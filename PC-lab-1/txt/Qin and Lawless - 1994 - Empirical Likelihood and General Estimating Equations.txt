Empirical Likelihood and General Estimating Equations
Author(s): Jin Qin and Jerry Lawless
Source: The Annals of Statistics, Vol. 22, No. 1 (Mar., 1994), pp. 300-325
Published by: Institute of Mathematical Statistics
Stable URL: https://www.jstor.org/stable/2242455
Accessed: 05-03-2019 15:15 UTC
JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide
range of content in a trusted digital archive. We use information technology and tools to increase productivity and
facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org.
Your use of the JSTOR archive indicates your acceptance of the Terms & Conditions of Use, available at
https://about.jstor.org/terms

Institute of Mathematical Statistics is collaborating with JSTOR to digitize, preserve and
extend access to The Annals of Statistics

This content downloaded from 134.174.144.116 on Tue, 05 Mar 2019 15:15:25 UTC
All use subject to https://about.jstor.org/terms

ne Annals of Statistics
1994, Vol. 22, No. 1, 300-325

EMPIRICAL LIKELIHOOD AND GENERAL ESTIMATING
EQUATIONS
BY JING QIN AND JERRY LAWLESS1

University of Waterloo
For some time, so-called empirical likelihoods have been used heuristi-

cally for purposes of nonparametric estimation. Owen showed that empirical likelihood ratio statistics for various parameters 0(F) of an unknown
distribution F have limiting chi-square distributions and may be used to
obtain tests or confidence intervals in a way that is completely analogous
to that used with parameteric likelihoods. Our objective in this paper is
twofold: first, to link estimating functions or equations and empirical like-

lihood; second, to develop methods of combining information about parameters. We do this by assuming that information about F and 0 is available
in the form of unbiased estimating functions. Empirical likelihoods for parameters are developed and shown to have properties similar to those for

parameteric likelihood. Efficiency results for estimates of both 0 and F are

obtained. The methods are illustrated on several problems, and areas for
future investigation are noted.

1. Introduction. Likelihood is arguably the most important concept for
inference in parameteric models. Recently it has also been shown to be useful
in nonparametric contexts. For some time it has been used to obtain nonparametric estimates of distribution functions [e.g., Kaplan and Meier (1958),
Vardi (1985). Recently Owen (1988, 1990, 1991), building on an earlier suggestion of Thomas and Grunkemeier (1975), has introduced an "empirical"
likelihood ratio statistic for nonparametric problems. Owen has shown that
the statistics have limiting chi-square distributions in certain situations, and
has shown how to obtain tests and confidence limits for parameters, expressed
as functionals 0(F) of an unknown distribution function F. Other asymptotic
properties-and the possibility of correcting likelihood ratio statistics or their
signed roots-have been studied by DiCiccio and Romano (1989), Hall (1990),
DiCiccio, Hall and Romano (1989, 1991) and others.
Empirical likelihood, described in Section 2, provides likelihood ratio statistics for parameters by profiling a nonparametric likelihood; the approach is
analoguous to that used for parameteric models, although it is computationally more complex. Owen (1990) showed that for independent and identically
distributed (i.i.d.) data the approach applies to quite general parameters O(F).
Owen (1991) made extensions to linear regression problems, and Kolaczyk
Received September 1991; revised December 1992.

1This research partially supported by a grant from the Natural Sciences and Engineering
Research Council of Canada.

AMS 1991 subject classification. Primary 62E20.
Key words and phrases. Asymptotic efficiency, auxiliary information, empirical likelihood, estimating equations, parameteric likelihood, semiparametric models, testing hypotheses,
Wilks' theorem.

300

This content downloaded from 134.174.144.116 on Tue, 05 Mar 2019 15:15:25 UTC
All use subject to https://about.jstor.org/terms

EMPIRICAL LIKELIHOOD AND GEE'S 301

(1992) and Owen (1992) have made further extensions to generalized linear and projection pursuit regression. Although further investigation of this
methodology is needed, especially in small to moderate size samples, it appears to provide a valuable approach to tests and interval estimation in nonparametric or distribution-free contexts.

Our objective in this paper is twofold: first, to link estimating equations
and empirical likelihood; second, to develop methods of combining information
about parameters. We achieve both for i.i.d. data as follows. Consider d-variate

i.i.d. random variables xl,... ,x,, with unknown distribution function F, and
a p-dimensional parameter 0 associated with F. We assume that information

about 0 and F is available in the form of r > p functionally independent

unbiased estimating functions, that is functions gj(x, 0), j = 1,2,... , r, such

that EF{gj(x, 0)} = 0. In vector form, we have

(1.1) g(X,0) = (gj (x,0).. 0))
where

(1.2) EF{g(x, 0) } = 0.

We will show how to use such inform
with empirical likelihood.

When r = p, our methods are the same as those of Owen (1988, 1990)
and provide (empirical) likelihood-based methods of interval estimation for
parameters with 0(F). Our main interest, however, is in the case where r > p.
This allows us to deal with the combination of pieces of information about a
distribution. For illustration we introduce some examples that we will return
to later.

EXAMPLE 1. Sometimes we have information relating the first and second
moments of a variable [e.g., Godamble and Thompson (1989) and McCullagh
and Nelder (1989)]. For example, lety1,... ,Yn be i.i.d., univariate observations
with mean 0, and suppose that it is known that E(y2) = m(0), where mW ) is
a known function. Our aim is to estimate 0. The information about F can be
expressed in the form (1.1), (1.2) by taking
g(y, 0) = (y _ 0,y2 _ m(0))T.

EXAMPLE 2. Let (x1,y1),..., (x,y ) be bivariate i.i.d. observations with
E(xi) = E(yi) = 0. In this case we can take g((xi,y1), 0) = (xi - O,yi - 0). A some-

what similar problem is when E(xi) = c is known and E(yi) = 0 is to be estimated, in which case we would have g((xi,yi),0) = (xi - c,yj - 0). Such prob-

lems are common in survey sampling [e.g., Kuk and Mak (1989) and Chen
and Qin (1993)].

EXAMPLE 3. Several authors have considered nonparametric estimation of
a distribution F when information about certain functionals of F is available.

This content downloaded from 134.174.144.116 on Tue, 05 Mar 2019 15:15:25 UTC
All use subject to https://about.jstor.org/terms

302

J.

QIN

AND

J.

LAWLESS

For example, Haberman (1984) and Sheehy (1988) consider estimation of F(x)

based on an i.i.d. sample xl, ... ,xn when it is known that EF{T(x)} = a, for
some specified function T(.). Our methods deal with this by taking g(x) =
T(x) - a; that is, r = 1 and the dimension p of 0 is 0.
We show in this paper that empirical likelihood may be brought to bear
on problems such as these. The basic idea is to maximize an empirical like-

lihood (see Section 2) subject to constraints provided by (1.2). We show how
estimators both of parameters 0 and the underlying distribution F may be
obtained and determine asymptotic normal distributions for the estimators.
We also demonstrate that empirical likelihood ratio statistics for parameters

have asymptotic x2 distributions. All of these results parallel closely simila
results for parametric likelihood inference. Section 2 reviews Owen's (1988,
1990) definition of empirical likelihood and the concept of optimal estimating
functions. Section 3 presents our methods and associated asymptotic results;
it is also shown that our method combines information in the form of estimating functions in an optimal way. Section 4 gives two other asymptotic results.
Section 5 presents several examples, and Section 6 discusses some additional

points. Outlines of proofs of the results in Sections 3 and 4 are provided in
the Appendix. Further details are given in a technical report available from
the authors.

2. Definition of empirical likelihood and optimal estimating functions. We first outline empirical likelihood as discussed by Owen (1988,

1990). Let x1,x2,...,x,, be i.i.d. observations from a d-variate distribu
having mean ,u and nonsingular covariance matrix. The empirical likelihood
function is
n

n

(2.1) L(F) = ]7dF(xi) = flpi,
i=1

i=1

where pi = dF(xi) = Pr(X
on each xi have nonzero likelihood, and (2.1) is maximized by the empirical

=

x

distribution function Fn(x) = n-lin I(xi < x). The empirical likelihood ratio
is then defined as R(F) = L(F)/L(Fn), and it is easily shown that this may be
written as
n

(2.2) R(F) = Jnpi.
i=l

We remark that formulas here and elsewhere in this paper do not require that
the xi's be distinct.
Suppose now that we want to estimate a parameter 0 = T(F): For simplicity

we consider the mean Xi of F. Tb obtain confidence regions for ,u, we define th

This content downloaded from 134.174.144.116 on Tue, 05 Mar 2019 15:15:25 UTC
All use subject to https://about.jstor.org/terms

EMPIRICAL LIKELIHOOD AND GEE'S 303

profile empirical likelihood ratio function
n

n

n

(2.3)
i=l

As

i=l

noted

exists,

i=1

by

RE
Owen

provided

th

explicit expression for RE(G) can be derived by a Lagrange multiplier argument: the maximum of HP 1npi subject to the constraints pi > 0, Ey2pi = 1 and
EI=lpixi = it is attained when

(2.4) Pi = Pi(H) = n-{1 + tr(xi where t = t(p) is a d x 1 vector given as the solution to
n

(2.5) {1 + t (xi - /,) }' (xi - IL) = 0.
i=1

Since Hinlpi is maximized unconditionally by Fn, it follows that RE(GI) is
maximized with respect to IL at i = x and that
n

(2.6) RE(I) = fJ {1 + t(-Xi

The empirical likelihood ratio statistic is WE(H) = -2 logR(p), that i
n

(2.7) WE(1= 2 Elog { 1 + tr (xi - y)}
i=l

Owen (1988, 1990) has proved under mild conditions that i

WEGIO) converges in distribution to X( as n -x oo. Approximate
dence regions for p may therefore be obtained as the set of poi

WE(,) < CO where c,a is defined such that Pr(X(d) < ca) = a. P

likelihood ratio statistics for subsets of pt = (pi,... , td ) can also
tain confidence regions for subsets of the parameters, in the u
(1990) has shown that the preceding approach applies to quite ge
eters 0(F), including multidimensional M-estimates. Owen (19
Kolaczyk (1992) have extended the methodology to a broad rang
problems involving linear, generalized linear and projection pur
Let g1(x, 0), . . . ,gr(x, 0) be a set of functionally independent e

tions, as in (1.1) and (1.2), where 0 is a p-dimensional param

estimates 9(x) may be obtained as roots of the corresponding es
tion g(x, 9) = 0. More generally, if the gj(x, 0)'s are r specified fu
consider the class of p-dimensional estimating functions
(2.8) 'I = {+b(x, 0) J4(x, 0) = A(0)g(x, 0) },

This content downloaded from 134.174.144.116 on Tue, 05 Mar 2019 15:15:25 UTC
All use subject to https://about.jstor.org/terms

304 J. QIN AND J. LAWLESS

where A(M) is a p x r matrix of real

[e.g., see Goldambe and Heyde (19
is called optimum in 'I if the estimator 0 from 4*(x, 0) = 0 has minimum
asymptotic variance.

3. Main results. We assume that xl, .. ., x,, are i.i.d. observations from an
unknown distribution F, that there is a p-dimensional parameter 9 associated
with F and that information about 0 and F is available in the form of r > p
functionally independent unbiased estimating functions, as described by (1.1)
and (1.2). We apply empirical likelihood to this framework by maximizing (2.1)
subject to restrictions

(3.1) pi > 0, ,pj = 1, Epig(xi,0) = 0.
i

i

For a given 0, a unique maximum exists, provided that 0 is inside the
convex hull of the points g(x1, 0), ... ,g(xn, 0). The maximum may be found via
Lagrange multipliers. Let

H=ZlogPi+A (1EPi) -ntTEpig(xi 0),
i

i

i

where A and t = (t1, t2,.. .,tr) are Lagrange multipliers. Taking derivatives
with respect to pi, we have

&H =- -A ntrg(xi, 0) = 0,

19Pi Pi

Pi a-p = n -A =O A=n
and

(3.2)

Pi

('

with the restriction from the third part of (3.1) that

(3.3) 0 = pig(xi, 0) =!n E 1 g(xi)
i

from

wh

Note that it is necessary that 0 < pi < 1, which implies that t and 0 must
satisfy 1+tTg(xi, 9) ? 1/n for each i. For fixed 9, let Do = {t: 1 + tTg(xi, 9) > 1/n};
Do is convex and closed, and it is bounded if 0 is inside the convex hull of the

g(xi, 0)'s. Moreover,

{n 1+ g (Xi O)} = __ E g(xi, )gr (xi, 9)

at n + tr g (xi, 0) {1 +trg (

This content downloaded from 134.174.144.116 on Tue, 05 Mar 2019 15:15:25 UTC
All use subject to https://about.jstor.org/terms

EMPIRICAL LIKELIHOOD AND GEE'S 305

is negative definite for t in Do, provided that EjI1g(x
definite. By the inverse function theorem, t = t(9) is thus a continuous differentiable function of 0.

The (profile) empirical likelihood function for 0 is now defined as

LE ( ( = r

i=1 n 1 + tr(0)g(xi, 0))

Since HM lpi is maximized for pi = n-1 in the absence of the parameteric
constraints we define, analogous to (2.6), the empirical log-likelihood ratio
n

(3.4) IE (0) 109lo [1 + t' (0)g(Xi, 0)].
i=l

Obviously (2.4) and (2.5) are special cases of (3.2) and (3.3), given by g(xi, At) =
xi - ,u, and 2-1WE(G) from (2.7) is a special case of (3.4).
We may minimize lE(O) to obtain an estimate 9 of the parameter 0, called
the maximum empirical likelihood estimate (MELE). In addition, this yields
estimates Pi, from (3.2), and an estimate for the distribution function F, as
n

(3.5) F(= ZinX(xi < x)
i=1

When r = p it is easily seen that 9 = 9 maximizes lE(9), where 0 is the solution

to the estimating equations Ei lg(xi, 9) = 0. In addition, ji = n1 and (3.5)
is the empirical cumulative distribution function. The empirical log-likelihood

ratio lE(O) covers as special cases (2.7) and similar statistics for other problems
considered by Owen (1990, 1991, 1992) and Kolaczyk (1992).
When r > p and when profile empirical likelihoods are wanted, computational issues arise as to the best ways to obtain 0 and profiles of lE(9). We
discuss this in Section 5, where we consider specific examples. The remainder

of this section presents first-order asymptotic properties of 9, Fn(x) and the
empirical log likelihood ratio statistics. Proofs for the various propositions are
given in the Appendix.
In the following, we use . to denote Euclidean norm.

LEMMA 1. Assume that E [g(x, OO)gT(x, Oo)] is positive definite, Og(x, 0)/&

continuous in a neighborhood of the true value 00, I1ag(x, 9)/aOll and IIg(X, 0

are bounded by some integrable function G(x) in this neighborhood, and the

rank of E[ag(x, 0o)/&a] is p. Then, as n -* oo, with probability 1 lE(O) attains
its minimum value at some point 9 in the interior of the ball 110 - 0olI < n113
and 0 and t = t(9) satisfy
(3.6) Q1n(9,t) = 0, Q2n(0,t) = 0,

This content downloaded from 134.174.144.116 on Tue, 05 Mar 2019 15:15:25 UTC
All use subject to https://about.jstor.org/terms

306 J. QIN AND J. LAWLESS
where

(3.7) Qln (0, t) = 1(xi, O),

(3.8) Q2n (Oi t) = n E t. tgx, )( a
n 1 +tr g(xi, 9) k~09
THEOREM 1. In addition to the conditions of Lemma 1, we assume that
c92g(x, 0)/909 09' is continuous in 0 in a neighborhood of the true value 00. Then

if 1jj2g(x, 9)/O09a T can be bounded by some integrable function G(x) in the
neighborhood, then

V/j(n- Oo) - N(O,V),~F (t - O) -- N (O,U),
V/n-(Fn (x) - F(x)) -+N (O,W(X)),
where
n

Fn(x) = Zfi1(xi < x),
i=l

n(n) i +jrg(xi,

V = [E 9g )r(Eggr) 'E 9g)
W (x) = F (x) (1 - F (x)) - B (x) UBr (x),

B(x) = E{g(xi, 9o)I(xi < x) },

U=[E(ggr)] K1{ I-( ) ( VE ) [((gg )]
and 0 and t are asymptotically uncorrelated.

We can use Theorem 1 to get approximate confidence limits for 9 or F(x
The asymptotic variance V of /ni(W - 00) is consistently estimated by
F

(

'~~

[IPi

'flT(

.rL-1

(90f

)

}

g

or
by
the
same
expre
We give some addition properties of empirical likelihood methods in the
following corollaries to Theorem 1. We assume throughout that the conditions

in Theorem 1 hold.

This content downloaded from 134.174.144.116 on Tue, 05 Mar 2019 15:15:25 UTC
All use subject to https://about.jstor.org/terms

EMPIRICAL LIKELIHOOD AND GEE'S 307

COROLLARY 1. When r > p, the asymptotic variance V
cannot decrease if an estimating equation is dropped.

COROLLARY 2. The MELE 9 based on gl(x, 0), ... ,gr(x, 0) is fully efficient in
the sense that it has the same asymptotic variance as the optimal estimator obtained from the class of p x 1 estimating equations that are linear combinations
of gi(x, 9), ... ,gr(x, 0); see (2.8).
The proofs of Corollaries 1 and 3 are sketched in the Appendix. Corollary
2 is obtained by direct comparison of V in Theorem 1 with the asymptotic
covariance matrix of the estimator obtained from the class of p x 1 estimating
equations based on g1(x, 9), . . .,gr(x, 0) [see, e.g., McCullagh and Nelder (1989),
page 341] or by noting the equivalence of MELE's based on equivalent sets of
estimating functions (i.e., sets which are in 1-1 correspondence) and applying
Corollary 1. See Section 6 for additional information.
We know that when the number of estimating equations and parameters
are equal, the score equations are optimal [see Godambe and Heyde (1987)].

Corollary 3 shows that ifp of the r estimating functions gj(x, 9) are actually

score functions, then, as seems obvious, the covariance matrix Vr for 4/9(9 - 0

is the same as that for the MLE, tr/-(f - 0). However, Fn(x) of (3.5) is gener
more efficient than the empirical c.d.f. Fj(x).
COROLLARY 3. If we know the distribution of x up to parameter, let g =
(h , hr), where
1 (l (, ) X. . gp(X ))T =(a log f (X, 0) a log f (X, 0))T

h2 = (gp+l (x, o), .. gr(x) 0))

and x is assumed to have density f(x, O), so that h, is the score. Then

Vr = Vp2 Wr > E(hi(X,0)1(X < x))TVpE(hj(X,0)1(X < x)),
where Vp and Vr are the asymptotic covariance matrices of the MELE's 0 based
on h1 and on (hl,h2), respectively.
Empirical likelihood provides a way to find efficient estimates in semiparametric models which are specified in terms of r > p estimating functions. It

also parallels likelihood in full parametric models with respect to the likelihood ratio statistic, as the next theorem shows.

THEOREM 2. The empirical likelihood ratio statistic for testing Ho: 0 = 90 is

(3.9) WE (00) = 21E (0) - 21E (),
where lE(O) is given by (3.4). Under the assumptions of
as n -+ oo, when Ho is true.

This content downloaded from 134.174.144.116 on Tue, 05 Mar 2019 15:15:25 UTC
All use subject to https://about.jstor.org/terms

308 J. QIN AND J. LAWLESS

Similarly we can prove the following corollary.
COROLLARY 4. In order to test model (1.2), we may consider the empirical
likelihood ratio statistic

W1= 2Elog [1 +itg(xi 0)]
i=1

Under the assumptions of Theorem 1, Wi is asymptotically X2_) if (1.2) is
correct.

COROLLARY 5. Let 0r = (01, 02)T, where 01 and 02 are q x 1 and (p - q) x 1
vectors, respectively. For HO: 01 = fl, the profile empirical likelihood ratio test
statistic is

(3.10) W2 = 21E (09, 92) - 21E (1, 9 2),

where O? minimizes lE(91, 02) with respect to 92. Under HO, W2 -- X(q) as
Theorem 2 and Corollary 5 allow us to use the empirical likelihood ratio
statistic for testing or obtaining confidence limits for parameters in a completely analogous way to that for parametric likelihoods. With full parametric
models, there are several likelihood-based statistics equivalent to the first or-

der for testing Ho: 9 = 90, including the likelihood ratio statistic, score statistic
and Wald's maximum likelihood estimate statistic. A similar equivalence exists here, but we will not explore this topic now.
4. Other results. In this section we mention two other results. The first
is that we may use results of Van der Vaart (1988) and Bickel, Klaassen, Ritov
and Wellner (1993) to derive a convolution theorem for "regular estimators'

of P0 and 9(P0), where Po is the probability measure corresponding to Fo, and
to show that our maximum empirical likelihood estimates are asymptotically
efficient in the sense of those authors and of Sheehy (1988). This gives the
following theorem.
THEOREM 3. Under the conditions of Theorem 1 and Lemma 2 in the Appendix, the MELE's for both the parameters and the distribution function
are asymptotically efficient in the sense of Van der Vaart (1988) and Bickel,
Klaassen, Ritov and Wellner (1993).
The second result is about local asymptotic normality of the empirical likelihood ratio statistic.

THEOREM 4. Under the conditions of Theorem 1, let

z e(u) n [1 + tT (9 + un (xi, 0 + un-1/2)]

This content downloaded from 134.174.144.116 on Tue, 05 Mar 2019 15:15:25 UTC
All use subject to https://about.jstor.org/terms

EMPIRICAL LIKELIHOOD AND GEE'S 309

be the normalized empirical likelihood ratio. Then we ha
resentation,
n

Zn,e(u) = exp { slog [1 + tT(O + un-1/2)g
n

- s log 1 + tT(O)g(xi, )]
i=l

= exp {uEe( ) [Ee(ggr)]1i a g(xi, )

+ - UrE. ( ) [E. (ggr)] 'E u + o
and

The proof is straightforward by Taylor expansion.

Note that Eo(&g/l90)[Eo(ggT)]-lEo(ag/90) is the inverse of the asymptotic
variance of V'i(W - 0), so this representation is similar to the representation
of the normalized parametric likelihood ratio [see Ibragimov and Has'minskii
(1981), page 114].

5. Examples. We consider several illustrations of the estimation procedures. We primarily consider large-sample aspects, but for the first example
we also present some numerical results.
Computational issues are discussed by Owen (1990, 1992) and by Qin and
Lawless (1992a). Additional experience with empirical likelihood methods is
needed before specific recommendations can be given, but a few points may
be mentioned. In order to evaluate IE(O) for a given 0, we have to solve (3.3)

for t(O); this is often handled well by Newton's method, bearing in mind, however, the remarks preceding Lemma 1. To obtain 0 we may proceed in two
stages, essentially obtaining lE(G) and then maximizing it. Alternatively, we
may attempt to solve (3.5) simultaneously for 0 and t; some care is needed be-

cause the solution sought is one of many saddlepoints of the function h(M, t) =
YEU7 log {1 + tTg(xi, 0)} and, in particular, must satisfy 1 + tTg(xi_ 0) > n1 for
each i.

EXAMLE 1 (Continued). Recall that Y1,Y2. . . ,y'n are i.i.d., with unknown
univariate distribution F and first and second moments ,1 = 0, and 12 = m(0),
respectively, where m(W) is a known function. We can apply the approach used

This content downloaded from 134.174.144.116 on Tue, 05 Mar 2019 15:15:25 UTC
All use subject to https://about.jstor.org/terms

310 J. QIN AND J. LAWLESS

in Section 3 leading to equations (3.6), (3.7) and (3.8). This yields

(5.1) l i+t( Yi - 0=+t2 [y -m(0)]=0

(5.2) n ~1+t1(Yi-O)+t2[Y?-M(O)]

(5.2) n-1 z y-(9) =0,l

(5.3) n-1 + tl - t2M'(0) = 0m
The third equation implies t, = -t2m'(0) and by substituting this into (5.1)
and (5.2) we may get two equations in t2 and 9 to solve. Recalling the discussion proceeding Lemma 1 in Section 3, we note that the desired solution 0, t2
must satisfy the conditions 1 - t2m'(0)(y1 - 0) + t2 [y2 - m(0)] > 1/n, for each
i = 1, .. ., n. In moderately large samples Newton iteration starting from the
initial value (9, t2) = (y, 0) often works; alternatively, we can obtain IE(O) by
finding t(9) (see the comments before Lemma 1) as an 'inner" iteration and
can iterate on 0 to find W.

The results of Theorem 1 show that .,/Ii(0 - 00) -- N(O, V) where V is given
in Theorem 1. Some algebra shows that

V = var(y) - A-1 [m'(0o)var(y) + Oom(9o) - E(y3)]2,
where A = E[m'(o)(y - So) + m(Oo) - y2]2 Thus V < var(y), which is the

variance of /(y - 00) and so 0 is asymptotically at least as efficient as y

In practice, any higher efficiency of 9 when the second moment relationship
E(y2) = m(0) holds will of course have to be balanced against a lack of robustness under departures from the relationship.
We consider for illustration a model with first and second moments satisfying Ex = 0 and E(x2) = 202 + 1. We generated 1000 pseudorandom samples
of sizes 15, 20, 30 and 40 from N(9, 02 + 1), for two values of 0. For each sam-

ple we obtained three estimates of 6: the sample mean, the MELE based on
the additional knowledge that E(x2) = 202 + 1 and the parametric MLE based
on the normal distribution. Table 1 shows that estimated mean and variance
of each estimator, obtained from the simulation. We see that the variance of
the MELE lies between that of the sample mean and the parametric maximum likelihood estimator. In Table 2 we compare three methods of obtaining
confidence intervals for 0. The first two are based on our empirical likelihood
methods: one (ELR) obtains confidence intervals from the empirical likelihood
ratio statistic (3.9) and the X(1) approximation of Theorem 2; the other (NCI)
is based on the limiting normal distribution for the MELE 0 given in Theorem
1 and the variance estimator following Theorem 1. The third method (PLR) is
based on the parametric likelihood ratio statistic from the normal distribution
N(9,02 + 1) from which the data were generated, with X(1, as the approximating distribution. Table 2 shows, for 1000 samples, of sizes n = 30 and 60, two

This content downloaded from 134.174.144.116 on Tue, 05 Mar 2019 15:15:25 UTC
All use subject to https://about.jstor.org/terms

EMPIRICAL LIKELIHOOD AND GEE'S 311

TABLE 1

Estimated mean and variance of three estimators of 0, fro
Sample

n

Mean

mean

Var

Mean

MELE

Var

MLE

Mean

Var

N(0, 1), true value of 0 = 0.0

15 0.004484 0.067624 0.006848 0.061824 0.006482 0.058516
20 0.000956 0.049740 0.001945 0.048108 -0.002313 0.045455
30 -0.005714 0.031004 -0.005119 0.030921 -0.004360 0.029835

40 0.000956 0.024572 0.002931 0.024221 -0.000947 0.023431
N(1,2), true value of 0 = 1.0
15 1.004317 0.128445 0.946416 0.086383 0.966406 0.083193
20 0.995677 0.106569 0.952668 0.062353 0.972931 0.059177
30 1.006338 0.068629 0.968523 0.035759 0.984540 0.034930
40 1.015897 0.044045 0.984512 0.021883 0.994275 0.020584

0 values and nominal 90% and 95% confidence intervals, the average length
(Avl) and empirical coverage (Ecv) for each type of interval. It is interesting
to note that the two empirical likelihood methods agree closely and that for
smaller samples their coverage probability is substantially less than the nominal 90 and 95%. By comparison, the parameteric likelihood yields intervals
with close to the nominal coverage. Further investigation of this is needed,
but these results raise the question of whether, even for small samples, like-

lihood ratio intervals are similar to ones based on normality of 9 for empirical
likelihood and whether higher-order corrections for both methods are needed.
Finally, as an example, Figure 1 depicts an empirical likelihood ratio curve
WE(0) (solid line) and a parametric likelihood ratio curve (dotted line), for a
particular sample with n = 30 and 0 = 1. We can see that for this example the
curves are very close.

EXAMPLE 2 [(Continued) Two-sample problem with common mean]. In this
case observations (xi, yi), i = 1, 2,... , n, occur in independent pairs and E(x,) =

E(yi) = 0. To estimate 0, we consider the estimating equations based on g, =

x - 0 and g2 = y - 9 and we associate the empirical likelihood probability Pi

with (xi,yi). After some simplification, from estimating equations (3.6)-(3.8)
we have

E[+!(xi -Yj)]/ [+ (xi -Yi)],
where t is determined by
n

(5.4) E { 8=0?

This content downloaded from 134.174.144.116 on Tue, 05 Mar 2019 15:15:25 UTC
All use subject to https://about.jstor.org/terms

312

J.

QIN

AND

J.

LAWLESS

TABLE

Average

length

and

90%
Avl
n

Ecv
=

2

coverage

for

three

co

95%
Avl

Ecv

30

N(0, 1) ELR 0.55064 85.8% 0.65714 92.4%
NCI 0.56965 86.0% 0.67889 92.0%
PLR 0.60197 89.6% 0.72441 94.3%

N(1, 2) ELR 0.56698 83.3% 0.67737 89.2%
NCI 0.56863 84.3% 0.67767 90.1%
PLR 0.61489 88.3% 0.73900 93.6%
n = 60

N(0, 1) ELR 0.41535 89.5% 0.49611 95.4%
NCI 0.41291 89.2% 0.49210 94.9%
PLR 0.42549 90.8% 0.50950 96.1%

N(1, 2) ELR 0.41200 88.6% 0.49267 94.1%
NCI 0.40933 89.0% 0.48782 93.2%
PLR 0.42845 91.3% 0.51265 96.1%

Moreover, we seek the solution t to (5.4) that satisfie
each i = 1, 2, ... , n. Such a solution exists if and only
of the same sign. In that case there is exactly one such value t, which lies in

the interval (tL, tu), where

tL = (1/n- 1) (1 -1/n)

= max[O, max(xi -yi)]' - min[O, max(yi -xi)]

The asymptotic covariance matrix for /ni(W - Oo) is given by V of Theorem
as

(, Q \T/ O -1 ~~~~a2 2 ~2

V (FO (E(ggT))lE =x x+?y -2a,'

where u2 = var(xi), o7 = var(yi) and ax = cov(xi,yi); V may be estima
the sample covariance matrix entries.

It is easily shown directly that 9 is asymptotically equivalent to the optimal

(minimum asymptotic variance) linear combination of x and y. In particular,
note that in the case where ax = cov(xi,yi) = 0 that V = U2o2/(c2 + o2), which
is the same as the variance of the optimal linear combination estimator

_____ + Y
U)U2
x +2))2Y
y)

This content downloaded from 134.174.144.116 on Tue, 05 Mar 2019 15:15:25 UTC
All use subject to https://about.jstor.org/terms

EMPIRICAL LIKELIHOOD AND GEE'S 313

N~~~;

1
0.4

1-

1

0.6

'

I

~

0.8

~

~

1.0

~~~~~~
1.2

I

I

1.4

Theta

FIG.

1.

Empirical

likelihood

This content downloaded from 134.174.144.116 on Tue, 05 Mar 2019 15:15:25 UTC
All use subject to https://about.jstor.org/terms

314 J. QIN AND J. LAWLESS

EXAMPLE 3 (Continued). Haberman (1984) and Sheehy (1988) have considered constrained estimation of probability measures based on i.i.d. sample

x1,x2, ... ,xn from a distribution P0, where it is assumed known that Ep0T(x) =
a, for some specified function T. The estimators they considered are based on
the minimization of Kullback-Leibler divergence from certain collections of

probability measures to the empirical measure of the xi's and Sheehy (1988)
has proved that the estimate is asymptotically efficient. We may apply empiri-

cal likelihood to this problem, utilizing the fixed constraint Si pi T(xi) -a = 0.
In this case earlier results about the c.d.f. still apply, and we obtain the esti-

mate FP(t) of the distribution function. It is easy to check that for this estimate

V/i(Fn(t) - Fo(t)) is asymptotically equivalent to

J [1(x < t) - Fo(t) - cov(l(x < t),T)(varT)-'(T - a)]
x d v/Hi (Fn (x) -Fo (x) ) + op(1).

This matches with Sheehy's Theorem 2'.
EXAMPLE 4 (An example of "semi-empirical" likelihood). The methods developed in this paper are also useful for dealing with incomplete information in
parametric or semiparametric models. As an example we consider a problem
arising in field studies of equipment failure [Kalbfleisch and Lawless (1988)].
In this situation, N items are in use and associated with item i are a time to

failure y1 > 0 and a vector of covariates xi; a regression model with densit
f(ylx; 0) specifies the distribution of yi given xi.

An incomplete data problem arises because only items that fail by some

time T are inspected: each item that fails at time yi < T is inspected, and
covariate values xi are determined. For items with yi > T, the xi values are
unknown. One approach is to base inferences about 0 on the likelihood function

for yi's such that yi < T:

L(0)= I f(YTi xi; 0)
where F(ylx; 0) is the c.d.f. of y given x. However, this does not use the infor-

mation that the remaining yi's exceed T. This information cannot be used in
a parametric likelihood framework without specifying a distribution for the
covariates, but it is possible to use empirical likelihood, as follows.
Consider y and x to be jointly distributed, and define estimating functions

g1 (y, x, 0) = I(y < T) 9[ logf (ylx; H) - logF(Tlx; 0)],

g2(y,x, ) = I(y?5 T) _ 1
F (T lx;90)

Note that Eg1 = Eg2 = 0, and EZ=1gl(xi, ti, 9) is the score function from L(O).
We now associate pi in the empirical likelihood formulation of Section 3 with

This content downloaded from 134.174.144.116 on Tue, 05 Mar 2019 15:15:25 UTC
All use subject to https://about.jstor.org/terms

EMPIRICAL LIKELIHOOD AND GEE'S 315

Pr(yi,xi), i = 1,2,.. ,N, and use (g1,g2) as our estimat
to an MELE 0 where, by Theorem 1, vW(9 - 0) - N(O, V) as N -- oc.
To illustrate, we consider the special case where f(y lx; 9) = 0 exp(-9y), that
is, there are no covariates. We find that V - 02/{1 - exp(-OT)}, which is
precisely the asymptotic variance of the MLE 0 obtain from the censored data
likelihood

L1= 1710 exp(-9yi) 11 exp(-OT).
i:yi<T

i:yi>T

Further investigation is needed for cases where covariates have an effect,
but it is interesting that in this special case the MELE is equivalent to the
fully efficient MLE. Qin and Lawless (1992b) consider similar applications of
empirical likelihood.

6. Additional remarks. Other approaches may be taken to combine estimating functions. First, we remark that likelihood is not the only distance
in the simplex for (P1,P2 ... ,Pn) that can be used to generate confidence sets
for 0 with a chi-square calibration. Efron (1981) and DiCiccio and Romano
(1990) consider the (n - 2)-dimensional subfamily of multinomials generated

by minimizing the Kullback-Leibler distance D(F, Fn) = Yipi log(npi) subject
to Eipixi = ,u and Eipi = 1. Owen (1991) has considered log Euclidean likelihood, defined as IEU = - ZEi(npi - 1)2, as an alternative to Sin log Pi. This is
quite tractable and leads to methods asymptotically equivalent to the ones in
this paper.

Another approach to combining estimating functions is to consider the optimal (minimum asymptotic variance) linear combination of the r estimating

functions gj(xi, 0), j = 1,2, ... , r, as mentioned in Corollary 2. This leads to
[e.g., see McCullagh and Nelder (1989), page 341] the estimating equations
n

(6.1) EDTv7lgi = 0,

where gi = (gi(xi, 9), ... ,gr(xi, 0))T, D

sumed nonsingular. The vi's are unknow
vi = v, which may be consistently estim
n

' 1 ^T1

v= n gi i,
i=l

where g is gi evaluated at any consistent estimate 0 of 0. We then obtain 9
by solving
n

(6.2)

EDTV-gig=0.
i=l

This content downloaded from 134.174.144.116 on Tue, 05 Mar 2019 15:15:25 UTC
All use subject to https://about.jstor.org/terms

316 J. QIN AND J. LAWLESS

The empirical likelihood approach has the potential advantage of providing
likelihood ratio statistics, upon which tests and confidence intervals may be
based. One might hope that these possess better small-sample properties than
methods based on approximate normality of the estimates. This was not apparent from our simulation in Example 1, however, and needs investigation.
The empirical likelihoods also appear to be Bartlett or signed square root correctable and may be generalized to handle independent but not identically
distributed data.
A good deal of work is needed to apply and assess the methods in practical situations. Experience is needed to determine how easily estimates can
be obtained in small- to moderate-size samples and what the properties of
the estimators and the empirical likelihood ratio statistics are in these situations. Higher-order asymptotic properties and comparisons with resampling
methods are also of interest. We hope to consider some of these topics in future communications.
APPENDIX

Here we give proofs for the results in Sections 3 and 4.

PROOF OF LEMMA 1. Denote 0 = 00 + un-1/3, for 0 E {0 1 1H0 - Ooll = n-1/3
where Ilull = 1. First, we give a lower bound for lE(O) on the surface of the ball.
Similar to the proof of Owen (1990), when EIIg(x, 0)113 < 0 and II0-OoIl < n1/3

we have

(1)t(o) = g- E (xi, O)gr (xi, 0)] n Eg(xi,0)] + o(n- 1/3) (a. s.)
(A. 1) n ~i=1Ln= J
=O(n-1/3) (a.s.),

uniformly about 0 E {0 1 110 - Ooll < n-1/3}
By this and Taylor expansion, we have (uniformly for u),

lE(0) = rtr(0)g(xi,9) - 2 E [tr(0)g(xi,0)]2 +o(n1/3) (a.s.)
[! g(xio)] [! Zg(xi, )gT(xi,0)] 1[ Eg(xj 0)]
2ni nin
+o(n1/3) (a. S)

l- Eg(xi, g) + - E 9(9 f 00) un-1/31 F-lEg(xi,

x [!Eg(xi, 0o) + - Og(x 00)un-1/3 + o(n1/3) (a.s.)

n n 190~~~~~~~

This content downloaded from 134.174.144.116 on Tue, 05 Mar 2019 15:15:25 UTC
All use subject to https://about.jstor.org/terms

EMPIRICAL LIKELIHOOD AND GEE'S 317

= 0 [o(n'1/2(loglogn)1/2) +E(ag0(x0))unl1/3]
x [E(g(x, Oo)gT (x, 9o)] -1

x [O (nl/ (log log n)/) + E un -gX 1o u /31 +
> (c - E)n/3, a.s.,
where c - E > 0 and c is the smallest eigenvalue of

E (gg (aX2o 0) 'r[E(g(X,00)gr(xs 0o)]-1E ag(a2a*))
Similarly,

lE(9) = 2[n Eg(xi, o)] [n gg(xi, 90)gT(xi,o9)]

2 n -n~~~~i

x [!g(xi, o)] + o(1) (a.s.)
= O(loglogn), (a. s.).

Since lE(O) is a continuous function about 0 as 0 belongs to the ball 110 - 0o
n"1/3, IE(O) has minimum value in the interior of this ball, and 9 satisfies

alE(0) |r (dtT(0)/0a)g(xi, 0) + (0g(xi, 0)/la) t(90)

a0 0=0 1 + tr(M)g(xi, 0)

1 + tTr (0)g (Xi 2 0) (0 0A) 1=0
-0.

o

PROOF

aQln (0,0) 1 ag(xi,9) aQln (,0) 1
i

i

i9Q2n(9,0) =, aQ2n (090) = (g(9X,i)
a9

a

t

)

Expanding

t

n

a0

Qln(W,!)2

This content downloaded from 134.174.144.116 on Tue, 05 Mar 2019 15:15:25 UTC
All use subject to https://about.jstor.org/terms

Q2n(

318 J. QIN AND J. LAWLESS
Lemma 1, we have

O=Qln (0)
(A2) Qln (00, 0) + aQln (0 0 ) _00) + a3Qin (Oo I 0) ct - ) +o(60
oo ig~~~~tT

O=Q2n (,t )

(A.3) Q2l( 9o+Q2n :~O(o0 9) ODQ2n (Oo,0) (-)
Q2n (00, 0) + dO )(_00) + 0)n(0 0) Op0 (n))
where 6n = II|6- Ooll + Ir1Il. We have

( _ 00o = n O ln 0(6n) ?Pn)
where

- Q1l DQ1) ( (ln 1 E2) = (-E(ggr) E( -)

(A.4) S DtT (90 ,)S12DO

From this and Qjn(9o, 0) = (1/n)Ei lg(xi, Oo) = Op(n
OP(n-1/2). Easily we have

n(6- 9o) = S22.S21S- 'v'Qln (9 0) + op(i)
where

V = S-'1 = {E (ag (Eggr) -4E (ag)}

The rest of the proof is similarly straightforward. O
PROOF OF COROLLARY 1. Write

Dr() =((%g ), T , (D r), (D0r)T) = (D(O), (Dgr)),

Cr(O) =E(ggr) C Kcll(o) c12(0))
C21 (o) C22 (o)

where C11(0) is an (r - 1) x (r - 1) matrix. For square matrices A and B of the

same order, let A > B denote that A - B is positive semidefinite. Then

7r =E() E(ggT)1E( )
=Dr (0)Cr-1(O)Dr(0)

> (Dr l(0) a@gr) ) 1 (Cll (0) O) (r0=
Vr-V-

This content downloaded from 134.174.144.116 on Tue, 05 Mar 2019 15:15:25 UTC
All use subject to https://about.jstor.org/terms

EMPIRICAL LIKELIHOOD AND GEE'S 319

PROOF OF COROLLARY 3. We know that

17V - ((E% 'f, (Er 2)) k(E(h,hM) E(hlh'r) J90

r (90 -i0o E(h2hl7) E(h2h2r) t(h)

(( ahl)7 E(3) (Ehlhlr)- E(hlhr) +E( ))

ao((E fl A2-1) K (Eh2 ha)(Ehoh7)-'E Oh1 +EOh2)

where

A22.1 = E(h2h2) -E(h2hT) (Eh,hT)'E(hlhE).

Since h1 is the score, (Ehlh'r) = -E(9h1/cO) and, taking derivatives abou
the equation E(h2) = 0, we have

ah2f (x, ) dX + J h2df (XI 0) dx =o,
that is

E (a2) +E(h2hr) = O.
Thus

r ( f9)X
A-'1)((0)
(ahj~0
(Ehlh'r-l
(Eh'hO) l

E ( Ah, 1) (Eh l h'r - )E ( 9h,) (Ehlh 1r
If O is the MLE for 0, then F(x, W) is the MLE for

variance Wr of -/n(Fn(x) - F(x)) is no less than th
\li(F(x, 0) - F(x)). We only need to calculate the asy
latter, which is easily found to be

E{hi(X, 0)1(X < x)}TVpE{hh(X, 0)1(X < x)},
and the corollary is proved. 0

This content downloaded from 134.174.144.116 on Tue, 05 Mar 2019 15:15:25 UTC
All use subject to https://about.jstor.org/terms

320 J. QIN AND J. LAWLESS

PROOF OF THEOREM 2. The log-empirical likelihood ratio test statistic is

WE (00) = 2{ log [ 1 + Q (xi, 00)]- log [1 * tr g(xi, W)]
Note that
n

lE(0,t) = Elog[1+?tg(xij,)] = -_QLn(00)0)AQ1n(0o00) +op(1)
i=l

where A = S-j1 {I + S12S'11S21Sj' } . Also under Ho,

1g 9(x.0o)=0 =~to =-Sj'Q1n(0020) +oP(i)

n E 1 + t1o0g(xi, 00)
and

Z log [1 + tOg(xis,0o)] =-2 Qn (10, 0)S1l'Qln (9o, 0) + Op (1)
Thus

WE(O0) =nQr (00, 0) (A-Sjl ) Q1n(0o0o)+op(1)
= nQ n(0, 0)S1'S12S_ 11S21S_ jQ1n (Go 0) + op(i)

= [(-s11) v Q (90 O)] [(-s1) /12S 2S1S211(-S1) -1/2]
x [(-ll )/ Qln (Oto O)] +12S2(1)21
Note that (0Sll)-1/2 /EQln(o, O) converges to a standard multivariate normal
distribution and that (-S11)-2S12S-1S2_(-S11)-/2 is symmetric and idempotent, with trace equal to p. Hence the empirical likelihood ratio statistic

WE(Oo) converges to p
PROOF OF COROLLARY 5. By Taylor expansion we have

W2= 21E (o, J)0 -21E (?1 02)

= [(_S)-1/2 /iQin(0o,0)]T(Eggr)-1/2
V&g\ In(0g0T 0

X I (E) (Egg<1 (E ] (E gg) )

-j.00 [\E0- 50 (Eggr )o OOj(
~(Eggr)

This content downloaded from 134.174.144.116 on Tue, 05 Mar 2019 15:15:25 UTC
All use subject to https://about.jstor.org/terms

EMPIRICAL LIKELIHOOD AND GEE'S 321

By a result in Rao [(1973), page 187], we only need to show

(E =( ) [ (E )r(Egg o0 ) (E )
> (E ag) [L (Ed) (EggT) -1(E )]1 a
In fact,

trE
g t L,ig
a 1tg LIt
t @@}
VE y(Eggr)
-1 ?ag
Ea9

> g E-Eg 'r ( (Eggr )- (E 0g)]( E(a

0, ((0a2
- Et 8 E0 9 [k} ae11o0
kOOi!
?)o1 tE( ) )

tvag a g E9 Og
=(00, [(E80g (Eggr)- (d0g )] (E90,
L

Thus W2 > X[r- p-q)-(r-p)] = x2 ?
Now we consider the proof of Theorem 3. For this we need the concept of
a tangent space T(PO) of L2(Po); see Sheehy (1988) for a concise description.
First we give a lemma; let

gA(x,0) = (g1(x,9), . gp (x,0))T, gB(X,0) = (gp+1(0) *.. gr(,x). ))
Eg(x,O)=O = EgA=0, andEgB=0.

LEMMA 2. Assume that (Epo 9gA/909)1 exists. Define

T*(Po) = {h: h E L2(Po) IhI < K, some K > 0, Eph =O,

Epo (B - Epo Q )E g A) gA h)

([gsEpo(09) (EPo) gA] h) = O h

Then T*(Po) is a maximal tangent space at Po E P.
Since T*(Po) is maximal we must have

Tm(Po) = {h: h E L2(Po),Epoh = 0,

This content downloaded from 134.174.144.116 on Tue, 05 Mar 2019 15:15:25 UTC
All use subject to https://about.jstor.org/terms

322 J. QIN AND J. LAWLESS

for any maximal tangent space Tm(Po).

PROOF. Consider a parametric sub
fgA (x, O(Fn)) dFn = 0, we have

J OgA(x,9l(F.,)) oo(F.R) dF, +Jg
that is,

J OgA(,OO) oo(F) dPo + gA(
or

(A.6) 19,q =o Epo (90 Epo[9A(XO)h][
Similarly,

j OgB(x, dP) O9(Fo) + JgB(x2Oo)hdPo = 0
that is,

Epo ( agB(x,) [Epo (aoA O))] Epo [gA (x 9o)h]}
+Epo[gB(x,Go)h]= 0
or

Epo ([SB - (Eo () (gao 9A] h) =0,
so

T(Po) C T*(P0) = {h: h e L2(Po),Ep0h = 0,

Epo ([B-Epo (o ) A]h) 0
In order to show that T*(Po) is itself a tangent space, we need to show that if
h E T*(Po) there exists {Pt: 0 < t < 1} c P so that

J [1 ((dPt)1/2 - (dPo)1/2) - 1h(dPo)1/2] - 0 as t -- 0.

This content downloaded from 134.174.144.116 on Tue, 05 Mar 2019 15:15:25 UTC
All use subject to https://about.jstor.org/terms

EMPIRICAL LIKELIHOOD AND GEE'S 323

It is easy to show that the sequence {Pt} defined by dPt/d
and we are done. 0

PROOF OF THEOREM 3. We will show that the MELE 9 is fully efficient.
Since the tangent space Tm(Po) is linear, it is sufficient to show that the
influence function of 0 is the projection of the (pathwise) derivative of the
functional 9(PO) onto the tangent space. Since

a9(F1)| = - [Ep(gA(Eo ))]Ep [gA(xioo)h],
the derivative 0 of 9 is
ba=_ (gA) gA

Noting that

= S221S21Sllg(X, 09) - jEpo( 4-) ) + S22 1 S21S&11g(X, 0O)
and

(0- - So) = EsS21Sjj'g(x, Go) +op(i),
we want to show that

(A.7) S2'12S2.iS2Sg(x, o) E Tm(Po),
(A.8) a: = { [Epo ( )] gA +S1 S21ST1 g(x,9o)} E Tm (Po).
By Bickel, Klaassen, Ritov and Wellner [(1993), Section 31, our empirical likelihood estimate for 0 is then efficient.
Easlly we can show

Epo { [gB -Epo ( ) (Ep ) A] [grSjjS112s2i ] } = ?.
Next we show a E TL(PO).
Note that S22.1 = Ep,(Og/a9)rE(ggr)Ep0(ag/a0), S12 = Sr = Ep0(Q9g/0a) and

Sl = -Epo(ggT), so

a=( 1 S22.1 [EPo(&8A )I g+ S21Si19g}
( ~~0

22hS21SiI' -(gR-M}
Tm(P)).
0 00 9A
In a similar way, we can establish the efficiency of the distribution function
estimate Fn. 0

This content downloaded from 134.174.144.116 on Tue, 05 Mar 2019 15:15:25 UTC
All use subject to https://about.jstor.org/terms

324 J. QIN AND J. LAWLESS

Acknowledgments. The authors wish to thank Professors J. Chen, V. P.
Godambe, J. D. Kalbfleisch, D. L. McLeish and A. B. Owen and two anonymous
referees for many helpful comments.

REFERENCES
BICKEL, P. J., KLAASSEN, C. A. J., Rrrov, Y. and WELLNER, J. A. (1993). Efficient and Adaptive
Estimation for Semiparametric Models. Johns Hopkins Univ. Press.
CHEN, J. and QIN, J. (1993). Empirical likelihood estimation for finite populations and the effective
usage of auxiliary information. Biometrika 80 107-116.

DiCiccio, T. J., HALL, P. and RoMANo, J. P. (1989). Comparison of parametric and empirical likelihood functions. Biometrika. 76 465-476.

DiCiCcio, T. J., HALL, P. and RoMANo, J. P. (1991). Empirical likelihood is Bartlett-correctable.
Ann. Statist. 19 1053-1061.

DiCiccio, T. J. and RoMANo, J. (1989). On adjustments to the signed root of the empirical likelihood ratio statistic. Biometrika 76 447-456.

DiCiccio, T. J. and ROMANO, J. (1990). Nonparametric confidence limits by resampling methods
and least favorable families. Internat. Statist. Rev. 58 59-76.

EFRON, B. (1981). Nonparametric standard errors and confidence intervals (with discussion).
Canad. J. Statist. 9 139-172.

GODAMBE, V. P. and HEYDE, C. C. (1987). Quasi-likelihood and optimal estimation. Internat.
Statist. Rev. 55 231-244.
GODAMBE, V. P. and THOMPSON, M. E. (1989). An extension of quasi-likelihood estimation (with
discussion). J. Statist. Plann. Inference 22 137-172.

HABERMAN, S. J. (1984). Adjustment by minimum discriminant information. Ann. Statist. 12 971988.

HALL, P. (1990). Pseudo-likelihood theory for empirical likelihood. Ann. Statist. 18 121-140.
IBRAGIMOV, I. A. and HAS'MINSKII, R. Z. (1981). Statistical Estimation, Asymptotic Theory.
Springer, New York.
KALBFLEISCH, J. D. and LAWLESS, J. F. (1988). Estimation of reliability in field-performance studies. Technometrics 30 365-388.

KAPLAN, E. L. and MEIER, P. (1958). Nonparametric estimation from incomplete observations. J.
Amer. Statist. Assoc. 53 457-481.

KoLAczYK, E. D. (1992). Empirical likelihood for generalized linear models. Technical Report 389,
Dept. Statistics, Stanford Univ.
KUK, A. Y. C. and MAK, T. K (1989). Median estimation in the presence of auxiliary information.
J. Roy. Statist. Soc. Ser. B 51261-269.
MCCULLAGH, P. and NELDER, J. (1989). Generalized Linear Models, 2nd ed. Chapman and Hall,
London.

OWEN, A. B. (1988). Empirical likelihood ratio confidence intervals for a single functional.
Biometrika 75 237-249.

OWEN, A. B. (1990). Empirical likelihood confidence regions. Ann. Statist. 18 90-120.
OWEN, A. B. (1991). Empirical likelihood for linear models. Ann. Statist. 19 1725-1747.
OWEN, A. B. (1992). Empirical likelihood and generalized projection pursuit. Technical Report
393, Dept. Statistics, Stanford Univ.

QIN, J. and LAWLESS, J. F. (1992a). Empirical likelihood estimation and tests of parameters subject
to constraints. Technical Report 92-13, Dept. Statistics, Univ. Waterloo.
QIN, J. and LAWLESS, J. F. (1992b). Some applications of semi-empirical likelihood in incomplete
data problems. Unpublished manuscript.
RAO, C. R. (1973). Linear Statistical Inference and Its Applications. Wiley, New York.

SHEEHxY, A. (1988). Kullback-Leibler constrained estimation of probability measures, Technical
Report 137, Dept. Statistics, Stanford Univ.

This content downloaded from 134.174.144.116 on Tue, 05 Mar 2019 15:15:25 UTC
All use subject to https://about.jstor.org/terms

EMPIRICAL LIKELIHOOD AND GEE'S 325

THOMAS, D. R. and GRUNKEMEIER, G. L. (1975). Confidence interval estimation of survival proba-

bilities for censored data. J. Amer. Statist. Assoc. 70 865-871.
VAN DER VAART, A. W. (1988). Statistical Estimation in Large Parameter Spaces. Centrum voor

Wiskunde en Informatica, Amsterdam.

VARDI, Y (1985). Empirical distributions in selection bias models. Ann. Statist. 13 178-203.

DEPARTMENT OF STATISTICS

AND ACTUARIAL SCIENCE
UNIVERSITY OF WATERLOO
WATERLOO, ONTARIO

CANADA N2L 3G1

This content downloaded from 134.174.144.116 on Tue, 05 Mar 2019 15:15:25 UTC
All use subject to https://about.jstor.org/terms

