Rejoinder: Boosting Algorithms: Regularization, Prediction and Model Fitting
Author(s): Peter BÃ¼hlmann and Torsten Hothorn
Source: Statistical Science, Vol. 22, No. 4 (Nov., 2007), pp. 516-522
Published by: Institute of Mathematical Statistics
Stable URL: https://www.jstor.org/stable/27645857
Accessed: 21-10-2019 14:59 UTC
JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide
range of content in a trusted digital archive. We use information technology and tools to increase productivity and
facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org.
Your use of the JSTOR archive indicates your acceptance of the Terms & Conditions of Use, available at
https://about.jstor.org/terms

Institute of Mathematical Statistics is collaborating with JSTOR to digitize, preserve and
extend access to Statistical Science

This content downloaded from 206.253.207.235 on Mon, 21 Oct 2019 14:59:11 UTC
All use subject to https://about.jstor.org/terms

Statistical Science
2007, Vol. 22, No. 4, 516-522

DOI: 10.1214/07-STS242REJ
Main article DOI: 10.1214/07-STS242
? Institute of Mathematical Statistics, 2007

Rejoinder: Boosting Algorithms:
Regularization, Prediction and Model
Fitting
Peter B?hlmann and Torsten Hothorn

1. DEGREES OF FREEDOM FOR BOOSTING
We are grateful that Hastie points out the connec
tion to degrees of freedom for LARS which leads
to another?and often better?definition of degrees of
freedom for boosting in generalized linear models.

As Hastie writes and as we said in the paper, our
formula for degrees of freedom is only an approxima
tion: the cost of searching, for example, for the best
variable in componentwise linear least squares or com
ponentwise smoothing splines, is ignored. Hence, our
approximation formula

df(ra) = trace (<Sm)
for the degrees of freedom of boosting in the rath it
eration is underestimating the true degrees of freedom.
The latter is defined (for regression with L2-I0SS) as
n

dftrue(m) = ]TCov(?>-, Yi)/a2, Y = ?mY,
i=\

cf. Efron et al. [5].
For fitting linear models, Hastie illustrates nicely that

for infinitesimal forward stagewise (iFSLR) and the
Lasso, the cost of searching can be easily accounted for
in the framework of the LARS algorithm. With k steps
in the algorithm, its degrees of freedom are given by

Note that the number of steps in dfLARS is not mean

ingful for boosting with componentwise linear least
squares while dfactset(^) f?r boosting with m itera
tions can be used (and often seems reasonable; see
below). We point out that df(ra) and dfactset(^) are ran
dom (and hence they cannot be degrees of freedom in
the classical sense). We will discuss in the following
whether they are good estimators for the true (nonran
dom) dftnie(m).

When using another base procedure than compo
nentwise linear least squares, for example, compo
nentwise smoothing splines, the notion of dfactset(^)

is inappropriate (the number of selected covariates

times the degrees of freedom of the base proce
dure is not appropriate for assigning degrees of free

dom).

We now present some simulated examples where we
can evaluate the true dftrUe for /^Boosting. The first
two are with componentwise linear least squares for
fitting a linear model and the third is with componen
twise smoothing splines for fitting an additive model.

The models are
p

Yi = Y,?jx(iJ)+?^ s> ~ ^(?> !) u-d"

dfLARs(k) = k.
For quite a few examples, this coincides with the num
ber of active variables (variables which have been se
lected) when using k steps in LARS, that is,

/ = 1, ..., n,

with fixed design from Mp(0, E), E/j = 0.5I?-

nonzero regression coefficients ?j and with param

dfLARsW-dfactsetW
= cardinality of active set.
Peter B?hlmann is Professor, Seminar f?r Statistik, ETH

Z?rich, CH-8092 Z?rich, Switzerland (e-mail:
buhlmann@stat.math.ethz.ch). Torsten Hothorn is
Professor, Institut f?r Statistik, Ludwig-Maximilians
Universit?t M?nchen, Ludwigstra?e 33, D-80539 M?nchen,
Germany (e-mail: Torsten.Hothorn@R-project.org).
Torsten Hothorn wrote this paper while he was a lecturer at

(1)
(2)

P = 10,

peff=l,

n =

100, ft = V34^, ft e= 0? 5*5),
P =200, .peff=l,
n ? 100,

?8 as in (1),

p = 200, peff =10,

(3) n = 100,

j8 = (1, 1, 1, 1, 1, 0.5, 0.5, 0.5, 0.5

the Universit?t Erlangen-N?rnberg.

516
This content downloaded from 206.253.207.235 on Mon, 21 Oct 2019 14:59:11 UTC
All use subject to https://about.jstor.org/terms

517

REJOINDER

df active set

df approximate hat matrix

o -f
oo H

<o H

cm H
o H

20 40 60 80 100

20 40 60 80 100

Number of boosting iterations

Number of boosting iterations

FIG. 1. Model (1) and boosting with componentwise linear least squares (v = 0.1). True degrees of f
and?i(m) (shaded gray lines, left panel) and dfactset(w) (shaded gray lines, right panel) from 100 sim

All models (l)-(3) have the same signal-to-noise
true degrees of
rafreedom. Hence, our penalty term
AIC or
information criteria tends to be
tio. In addition, we consider the Friedman
#1similar
additive
model with p = 20 and peff = 5:

Yi = lOsin^jc^jcP) + 20(xf} -

small. Furthermore, our df(m) is less variable th

dfactset(w)- When looking in detail to the sparse ca
0.5)2
from model (1) and (2) in Figures 1 and 2, respective

our df(m)
+ I0x?4)+5x?5)+sii f = (i)
1./i,

is accurate for the range of iterati
which are reasonable (note that we should not sp
with fixed design from K[0, l]20 and i.i.d. errors e? ~
more degrees of freedom than, say, 2-3 if peff =

Jf(0, <j2), i = 1,..., n where

OLS on the single effective variable, including an in
cept, would have dftme = 2); (ii) the active set degre
of freedom are too large for the first few values of
that is, dfactset(w) = 2 (one variable and the interce

(4) <x2 = l,

(5) a] = 10.

although
dftrue(m) < 1.5 for m < 5. Such a behav
Figures 1-4 display the results. As already
men
tioned, our approximation df(m) underestimates
disappears
the
in the less sparse case in model (3), wh

df active set

df approximate hat matrix

8
? 4
o J

? H

o'

r-\_
r^

I

oh

0 20 40

100

Number of boosting iterations Number of boosting

FIG. 2. Model (2) and boosting with componentwise linear least squares (v = 0.1). Other

This content downloaded from 206.253.207.235 on Mon, 21 Oct 2019 14:59:11 UTC
All use subject to https://about.jstor.org/terms

518

P. B?HLMANN AND T. HOTHORN

df active set

df approximate hat matrix
O

o
CM

CM

E
o

lO J

* o I

o J

o
<D

(ft

<D

O)
(D
"O

10 H

*o -i

o

H

I

o

-L

60 80 100 0 20 40 60 80 100

Number of boosting iterations Number of b

Fig. 3. Model (3) and boosting with componentwise linear least squares (v =

is an example where df(m) underestimates
very
heav
will never be
exact.
It seems that a

ily; see Figure 3.
grees of freedom for boosting is m
Despite some (obvious) drawbacks
forof
LARS.
dfactset(tfO,
For other
it learners, for

works reasonably well. Hastieponentwise
has asked us
to give
smoothing
spline, we
a correction formula for ourbetter
df(m).
His discussion
approximation
for degrees o
summarizing the nice relation
between
LARS,
iFSLR
mula
df(m)
worked
reasonably we
and Z^Boosting, together with
our
simulated
exam
(4)
and
(5); changing
the signal-to-n

10 gave
almost identical results
ples, suggests that dfactSet(w) istor
a better
approximation

for degrees of freedom for boosting
prioriwith
because
the df(m)
compodepends on t
nentwise linear base procedure. no
Weguarantee
have implemented
for generalizing to ot
dfactset(w) in version 1.0-0 of the
sence
mboost
of a better
package
approximation
[9]
for
still
that
ourcom
df(m) formula
for assigning degrees of freedom
of think
boosting
with
ponentwise linear least squaresapproximation
for generalized
forlinear
degrees of freed
models. Unfortunately, in contrast
to LARS, dfactset(w)
componentwise
smoothing splines.

df approximate hat matrix
df approximate hat matr

i-r-1-1-r

h-i-1?:?i-r

0 50 100 150 200 0 50 100 150 200

Number of boosting iterations Number of

FIG. 4. Left: model (4). Right: model (5). Boosting with componentwise smoo
degrees of freedom dftrue(m) (dashed black line) and df(m) (shaded gray lines,

This content downloaded from 206.253.207.235 on Mon, 21 Oct 2019 14:59:11 UTC
All use subject to https://about.jstor.org/terms

REJOINDER 519
Hastie that cross-validation is a valuable alternative for
the task of estimating the stopping point of boosting
iterations.

2. HISTORICAL REMARKS AND NUMERICAL

OPTIMIZATION

Buja, Mease and Wyner (BMW hereafter) make a
very nice and detailed contribution regarding the his
tory and development of boosting.

BMW also ask why we advocate Friedman's gradi
ent descent as the boosting standard. First, we would
like to point out that computational efficiency in boost
ing does not necessarily yield better statistical perfor

mance. For example, a small step-size may be bene
ficial in comparison to step-size v ? 1, say. Related
to this fact, the quadratic approximation of the loss

function as described by BMW may not be better
than the linear approximation. To exemplify, take the

negative log-likelihood loss function in (3.1) for bi
nary classification. When using the linear approxima
tion, the working response (i.e., the negative gradient)

is

ZUinapp = 2(y/ - p(xi)), y i G {0, 1}.
In contrast, when using the quadratic approximation,
we end up with LogitBoost as proposed by Friedman,

Hastie and Tibshirani [7]. The working response is

then

_ 1 yj - p(xj)

Zi^d^-2P(xl)(l-P(xi)Y
The factor 1/2 appears in [7] when doing the lin
ear update but not for the working response. We see
that z/,quadapp *s numerically problematic whenever

p(xi) is close to 0 or 1, and [7], on pages 352?
353, address this issue by thresholding the value of
?/,quadapp to an "ad hoc" upper limit. On the other hand,

with the linear approximation and z/jinapp, such nu
merical problems do not arise. This is a reason why
we generally prefer to work with the linear approx
imation and Friedman's gradient descent algorithm

[6].

BMW also point out that there is no "random ele
ment" in boosting. In our experience, aggregation in
the style of bagging is often very useful. A combi
nation of boosting with bagging has been proposed
in B?hlmann and Yu [2] and similar ideas appear in
Friedman [8] and Dettling [4]. In fact, random forests
[1] also involve some bootstrap sampling in addition
to the random sampling of covariates in the nodes of
the trees; without the bootstrap sampling, it would not

work as well. We agree with BMW that quite a few
methods actually benefit from additional bootstrap ag
gregation. Our paper, however, focuses solely on boost

ing as a "basic module" without (or before) random
sampling and aggregation.

3. LIMITATIONS OF THE "STATISTICAL VIEW" OF

BOOSTING

BMW point out some limitations of the "statistical
view" (i.e., the gradient descent formulation) of boost

ing. We agree only in part with some of their argu
ments.

3.1 Conditional Class Probability Estimation
BMW point out that conditional class probabilities
cannot be estimated well by either AdaBoost or Logit
Boost, and later in their discussion they mention that

overfitting is a severe problem. Indeed, the amount
of regularization for conditional class probability es
timation should be (markedly) different than for clas
sification. For probability estimation we typically use
(many) fewer iterations, that is, a less complex fit, than
for classification. This fits into the picture of the re

joinder in [7] and [2], saying that the 0-1 misclas
sification loss in (3.2) is much more insensitive to
overfitting. For accurate conditional class probability
estimation, we should use the surrogate loss, for exam

ple, the negative log-likelihood loss in (3.1), for esti
mating (e.g., via cross-validation) a good stopping iter
ation. Then, conditional class probability estimates are
often quite reasonable (or even very accurate), depend
ing of course on the base procedure, the structure of the

underlying problem and the signal-to-noise ratio. We
agree with BMW that AdaBoost or LogitBoost overfit
for conditional class probability estimation when using
the wrong strategy?namely, tuning the boosting algo
rithm according to optimal classification. Thus, unfor
tunately, the goals of accurate conditional class proba
bility estimation and good classification are in conflict

with each other. This is a general fact (see rejoinder
by Friedman, Hastie and Tibshirani [7]) but it seems to
be especially pronounced with boosting complex data.

Having said that, we agree with BMW that AIC/BIC
regularization with the negative log-likelihood loss in
(3.1) for binary classification will be geared toward es

timating conditional probabilities, and for classifica
tion, we should use more iterations (less regulariza
tion).

This content downloaded from 206.253.207.235 on Mon, 21 Oct 2019 14:59:11 UTC
All use subject to https://about.jstor.org/terms

520 P. B?HLMANN AND T. HOTHORN

3.2 Robustness
For classification, BMW argue that robustness in the
response space is not an issue since, "binary responses
have no problem of vertically outlying values." We dis
agree with the relevance of their argument. For logistic
regression, robustification of the MLE has been studied
in detail. Even though the MLE has bounded influence,
the bound may be too large and for practical problems
this may matter a lot. K?nsch, Stefanski and Carroll
[10] is a good reference which also cites earlier papers
in this area. Note that with the exponential loss, the
issue of too large influence is even more pronounced
than with the log-likelihood loss corresponding to the

MLE.

4. EXEMPLIFIED LIMITATIONS OF THE
"STATISTICAL VIEW"

[11] created a simulation model which is additive
as a decision function but nonadditive on the logit
scale for the conditional class probabilities; and they
showed that larger trees are then better than stumps

(which is actually consistent with what we write in
our paper). We think that this is the main reason

why Mease and Wyner [11] found "contrary evi

dence."

We illustrate in Figure 5 that our heuristics to pre
fer stumps over larger trees is useful if the underly
ing model is additive for the logit of the conditional

class probabilities. The simulation model here is the
same as in [3] which we used to address the "contrary

evidence" findings in [11]; our model is inspired by
Mease and Wyner [11] but we make the conditional
class probabilities additive on the logit-scale:
5

logit(p(X)) = 8^(X(^-0.5),

The paper by Mease and Wyner [11] presents some
"contrary evidence" to the "statistical view" of boost (6)
y~ Bernoulli(p(X)),
ing. We repeat some of the points made by B?hlmann
and Yu [3] in the discussion of Mease and Wyner's pa and X - U[0, I]20 (i.e., i.i.d. U[0, 1]). This model
per.
has Bayes error rate approximately equal to 0.1 (as

4.1 Stumps Should be Used for Additive Bayes

Decision Rules

The sentence in the subtitle which is put forward,

discussed and criticized by BMW never appears in
our paper. The main source of confusion seems to be
the concept of "additivity" of a function. It should be

considered on the logit-scale (for AdaBoost, Logit

in [11]). We use n = 100, p = 20 (i.e., 15 inef

fective predictors), and we generate test sets of size
2000. We consider BinomialBoosting with stumps
and with larger trees whose varying size is about 6
8 terminal nodes. We consider the misclassification
test error, the test-set surrogate loss with the nega
tive log-likelihood and the absolute error for proba

bilities

Boost or BinomialBoosting), since the population min

j 2000

imizer of AdaBoost, LogitBoost or BinomialBoost
ing is half of the log-odds ratio. Mease and Wyner
misclassification test error

5555l = Dior,)-**,)!.
\
surrogate test error

absolute error for probabilities

i

200 400 600 800

1000

Number of boosting iterations

600 800 1000
Number of boosting iterations

800 1000
Number of boosting iterations

FlG. 5. BinomialBoosting (v = 0.1) with stumps (solid line) and larger trees (dashed line) for model (6). Left panel: test-set

tion error; middle panel: test-set surrogate loss; right panel: test-set absolute error for probabilities. Averaged over 50 simulati

This content downloaded from 206.253.207.235 on Mon, 21 Oct 2019 14:59:11 UTC
All use subject to https://about.jstor.org/terms

REJOINDER 521
where averaging is over the test set. Figure 5 displays
the results (the differences between stumps and larger

trees are significant) which are in line with the ex
planations and heuristics in our paper but very dif
ferent from what BMW describe. To reiterate, we
think that the reason for the "contrary evidence" in
Mease and Wyner [11] comes from the fact that their

model is not additive on the logit-scale. We also see
from Figure 5 that early stopping is important for
probability estimation, in particular when measur
ing in terms of test-set surrogate loss; a bit surpris

ingly, BinomialBoosting with stumps does not over
fit within the first 1000 iterations in terms of ab
solute errors for conditional class probabilities (this
is probably due to the low Bayes error rate of the
model; eventually, we will see overfitting here as well).

Finally, B?hlmann and Yu [3] also argue that the
findings here also appear when using "discrete Ad

aBoost."

In our opinion, it is exactly the "statistical view"
which helps to explain the phenomenon in Figure 5.
The "parameterization" with stumps is only "efficient"
if the model for the logit of the conditional class prob

abilities is additive; if it is nonadditive on the logit
scale, it can easily happen that larger trees are better
base procedures, as found indeed by Mease and Wyner

[11].

4.2 Early Stopping Should be Used to Prevent
Overfitting

BMW indicate that early stopping is often not
necessary?or even degrades performance. One should
be aware that they consider the special case of binary
classification with "discrete AdaBoost" and use trees
as the base procedure. Arguably, this is the original
proposal and application of boosting.

In our exposition, though, we not only focus on

4.3 Shrinkage Should be Used to Prevent
Overfitting
We agree with BMW that shrinkage does not always
improve performance. We never stated that shrinkage
would prevent overfitting. In fact, in linear models,
infinitesimal shrinkage corresponds to the Lasso (see

Section 5.2.1) and clearly, the Lasso is not free of
overfitting. In our view, shrinkage adds another di
mension of regularization. If we do not want to tune
the amount of shrinkage, the value v = 0.1 is often
a surprisingly good default value. Of course, there
are examples where such a default value is not opti

mal.

4.4 The Role of the Surrogate Loss Function and

Conclusions From BMW

BMW's comments on the role of the surrogate loss
function when using a particular algorithm are intrigu

ing. Their algorithm can be viewed as an ensemble
method; whether we should call it a boosting algo

rithm is debatable. And for sure, their method is not

within the framework of functional gradient descent al
gorithms.
BMW point out that there are still some mysteries
about AdaBoost. In our view, the overfitting behavior
is not well understood while the issue of using stumps
versus larger tree base procedures has a coherent expla
nation as pointed out above. There are certainly exam
ples where overfitting occurs with AdaBoost. The (the

oretical) question is whether there is a relevant class
of examples where AdaBoost is not overfitting when
running infinitely many iterations. We cannot answer
the question with numerical examples since "infinitely
many" can never be observed on a computer. The ques
tion has to be answered by rigorous mathematical argu
ments. For practical purposes, we advocate early stop
ping as a good and important recipe.

binary classification but on many other things, such

ACKNOWLEDGMENTS

as estimating class conditional probabilities, regres
sion functions and survival functions. As BMW write,

when using the surrogate loss for evaluating the per
formance of boosting, overfitting kicks in quite early

and early stopping is often absolutely crucial. It is
dangerous to present a message that early stopping
might degrade performance: the examples in Mease
and Wyner [11] provide marginal improvements of
about 1-2% without early stopping (of course, they
also stop somewhere) while the loss of not stopping
early can be huge in applications other than classifica

tion.

We are much obliged that the discussants provided
many thoughtful, detailed and important comments.
We also would like to thank Ed George for organizing
the discussion.

REFERENCES
[1] Breiman, L. (2001). Random forests. Machine Learning 45

5-32.

[2] B?HLMANN, P. and Yu, B. (2000). Discussion of "Addi
tive logistic regression: A statistical view," by J. Friedman,
T. Hastie and R. Tibshirani. Ann. Statist. 28 377-386.

This content downloaded from 206.253.207.235 on Mon, 21 Oct 2019 14:59:11 UTC
All use subject to https://about.jstor.org/terms

522 P. B?HLMANN AND T. HOTHORN
[31 B?HLMANN, P. and Yu, B. (2008). Discussion of "Evidence
contrary to the statistical view of boosting," by D. Mease and

A. Wyner. J. Machine Learning Research 9 187-194.
[4] DETTLING, M. (2004). BagBoosting for tumor classification
with gene expression data. Bioinformatics 20 3583-3593.

[5] Efron, B., Hastie, T., Johnstone, I. and Tibshirani,
R. (2004). Least angle regression (with discussion). Ann. Sta

tist. 32 407-499. MR2060166

[6] Friedman, J. (2001). Greedy function approximation:
A gradient boosting machine. Ann, Statist. 29 1189-1232.

MR1873328

[7] Friedman, J., Hastie, T. and Tibshirani, R. (2000). Ad
ditive logistic regression: A statistical view of boosting (with

discussion). Ann. Statist. 28 337-407. MR1790002

[8] Friedman, J. H. (2002). Stochastic gradient boosting. Corn
put. Statist. Data Anal. 38 367-378. MR1884869

[9] HOTHORN, T., B?HLMANN, P., Kneib, T. and SCHMID, M.

(2007). Mboost: Model-based boosting. R package version
1.0-0. Available at http://CRAN.R-project.org.

[10] K?nsch, H.-R., Stefanski, L. A. and Carroll, R. J.
(1989). Conditionally unbiased bounded-influence estimation

in general regression models, with applications to gener
alized linear models. J. Amer. Statist. Assoc. 84 460-466.

MR1010334

[11] Mease, D. and Wyner, A. (2008). Evidence contrary to the
statistical view of boosting. J. Machine Learning Research 9

131-156.

This content downloaded from 206.253.207.235 on Mon, 21 Oct 2019 14:59:11 UTC
All use subject to https://about.jstor.org/terms

