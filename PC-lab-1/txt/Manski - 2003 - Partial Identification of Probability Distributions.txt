Springer Series in Statistics
Advisors:
P. Bickel, P. Diggle, S. Fienberg, K. Krickeberg,
I. Olkin, N. Wermuth, S. Zeger

This page intentionally left blank

Charles F. Manski

Partial Identification of
Probability Distributions

Charles F. Manski
Department of Economics
Northwestern University
2001 Sheridan Road
Evanston, IL 60208-2600
USA
cfmanski@northwestern.edu

Library of Congress Cataloging-in-Publication Data
Manski, Charles F.
Partial identification of probability distributions / Charles F. Manski.
p. cm. — (Springer series in statistics)
Includes bibliographical references and index.
ISBN 0-387-00454-8 (alk. paper)
1. Distribution (Probability theory) 2. Regression analysis. I. Title. II. Series.
QA273.6.M294 2003
519.2′4—dc21
2003042476
ISBN 0-387-00454-8

Printed on acid-free paper.

 2003 Springer-Verlag New York, Inc.
All rights reserved. This work may not be translated or copied in whole or in part without the
written permission of the publisher (Springer-Verlag New York, Inc., 175 Fifth Avenue, New York,
NY 10010, USA), except for brief excerpts in connection with reviews or scholarly analysis. Use
in connection with any form of information storage and retrieval, electronic adaptation, computer
software, or by similar or dissimilar methodology now known or hereafter developed is forbidden.
The use in this publication of trade names, trademarks, service marks, and similar terms, even if
they are not identified as such, is not to be taken as an expression of opinion as to whether or not
they are subject to proprietary rights.
Printed in the United States of America.
9 8 7 6 5 4 3 2 1

SPIN 10911921

Typesetting: Pages created using the author’s WordPerfect files.
www.springer-ny.com
Springer-Verlag New York Berlin Heidelberg
A member of BertelsmannSpringer Science+Business Media GmbH

To Arthur S. Goldberger, who encouraged me to fly

v

This page intentionally left blank

Preface
Early on, when my research on partial identification was a lonely undertaking, Arthur Goldberger saw its potential and encouraged me to keep at it.
He especially lifted my spirits when, reacting to a new finding that I had
excitedly shown him, he remarked “now you are flying.” I have not had
many ways to express how important Art has been to me over the years as
colleague, critic, and friend. Dedicating this book to him is one.
The enterprise has gradually become less lonely as others have become
interested in, and contributed to, research on partial identification. Four of
the ten chapters in this book are based on joint work with co-authors with
whom I have enjoyed very productive collaborations. Chapters 3 and 4 are
based on several published articles co-authored with Joel Horowitz. Chapter
5 is based on an article co-authored with Philip Cross and Chapter 9 on one
co-authored with John Pepper. I have benefitted greatly from the opportunity to work with Joel, Phil, and John on these specific projects as well as
from our many discussions of subjects of mutual interest.
Jeff Dominitz, Francesca Molinari, John Pepper, and Daniel Scharfstein
provided detailed and constructive comments on a draft of this book,
completed in the summer of 2002. I was fortunate that all four have been
interested in the book and eager to help me improve its coverage and
exposition. I am grateful to the students enrolled in my Ph.D. field courses
in econometrics at Northwestern University in spring and fall 2002. I tried
out early versions of various book chapters on the spring 2002 class and
worked through the entire draft with the fall 2002 class. I am also grateful
to Joerg Stoye, who read the revised manuscript.
The National Science Foundation has supported my research program
through a succession of grants. My preparation of the book itself was
supported in part under grant SES-0001436.
Chicago, Illinois
January 2003

Charles F. Manski

vii

This page intentionally left blank

Contents
Preface

vii

Introduction: Partial Identification and Credible Inference

1

1
Missing Outcomes

6

1.1. Anatomy of the Problem
1.2. Means
1.3. Parameters that Respect Stochastic Dominance
1.4. Combining Multiple Sampling Processes
1.5. Interval Measurement of Outcomes
Complement 1A. Employment Probabilities
Complement 1B. Blind-Men Bounds on an Elephant
Endnotes

6
8
11
13
17
18
21
23

2
Instrumental Variables

26

2.1. Distributional Assumptions and Credible Inference
2.2. Some Assumptions Using Instrumental Variables
2.3. Outcomes Missing-at-Random
2.4. Statistical Independence
2.5. Mean Independence and Mean Monotonicity
2.6. Other Assumptions Using Instrumental Variables
Complement 2A. Estimation with Nonresponse Weights
Endnotes

26
27
29
30
32
36
37
38

ix

x

Contents

3
Conditional Prediction with Missing Data

40

3.1. Prediction of Outcomes Conditional on Covariates
3.2. Missing Outcomes
3.3. Jointly Missing Outcomes and Covariates
3.4. Missing Covariates
3.5. General Missing-Data Patterns
3.6. Joint Inference on Conditional Distributions
Complement 3A. Unemployment Rates
Complement 3B. Parametric Prediction with Missing Data
Endnotes

40
41
41
46
49
53
55
56
58

4
Contaminated Outcomes

60

4.1. The Mixture Model of Data Errors
4.2. Outcome Distributions
4.3. Event Probabilities
4.4. Parameters that Respect Stochastic Dominance
Complement 4A. Contamination Through Imputation
Complement 4B. Identification and Robust Inference
Endnotes

60
62
63
65
68
70
72

5
Regressions, Short and Long

73

5.1. Ecological Inference
5.2. Anatomy of the Problem
5.3. Long Mean Regressions
5.4. Instrumental Variables
Complement 5A. Structural Prediction
Endnotes

73
74
76
81
84
85

6
Response-Based Sampling

87

6.1. Reverse Regression
6.2. Auxiliary Data on Outcomes or Covariates
6.3. The Rare-Disease Assumption
6.4. Bounds on Relative and Attributable Risk

87
89
89
91

Contents

xi

6.5. Sampling from One Response Stratum
Complement 6A. Smoking and Heart Disease
Endnotes

94
97
98

7
Analysis of Treatment Response

99

7.1. Anatomy of the Problem
7.2. Treatment Choice in Heterogeneous Populations
7.3. The Selection Problem and Treatment Choice
7.4. Instrumental Variables
Complement 7A. Identification and Ambiguity
Complement 7B. Sentencing and Recidivism
Complement 7C. Missing Outcome and Covariate Data
Complement 7D. Study and Treatment Populations
Endnotes

99
102
105
108
110
112
114
117
118

8
Monotone Treatment Response

120

8.1. Shape Restrictions
8.2. Monotonicity
8.3. Semi-monotonicity
8.4. Concave Monotonicity
Complement 8A. Downward-Sloping Demand
Complement 8B. Econometric Response Models
Endnotes

120
123
127
132
136
138
139

9
Monotone Instrumental Variables

141

9.1. Equalities and Inequalities
9.2. Mean Monotonicity
9.3. Mean Monotonicity and Mean Treatment Response
9.4. Variations on the Theme
Complement 9A. The Returns to Schooling
Endnotes

141
143
145
149
149
153

xii

Contents

10
The Mixing Problem

154

10.1. Within-Group Treatment Variation
10.2. Known Treatment Shares
10.3. Extrapolation from the Experiment Alone
Complement 10A. Experiments Without Covariate Data
Endnotes

154
157
160
161
165

References

167

Index

175

Introduction: Partial Identification
and Credible Inference
Statistical inference uses sample data to draw conclusions about a population of interest. However, data alone do not suffice. Inference always
requires assumptions about the population and the sampling process.
Statistical theory illuminates the logic of inference by showing how data
and assumptions combine to yield conclusions.
Empirical researchers should be concerned with both the logic and the
credibility of their inferences. Credibility is a subjective matter, yet I take
there to be wide agreement on a principle that I shall call:
The Law of Decreasing Credibility: The credibility of inference decreases
with the strength of the assumptions maintained.
This principle implies that empirical researchers face a dilemma as they
decide what assumptions to maintain: Stronger assumptions yield inferences
that may be more powerful but less credible. Statistical theory cannot
resolve the dilemma but can clarify its nature.
It is useful to distinguish combinations of data and assumptions that
point-identify a population parameter of interest from ones that place the
parameter within a set-valued identification region. Point identification is
the fundamental necessary condition for consistent point estimation of a
parameter. Strengthening an assumption that achieves point identification
may increase the attainable precision of estimates of the parameter.
Statistical theory has had much to say about this matter. The classical theory
of local asymptotic efficiency characterizes, through the Fisher information
matrix, how attainable precision increases as more is assumed known about
a population distribution. Nonparametric regression analysis shows how the
attainable rate of convergence of estimates increases as more is assumed
about the shape of the regression. These and other achievements provide
1

2

Introduction

important guidance to empirical researchers as they weigh the credibility
and precision of alternative point estimates.
Statistical theory has had much less to say about inference on population
parameters that are not point-identified (see the historical note at the end of
this Introduction). It has been commonplace to think of identification as a
binary event—a parameter is either identified or it is not—and to view point
identification as a precondition for meaningful inference. Yet there is
enormous scope for fruitful inference using data and assumptions that
partially identify population parameters. This book explains why and shows
how.

Origin and Organization of the Book
The book has its roots in my research on nonparametric regression analysis
with missing outcome data, initiated in the late 1980s. Empirical researchers
estimating regressions commonly assume that missingness is random, in the
sense that the observability of an outcome is statistically independent of its
value. Yet this and other point-identifying assumptions have regularly been
criticized as implausible. So I set out to determine what random sampling
with partial observability of outcomes reveals about mean and quantile
regressions if nothing is known about the missingness process or if
assumptions weak enough to be widely credible are imposed. The findings
were sharp bounds whose forms vary with the regression of interest and
with the maintained assumptions. These bounds can readily be estimated
using standard methods of nonparametric regression analysis.
Study of regression with missing outcome data stimulated investigation
of more general incomplete data problems. Some sample realizations may
have unobserved outcomes, some may have unobserved covariates, and
others may be entirely missing. Sometimes interval data on outcomes or
covariates are available, rather than point measurements. Random sampling
with incomplete observation of outcomes and covariates generically yields
partial identification of regressions. The challenge is to describe and
estimate the identification regions produced by incomplete-data processes
when alternative assumptions are maintained.
Study of regression with missing outcome data also naturally led to
examination of inference on treatment response. Analysis of treatment
response must contend with the fundamental problem that counterfactual
outcomes are not observable; hence my findings on partial identification of
regressions with missing outcome data were directly applicable. Yet
analysis of treatment response poses much more than a generic missing-data
problem. One reason is that observations of realized outcomes, when
combined with suitable assumptions, can provide information about
counterfactual ones. Another is that practical problems of treatment choice

Introduction

3

motivate much research on treatment response and thereby determine what
population parameters are of interest. So I found it productive to examine
inference on treatment response as a subject in its own right.
Another subject of study has been inference on the components of finite
probability mixtures. The mathematical problem of decomposition of finite
mixtures arises in many substantively distinct settings, including contaminated sampling, ecological inference, and regression with missing covariate
data. Findings on partial identification of mixtures have application to all
of these subjects and more.
This book presents the main elements of my research on partial
identification of probability distributions. Chapters 1 through 3 form a unit
on prediction with missing outcome or covariate data. Chapters 4 and 5
form a unit on decomposition of finite mixtures. Chapter 6 is a stand-alone
analysis of response-based sampling. Chapters 7 through 10 form a unit on
the analysis of treatment response.
Whatever the particular subject under study, the presentation follows a
common path. I first specify the sampling process generating the available
data and ask what may be inferred about population parameters of interest
in the absence of assumptions restricting the population distribution. I then
ask how the (typically) set-valued identification regions for these parameters shrink if certain assumptions are imposed. There are, of course,
innumerable assumptions that could be entertained. I mainly study statistical
independence and monotonicity assumptions.
The approach to inference that runs throughout the book is deliberately
conservative and thoroughly nonparametric. The traditional way to cope
with sampling processes that partially identify population parameters has
been to combine the available data with assumptions strong enough to yield
point identification. Such assumptions often are not well motivated, and
empirical researchers often debate their validity. Conservative
nonparametric analysis enables researchers to learn from the available data
without imposing untenable assumptions. It enables establishment of a
domain of consensus among researchers who may hold disparate beliefs
about what assumptions are appropriate. It also makes plain the limitations
of the available data. When credible identification regions turn out to be
uncomfortably large, researchers should face up to the fact that the available
data do not support inferences as tight as they might like to achieve.
By and large, the analysis of the book rests on the most elementary
probability theory. As will become evident, an enormous amount about
identification can be learned from judicious application of the Law of Total
Probability and Bayes Theorem. To keep the presentation simple without
sacrificing rigor, I suppose throughout that conditioning events have
positive probability. With appropriate attention to smoothness and support

4

Introduction

conditions, the various propositions that involve conditioning events hold
more generally.
The book maintains a consistent notation and usage of terms throughout
its ten chapters, with the most basic elements set forth in Chapter 1 and
elaborations introduced later as required. Random variables are always in
italics and their realizations in normal font. The main part of each chapter
is written in textbook style, without references to literature. However, each
chapter has complements and endnotes that place the analysis in context and
elaborate in eclectic ways. The first endnote of each chapter cites the
sources on which the chapter draws. These primarily are research articles
that I have written, often with co-authors, over the period 1989–2002.
This book complements my earlier book Identification Problems in the
Social Sciences (Manski, 1995), which exposits basic themes and findings
on partial identification in an elementary way intended to be broadly
accessible to students and researchers in the social sciences. The present
book develops the subject in a rigorous, thorough manner meant to provide
the foundation for further study by statisticians and econometricians.
Readers who are entirely unfamiliar with partial identification may want to
scan at least the introduction and first two chapters of the earlier book
before beginning this one.

Identification and Statistical Inference
This book contains only occasional discussions of problems of finite-sample
statistical inference. Identification and statistical inference are sufficiently
distinct for it to be fruitful to study them separately. As burdensome as
identification problems may be, they at least have the analytical clarity of
exercises in deductive logic. Statistical inference is a more murky matter of
induction from samples to populations.
The usefulness of separating the identification and statistical components
of inference has long been recognized. Koopmans (1949, p. 132) put it this
way in the article that introduced the term identification into the literature:
In our discussion we have used the phrase “a parameter that can be determined
from a sufficient number of observations.” We shall now define this concept
more sharply, and give it the name identifiability of a parameter. Instead of
reasoning, as before, from “a sufficiently large number of observations” we shall
base our discussion on a hypothetical knowledge of the probability distribution
of the observations, as defined more fully below. It is clear that exact knowledge of this probability distribution cannot be derived from any finite number
of observations. Such knowledge is the limit approachable but not attainable by
extended observation. By hypothesizing nevertheless the full availability of such
knowledge, we obtain a clear separation between problems of statistical

Introduction

5

inference arising from the variability of finite samples, and problems of
identification in which we explore the limits to which inference even from an
infinite number of observations is suspect.

Historical Note
Partial identification of population parameters has a long but sparse history
in statistical theory. Frisch (1934) developed sharp bounds on the slope
parameter of a simple linear regression when the covariate is measured with
mean-zero errors; fifty years later, his analysis was extended to multiple
regression by Klepper and Leamer (1984). Frechét (1951) studied the
conclusions about a joint probability distribution that may be drawn given
knowledge of its marginals; see Ruschendorf (1981) for subsequent
findings. Duncan and Davis (1953) used a numerical example to show that
ecological inference is a problem of partial identification, but formal
characterization of identification regions had to wait more than forty years
(Horowitz and Manski, 1995; Cross and Manski, 2002). Cochran,
Mosteller, and Tukey (1954) suggested conservative analysis of surveys
with missing outcome data due to nonresponse by sample members, but
Cochran (1977) subsequently downplayed the idea. Peterson (1976)
initiated study of partial identification of the competing risk model of
survival analysis; Crowder (1991) and Bedford and Meilijson (1997) have
carried this work further.
Throughout this book, I begin with the identification region obtained
using the empirical evidence alone and study how distributional assumptions may shrink this region. A mathematically complementary approach is
to begin with some point-identifying assumption and examine how
identification decays as this assumption is weakened in specified ways.
Methodological research of the latter kind is variously referred to as
sensitivity, perturbation, or robustness analysis. For example, studying the
problem of missing outcome data, Rosenbaum (1995) and Scharfstein,
Rotnitzky, and Robins (1999) investigate classes of departures from the
point-identifying assumption that data are missing at random.

1
Missing Outcomes
1.1. Anatomy of the Problem
To begin, suppose that each member j of a population J has an outcome yj
in a space Y. The population is a probability space (J, 6, P) and y: J  Y is
a random variable with distribution P(y). A sampling process draws persons
at random from J. A realization of y may or may not be observable, as
indicated by the realization of a binary random variable z. Thus y is
observable if z = 1 and not observable if z = 0. The problem is to use the
available data to learn about P(y).
The structure of this familiar problem of inference with missing outcome
data is displayed by the Law of Total Probability
P(y) = P(y z = 1)P(z = 1) + P(y z = 0)P(z = 0).

(1.1)

The sampling process asymptotically reveals the distribution of
observable outcomes, P(y z = 1), and the distribution of observability, P(z).
The sampling process is uninformative regarding the distribution of missing
outcomes, P(y z = 0). Hence, the empirical evidence asymptotically reveals
that P(y) lies in the identification region

[P(y)]  [P(y z = 1)P(z = 1) + P(z = 0),



],

Y

(1.2)

where Y denotes the space of all probability measures on Y. The feasible
values of P(y) are the mixtures of P(y z = 1) and all elements of Y, with
mixing probabilities P(z = 1) and P(z = 0). The identification region is a
proper subset of Y whenever the probability P(z = 0) of missing data is less
6

1.1. Anatomy of the Problem

7

than one, and is a singleton when P(z = 0) = 0. Hence P(y) is partially
identified when 0 < P(z = 0) < 1 and is point-identified when P(z = 0) = 0.
Distributional assumptions may have identifying power. One may assert
that the distribution P(y z = 0) of missing outcomes lies in some set 0Y G
Y. Then the identification region shrinks from [P(y)] to

1[P(y)]  [P(y z = 1)P(z = 1) + P(z = 0),



].

0Y

(1.3)

Or one may assert that the distribution of interest, P(y), lies in some set
0[P(y)] G Y. Then the identification region shrinks from [P(y)] to

1[P(y)]  0[P(y)] B [P(y)].

(1.4)

Assumptions of the former and latter types differ in that the former are
necessarily non-refutable but the latter may be refutable. An assumption
that restricts P(y z = 0) is non-refutable because, after all, one does not
observe the missing data. In contrast, an assumption that restricts P(y) may
be incompatible with the available empirical evidence. If the intersection of
0[P(y)] and [P(y)] is empty, one should conclude that P(y) does not lie
in the set 0[P(y)].
The above concerns identification of the entire outcome distribution. A
common objective of empirical research is to infer a parameter of this
distribution; for example, one may want to learn the mean of y. Viewing
this in abstraction, let -(#): Y  , be a function mapping probability
distributions on Y into a space , and consider the problem of inference on
the parameter -[P(y)]. The identification region for -[P(y)] is

{-[P(y)]} = {-(),   [P(y)]}

(1.5)

if only the empirical evidence is available and is

1{-[P(y)]} = {-(),   1[P(y)]}

(1.6)

given distributional assumptions as discussed above.

Statistical Inference
The fundamental problem posed by missing data is identification, so it is
analytically convenient to suppose that one knows the distributions that are
asymptotically revealed by the sampling process, namely P(y z = 1) and
P(z). Of course, an empirical researcher observing a sample of finite size N
must contend with issues of statistical inference as well as identification. I

8

1. Missing Outcomes

shall not dwell on these here, but merely point out that the empirical
distributions PN(y z = 1) and PN(z) almost surely converge to P(y z = 1) and
P(z) respectively. Hence, a natural nonparametric estimate of the identification region [P(y)] is the sample analog

N[P(y)]  [PN(y z = 1)PN(z = 1) + PN(z = 0),



Y

]

(1.7)

and a natural nonparametric estimate of {-(),   [P(y)]} is {-(),  
N[P(y)]}. Sample analogs may also be used to estimate 1[P(y)] in the
presence of distributional assumptions.

The Task Ahead
The above, in a nutshell, is the story of identification when the data are
generated by random sampling and some outcome realizations are not
observable. The task ahead is to flesh out and elaborate on this story.
The remainder of this chapter studies identification using the empirical
evidence alone. Sections 1.2 and 1.3 describe the identification regions for
particular parameters of interest: means of real-valued functions of y and
parameters that respect stochastic dominance. Section 1.4 generalizes the
premise of random sampling to cases in which data are available from
multiple sampling processes, each process drawing persons at random and
each having some missing data. Section 1.5 extends the scope of the
analysis from missing data to interval measurement of outcomes.
Chapter 2 examines a broad class of distributional assumptions that use
instrumental variables to help identify the distribution of outcomes. One
such assumption is the long-familiar supposition that data are missing at
random. Another is the premise that outcomes are statistically independent
of an instrumental variable. Yet another is that mean outcomes vary
monotonically with an instrumental variable.
The analysis here and in Chapter 2 extends immediately to inference on
conditional outcome distributions if the conditioning event is always
observed. One simply needs to redefine the population of interest to be the
sub-population for which the conditioning event holds. Chapter 3 examines
inference on conditional distributions when data on outcomes and/or
conditioning events may be missing.

1.2. Means
Let R  [, ] be the extended real line. Let G be the space of measurable
functions that map Y into R and that attain their lower and upper bounds g0

1.2. Means

9

 inf y  Y g(y) and g1  sup y  Y g(y). Thus g  G if there exists a y0g  Y
such that g(y0g) = g0 and a y1g  Y such that g(y1g) = g1. The lower bound
g0 may be finite or may be ; similarly, g1 may be finite or may be .
Let the problem of interest be to infer the expectation E[g(y)] using only
the empirical evidence. The Law of Iterated Expectations gives
E[g(y)] = E[g(y) z = 1]P(z = 1) + E[g(y) z = 0]P(z = 0).

(1.8)

The sampling process asymptotically reveals E[g(y) z = 1] and P(z).
However, it is uninformative regarding E[g(y) z = 0], which can take any
value in the interval [g0, g1]. Hence we have this simple, important result:
Proposition 1.1: Let g  G. Given the empirical evidence alone, the
identification region for E[g(y)] is the closed interval

{E[g(y)]} = [E[g(y) z = 1]P(z = 1) + g0P(z = 0),
E[g(y) z = 1]P(z = 1) + g1P(z = 0)].

(1.9)
a

If the function g does not attain its lower (upper) bound on Y, Proposition
1.1 remains valid with the closed interval on the right side of (1.9) replaced
by one that is open from below (above).
Observe that {E[g(y)]} is a proper subset of [g0, g1], and hence
informative, whenever the probability P(z = 0) of missing data is less than
one and g has finite range. The width of the region is (g1  g0)P(z = 0).
Thus, the severity of the identification problem varies directly with the
probability P(z = 0) of missing data.
The situation changes if g0 =  or g1 = . The identification region is
the tail interval [, E[g(y) z = 1]P(z = 1) + g1P(z = 0)] in the former case
and [E[g(y) z = 1]P(z = 1) + g0P(z = 0), ] in the latter. In both cases, the
region remains informative but has infinite length. The region is [, ] if
g is unbounded from both below and above. Thus, credible prior information is a prerequisite for inference on the mean of an unbounded random
variable.

Probabilities of Events
Proposition 1.1 has many applications. Perhaps the most far-reaching is the
identification region it implies for the probability that y lies in any nonempty, proper, measurable set B G Y. Let gB(#) be the indicator function

10

1. Missing Outcomes

gB(y)  1[y  B]; that is, gB(y) = 1 if y  B and gB(y) = 0 otherwise. Then
gB(#) attains its lower and upper bounds on Y, these being 0 and 1.
Moreover, E[gB(y)] = P(y  B) and E[gB(y) z = 1] = P(y  B z = 1). Hence,
Proposition 1.1 has this corollary.
Corollary 1.1.1: Let B be a non-empty, proper, and measurable subset of Y.
Given the empirical evidence alone, the identification region for P(y  B)
is the closed interval

[P(y  B)] = [P(y  B z = 1)P(z = 1),
P(y  B z = 1)P(z = 1) + P(z = 0)].

(1.10)
a

The width of this interval is P(z = 0), whatever the set B may be. The
location of the interval does vary with B. In particular, if B1 G B, the
interval [P(y  B)] shifts the interval [P(y  B1)] rightward.
Statistical Inference
The natural nonparametric estimate of the identification region for E[g(y)]
is its sample analog. The sampling distribution of this estimate is particularly simple to analyze if one rewrites (1.9) in the alternative form

{E[g(y)]} = [E[g(y)z + g0(1  z)], E[g(y)z + g1(1  z)]].

(1.9')

The sample analog of (1.9') is the interval

N{E[g(y)]} = [EN[g(y)z + g0(1  z)], EN[g(y)z + g1(1  z)]]

(1.11)

connecting the sample averages of g(y)z + g0(1  z) and g(y)z + g1(1  z).
Hence, analysis of the sampling distribution of N{E[g(y)]} is the
elementary problem of analysis of the sampling distribution of a bivariate
sample average.

Point Estimation and Tests of Hypotheses
Empirical researchers confronting missing data routinely combine the
empirical evidence with distributional assumptions to produce point
estimates of parameters of interest. These assumptions may or may not be
correct, so it is of interest to examine point estimation from the present
nonparametric perspective.

1.3. Parameters that Respect Stochastic Dominance

11

Let N denote a point estimate of E[g(y)] obtained in a sample of size N,
and let  be its probability limit. Suppose that  lies outside the identification region {E[g(y)]}. Then E[g(y)] cannot equal . Hence, the asserted
assumptions must be incorrect.
Of course, such a definitive conclusion cannot be drawn from finitesample data. However, one can compare the point estimate N with the
identification-region estimate N{E[g(y)]}. This suggests statistical tests of
the form: Reject the asserted assumptions if the point N is sufficiently
distant from the interval N{E[g(y)]}.
Such tests are applicable only when assumptions are refutable, in the
sense that  can logically lie outside the identification region {E[g(y)]}.
Chapter 2 studies a variety of assumptions, of which some are refutable and
others are not.1

1.3. Parameters that Respect Stochastic Dominance
This section generalizes Proposition 1.1 from means of functions of y to
parameters that respect stochastic dominance.
Parameters that Respect Stochastic Dominance (D-parameters): Let R be
the space of probability distributions on the extended real line R. Distribution F  R stochastically dominates distribution F1  R if F[, t] 
F1[, t] for all t  R. An extended real-valued function D(#): R  R
respects stochastic dominance (is a D-parameter) if D(F)  D(F1) whenever
F stochastically dominates F1.
Leading examples of D-parameters are the mean and quantiles of real
random variables. Spread parameters such as the variance or interquartile
range do not respect stochastic dominance.
Here is the result:
Proposition 1.2: Let D(#) respect stochastic dominance. Let g  G. Let Rg
 [g(y), y  Y] be the range set of g. Let g be the space of probability
distributions on Rg. Let 0g  g and 1g  g be the degenerate distributions
that place all mass on g0 and g1 respectively. Given the empirical evidence
alone, the smallest and largest points in the identification region for
D{P[g(y)]} are D{P[g(y) z = 1]P(z = 1) + 0gP(z = 0)} and
a
D{P[g(y) z = 1]P(z = 1) + 1gP(z = 0)}.
Proof: The identification region for distribution P[g(y)] is

12

1. Missing Outcomes

{P[g(y)]}  {P[g(y) z = 1]P(z = 1) + P(z = 0),



}.

g

(1.12)

Consider distribution P[g(y) z = 1]P(z = 1) + 0gP(z = 0), which supposes
that all missing data take a value y0g that minimizes g. This distribution
belongs to {P[g(y)]} and is stochastically dominated by all other members
of {P[g(y)]}. Similarly, P[g(y) z = 1]P(z = 1) + 1gP(z = 0) belongs to
{P[g(y)]} and stochastically dominates all other members of {P[g(y)]}.
The result follows.
Q. E. D.

Proposition 1.2 determines sharp lower and upper bounds on D{P[g(y)]},
but it does not assert that the identification region is the entire interval
connecting these bounds. Proposition 1.1 showed that the identification
region is this interval if D is the expectation parameter. However, the
interval may contain non-feasible values if D is another parameter that
respects stochastic dominance.
A particularly simple example occurs when g(y) is a binary random
variable and D is a quantile of P[g(y)]. A quantile must be an element of the
range set Rg. Hence, D{P[g(y)]} cannot take a value in the interior of the
interval [0, 1].

Quantiles
Quantiles are familiar parameters that respect stochastic dominance. For 
 (0, 1), the –quantile of P[g(y)] is Q[g(y)]  min t: {P[g(y)  t]  }.
Proposition 1.2 shows that the smallest feasible value of Q[g(y)] is the
–quantile of distribution P[g(y) z = 1]P(z = 1) + 0gP(z = 0) and the largest
feasible value is the –quantile of P[g(y) z = 1]P(z = 1) + 1gP(z = 0).
Examination of these quantities yields the following Corollary.
Corollary 1.2.1: Let   (0,1). Define r() and s() as follows:
r()  [1  (1  )/P(z = 1)]–quantile of P[g(y) z = 1] if P(z = 1) > 1  
 g0 otherwise.
s()  [/P(z = 1)]–quantile of P[g(y) z = 1] if P(z = 1)  
 g1 otherwise.
The smallest and largest points in the identification region for Q[g(y)] are
a
r() and s().

1.4. Combining Multiple Sampling Processes

13

Observe that r() and s() are weakly increasing functions of ; hence, the
identification region for Q[g(y)] shifts to the right as  increases.
For any value of , the lower and upper bounds r() and s() are
generically informative if P(z = 1) > 1   and P(z = 1)  , respectively.
This holds whether or not the function g has finite range. Clearly, the
implications of missing data for inference on quantiles are quite different
from the implications for inference on means.

Outer Bounds on Differences between D-Parameters
Sometimes the parameter of interest is the difference between two specified
D-parameters; that is, a parameter of the form -21{P[g(y)]}  D2{P[g(y)]}
 D1{P[g(y)]}. For example, the interquartile range Q0.75[g(y)]  Q0.25[g(y)]
is a familiar measure of the spread of a distribution. The meanmedian
difference E[g(y)]  Q0.5[g(y)] measures skewness.
In general, differences between D-parameters are not themselves Dparameters. Nevertheless, Proposition 1.2 may be used to obtain informative
outer bounds on such differences. A lower bound on -21{P[g(y)]} is the
proposition’s lower bound on D2{P[g(y)]} minus its upper bound on
D1{P[g(y)]}; similarly, an upper bound on -21{P[g(y)]} is the proposition’s
upper bound on D2{P[g(y)]} minus its lower bound on D1{P[g(y)]}.
The bound on -21{P[g(y)]} obtained in this manner generally is nonsharp; hence the term outer bound. Consider the lower bound constructed
above for -21{P[g(y)]}. For this to be sharp, there would have to exist a
distribution of missing data that jointly makes D2{P[g(y)]}attain its sharp
lower bound and D1{P[g(y)]} attain its sharp upper bound. However,
D2{P[g(y)]}attains its sharp lower bound if all missing data take a value y0g
that minimizes g, and D1{P[g(y)]}attains its sharp upper bound if all
missing data take a value y1g that maximizes g. These two requirements are
compatible with one another only in degenerate cases.

1.4. Combining Multiple Sampling Processes
I have so far presumed that the available data are generated by random
sampling, with y observable if z = 1. This section generalizes the analysis
to cases in which data from multiple sampling processes are available. Each
sampling process draws persons at random from population J, and each
yields some observable outcomes. Outcomes that are observable under
some sampling processes may be missing under others. The objective is to
combine the data generated by the sampling processes to learn as much as
possible about P(y).2

14

1. Missing Outcomes

The possibility of combining data from multiple sampling processes
arises often in survey research. One survey of a population of interest may
attempt to interview respondents face-to-face, another by telephone, and
another by mail or e-mail. Each interview mode may yield its own pattern
of nonresponse.

Identification of P(y)
Let M denote the set of sampling processes. For each j  J and m  M, let
zjm = 1 if yj is observable under sampling process m; let zjm = 0 otherwise.
For each m  M, the Law of Total Probability gives
P(y) = P(y zm = 1)P(zm = 1) + P(y zm = 0)P(zm = 0).

(1.13)

Sampling process m asymptotically reveals the distribution of observable
outcomes, P(y zm = 1), and the distribution of observability, P(zm). Hence
P(y)  [P(y zm = 1)P(zm = 1) +

P(zm = 0),

m

m



].

(1.14)

Y

The set of sampling processes collectively reveal that P(y) lies in the
intersection of the sets of distributions on the right side of (1.14). Hence,
we have the following proposition.
Proposition 1.3: The identification region for P(y) is

M[P(y)]  B [P(y zm = 1)P(zm = 1) +

P(zm = 0),

m

m



]. (1.15)

Y

mM

a
Proposition 1.3 is simple in form but is too abstract to communicate
much about the size and shape of the identification region. Corollary 1.3.1
gives a useful alternative characterization of the region when M is finite.
Part (a) shows that a distribution is a feasible value for P(y) if and only if
the probability that it places on each measurable subset of Y is no less than
an easily computed lower bound. This characterization further simplifies
when Y is countable. Then, part (b) shows that one need only consider the
probability placed on each atom of Y. This finding yields a simple
necessary and sufficient condition for existence of a unique feasible
distribution, given in part (c).
Corollary 1.3.1: Let M[P(y)] be given by (1.15), with M finite. Let  
Y. For each measurable set B G Y, define

1.4. Combining Multiple Sampling Processes

%M(B)  max m  M P(y  B zm = 1)P(zm = 1).

15
(1.16)

(a) Then   M[P(y)] if and only if (B)  %M(B), ~ B G Y.
(b) Let Y be countable. Then   M[P(y)] if and only if (y)  %M(y),
~ y  Y.
(c) Let Y be countable. Let SM  y  Y %M(y). The region M[P(y)] contains
multiple distributions if SM < 1 and a unique distribution if SM = 1. If SM =
a
1, the unique feasible distribution is M(y)  %M(y), y  Y.
Proof: (a) Let   M[P(y)]. For each m  M, there exists a distribution m
 Y such that  = P(y zm = 1)P(zm = 1) + mP(zm = 0). Hence (B) 
P(y  B zm = 1)P(zm = 1), ~ B G Y. Hence (B)  %M(B), ~ B G Y.
Let (B)  %M(B), B G Y. Let m  [  P(y zm = 1)P(zm = 1)]/P(zm = 0).
Then m is a probability measure. Moreover,  = P(y zm = 1)P(zm = 1) +
mP(zm = 0). Hence   M[P(y)].
(b) Part (a) shows directly that   M[P(y)] < (y)  %M(y), ~ y  Y. Let
(y)  %M(y), y  Y. Then for all B G Y,

(B) =  y  B (y)   y  B %M(y)  %M(B),

(1.17)

where the final inequality holds because %M(#) is sub-additive. Hence  
M[P(y)], again by part (a).
(c) If SM < 1, the empirical evidence leaves indeterminate the allocation of
(1  SM) of probability mass among the atoms of Y; Hence M[P(y)]
contains multiple distributions.
If SM = 1, then M is a probability measure. Part (b) shows that M 
M[P(y)]. Let  be a measure with (y)  %M(y), y  Y, and (y) > %M(y)
for some y  Y. Then (Y) > 1, so  is not a probability measure. Hence M
is the only element of M[P(y)].
Note that M is distinct from %M, which is sub-additive and hence not a
probability distribution. That is, M(y) = %M(y) for y  Y but M(B)  %M(B)
for B G Y.
Q. E. D.
Corollary 1.3.1 shows that when Y is countable, a sufficient statistic for
M[P(y)] is the vector [%M(y), y  Y]. We see immediately that M[P(y)]
shrinks as [%M(y), y  Y] increases. Moreover, we can measure the size of
M[P(y)]. As observed in the proof to part (c), the empirical evidence leaves

16

1. Missing Outcomes

indeterminate the allocation of (1  SM) of probability mass among the
atoms of Y, where SM  y  Y%M(y). Hence, the size of M[P(y)] as
measured by the sup norm is

Msup =

sup

sup

(, 1)  M × M

BGY

(B)  1(B) = 1  SM.

(1.18)

For given values of [P(y zm = 1), m  M], SM rises as the vector [P(zm = 1),
m  M] increases. Some insight into the role of [P(y zm = 1), m  M] in the
determination of SM may be obtained from the inequality
SM  y  Y max m  M P(y = y zm = 1)P(zm = 1)

 max m  M [y  Y P(y = y zm = 1)]P(zm = 1)
= max m  M P(zm = 1).

(1.19)

The lower bound on SM in (1.19) is attained if the distributions P(y zm = 1),
m  M are identical to one another; then M[P(y)] = m*[P(y)], where m*
 argmax m  M P(zm = 1). Thus, combining multiple sampling processes is
least informative when all sampling processes generate the same observed
distribution of y.
The event SM > 1 cannot occur if all sampling processes draw realizations at random from population J. If SM > 1, a measure  that satisfies part
(b) has (Y) > 1 and so is not a probability measure. Hence M[P(y)] is
empty. If one finds that SM > 1, one should conclude that some sampling
process does not draw realizations at random from J.

Parameters that Respect Stochastic Dominance
When Y is a countable subset of the real line, Corollary 1.3.1 implies
simple characterizations of the identification regions for parameters that
respect stochastic dominance. Corollary 1.3.2 gives the result. Part (a)
determines the endpoints of the identification region for any D-parameter.
Part (b) focuses on the important special case of the expectation parameter
and shows that its identification region is the closed interval connecting the
endpoints determined in part (a).
Corollary 1.3.2: Let M[P(y)] be given by (1.15), with M finite. Let Y be
a countable subset of R, and let Y contain its lower and upper bounds y0 
inf y  Y and y1  sup y  Y. Let 0 and 1 be probability distributions on Y
such that, for each y  Y,

1.5. Interval Measurement of Outcomes

17

0(y) = %M(y) if y > y0 and 0(y0) = %M(y0) + (1  SM),

(1.20a)

1(y) = %M(y) if y < y1 and 1(y1) = %M(y1) + (1  SM).

(1.20b)

(a) Let D(#) respect stochastic dominance. Then the smallest and largest
elements of M{D[P(y)]} are D(0) and D(1).
(b) The closed interval

M[E(y)] = [y  Y y%M(y) + (1SM)y0, y  Y y%M(y) + (1SM)y1] (1.21)
is the identification region for E(y).

a

Proof:
(a) Corollary 1.3.1 showed that   M[P(y)] if and only if (y)  %M(y),
~ y  Y. By construction, 0 and 1 are members of M[P(y)]. Indeed, 0 is
stochastically dominated by all members of M[P(y)], and 1 stochastically
dominates all members of M[P(y)]. Hence, the smallest and largest
elements of M{D[P(y)]} are D(0) and D(1).
(b) The expectation parameter respects stochastic dominance. Hence,
part (a) shows that the smallest and largest elements of M[E(y)] are ,ydP0
and ,ydP1, which equal the endpoints of the interval on the right side of
(1.21). For any  [0, 1], the mixture 0 + (1  )1 belongs to M[P(y)].
Hence M[E(y)] is the entire interval on the right side of (1.21).
Q. E. D.

1.5. Interval Measurement of Outcomes
The phenomenon of missing outcomes juxtaposes extreme observational
states: each realization of y is either observed completely or not observed
at all. Empirical researchers sometimes encounter intermediate observational states, in which realizations of y are observed to lie in proper but nonunitary subsets of the outcome space Y. A particularly common intermediate observational state is interval measurement of real-valued outcomes.
To formalize interval measurement, let Y G R. Let each j  J have a triple
(yj, yj, yj+)  Y3. Let the random variable (y, y, y+): J  Y3 have a
distribution P(y, y, y+) such that
P(y  y  y+) = 1.

(1.22)

Let a sampling process draw persons at random from J. Then we have

18

1. Missing Outcomes

interval measurement of outcomes if realizations of (y, y+) are observable
but realizations of y are not directly observable. (I say that y is not “directly” observable to cover the possibility that y = y+, in which case
observation of (y, y+) implies observation of y.)
Sampling with missing outcomes is a special case of interval measurement. Let y0  inf y  Y and y1  sup y  Y. A realization of y is effectively
observed when (y = y+) and missing when (y = y0, y+ = y1). Hence,
sampling with missing outcomes is the special case of (1.22) in which
P(y = y+) + P(y = y0, y+ = y1) = 1.

(1.23)

Interval measurement of outcomes yields very simple lower and upper
bounds on any parameter that respects stochastic dominance. The distribution P(y+) is a feasible value of P(y) and stochastically dominates all other
feasible values of P(y); Hence D[P(y+)] is the largest feasible value of
D[P(y)]. The distribution P(y) is a feasible value of P(y) and is
stochastically dominated by all other feasible values of P(y); Hence
D[P(y)] is the smallest feasible value of D[P(y)]. Thus, we have Proposition 1.4.
Proposition 1.4: Let Y G R, let (y, y+) be observable, and let (1.22) hold.
Let D respect stochastic dominance. Given the empirical evidence alone, the
smallest and largest points in the identification region for D[P(y)] are
a
D[P(y-)] and D[P(y+)].

Complement 1A. Employment Probabilities
This complement presents an empirical example illustrating Corollary 1.1.1.
Horowitz and Manski (1998) used data from the National Longitudinal
Survey of Youth (NLSY) to estimate the probability that a member of the
surveyed population is employed in 1991. The surveyed population consists
of persons born between January 1, 1957 and December 31, 1964 who
resided in the United States in 1979. From 1979 on, the NLSY has
periodically sought to interview a random sample of this population and
supplemental samples of some sub-populations (Center for Human
Resource Research, 1992). The random-sample data are used here.
In this illustration, the outcome y indicates an individual’s employment
status at the time of the 1991 interview. In the 1979 base year, the NLSY
sought to interview a random sample of 6812 individuals and succeeded in
obtaining interviews from 6111 of the sample members. Data on employment status in 1991 are available for 5556 of the 6111 individuals inter-

1A. Employment Probabilities

19

viewed in the base year. The remaining 555 are nonrespondents, some
because they declined to be interviewed in 1991 and some because they did
not answer the employment-status question in their 1991 interviews. Table
1.1 presents these response statistics and the frequencies with which
different outcome values are reported.
Table 1.1: 1991 Employment Status of NLSY Respondents
Employment Status

Number of Respondents

Employed (y = 2)
Unemployed (y = 1)
Out of Labor Force (y = 0)
Ever-interviewed Nonrespondents
Never-interviewed Nonrespondents

4332
297
927
555
701

Total

6812

The discussion below first uses these frequencies to generate empirical
probabilities of events and then interprets these empirical probabilities as
finite-sample estimates of population quantities.
The empirical nonresponse rate, which takes account of sample members
who were never interviewed, is P(z = 0) = 1256/6812 = 0.184. Researchers
computing nonresponse rates to questions in the later years of longitudinal
surveys often condition on the event that a sample member was interviewed
in the base year. Let this event, which is always observed, be denoted
BASE. Then, the “ever-interviewed” nonresponse rate for employment
status in 1991 is P(z = 0*BASE) = 555/6111 = 0.091.
The empirical probability of employment for the 5556 individuals who
responded to the 1991 employment-status question is P(y = 2* z = 1) =
4332/5556 = 0.780. The probability of employment among nonrespondents
can take any value in the interval [0, 1]. Hence, Corollary 1.1.1 yields the
following identification regions for the population and the ever-interviewed
empirical employment probabilities P(y = 2) and P(y = 2*BASE):
P(y = 2)  [(0.780)(0.816), (0.780)(0.816) + (0.184)] = [0.636, 0.820],
P(y = 2*BASE)  [(0.780)(0.909), (0.780)(0.909) + (0.091)]
= [0.709, 0.800].

20

1. Missing Outcomes

Sampling Variation
The focus of this book is identification, but empirical research must also be
concerned with sampling variation. Thus, let us now consider the empirical
probabilities analyzed above to be random sample estimates of corresponding population quantities. Then the effects of sampling variation may be
characterized by confidence intervals on the identification regions obtained
above.
Horowitz and Manski (1998) presented Bonferroni intervals based on
local asymptotic theory.3 Consider P(y = 2). The identification region (1.10)
may be rewritten as

[P(y = 2)] = [P(y = 2, z = 1), 1  P(y =/ 2, z = 1)].
The asymptotic standard errors of the sample estimates of the lower and
upper bounds of this interval are
CL = {P(y = 2, z = 1)[1  P(y = 2, z = 1)]/N}1/2,
CU = {P(y =/ 2, z = 1)[1  P(y =/ 2, z = 1)]/N}1/2,
where N = 6812 is the sample size. A Bonferroni asymptotic joint
confidence region with level at least 95 percent is obtained by forming the
intersection of individual 97.5 percent regions. These regions are the point
estimates of the lower and upper bounds ± (2.24)CL and ± (2.24)CU
respectively.
Substituting sample frequencies for population probabilities yields
P(y = 2, z = 1) = 4332/6812 = 0.636,
P(y =/ 2, z = 1) = 1224/6812 = 0.180,
CL = [(0.636)(1  0.636)(1/6812)]1/2 = 0.0058,
CU = [(0.180)(1  0.180)(1/6812)]1/2 = 0.0047,
so the estimated asymptotic joint Bonferroni 95 percent intervals are
0.623  lower bound on P(y = 2)  0.649,
0.810  upper bound on P(y = 2)  0.831.
Analogous computations conditioning on the event BASE yield
0.696  lower bound on P(y = 2*BASE)  0.722,
0.788  upper bound on P(y = 2*BASE)  0.811.

1B. Blind-Men Bounds on an Elephant

21

These confidence intervals are much narrower than the widths of the
identification regions. Thus, identification is the dominant problem in
inference on P(y = 2) and P(y = 2*BASE) from the NLSY data; sampling
variation is a second-order concern. This conclusion holds except when the
sample size is quite small or the response rate is very close to one.

Complement 1B. Blind-Men Bounds on an Elephant
The ancient Indian fable The Blind Men and the Elephant exemplifies the
problem of combining empirical evidence from multiple sampling
processes, each of which partially identifies a population distribution of
interest. Modern renditions of the fable agree on the inferential problem but
differ on its resolution.
The nineteenth-century American poem of John Godfrey Saxe,
reproduced here, concludes pessimistically; the six disputatious blind men
fail to recognize that each has observed a different feature of the elephant.
In other renditions, the blind men learn to combine their observations for
their common benefit. For example, in a version of the fable prepared for
classroom use, the blind men are interrupted by a listening Rajah who
counsels as follows: “The elephant is a very large animal. Each man
touched only one part. Perhaps if you put the parts together, you will see
the truth.” 4
The Rajah may have been optimistic in suggesting that six partial
observations of an elephant may reveal the full truth about the creature, but
he was sensible to counsel that six partial observations are more informative
than one. All too often, researchers act like the disputatious blind men of the
Saxe poem, each failing to recognize that he or she has observed a different
feature of the same population. They should instead combine their
observations, as counseled by the Rajah.
“The Blind Men and the Elephant”, by
John Godfrey Saxe (1816–1887)
It was six men of Indostan
To learning much inclined,
Who went to see the Elephant
(Though all of them were blind),
That each by observation
Might satisfy his mind

22

1. Missing Outcomes
The First approached the Elephant,
And happening to fall
Against his broad and sturdy side,
At once began to bawl:
“God bless me! but the Elephant
Is very like a wall!”
The Second, feeling of the tusk,
Cried, “Ho! what have we here
So very round and smooth and sharp?
To me ’tis mighty clear
This wonder of an Elephant
Is very like a spear!”
The Third approached the animal,
And happening to take
The squirming trunk within his hands,
Thus boldly up and spake:
“I see,” quoth he, “the Elephant
Is very like a snake!”
The Fourth reached out an eager hand,
And felt about the knee.
“What most this wondrous beast is like
Is mighty plain,” quoth he;
“ ‘Tis clear enough the Elephant
Is very like a tree!”
The Fifth, who chanced to touch the ear,
Said: “E’en the blindest man
Can tell what this resembles most;
Deny the fact who can
This marvel of an Elephant
Is very like a fan!?
The Sixth no sooner had begun
About the beast to grope,
Than, seizing on the swinging tail
That fell within his scope,
“I see,” quoth he, “the Elephant
Is very like a rope!”
And so these men of Indostan
Disputed loud and long,
Each in his own opinion
Exceeding stiff and strong,
Though each was partly in the right,
And all were in the wrong!

Endnotes

23

Endnotes
Sources and Historical Notes
The basic ideas in Sections 1.1 and 1.2 were first presented in Manski
(1989) and were developed more fully in Manski (1994). Corollary 1.2.1
was proved directly, without reference to D-parameters, in Manski (1994,
Proposition 2). Proposition 1.3 and its corollaries re-interpret and extend
results proved in Manski (2003). Proposition 1.4 is based on Manski and
Tamer (2002, Proposition 1).
The class of parameters that respect stochastic dominance was introduced in Horowitz and Manski (1995) and studied further in Manski
(1997a). Many partial identification results for means of random variables
extend easily to this class of parameters, as shown throughout this book.
As described in Manski (1989, 1995), my study of inference with
missing outcome data grew out of a specific inquiry by Irving Piliavin in the
spring of 1987. Piliavin and his colleague Michael Sosin had interviewed
a sample of 137 individuals who were homeless in Minneapolis in late
December 1985. Six months later, they attempted to re-interview these
respondents to measure an outcome of interest but succeeded in locating
only 78. Piliavin told me that he felt it implausible to assume that
nonresponse to the second survey was random. Nor was he comfortable
making other assumptions about the nonresponse process. He asked whether
it was possible to draw inferences without imposing such assumptions.
Fifty years ago, in a study of the statistical problems of the Kinsey report
on sexual behavior, Cochran, Mosteller, and Tukey (1954, pp. 274–282)
used essentially Corollary 1.1.1 to express the possible effects of
nonresponse to the Kinsey survey. However, the subsequent literature on
missing data in surveys did not pursue the idea, preferring instead to impose
distributional assumptions that yield point identification (e.g., Little and
Rubin, 1987). I learned of the Cochran, Mosteller, and Tukey work in the
early 1990s, and wondered why the authors did not pursue the idea of
inference using the empirical evidence alone. I found that Cochran (1977)
had subsequently dismissed such inference as uninformative in practice.
Using the symbol W2 to denote the probability of missing data, he wrote (p.
362): “The limits are distressingly wide unless W2 is very small.” Cochran
did not discuss what a researcher should do in the absence of credible
assumptions that shrink these bounds.

24

1. Missing Outcomes

Text Notes
1. Point estimates obtained by imputation of missing values are nonrefutable using the empirical evidence alone. Imputation methods assign to
each person with a missing realization of y some logically possible value,
say y*. This done, E[g(y)] is estimated by the sample average
1
N
N = )  g(yi)zi + g(y*i)(1 zi).
N i=1
By the Strong Law of Large Numbers, N almost surely converges to

  E[g(y)*z = 1]$P(z = 1) + E[g(y*)*z = 0]$P(z = 0).
This  necessarily lies in {E[g(y)]} but does not necessarily equal E[g(y)].
The latter holds if and only if E[g(y*)*z = 0] = E[g(y)*z = 0].
2. Although there has been much research on inference combining multiple
sampling processes, the problem of partial identification examined here has
not previously been addressed. The statistical literature on meta-analysis
has supposed that each sampling process independently point-identifies the
distribution of interest; that said, the concern has been to combine the
available data sources in a statistically efficient manner. Econometric
research on sample augmentation has considered situations in which each
sampling process incompletely identifies the distribution of interest, but
combining multiple sampling processes with suitable assumptions achieves
point identification; see, for example, Hsieh, Manski, and McFadden (1985)
and Hirano, Imbens, Ridder, and Rubin (2001).
3. The problem of obtaining a joint confidence interval for a pair of lower
and upper bounds is examined more fully in Horowitz and Manski (2000).
There we consider construction of a confidence interval with known
(asymptotic) probability of containing both the lower and the upper bound
on a partially identified population parameter. We focus on intervals of the
form [LN  zN, UN + zN], where N is sample size and LN and UN are
estimates of the lower and upper bounds L and U on the parameter of
interest. The number zN is chosen so that P(LN  zN  L, U  UN + zN) =
1   asymptotically. One way of obtaining zN is to derive it from an
analytic expression for the asymptotic distribution of (LN, UN). Another is
to use the bootstrap.

Endnotes

25

4. A World Wide Web search on the phrase “The Blind Men and the
Elephant” yields many versions of the fable. One with the Rajah’s advice
is at the URL www.peacecorps.gov/wws/guides/looking/story22.html.

2
Instrumental Variables
2.1. Distributional Assumptions and Credible Inference
Distributional assumptions may enable one to shrink identification regions
obtained using empirical evidence alone. When facing the problem of
missing outcome data, researchers have generally imposed distributional
assumptions that point-identify the outcome distribution P(y). When a
single random sampling process generates the available data, it has been
particularly common to assert that observed and missing outcomes have the
same distribution; that is,
P(y) = P(y z = 0) = P(y z = 1).

(2.1)

The distribution P(y z = 1) is revealed by the sampling process, so P(y) is
point-identified. Someone asserting (2.1) cannot be proved wrong; after all,
the empirical evidence reveals nothing about P(y z = 0).
An assumption may be non-refutable and yet not credible. Researchers
who assert (2.1) almost inevitably find this assumption difficult to justify.
Analysts who assert other point-identifying assumptions regularly encounter
the same difficulty. This should not be surprising. The empirical evidence
reveals nothing at all about the distribution of missing data. An assumption
must be quite strong to pick out one among all possible distributions.
There is a fundamental tension between the credibility and strength of
conclusions, which I have called the Law of Decreasing Credibility.
Inference using the empirical evidence alone sacrifices strength of
conclusions in order to maximize credibility. Inference invoking pointidentifying distributional assumptions sacrifices credibility in order to
26

2.2. Some Assumptions Using Instrumental Variables

27

achieve strong conclusions. Between these poles, there is a vast middle
ground of possible modes of inference asserting assumptions that may
shrink the identification region [P(y)] but not reduce it to a point.
This chapter examines the identifying power of various distributional
assumptions that make use of instrumental variables. Some such assumptions imply point identification, whereas others have less identifying power
and, perhaps, greater credibility. For simplicity, the analysis below
presumes that a single random sampling process generates the available
data, that realizations of y are either completely observed or entirely
missing, and that all realizations of the instrumental variable are observed.
Distributional assumptions using instrumental variables may also be applied
when data are available from multiple sampling processes, when interval
measures of outcomes are observed, and when some realizations of the
instrumental variable are missing.

2.2. Some Assumptions Using Instrumental Variables
As in Chapter 1, suppose that a sampling process draws persons at random
from population J and that the outcome y is observable if z = 1. Moreover,
suppose now that each person j is characterized by a covariate vj in a space
V. Let v: J  V be the random variable mapping persons into covariates and
let P(y, z, v) denote the joint distribution of (y, z, v). Suppose that all
realizations of v are observable. Observability of v provides an instrument
or tool that may help to identify the outcome distribution P(y). Thus v is
said to be an instrumental variable.
The sampling process asymptotically reveals the distributions P(z),
P(y, v z = 1), and P(v z = 0). It is uninformative about the conditional
distributions [P(y v = v, z = 0), v  V]. The presence of an instrumental
variable does not, per se, help to identify P(y). However, observability of
v may be useful when combined with distributional assumptions. This
chapter examines the identifying power of six such assumptions.
Sections 2.3 and 2.4 study identification of P(y) under assumptions that
assert forms of statistical independence among the random variables
(y, z, v). Section 2.3 assumes that observed and missing outcomes have the
same distribution conditional on v; that is, outcomes are missing-at-random
conditional on v.
Outcomes Missing-at-Random (Assumption MAR):
P(y v) = P(y v, z = 0) = P(y v, z = 1).

(2.2)

28

2. Instrumental Variables

Section 2.4 assumes that y is statistically independent of v; that is:
Statistical Independence of Outcomes and Instruments (Assumption SI):
P(y v) = P(y).

(2.3)

Section 2.5 studies identification of the expectation E[g(y)] of a real-valued
function g(y) under assumptions that are weaker than Assumptions MAR
and SI. First the forms of statistical independence asserted in (2.2) and
(2.3) are weakened to the mean-independence assumptions
Means Missing-at-Random (Assumption MMAR):
E[g(y) v] = E[g(y) v, z = 0] = E[g(y) v, z = 1]

(2.4)

and
Mean Independence of Outcomes and Instruments (Assumption MI):
E[g(y) v] = E[g(y)],

(2.5)

respectively. Then, Assumptions MMAR and MI are weakened to the
monotonicity assumptions
Mean Missing Monotonically (Assumption MMM):
E[g(y) v, z = 1]  E[g(y) v]  E[g(y) v, z = 0]

(2.6)

and
Mean Monotonicity of Outcomes and Instruments (Assumption MM): Let
V be an ordered set.
E[g(y) v = v]  E[g(y) v = v1], ~ (v, v1)  V × V such that v  v1. (2.7)
Taken together, these six distributional assumptions provide a variety of
ways in which a researcher can use instrumental variables to help identify
outcome distributions when some outcome data are missing. Researchers
contemplating the use of instrumental variables should, of course, pay due
attention to the credibility of these and other assumptions. Empirical
researchers often ask whether some observable covariate is or is not a “valid
instrument” in an application of interest. The expression “valid instrument”

2.3. Outcomes Missing-at-Random

29

is imprecise because it focuses attention on the covariate used in the role of
v. Credibility depends not on the covariate per se but on the assumption that
the distribution P(y, z, v) is assumed to satisfy.
To simplify the presentation, the analysis below supposes that the
covariate space V is finite and that P(v = v, z = 1) > 0 for all v  V. These
regularity conditions are maintained without further reference.

2.3. Outcomes Missing-at-Random
Assumption MAR is a non-refutable hypothesis that point-identifies P(y).
Proposition 2.1 shows how.1
Proposition 2.1: Let assumption MAR hold. Then P(y) is point-identified
with
P(y) = v  V P(y v = v, z = 1)P(v = v).
Assumption MAR is non-refutable.

(2.8)

a

Proof: The Law of Total Probability gives
P(y) = v  V P(y v = v)P(v = v).

(2.9)

Assumption MAR states that
P(y v) = P(y v, z = 1).

(2.10)

Applying (2.10) to (2.9) yields (2.8). The right side of (2.8) is pointidentified by the sampling process, so P(y) is point-identified. Assumption
MAR is non-refutable because the empirical evidence reveals nothing about
P(y v, z = 0).
Q. E. D.
A researcher applying assumption MAR must specify the instrumental
variable v for which the assumption holds. Assumption (2.1) is the special
case in which v has a degenerate distribution. As in that case, the credibility
of assumption MAR is regularly a matter of controversy.2

30

2. Instrumental Variables

2.4. Statistical Independence
Assumption SI has the same identifying power as does observation of data
from multiple sampling processes. The space V of values for the instrumental variable plays the same role here as did the set M of sampling processes
in Section 1.4. Proposition 2.2 gives the basic result, and two corollaries
flesh it out.
Proposition 2.2: (a) Let assumption SI hold. Then the identification region
for P(y) is

SI[P(y)] =
B {P(y v = v, z = 1)P(z = 1 v = v) +

#P(z = 0 v = v),

v

v



}.

Y

vV

(2.11)
(b) Let the set SI[P(y)] be empty. Then assumption SI does not hold.

a

Proof: (a) Application of equation (1.2) to each conditional distribution
P(y v = v), v  V gives the identification region for this distribution using
the empirical evidence alone; that is,

[P(y v = v)] =
[P(y v = v, z = 1)P(z = 1 v = v) +

#P(z = 0 v = v),

v

v



].

Y

(2.12)
Moreover, the identification region for the set of distributions [P(y v = v),
v  V] is the Cartesian product × v  V [P(y v = v)].
Assumption SI states that the distributions P(y v = v), v  V coincide, all
being equal to P(y). Hence P(y) must lie in  v  V [P(y v = v)]. Any
distribution in this intersection is feasible, so SI[P(y)] is the identification
region.
(b) If assumption SI holds, the set SI[P(y)] is necessarily non-empty.
Hence, the assumption cannot hold if SI[P(y)] is empty.
Q. E. D.
Part (a) of the proposition shows that the identifying power of assumption SI can range from point identification of P(y) to no power at all,

2.4. Statistical Independence

31

depending on the nature of the instrumental variable. Point identification
occurs if there exists a v  V such that P(z = 1 v = v); then one of the sets
whose intersection is taken in (2.11) is a singleton. When Y is countable,
Corollary 2.2.1 below gives a simple necessary and sufficient condition for
point identification.
Assumption SI has no identifying power if (a) z is statistically independent of v and (b) y is statistically independent of v conditional on the event
{z = 1}; that is, if P(z v) = P(z) and P(y v, z = 1) = P(y z = 1). Then
[P(y v = v)], v  V are all the same as the identification region obtained
using the empirical evidence alone. This shows that identification cannot be
achieved by construction of a trivial instrumental variable that uses a
randomization device to assign a covariate value to each member of the
population. A covariate v generated by a randomization device is necessarily statistically independent of the pair (y, z). Such a covariate satisfies
assumption SI but has no identifying power.
Part (b) of the proposition shows that assumption SI is refutable. If
SI[P(y)] is empty, the assumption logically cannot hold. Of course, nonemptiness of SI[P(y)] does not imply that the assumption is correct.
Observe that the identification region SI[P(y)] has the same structure as
the region M[P(y)] obtained by combining data from multiple sampling
processes (see Proposition 1.3), with V here playing the role of M there.
Hence, there are instrumental-variable analogs to Corollaries 1.3.1 and
1.3.2. These are given in Corollaries 2.2.1 and 2.2.2 below. The proofs are
analogous to those of the earlier corollaries and so are omitted.
Corollary 2.2.1: Let assumption SI hold. Let  

. For B G Y, define

Y

%V(B)  max v  V P(y  B v = v, z = 1)P(z = 1 v = v).

(2.13)

(a) Then   SI[P(y)] if and only if (B)  %V(B), ~ B G Y.
(b) Let Y be countable. Then   SI[P(y)] if and only if (y)  %V(y), ~
y  Y.
(c) Let Y be countable. Let SV  y  Y %V(y). Then SI[P(y)] contains
multiple distributions if SV < 1 and a unique distribution if SV = 1. If SV =
1, the unique feasible distribution is V(y)  %V(y), y  Y. If SV > 1, then
a
assumption SI does not hold.
Corollary 2.2.2: Let assumption SI hold. Let Y be a countable subset of R,
and let Y contain its lower and upper bounds y0  inf y  Y and y1  sup y  Y.
Let 0 and 1 be probability distributions on Y such that, for each y  Y,

32

2. Instrumental Variables

0(y) = %V(y) if y > y0 and 0(y0) = %V(y0) + (1  SV)

(2.14a)

1(y) = %V(y) if y < y1 and 1(y1) = %V(y1) + (1  SV).

(2.14b)

(a) Let D(#) respect stochastic dominance. Then the smallest and largest
elements of SI{D[P(y)]} are D(0) and D(1).
(b) The closed interval

SI[E(y)] = [ y  Y y%V(y) + (1  SV)y0,  y  Y y%V(y) + (1  SV)y1]
(2.15)
is the identification region for E(y).

a

2.5. Mean Independence and Mean Monotonicity
This section studies identification of the expectations of real-valued
functions of the outcome. The distributional assumptions considered here
are weaker than Assumptions MAR and SI. Throughout this section, g(#) is
a real-valued function that attains its lower and upper bounds.

Mean Independence
Assumptions MMAR and MI weaken the forms of statistical independence
asserted in assumptions MAR and SI to corresponding forms of mean
independence. Assumption MMAR is a non-refutable hypothesis that pointidentifies E[g(y)]. Assumption MI is a refutable hypothesis that generically
shrinks the identification region obtained using the empirical evidence
alone, but point-identifies E[g(y)] only in special cases. Propositions 2.3 and
2.4 give the results.
Proposition 2.3: Let assumption MMAR hold. Then E[g(y)] is pointidentified with
E[g(y)] = v  V E[g(y) v = v, z = 1]P(v = v).
Assumption MMAR is non-refutable.
Proof: The Law of Iterated Expectations gives

(2.16)

a

2.5. Mean Independence and Mean Monotonicity
E[g(y)] =  v  V E[g(y) v = v]P(v = v).

33
(2.17)

Assumption MMAR states that
E[g(y) v] = E[g(y) v, z = 1].

(2.18)

Applying (2.18) to (2.17) yields (2.16). The right side of (2.16) is pointidentified by the sampling process, so E[g(y)] is point-identified. Assumption MMAR is non-refutable because the empirical evidence reveals
nothing about E[g(y) v, z = 0].
Q. E. D.
Proposition 2.4: (a) Let assumption MI hold. Then the closed interval

MI{E[g(y)]} =

[max v  V E[g(y)z + g0(1  z) v = v], min v  V E[g(y)z + g1(1  z) v = v]].
(2.19)
is the identification region for E[g(y)].
(b) Let MI{E[g(y)]} be empty. Then assumption MI does not hold.

a

Proof: (a) Application of equation (1.91) to each conditional expectation
E[g(y) v = v], v  V gives its identification region using the empirical
evidence alone; that is, the closed interval

{E[g(y) v = v]} = [E[g(y)z + g0(1  z) v = v], E[g(y)z + g1(1  z) v = v]].
(2.20)
Moreover, the identification region for {E[g(y) v = v], v  V} is the V dimensional rectangle × v  V {E[g(y) v = v]}.
Assumption MI states that the expectations E[g(y) v = v], vV coincide,
all being equal to E[g(y)]. Hence E[g(y)] must lie in  v  V {E[g(y) v=v]}.
Any value in this set is feasible, so MI{E[g(y)]} is the identification region.
(b) If assumption MI holds, the set MI{E[g(y)]} is necessarily non-empty.
Hence, the assumption cannot hold if MI{E[g(y)]} is empty.
Q. E. D.

34

2. Instrumental Variables

As with assumption SI, the identifying power of assumption MI can
range from point identification to no power at all, depending on the nature
of the instrumental variable. Point identification of E[g(y)] occurs if there
exists a v  V such that P(z = 1 v = v); then E[g(y)] = E[g(y) v = v]. There
is no identifying power if the pair (y, z) is statistically independent of v.
Then MI{E[g(y)]} = {E[g(y)]}.

Mean Monotonicity
Although mean independence is a weaker property than statistical independence, empirical researchers often find that assertions of mean independence
are still too strong to be credible. There is therefore reason to ask whether
Assumptions MMAR and MI may be weakened in ways that enhance
credibility while preserving some identifying power. A simple way to do
this is to change the equalities in equations (2.4) and (2.5) to the weak
inequalities in equations (2.6) and (2.7).
Weakening assumption MMAR in this way yields assumption MMM,
which asserts that, for each realization of v, the mean value of g(y) when y
is observed is greater than or equal to the mean value of g(y) when y is
missing. (The direction of the inequality can be reversed by applying the
assumption to the function g(y).) Weakening assumption MI in this way
yields assumption MM, which presumes that the set V has been preordered. Propositions 2.5 and 2.6 characterize the identifying power of
these monotonicity assumptions.
Proposition 2.5: Let assumption MMM hold. Then the identification region
for E[g(y)] is the closed interval

MMM{E[g(y)]} =

[E[g(y) z = 1]P(z = 1) + g0P(z = 0), v  V E[g(y) v = v, z = 1]P(v = v)].
(2.21)
Assumption MMM is non-refutable.

a

Proof: Let v  V. Under assumption MMM, the identification region for
E[g(y) v = v, z = 0] is the closed interval

MMM{E[g(y) v = v, z = 0]} = [g0, E[g(y) v = v, z = 1]].

(2.22)

Moreover, the joint identification region for {E[g(y) v = v, z = 0], v  V}

2.5. Mean Independence and Mean Monotonicity

35

is the V -dimensional rectangle × v  V MMM{E[g(y) v = v, z = 0]}. The
Law of Iterated Expectations gives
E[g(y)] =  v  V E[g(y) v = v, z = 1]P(v = v, z = 1)
+ E[g(y) v = v, z = 0]P(v = v, z = 0).

(2.23)

Applying (2.22) to (2.23) yields (2.21). Assumption MMM is non-refutable
because the empirical evidence reveals nothing about {E[g(y) v = v, z = 0],
v  V}.
Q. E. D.
Proposition 2.6: (a) Let V be an ordered set. Let assumption MM hold.
Then the identification region for E[g(y)] is the closed interval

MM{E[g(y)]} = [  P(v = v){max E[g(y)z + g0(1  z) v = v1]},
vV

v1  v

 P(v = v) {min E[g(y)z + g1(1  z) v = v1]}].
vV

v1  v

(2.24)
(b) Let MM{E[g(y)]} be empty. Then assumption MM does not hold. a
Proof: (a) The proof to Proposition 2.4 showed that, using the empirical
evidence alone, the identification region for the expectations {E[g(y) v = v],
v  V} is the V -dimensional rectangle × v  V {E[g(y) v = v]}. Under
assumption MM, a point d  RV belongs to the identification region for
{E[g(y) v = v], v  V} if and only if d is an element of this rectangle whose
components (d1, d2, . . . , d V ) form a weakly increasing sequence. Applying
this to the Law of Iterated Expectations (2.17) yields (2.24).
(b) If assumption MM holds, the set MM{E[g(y)]} is necessarily nonempty. Hence, the assumption cannot hold if MM{E[g(y)]} is empty.
Q. E. D.
Proposition 2.5 shows that, under assumption MMM, the identification
region for E[g(y)] is a right-truncated subset of the region obtained using
the empirical evidence alone. The smallest feasible value of E[g(y)] is the
same as when using the empirical evidence alone. The largest is the value
that E[g(y)] would take under assumption MMAR.

36

2. Instrumental Variables

Proposition 2.6 shows that the identification region under assumption
MM is a subset of the region obtained using the empirical evidence alone
and a superset of the one obtained under assumption MI. The identifying
power of assumption MM depends on how the regions {{E[g(y) v = v]},
vV} vary with v. The extreme possibilities occur if this sequence of
intervals shifts to the left or right as v increases. In the former case, the
identification region under assumption MM is the same as under assumption
MI. In the latter case, assumption MM has no identifying power.

2.6. Other Assumptions Using Instrumental Variables
This chapter has examined assumptions that help to identify outcome
distributions when an instrumental variable is observed. The tension
between the credibility and strength of conclusions is especially evident as
one weakens assumption SI to assumption MI and then to assumption MM.
Each successive assumption is more plausible but has less identifying
power.
It is easy to think of other assumptions that make different tradeoffs
between credibility and identifying power. For example, assumption MI
could be weakened not to the monotonicity asserted in assumption MM but
rather to some form of “approximate” mean independence. A way to
formalize this is to assert that, for all pairs (v, v1)  V × V,

E[g(y) v = v1]  E[g(y) v = v]  C,

(2.25)

where C > 0 is a specified constant. Recall that the empirical evidence alone
restricts the vector of expectations E[g(y) v] to the V -dimensional
rectangle × v  V {E[g(y) v = v]}. Relationship (2.25) further restricts
E[g(y) v] to points in R V that satisfy specified linear inequalities.
Alternatively, assumption MI could be weakened to the zero-covariance
assumption
E[g(y)#v]  E[g(y)]E(v) = 0,

(2.26)

which may be rewritten as

 v  V P(v = v) [v  E(v)]E[g(y) v = v] = 0.

(2.27)

The empirical evidence point-identifies E(v). Hence, equation (2.27)
establishes a linear constraint among the elements of E[g(y) v].

2A. Estimation with Nonresponse Weights

37

Complement 2A. Estimation with Nonresponse Weights
Organizations conducting major surveys commonly release public data files
that provide nonresponse weights to be used for estimating means and other
parameters of outcome distributions when data are missing. Nonresponse
weights are distinct from design weights, which are used to compensate for
planned variation in sampling rates across strata of the population.
The standard construction of nonresponse weights presumes the
existence of an instrumental variable v. The standard use of such weights to
infer a population mean E[g(y)] yields a consistent estimate if assumption
MMAR holds but not otherwise. Hence, empirical researchers contemplating application of nonresponse weights need to take care.

Weighted Sample Averages
Suppose that a random sample of size N has been drawn from population
J. Let N(1) denote the sample members for whom z = 1, and let N1 be the
cardinality of N(1). Let s(v): V  [0, ) be a weighting function. Consider
estimation of E[g(y)] by the weighted sample average
1
N  ))
N1



s(vi)$g(yi).

i  N(1)

By the Strong Law of Large Numbers, lim N   N = a.s. E[s(v)$g(y)*z = 1].
The standard weights provided by survey organizations have the form
P(v = v)
s(v) = )))))))))) ,
P(v = v*z = 1)

v  V.

With such weights,
E[s(v)$g(y)* z = 1] =  v V E[s(v)$g(y)*v = v, z = 1]$P(v = v*z = 1)
=  v V E[g(y)*v = v, z = 1]$P(v = v)
=  v V E[g(y)*v = v, z = 1]$P(v = v*z = 1)$P(z = 1)
+  v V E[g(y)*v = v, z = 1]$P(v = v*z = 0)$P(z = 0)

38

2. Instrumental Variables
= E[g(y)*z = 1]$P(z = 1)
+  v V E[g(y)*v = v, z = 1]$P(v = v*z = 0)$P(z = 0).

The right side of this equation equals E[g(y)] if assumption MMAR holds,
but it generically differs from E[g(y)] otherwise.

Endnotes
Sources and Historical Notes
My work on the identifying power of assumptions using instrumental
variables began with Proposition 2.4, which was introduced in Manski
(1990) and developed more fully in Manski (1994, Proposition 6). The
monotonicity ideas in Propositions 2.5 and 2.6 are based on Manski and
Pepper (2000, Proposition 1).
The term instrumental variable is due to Reiersol (1945) who, along
with other econometricians of his time, studied the identification of linear
structural equation systems. Goldberger (1972), in a review of this
literature, dates the use of instrumental variables to identify linear structural
equations back to Wright (1928). Modern econometric research uses
instrumental variables to address this and many other identification
problems. However, the practice invariably is to assert assumptions strong
enough to yield point identification of quantities of interest.
It is revealing to consider some history within economics. Until the early
1970s, empirical researchers confronting missing outcome data essentially
always used assumption (2.1), although often without explicit discussion.
At that time, the credibility of this assumption was questioned sharply when
researchers observed that, in many economic settings, the process by which
observations on y become missing is related to the value of y; see, for
example, Gronau (1974). Econometricians subsequently developed a variety
of models of missing data that do not assert (2.1) but instead use instrumental variables and parametric restrictions on the shape of the distribution
P(y, z) to point-identify P(y); see, for example, Heckman (1976) and
Maddala (1983). These developments were initially greeted with widespread enthusiasm, but methodological studies soon showed that seemingly
minor changes in the assumptions imposed could generate large changes in
the implied value of P(y); see, for example, Arabmazar and Schmidt (1982),
Goldberger (1983), and Hurd (1979).

Endnotes

39

Text Notes
1. Proposition 2.1 has long been well-known, so much so that it is unclear
when the idea originated. In the survey sampling literature, this proposition
provides the basis for construction of sampling weights that aim to enable
population inference in the presence of missing data; see Complement 2A.
Rubin (1976) introduced the term missing at random. In applied
econometric research, assumption (2.1) is sometimes called selection on
observables; see Fitzgerald, Gottschalk, and Moffitt (1998, Section IIIA)
for discussion of the history of the concept and term.
2. Empirical researchers sometimes assert that assumption MAR becomes
more credible as the instrumental variable partitions the population into
more refined sub-populations. That is, if v1 and v2 are alternative
specifications of the instrumental variable, with P(v1 v2) degenerate,
researchers may assert that v2 is a more credible instrumental variable than
is v1. Unfortunately, this assertion typically is backed up by nothing more
than the empty statement that v2 “controls for” more determinants of
missing data than v1. In principle, assumption MAR could hold for both,
either, or neither of v1 and v2.

3
Conditional Prediction with Missing Data
3.1. Prediction of Outcomes Conditional on Covariates
A large part of statistical practice aims to predict outcomes conditional on
covariates. Suppose that each member j of population J has an outcome yj
in a space Y and a covariate xj in a space X. Let the random variable (y, x):
J  Y × X have distribution P(y, x). In general terms, the objective is to
learn the conditional distributions P(y x = x), x X. A particular objective
may be to learn the conditional expectation E(y x = x), conditional median
M(y x = x), or another point predictor of y conditional on an event {x = x}.
This chapter studies the feasibility of prediction when a sampling
process draws persons at random from J and realizations of (y, x) may be
observable in whole, in part, or not at all. Two binary random variables
(zy, zx) now indicate observability. A realization of y is observable if zy = 1
but not if zy = 0; a realization of x is observable if zx = 1 but not if zx = 0.
The sampling process reveals distributions P(zy, zx), P(y, x zy = 1, zx = 1),
P(y zy = 1, zx = 0), and P(x zy = 0, zx = 1). The problem is to use this
empirical evidence to infer P(y x = x), x  X.
In practice, empirical researchers may face complex patterns of missing
data; some sample members may have missing outcome data, others may
have missing covariate data, and others may have jointly missing outcomes
and covariates. Nevertheless, it is instructive to study the polar cases in
which all missing data are of the same type. Section 3.2 briefly reviews the
case in which only outcome data are missing; here P(zx = 1) = 1. Section 3.3
studies inference when all sample members with missing data have jointly
missing outcomes and covariates; here P(zy = zx = 1) + P(zy = zx = 0) = 1.
Section 3.4 supposes that only covariate data are missing, so P(zy = 1) = 1.
40

3.2. Missing Outcomes

41

With these polar cases understood, Section 3.5 examines general patterns
of missing data. Throughout Sections 3.2 to 3.5, the object of interest is the
distribution P(y*x = x) evaluated at a given x  X. Section 3.6 studies joint
inference on the set of conditional distributions [P(y*x = x), x  X].
To simplify the presentation, I suppose throughout this chapter that
realizations of outcomes or covariates are either completely observed or
entirely missing. Thus, I do not consider interval measurement of (y, x) or
partial observability of covariate vectors, with realizations of x having some
components observed and others not. I also suppose that the covariate space
X is finite and that P(x = x, zx = 1) > 0 for all x  X. These regularity
conditions are maintained without further reference.

3.2. Missing Outcomes
Chapters 1 and 2 studied identification of the marginal distribution P(y)
when some realizations of y may be missing. The results obtained there
apply immediately to P(y x = x) if realizations of x are always observable.
One simply needs to redefine the population of interest to be the subpopulation of J for which {x = x}. Then equation (1.2) shows that the
identification region using the empirical evidence alone is

[P(y x = x)] =
[P(y x = x, zy = 1)P(zy = 1 x = x) + P(zy = 0 x = x),



].

Y

(3.1)
Similarly, the other findings reported in Chapters 1 and 2 continue to hold
if one conditions all distributions on the event {x = x} and replaces all
occurrences of z with zy.

3.3. Jointly Missing Outcomes and Covariates
Jointly missing outcomes and covariates is a regular occurrence in survey
research. Realizations of (y, x) may be missing in their entirety when sample
members refuse to be interviewed or cannot be contacted by survey
administrators. Joint missingness also occurs when outcomes are missing
and the objective is to learn a distribution of the form P(y*y  B), where B
G Y. If the outcome y is not observed, then the conditioning event {y  B}
necessarily is not observed.

42

3. Conditional Prediction with Missing Data

In principle, identification of P(y x = x) when (y, x) realizations are
jointly missing may be studied as an instance of the problem of missing
outcomes set out in Section 1.1. Let the outcome of interest be (y, x) rather
than y alone. Let Y × X denote the space of all probability distributions on
Y × X. Let zyx = 1 if zy = zx = 1 and zyx = 0 otherwise. Then equation (1.2)
yields the identification region for the joint distribution P(y, x), namely

[P(y, x)] = [P(y, x zyx = 1)P(zyx = 1) + P(zyx = 0),



].

Y×X

(3.2)

Now let -(#): Y × X  Y be the function that maps P(y, x) into the distribution P(y x = x). Then equation (1.5) yields

[P(y x = x)] = {-(),   [P(y, x)]}.

(3.3)

Similarly, equation (1.6) may be applied if distributional assumptions have
been imposed. Thus, all of the results obtained in Chapters 1 and 2 may, in
principle, be used to study identification of P(y x = x) when (y, x) realizations are jointly missing.

Identification Using the Empirical Evidence Alone
Although equations (3.2) and (3.3) describe the identification region
[P(y x = x)] in principle, they do not provide a transparent description.
Proposition 3.1 shows directly that the region has a simple structure.
Proposition 3.1: Let P(zy = zx = 1) + P(zy = zx = 0) = 1. Then

[P(y*x = x)] = {P(y*x = x, zyx = 1)r(x) + [1r(x)],



},

Y

(3.4a)

where
P(x = x*zyx = 1)P(zyx = 1)
r(x)  )))))))))))))))))))))))))) .
P(x = x*zyx = 1)P(zyx = 1) + P(zyx = 0)

(3.4b)

a
Proof: The Law of Total Probability gives
P(y*x = x) =
P(y*x = x, zyx = 1)P(zyx = 1 x = x) + P(y*x = x, zyx = 0)P(zyx = 0 x = x).
(3.5)
For i = 0 or 1, Bayes Theorem gives

3.3. Jointly Missing Outcomes and Covariates
P(zyx = i*x = x) =

43

P(x = x*zyx = i)P(zyx = i)
))))))))))))))))))))))))))))))))))))) .
P(x = x*zyx = 1)P(zyx = 1) + P(x = x*zyx = 0)P(zyx = 0)
(3.6)

Inserting (3.6) into (3.5) yields
P(y*x = x) =
P(x = x*zyx = 1)P(zyx = 1)
P(y*x = x, zyx = 1) )))))))))))))))))))))))))))))))))))))
P(x = x*zyx = 1)P(zyx = 1) + P(x = x*zyx = 0)P(zyx = 0)
P(x = x*zyx = 0)P(zyx = 0)
+ P(y*x = x, zyx = 0) ))))))))))))))))))))))))))))))))))) .
P(x = x*zyx= 1)P(zyx= 1) + P(x = x*zyx= 0)P(zyx= 0)
(3.7)
Consider the right side of equation (3.7). The sampling process identifies
P(zyx), P(x = x*zyx = 1), and P(y*x = x, zyx = 1). It is uninformative about
P(x = x*zyx = 0) and P(y*x = x, zyx = 0). Hence, the identification region for
P(y*x = x) is

[P(y*x = x)] =
P(x = x*zyx = 1)P(zyx = 1)

A

{P(y*x = x, zyx = 1)

p  [0, 1]

)))))))))))))))))))))))))))

P(x = x*zyx = 1)P(zyx = 1) + pP(zyx = 0)
pP(zyx = 0)

+

))))))))))))))))))))))))))) ,
P(x = x*zyx = 1)P(zyx = 1) + pP(zyx = 0)



}.

Y

(3.8)
For each p  [0, 1], the distributions in brackets are mixtures of P(y*x = x,
zyx = 1) and arbitrary distributions on Y. The set of mixtures enlarges as p
increases from 0 to 1. Hence, it suffices to set p = 1. This yields (3.4).
Q. E. D.

44

3. Conditional Prediction with Missing Data

It is of interest to compare the identification region for P(y*x = x) in
(3.4) with the region (3.1) obtained when only realizations of y are missing.
The two regions have the same form, with r(x) here replacing P(z = 1*x = x)
there. The quantity r(x) is the smallest feasible value of P(zyx = 1*x = x) and
is obtained by conjecturing that all missing covariate realizations have the
value x. Thus, joint missingness of (y, x) exacerbates the identification
problem produced by missingness of y alone.
The degree to which joint missingness exacerbates the identification
problem depends on the prevalence of the value x among the observable
realizations of x. Inspection of (3.4b) shows that r(x) = P(zyx = 1) when
P(x = x*zyx = 1) = 1 and decreases to zero as P(x = x*zyx = 1) falls to zero.
Thus, region (3.4) is uninformative if the observable covariate distribution
P(x*zyx = 1) places zero mass on the value x.
With r(x) replacing P(z = 1*x = x) and zyx replacing z, all results obtained
in Chapter 1 hold when realizations of (y, x) are jointly missing. Proposition
1.2 has this analog.
Proposition 3.2: Let D respect stochastic dominance. Let g  G. Let P(zy =
zx = 1) + P(zy = zx = 0) = 1. Then the smallest and largest points in the
identification region for D{P[g(y)]}are D{P[g(y) zyx = 1]r(x) + 0g[1 r(x)]}
a
and D{P[g(y) zyx = 1]r(x) + 1g[1r(x)]}.
Proposition 1.3 has the following generalization to settings in which data
are available from multiple sampling processes, with only outcomes missing
in some sampling processes and (y, x) jointly missing in the others.
Proposition 3.3: Let there be a set M of sampling processes for which
P(zx = 1) = 1 and a set M1 of sampling processes for which P(zy = zx = 1) +
P(zy = zx = 0) = 1. Then

(M, M1) [P(y*x = x)] =
B

[P(y x = x, zmy = 1)P(zmy = 1*x = x) +

P(zmy = 0*x = x),

m

m



]

Y

m M

B

[P(y x = x, zmyx = 1)rm(x) +

m

[1  rm(x)],

m



]

Y

m  M1

(3.9)
is the identification region for P(y*x = x).

a

3.3. Jointly Missing Outcomes and Covariates

45

Distributional Assumptions
When (y, x) are jointly missing, empirical researchers often assume that
observed and missing outcomes have the same distribution conditional on
the event {x = x}; that is,
P(y*x = x) = P(y*x = x, zyx = 0) = P(y*x = x, zyx = 1).

(3.10)

Suppose that P(zyx = 1) > 0. Then P(y*x = x, zyx = 1) is revealed by the
sampling process, so P(y*x = x) is point-identified under assumption (3.10).
However, the credibility of this non-refutable assumption is often suspect.
Distributional assumptions that use instrumental variables have
identifying power when (y, x) are jointly missing. Consider Assumptions
MAR and SI. Applied to the sub-population of J for which {x = x}, these
assumptions are respectively
P(y v, x = x) = P(y v, x = x, zyx = 0) = P(y v, x = x, zyx = 1)

(3.11)

and
P(y v, x = x) = P(y x = x).

(3.12)

The identifying power of assumption SI follows easily from Propositions
3.1 and 2.2. Let v  V. Applying Proposition 3.1 to the sub-population of
J with {v = v, x = x} yields

[P(y*v = v, x = x)] =
{P(y*v = v, x = x, zyx = 1)r(v, x) +

v

[1  r(v, x)],

v



},

Y

(3.13a)
where
r(v, x) 

P(x = x*v = v, zyx = 1)P(zyx = 1 v = v)
))))))))))))))))))))))))))))))))))))))) .
P(x = x*v = v, zyx = 1)P(zyx = 1 v = v) + P(zyx = 0 v = v)
(3.13b)

Emulation of the proof to Proposition 2.2 then yields the following
proposition.

46

3. Conditional Prediction with Missing Data

Proposition 3.4: (a) Let assumption SI hold, as in (3.12). Let P(zy = zx = 1)
+ P(zy = zx = 0) = 1. Then the identification region for P(y*x = x) is

SI[P(y*x = x)] =
B {P(y v = v, x = x, zyx = 1)r(v, x) +

[1  r(v, x)],

v

v



}.

Y

vV

(3.14)
(b) Let SI[P(y x = x)] be empty. Then (3.12) does not hold.

a

It is a more complex matter to determine the identifying power of
assumption MAR when (y, x) are jointly missing. When (3.11) holds,
emulation of the proof to Proposition 2.1 shows that
P(y x = x) =  v  V P(y v = v, x = x, zyx = 1)P(v = v x = x).

(3.15)

If only outcomes were missing, the empirical evidence would reveal all
quantities on the right side of (3.15). However, with (y, x) jointly missing,
the empirical evidence does not reveal P(v x = x); hence P(y x = x) is not
point-identified.
To describe the identification region for P(y x = x), we need to know the
identification region for P(v x = x). Given that v is always observed,
inference on P(v x = x) is a problem of prediction when only covariates are
missing. This problem is the subject of the next section.

3.4. Missing Covariates
Suppose now that realizations of the outcome y are always observed but
realizations of the covariate x may be missing. Proposition 3.5 gives the
identification region for P(y*x = x) using the empirical evidence alone.
Proposition 3.5: Let P(zy = 1) = 1. Then

[P(y*x = x)] = A
p  [0, 1]

P(x = x*zx = 1)P(zx = 1)

{ P(y*x = x, zx = 1)

))))))))))))))))))))))))))
P(x = x*zx = 1)P(zx = 1) + pP(zx = 0)

3.4. Missing Covariates

47

pP(zx = 0)
+  )))))))))))))))))))))))))) ,  
P(x = x*zx = 1)P(zx = 1) + pP(zx = 0)

(p)},

Y

(3.16a)
where
(p) 

Y

Y

B {[P(y*zx = 0)  (1  p)]/p,



}.

Y

(3.16b)
a

Proof: Applying the Law of Total Probability and Bayes Theorem as in
(3.5) and (3.6) yields
P(y*x = x) =
P(x = x*zx = 1)P(zx = 1)
P(y*x = x, zx = 1) ))))))))))))))))))))))))))))))))))))
P(x = x*zx = 1)P(zx = 1) + P(x = x*zx = 0)P(zx = 0)
P(x = x*zx = 0)P(zx = 0)
+ P(y*x = x, zx = 0) )))))))))))))))))))))))))))))))))))) .
P(x = x*zx = 1)P(zx = 1) + P(x = x*zx = 0)P(zx = 0)
(3.17)
Of the quantities on the right side of equation (3.17), the sampling process
reveals P(zx), P(x = x*zx = 1), and P(y*x = x, zx = 1), but not P(x = x*zx=0)
and P(y*x = x, zx = 0). It does reveal P(y*zx = 0), which is related to
P(x = x*zx = 0) and P(y*x = x, zx = 0) by the Law of Total Probability
P(y*zx = 0) =
P(y*x = x, zx = 0)P(x = x*zx = 0) + P(y*x g x, zx = 0)P(x g x*zx = 0).
(3.18)
To determine the identifying power of (3.18), suppose that P(x = x*zx = 0)
= p. Then Y(p) given in (3.16b) is the set of values of P(y*x = x, zx = 0)
that are consistent with (3.18). Now let p range over the interval [0, 1]. This
yields (3.16a).
Q. E. D.

48

3. Conditional Prediction with Missing Data

The restriction imposed by equation (3.18) makes the problem of
missing covariates qualitatively different from the missing data problems
studied earlier in this chapter.1 For given p  [0, 1], the set of distributions
Y(p) are the solutions to a mixture problem that will be studied in depth in
Chapter 4. I defer discussion of the structure of Y(p) until then.
Missing covariates pose a less severe observational problem than do
jointly missing outcomes and covariates. Hence, the identification region
derived in Proposition 3.5 necessarily is a subset of the one obtained in
Proposition 3.1. Comparison of (3.16) with (3.8) makes this precise.
Whereas could range over the space Y of all distributions in (3.8), it can
only range over the restricted spaces Y(p), p  [0, 1] in (3.16).
Indeed, P(y*x = x) may even be point-identified. This happens if
P(y*x = x, zx = 1) = P(y*zx = 0), and these distributions are degenerate.
Then region (3.16) contains only one element, P(y*x = x, zx = 1).

Assumptions MAR and SI
Proposition 3.5 enables description of the identifying power of assumption
MAR when (y, x) are jointly missing, a question that was left open in
Section 3.3. Recall equation (3.15). Assuming that P(zyx = 1) > 0, the only
quantity not identified on the right side of (3.15) is P(v*x = x). Hence, joint
missingness of (y, x) has the same consequences as missingness of x alone.
Proposition 3.5 gives the identification region for P(v*x = x). From this,
Proposition 3.6 follows.
Proposition 3.6: Let assumption MAR hold, as in (3.11). Let P(zyx = 1) > 0.
Let [P(v*x = x)] be the identification region applying Proposition 3.5 to
P(v*x = x). Then the identification region for P(y*x = x) is

MAR[P(y*x = x)] =

{  v  V P(y v = v, x = x, zyx = 1)(v = v),   [P(v*x = x)]}. (3.19)
a
Proposition 3.5 immediately gives the identifying power of assumption
SI when covariates are missing. This result is Proposition 3.7.
Proposition 3.7: (a) Let assumption SI hold, as in (3.12). Let P(zy = 1) = 1.
For v  V, let [P(y*v = v, x = x)] be the identification region obtained by
applying Proposition 3.5 to P(y*v = v, x = x). Then the identification region
for P(y*x = x) is

3.5. General Missing-Data Patterns

49

SI[P(y*x = x)] = B [P(y*v = v, x = x)].

(3.20)

vV

(b) Let SI[P(y x = x)] be empty. Then (3.12) does not hold.

a

3.5. General Missing-Data Patterns
Consider now a sampling process with a general pattern of missing data in
which some realizations of (y, x) may be completely observed, others
observed in part, and still others not observed at all. The structure of the
problem of inference on P(y*x = x) is displayed by the Law of Total
Probability and Bayes Theorem, which give
P(y*x = x) =
P(x = x*zx = j, zy = k)P(zx = j, zy = k)

  P(y*x = x, zx = j, zy = k) )))))))))))))))))))))))))))))) .
j k
  P(x = x*zx= 5, zy= m)P(zx = 5, zy = m)
5 m

(3.21)
Examine the right side of (3.21). The sampling process identifies P(zx, zy),
P(x = x*zx = 1, zy), and P(y*x = x, zx = 1, zy = 1). It does not identify
P(x = x*zx = 0, zy), P(y*x = x, zx = 0, zy = 1), or P(y*x = x, zx, zy = 0). The
sampling process does, however, reveal P(y*zx = 0, zy = 1), which is related
to P(x = x*zx = 0, zy = 1) and P(y*x = x, zx = 0, zy = 1) by the Law of Total
Probability
P(y*zx = 0, zy = 1) = P(y*x = x, zx = 0, zy = 1)P(x = x*zx = 0, zy = 1)
+ P(y*x g x, zx = 0, zy = 1)P(x g x*zx = 0, zy = 1).
(3.22)
Thus, using the empirical evidence alone, the identification region for
P(y*x = x) has the following form.
Proposition 3.8: Let Pjk  P(zx = j, zy = k) for j, k = 0 or 1. Then

[P(y*x = x)] =

50

3. Conditional Prediction with Missing Data
P(x = x*zx = 1, zy = 1)P11

{ P(y*x = x, zx = 1, zy = 1)

))))))))))))))))))))))))))))))))
 P(x = x*zx = 1, zy = k)P1k + p0P00 + p1P01
k

P(x = x*zx = 1, zy = 0)P10

+

10 ))))))))))))))))))))))))))))))))
 P(x = x*zx = 1, zy = k)P1k + p0P00 + p1P01
k

+

p0P00
00 ))))))))))))))))))))))))))))))))
 P(x = x*zx = 1, zy = k)P1k + p0P00 + p1P01
k

+

p1P01
01 )))))))))))))))))))))))))))))))) ,
 P(x = x*zx = 1, zy = k)P1k + p0P00 + p1P01
k

(10, 00, 01) 

Y

×

Y

×

(p1) ; (p0, p1)  [0, 1]2 },

Y

(3.23a)
where
(p1) 

Y

Y

B {[P(y*zx = 0, zy = 1)  (1  p1)]/p1,



}.

Y

(3.23b)
a

This identification region is generally complex whenever there is positive
probability P01 that realizations of outcomes are observable but those of
covariates are not. Nevertheless, Proposition 3.8 yields a relatively simple
closed-form identification region for the probability P(y  B*x = x) that y
lies in any set B. Corollary 3.8.1 gives this result.2
Corollary 3.8.1: Let B be a non-empty, proper, and measurable subset of Y.
Define
R(x)  P(y  B*x = x, zx = 1, zy = 1)P(x = x*zx = 1, zy = 1)P11
+ P(x = x*zx = 1, zy = 0)P10 + P00 + P(y  B*zx = 0, zy = 1)P01,

3.5. General Missing-Data Patterns

51

S(x)   P(x = x*zx = 1, zy = k)P1k + P00 + P(y  B*zx = 0, zy = 1)P01,
k

T(x)   P(x = x*zx = 1, zy = k)P1k + P00 + P(y Õ B*zx = 0, zy = 1)P01,
k

P(y  B*x = x, zx = 1, zy = 1)P(x = x*zx = 1, zy = 1)P11
L(x)  ))))))))))))))))))))))))))))))))))))))) ,
T(x)
and U(x)  R(x)/S(x). Then the identification region for P(y  B*x = x) is

[P(y  B*x = x)] = [L(x), U(x)].

(3.24)
a

Proof: Proposition 3.8 shows that

[P(y  B*x = x)] =
P(x = x*zx = 1, zy = 1)P11

{P(y  B*x = x, zx = 1, zy = 1)

))))))))))))))))))))))))))))))

 P(x = x*zx = 1, zy = k)P1k + p0P00 + p1P01
k

+

P(x = x*zx = 1, zy = 0)P10
10(B) )))))))))))))))))))))))))))))))
 P(x = x*zx = 1, zy = k)P1k + p0P00 + p1P01
k

+

p0P00
00(B) )))))))))))))))))))))))))))))))
 P(x = x*zx = 1, zy = k)P1k + p0P00 + p1P01
k

+

p1P01
01(B) )))))))))))))))))))))))))))))))) ,
 P(x = x*zx = 1, zy = k)P1k + p0P00 + p1P01
k

[10(B), 00(B), 01(B)]  [0, 1]2 × I(p1); (p0, p1)  [0, 1]2 },
(3.25a)

52

3. Conditional Prediction with Missing Data

where
I(p1)  [0, 1] B {[P(y  B*zx= 0, zy= 1)  (1p1)]/p1,   [0, 1]}. (3.25b)
Hold p1 fixed, and vary  over its range [0 ,1]. This shows that

01(B)  [max{0, [A  (1  p1)]/p1}, min{1, A/p1}],

(3.26)

where A  P(y  B*zx = 0, zy = 1). Continue to hold p1 fixed and vary
[10(B), 00(B), p0]  [0, 1]3. This shows that
P(y  B*x = x)  [L*(x, p1) , U*(x, p1)],

(3.27a)

where
L*(x, p1) 
P(x = x*zx = 1, zy = 1)P11
P(y  B*x = x, zx = 1, zy = 1) ))))))))))))))))))))))))))))))
 P(x = x*zx = 1, zy = k)P1k + P00 + p1P01
k

p1P01
+ max{0, [A  (1  p1)]/p1} ))))))))))))))))))))))))))))))
 P(x = x*zx = 1, zy = k)P1k + P00 + p1P01
k

(3.27b)
and
U*(x, p1) 

P(x = x*zx = 1, zy = 1)P11

P(y  B*x = x, zx = 1, zy = 1) ))))))))))))))))))))))))))))))
 P(x = x*zx = 1, zy = k)P1k + P00 + p1P01
k

P(x = x*zx = 1, zy = 0)P10 + P00

+

)))))))))))))))))))))))))))))

 P(x = x*zx = 1, zy = k)P1k + P00 + p1P01
k

3.6. Joint Inference on Conditional Distributions
+

53

p1P01
min(1, A/p1) )))))))))))))))))))))))))))) .
 P(x = x*zx = 1, zy = k)P1k + P00 + p1P01
k

(3.27c)
Finally, minimize L*(x, p1) and maximize U*(x, p1) over p1  [0, 1]. The
function L*(x, #) is unimodal with unique minimum at p1 = 1  A. This
yields the overall lower bound L(x) = L*(x, 1  A). The function U*(x, #) is
unimodal with unique maximum at p1 = A. This yields the overall upper
bound U(x) = U*(x, A).
Q. E. D.

3.6. Joint Inference on Conditional Distributions
In Sections 3.2 through 3.5, the objective was presumed to be prediction of
the outcome y conditional on the covariate x taking a specified value x;
hence the object of interest was the conditional distribution P(y*x = x).
Researchers often want to predict outcomes when covariates take multiple
values. Then the object of interest is the set of conditional distributions
[P(y*x = x), x  X] or some functional thereof.
The identification region for [P(y*x = x), x  X] necessarily is a subset
of the Cartesian product of the identification regions for each component
distribution. Using the empirical evidence alone, that is

[P(y*x = x), x  X]

G

× x  X [P(y*x = x)].

(3.28)

Relationship (3.28) follows immediately from the definition of an identification region. Region [P(y*x = x), x  X] gives all jointly feasible values
of [P(y*x = x), x  X]. For each x  X, [P(y*x = x)] gives all feasible
values of P(y*x = x). Joint feasibility implies component-by-component
feasibility, so (3.28) must hold.
To go beyond (3.28) in characterizing the problem of joint inference, one
must specify the nature of the missing-data problem. The structure of the
joint identification region is complex for sampling processes with general
patterns of missing data, but simple results hold if only outcomes are
missing or if (y, x) are jointly missing.3

Missing Outcomes
Suppose that only outcome data are missing and that no distributional

54

3. Conditional Prediction with Missing Data

assumptions are imposed. The Law of Total Probability gives
[P(y*x = x), x  X] =
[P(y x = x, zy = 1)P(zy = 1 x = x) + P(y x = x, zy = 0)P(zy = 0 x = x), x  X].
(3.29)
The sampling process identifies all quantities on the right side of (3.29)
except for the set of distributions [P(y x = x, zy = 0), x  X], which can take
any value in × x  X Y. Hence, we have Proposition 3.9.
Proposition 3.9: Let P(zx = 1) > 0. Then

[P(y*x = x), x  X] = × x  X [P(y*x = x)]
= × x  X [P(y x = x, zy = 1)P(zy = 1 x = x) +

P(zy = 0 x = x),

x

x



].

Y

(3.30)
a
Analogous findings hold if data from multiple sampling processes are
available or if distributional assumptions using instrumental variables are
imposed. In all of the settings considered in Chapters 1 and 2, joint
inference on [P(y*x = x), x  X] is equivalent to component-by-component
inference on P(y*x = x), x  X.

Jointly Missing Outcomes and Covariates
When realizations of (y, x) are jointly missing, inference on the set of
distributions [P(y*x = x), x  X] is not equivalent to component-bycomponent inference on P(y*x = x), x  X. The reason is that a missing
covariate realization cannot simultaneously have multiple values.
Recall Proposition 3.1, which gave the identification region for
P(y*x = x) at a specified value x. Equation (3.8) showed that the set of
feasible values for P(y*x = x) enlarges with the probability P(x = x*zyx = 0)
that a missing realization of x takes the value x; hence [P(y*x = x)]
emerged by setting P(x = x*zyx = 0) = 1.
Now consider any other covariate value x1. If P(x = x*zyx = 0) = 1, then
P(x = x1*zyx = 0) = 0. Hence, by (3.7), P(y*x = x1) = P(y*x = x1, zyx = 1).
Thus P(y*x = x) can range over its entire identification region only if the
distributions [P(y*x = x1), x1  X, x1gx] take particular values. Thus

3A. Unemployment Rates

55

[P(y*x = x), x  X] is a proper subset of × x  X [P(y*x = x)].
Proposition 3.10 characterizes the joint region.
Proposition 3.10: Let P(zy = zx = 1) + P(zy = zx = 0) = 1. Let S denote the
unit simplex in R X . Then

[P(y*x = x), x  X] =

A
(px, x  X)  S

P(x = x*zyx = 1)P(zyx = 1)

{ × x  X [P(y*x = x, zyx = 1)

))))))))))))))))))))))))))))
P(x = x*zyx = 1)P(zyx = 1) + pxP(zyx = 0)

px P(zyx = 0)
+

x

)))))))))))))))))))))))))))) ,
P(x = x*zyx = 1)P(zyx = 1) + px P(zyx = 0)

x



]}.

Y

(3.31)

a
Proof: The vector [P(x = x*zyx = 0), x  X] can take any value in the unit
simplex. For any feasible value of this vector, the set of feasible values of
[P(y*x = x), x  X] is the Cartesian product of the sets of distributions in
brackets in (3.31).
Q. E. D.

Complement 3A. Unemployment Rates
Complement 1A used NLSY data to estimate the probability that a member
of the surveyed population was employed in 1991. Now consider the
problem of inference on the official unemployment rate as measured in the
United States by the Bureau of Labor Statistics. This rate is the probability
of unemployment within the sub-population of persons who are in the labor
force. When the 1991 employment status of an NLSY sample member is
not reported, data are missing not only on that person’s unemployment
outcome but also on his or her membership in the labor force. Thus,
inference on the official unemployment rate poses a problem of jointly
missing outcome and covariate data.
As in Complement 1A, the quantity of interest is P[y = 1*y  {1, 2}] or,
perhaps, P[y = 1*BASE, y  {1, 2}]. The data in Table 1.1 show that the
empirical unemployment rate among the individuals who responded to the

56

3. Conditional Prediction with Missing Data

1991 employment-status question and who reported that they were in the
labor force is P[y = 1*y  {1, 2}, z = 1] = 297/4629 = 0.064. In addition,
P(z = 1) = 5556/6812 = 0.816 and P[y  {1, 2}*z = 1] = (4332 + 297)/5556
= 0.833. Hence r(x) defined in equation (3.4b) has the value 0.787; here x
is the event {y  {1, 2}}. Proposition 3.1 now yields this identification
region for the official unemployment rate:
P[y = 1*y  {1, 2}]  [(0.064)(0.787), (0.064)(0.787) + 0.213]
= [0.050, 0.263].
Similar computations yield P[y = 1*BASE, y  {1, 2}]  [0.057, 0.164].

Complement 3B. Parametric Prediction with Missing Data
Whereas this chapter has studied nonparametric prediction of outcomes
conditional on covariates, researchers often specify a parametric family of
predictor functions and seek to infer a member of this family that minimizes
expected loss with respect to some loss function. Let the outcome y be realvalued. Let  be the parameter space and f(#, #): X ×   R be the family of
predictor functions. Let L(#): R  [0, ] be the loss function. The immediate
objective is to find a *   such that

*  argmin    E{L[y  f(x, )]}.
Then f(#, *) is called a best f(#, #)-predictor of y given x under loss function
L. Under usual regularity conditions, * is unique.
For example, consider the familiar problem of best linear prediction
under square loss. Here f(x, ) = x1 and L[y  f(x, )] = (y  x1)2. As is
well known, * = E(xx1)-1E(xy), provided that E(xx1) is non-singular.

Prediction Ignoring Missing Data
Empirical researchers routinely discard all realizations of (y, x) that are
incompletely observed. Suppose that a random sample of size N has been
drawn from population J. Let N(1) denote the sample members for which
zyx = 1, and let N1 be the cardinality of N(1). Then it is routine to estimate
* by a N   such that

3B. Parametric Prediction with Missing Data

57

1

N  argmin


))



N1

L[yi  f(xi, )].

i  N(1)

Under usual regularity conditions, N almost surely is unique and
lim N = argmin E{L[y  f(x, )] zyx = 1}, a. s.
N



Thus N is a consistent estimate of * if and only if
argmin    E{L[y  f(x, )] zyx = 1} = argmin    E{L[y  f(x, )]}.

Prediction Using the Empirical Evidence Alone
Consider the problem of parametric prediction using the empirical evidence
alone. The identification region for * is the set of parameter values that
minimize expected loss under some feasible distribution for the missing
data. It is easy enough to characterize this region, but it may be rather
difficult to estimate it.
Emulating the construction of [P(y*x = x)] for general missing-data
patterns in Proposition 3.8, the identification region for * is

(*) =

A
(10, 00, 01) 

10

×

00

×

01

{argmin    P(zyx = 1) E{L[y  f(x, )] zyx = 1}
+ P(zx = 1, zy = 0)#,L[y  f(x, )]d10
+ P(zx = 0, zy = 0)#,L[y  f(x, )]d00
+ P(zx = 0, zy = 1)#,L[y  f(x, )]d01}.
Here 10 is the set of all distributions on Y × X with x-marginal P(x zx = 1,
zy = 0), 00 is the set of all distributions on Y × X, and 01 is the set of all
distributions on Y × X with y-marginal P(y zx = 0, zy = 1).
The natural estimate of (*) is its sample analog, which uses the
empirical distribution of the data to estimate P(zyx), P[(y, x) zyx = 1],
P(x zx = 1, zy = 0), and P(y zx = 0, zy = 1). However, computation of this
estimate can pose a considerable challenge. This is so even in the relatively
benign setting of best linear prediction under square loss, where the sample

58

3. Conditional Prediction with Missing Data

analog of (*) is the set of least squares estimates produced by conjecturing all possible values for missing outcome and covariate data; see
Horowitz and Manski (2001).

Prediction Assuming that f(#, *) is Best Nonparametric
Researchers posing parametric prediction problems often combine the
empirical evidence with distributional assumptions. In particular, it is
common to assume that the best f(#, #)-predictor of y given x under loss
function L is best nonparametric; that is,
f(x, *)  argmin c  R E[L(y  c) x = x], ~ x  X.
This assumption may enable a researcher to shrink (*).
For example, researchers seeking the best linear predictor under square
loss often assume that the best nonparametric predictor, the conditional
expectation E(y x), is a linear function of x. Let [E(y x)] be the identification region for E(y x) using the empirical evidence alone. Then the
assumption of linear mean regression implies that  is a feasible solution
to the problem of best linear prediction under square loss if and only if x
 [E(y x)].

Endnotes
Sources and Historical Notes
Much of the analysis in this chapter was originally developed in Horowitz
and Manski (1998, 2000). In particular, Propositions 3.1 and 3.5 are based
on Horowitz and Manski (1998). Corollary 3.8.1 is based on Horowitz and
Manski (2000, Theorem 1).
Whereas prediction with missing outcome data has long been a
prominent concern of statistics, serious attention to missing covariate data
is relatively recent and far less common. When statisticians have studied
missing covariate data, they have invariably imposed assumptions that
point-identify P(y x). Having done this, their main concern has been to
understand the finite-sample properties of point estimates of conditional
predictors (see, for example, Little, 1992; Robins, Rotnitzky, and Zhao,
1994; Wang, Wang, Zhao, and Ou, 1997).

Endnotes

59

Text Notes
1. Likewise, interval measurement of real-valued covariates is qualitatively
different from interval measurement of outcomes. It was shown in Section
1.5 that, using the empirical evidence alone, interval measurement of
outcomes yields simple bounds on parameters that respect stochastic
dominance. Corresponding findings when covariates are interval-measured
are not available. However, Manski and Tamer (2002) show that imposition
of certain distributional assumptions does produce findings of interest.
Let X G R. Let each j  J have a triple (xj, xj, xj+)  X3. Let the random
variable (x, x, x+): J  X3 have a distribution P(x, x, x+) such that P(x  x
 x+) = 1. Then we have interval measurement of covariates if realizations
of (x, x+) are observable but realizations of x are not directly observable.
Now suppose that these distributional assumptions hold:
Monotonicity: E(y x) is weakly increasing in x.
Mean Independence: E(y x, x, x+) = E(y x).
Manski and Tamer (2002, Proposition 1) show that, for any x  X,
sup [E(y x = x, x+ = x+)]  E(y x = x)  inf [E(y x = x, x+ = x+)].
(x, x+) s.t. x  x  x+
(x, x+) s.t. x  x+  x
2. Further analysis of conditional prediction with general missing data
patterns is presented in Zaffalon (2002). He supposes that the set Y × X is
finite and shows that the smallest and largest feasible values of E(y x = x)
can be obtained by solving fractional linear programming problems.
3. The only finding to date for general missing-data processes concerns
functionals of the form P(y  B*x = x)  P(y  B*x = x1), where B G Y and
where (x, x1) are distinct covariate values. Through a laborious derivation,
Horowitz and Manski (2000) obtain closed-form expressions for the
minimum and maximum feasible values of this functional.

4
Contaminated Outcomes
4.1. The Mixture Model of Data Errors
Throughout the analysis of missing-data problems in Chapters 1 through 3,
it was assumed that the available data are realizations of the outcomes and
covariates of interest. Researchers use the broad term data errors to
describe situations in which the available data imperfectly measure
variables of interest. In general, data errors produce identification problems.
The specific nature of the problem depends on how the available data may
be related to the variables of interest.
One prominent conceptualization of data errors has been the mixture
model, which views the available data as realizations of a probability
mixture of the variable of interest and of another random variable. Let each
member j of population J have a pair of outcomes (y*j, ej) in the space Y× Y.
Let the random variable (y*, e): J  Y × Y have distribution P(y*, e). Let y*
be the outcome of interest. Let a sampling process draw persons at random
from J. The mixture model views the available data as realizations of the
probability mixture
y  y*z + e(1  z),

(4.1)

where z is an unobservable binary random variable indicating whether e or
y* is observed; Thus y* is observed if z = 1 and e is observed if z = 0.
Realizations of y with z = 0 are said to be data errors, those with z = 1 are
said to be error-free, and y itself is said to be a contaminated measure of y*.
The mixture model has no content per se but may be informative about
*
y when combined with distributional assumptions. Researchers often
60

4.1. The Mixture Model of Data Errors

61

assume that the error probability P(z = 0) is known, or at least that it can be
bounded non-trivially from above. This chapter studies identification of
outcome distributions under such assumptions.
Let p  P(z = 0) denote the probability of a data error. The inferential
problem is displayed by the Law of Total Probability in equations (4.2) and
(4.3):
P(y) = P(y z = 1)(1  p) + P(y z = 0)p

(4.2)

P(y*) = P(y z = 1)(1  p) + P(y* z = 0)p.

(4.3)

and

The sampling process reveals only the distribution P(y) on the left side of
(4.2). Empirical knowledge of P(y) per se is uninformative about P(y z = 1)
and, hence, about P(y*). However, informative identification regions emerge
if knowledge of P(y) is combined with a non-trivial upper bound on p.
An upper bound on the probability of data errors has been a central
assumption of research on robust inference in the presence of data errors
(see Complement 4B). In some applications, the probability of a data error
can be estimated from a validation data set. In other applications, data errors
arise out of the efforts of analysts to impute missing values; the fraction of
imputed values then provides the upper bound on the error probability (see
Complement 4A). There are, of course, many applications in which there
is no obvious way to set a firm upper bound on the probability of a data
error. In these cases, it may still be of interest to determine how inference
on population parameters degrades as the error probability increases.
This chapter studies the identification of two outcome distributions. One
is the distribution
P(y z = 1)  P(y* z = 1)

(4.4)

of error-free realizations. The other is the marginal distribution P(y*) of the
outcome of interest. These two distributions generally are distinct, but they
are identical if the occurrence of data errors is statistically independent of
the outcome of interest. Thus, the chapter effectively presents two parallel
sets of findings on identification of P(y*). One set of findings assumes only
an upper bound on the error probability, and the other also assumes that
P(y*) = P(y* z = 1).

(4.5)

62

4. Contaminated Outcomes

Section 4.2 develops the identification regions for P(y z = 1) and P(y*).
These abstract findings are fleshed out in Sections 4.3 and 4.4, which derive
simple identification regions for event probabilities and for parameters that
respect stochastic dominance. All results apply to inference on conditional
distributions P(y x = x, z = 1) and P(y* x = x) if the covariates x are known
to be measured without error; one simply redefines the population of
interest to be the sub-population for which {x = x}. The present analysis
does not cover cases in which covariates are measured with error.
Considered abstractly, this chapter studies identification of the components of a probability mixture. Contaminated outcomes is only one of many
manifestations of this basic identification problem. In Chapter 3, the
problem appeared in the analysis of missing covariate data. In Chapters 5
and 10, it will arise when we study ecological inference and the mixing
problem of program evaluation.

4.2. Outcome Distributions
Part (a) of Proposition 4.1 shows that, if p is known, P(y z = 1) belongs to
the identification region p[P(y z = 1)] defined below and P(y*) belongs to
a larger region p[P(y*)]. Part (b) shows that these regions expand as the
error probability increases. This implies, in part (c), that the identification
regions given an upper bound, say , on p are [P(y z = 1)] and [P(y*)].
Proposition 4.1: (a) Let p be known, with p < 1. Then the identification
regions for P(y z = 1) and P(y*) are

p[P(y z = 1)] 

Y

B {[P(y)  p ]/(1  p),



}

(4.6)

Y

and

p[P(y*)]  {(1  p) + p , (, )  p[P(y z = 1)] ×

}. (4.7)

Y

(b) Let > 0 and p + < 1. Then p[P(y z = 1)] G p+ [P(y z = 1)] and
p[P(y*)] G p+ [P(y*)].
(c) For given  < 1, let it be known that p  . Then the identification
a
regions for P(y z = 1) and P(y*) are [P(y z = 1)] and [P(y*)].
Proof: (a) By (4.2), the joint identification region for [P(y z = 1), P(y z = 0)]
is

4.3. Event Probabilities

p[P(y z = 1), P(y z = 0)]  {(, ) 

63
Y

×

: P(y) = (1  p) + p }.

Y

Equation (4.6) follows immediately. Equation (4.7) follows from (4.3)
because the sampling process is uninformative about P(y* z = 0).
(b) To show that p[P(y z = 1)] G p+ [P(y z = 1)], consider (, ) 
p[P(y z = 1), P(y z = 0)]. Let the error probability increase from p to p+ .
Let  ( p +  )/(p + ). Then is a probability distribution and (, )
 Y × Y solves the equation
P(y) = (1  p  ) +

(p + ).

Hence (, )  p+ [P(y z = 1), P(y z = 0)].
That p[P(y*)] G p+ [P(y*)] follows from the result above and (4.7).
(c) The identification region for P(y z = 1) is Fp p[P(y z = 1)]. Part (b)
showed that F p p[P(y z = 1)] = [P(y z = 1)]. Similarly, the identification region for P(y*) is Fp p[P(y*)], and part (b) showed that Fp p[P(y*)]
= [P(y*)].
Q. E. D.
Observe that, whatever the error probability p may be, the distribution
P(y) of observed outcomes necessarily belongs to p[P(y z = 1)]. Hence the
hypothesis {P(y*) = P(y z = 1) = P(y)} is not refutable.

4.3. Event Probabilities
Let B be a measurable subset of Y. Proposition 4.1 implies simple
identification regions for the event probabilities P(y  B z = 1) and
P(y*  B). Proposition 4.2 derives these regions. The proposition shows that
there are informative lower bounds on both P(y  B z = 1) and P(y*  B) if
P(y  B) >  and informative upper bounds if P(y  B)  1  . Thus  <
½ is a necessary condition for there to be both informative lower and upper
bounds on event probabilities.
Proposition 4.2: (a) Let p be known, with p < 1.Then the identification
regions for P(y  B z = 1) and P(y*  B) are the intervals

p[P(y  B z = 1)] = [0, 1] B [[P(y  B)  p]/(1  p), P(y  B)/(1  p)],
(4.8)

64

4. Contaminated Outcomes

p[P(y*  B)] = [0, 1] B [P(y  B)  p, P(y  B) + p].

(4.9)

(b) For given <1, let it be known that p. Then the identification regions
a
for P(y  B z = 1) and P(y*  B) are [P(y z = 1)] and [P(y*)].
Proof: (a) The first task is to prove that (4.8) gives the identification region
for P(y  B z = 1). Proposition 4.1 implies that P(y  B z = 1) lies in the
interval
[0, 1] B {[P(y  B)  pa]/(1  p), a  [0, 1]} =
[0, 1] B [[P(y  B)  p]/(1  p), P(y  B)/(1  p)].
Thus, the identification region for [P(y  B z = 1)] is a subset of the set on
the right side of (4.8). We must prove that all elements of this set are
feasible. This is so if, for all c  p[P(y  B z = 1)], there exist distributions
(, )  Y × Y such that (B) = c and P(y) = (1  p) + p .
To prove that such distributions exist, let c  p[P(y  B z = 1)] and let
d solve the equation
P(y  B) = (1  p)c + pd.
Equation (4.8) implies that d  [0, 1]. Now choose (, ) as follows:
If P(y  B) > 0 and A G B, let
(A) = [P(y  A)/P(y  B)]c,
(A) = [P(y  A)/P(y  B)]d.
If P(y  B) = 0 and A G B, let (A) = (A) = 0.
If P(y  Y  B) > 0 and A G Y  B, let
(A) = [P(y  A)/P(y  Y  B)](1  c),
(A) = [P(y  A)/P(y  Y  B)](1  d).
If P(y  Y  B) = 0 and A G Y  B, let (A) = (A) = 0.
Then (B) = c and P(y  A) = (1  p)(A) + p (A) for all A G B and for
all A G Y  B. Hence P(y) = (1  p) + p .
The second task is to prove that (4.9) gives the identification region for
P(y*  B). The sampling process is uninformative about P(y*  B z = 0).
Hence the identification region for P(y*  B) is the set

{(1  p)c + pa, c  p[P(y  B z = 1)], a  [0, 1]}
= {[0, 1  p] B [P(y  B)  p, P(y  B)] + p[0, 1]}
= [0, 1] B [P(y  B)  p, P(y  B) + p].

4.4. Parameters that Respect Stochastic Dominance

65

(b) The identification regions for P(y  B z = 1) and P(y*  B) are
F p p[P(y  B z = 1)] and F p p[P(y*  B)]. It follows from part (a) that

Ap   p[P(y  B z = 1)] = [P(y  B z = 1)]
Ap   p[P(y*  B)] = [P(y*  B)].
Q. E. D.

4.4. Parameters that Respect Stochastic Dominance
Suppose now that the outcome y is real-valued. Proposition 4.3 shows that,
for each p  [0, 1], the identification region p[P(y z = 1)] contains a
“smallest” member Lp that is stochastically dominated by all feasible values
of P(y z = 1) and a “largest” member Up that stochastically dominates all
feasible values of P(y z = 1). These smallest and largest distributions are
truncated versions of the distribution P(y) of observed outcomes: Lp righttruncates P(y) at its (1p)–quantile and Up left-truncates P(y) at its
p–quantile. Proposition 4.3 uses distributions Lp and Up to determine the
smallest and largest feasible values for parameters that respect stochastic
dominance.
Proposition 4.3: Let Y be a subset of R that contains its lower and upper
bounds, y0 and y1. Let D(#) respect stochastic dominance. For   [0, 1], let
Q(y) denote the –quantile of P(y). Define probability distributions L and
U on R as follows:
L[, t]  P(y  t)/(1  )
 1

for t < Q(1)(y)
for t  Q(1)(y).

U[, t]  0
for t < Q(y)
 [P(y  t) ]/(1  ) for t  Q(y).

(4.10a)

(4.10b)

Let 0  Y and 1  Y be the degenerate distributions placing all mass on
y0 and y1 respectively.
(a) Let p be known, with p < 1. Then sharp lower and upper bounds on
D[P(y z = 1)] are D(Lp) and D(Up). Sharp bounds on D[P(y*)] are
D[(1  p)Lp + p 0] and D[(1  p)Up + p 1].
(b) For given  < 1, let it be known that p  . Then sharp lower and upper
bounds on D[P(y z = 1)] are D(L) and D(U). Sharp bounds on D[P(y*)] are

66

4. Contaminated Outcomes

D[(1  )L +  0] and D[(1  )U +  1].

a

Proof: (a) I first show that D(Lp) is the sharp lower bound on D[P(y z = 1)].
D(Lp) is a feasible value for D[P(y z = 1)] because
P(y  t) = (1  p)Lp[, t] + pU(1  p)[, t], ~ t  R.
Thus (Lp, U(1  p))  p[P(y z = 1), P(y z = 0)]. D(Lp) is the smallest feasible
value for D[P(y z = 1)] because Lp is stochastically dominated by every
member of p[P(y z = 1)]. To prove this, one needs to show that Lp[, t]
 [, t] for all   p[P(y z = 1)] and all t  R. Fix . If t  Q(1p)(y),
then
Lp[, t]  [, t] = 1  [, t]  0.
If t < Q(1p)(y), then

[, t] > Lp[, t] < (1  p)[, t] > P(y  t)
< (1  p)[, t] + p [, t] > P(y  t)
for all  Y. This contradicts the supposition that   p[P(y z = 1)], so
[, t]  Lp[, t] for all t.
Now consider D(Up). This is a feasible value for D[P(y z = 1)] because
P(y  t) = (1  p)Up[, t] + pL(1p)[, t], ~ t  R.
Thus (Up, L(1  p))  p[P(y z = 1), P(y z = 0)]. D(Up) is the largest feasible
value for D[P(y z = 1)] because Up stochastically dominates every member
of p[P(y z=1)]. To prove this, one needs to show that Up[, t]  [, t]
for all   p[P(y z = 1)] and all t  R. Fix . If t < Qp(y), then
Up[, t]  [, t] = 0  [, t]  0.
If t  Qp(y), then

[, t] < Up[, t] < (1  p)[, t] < P(y  t)  p
< (1  p)[, t] + p [, t] < P(y  t)
for all  Y. This contradicts the supposition that   p[P(y z = 1)], so
Up[, t]  [, t] for all t.

4.4. Parameters that Respect Stochastic Dominance

67

Now consider D[P(y*)]. By Proposition 4.1, the identification region for
P(y*) is

p[P(y*)]  {(1  p) + p , (, )  p[P(y z = 1)] ×

}.

Y

We have found that Lp  p[P(y z = 1)] and that Lp is stochastically
dominated by all members of p[P(y z = 1)]. Distribution 0 belongs to Y
and is stochastically dominated by all members of Y. Hence distribution
(1  p)Lp + p 0 belongs to p[P(y*)] and is stochastically dominated by all
members of p[P(y*)]. Hence D[(1  p)Lp + p 0] is the smallest feasible
value for D[P(y*)]. The proof for the upper bound is analogous.
(b) By part (a), the sharp lower and upper bounds on D[P(y z = 1)] are
inf p D(Lp) and sup p D(Up). Consider the lower bound. Part (b) of
Proposition 4.1 and part (a) of the present proposition imply that Lp 
[P(y z = 1)], ~ p  . Moreover, part (a) showed that L is stochastically
dominated by all elements of [P(y z = 1)]. Hence inf p D(Lp) = D(L ).
The same reasoning shows that sup p D(Up) = D(U ). Given these results,
the proof for D[P(y*)] is the same as in part (a).
Q. E. D.

Quantiles
Proposition 4.3 yields simple sharp lower and upper bounds on quantiles of
P(y z = 1) and P(y*). Corollary 4.3.1 shows that the bounds on quantiles of
P(y z = 1) are informative whenever the error probability is known to be
less than one. However, the bounds on quantiles of P(y*) are informative
only if the error probability is sufficiently small. For   (0, 1), there is an
informative lower bound on the –quantile of P(y*) only if  >  and an
informative upper bound only if   1  .
Corollary 4.3.1: Let Y be a subset of R that contains its lower and upper
bounds, y0 and y1. Let   (0, 1). For a  R, define
ra(y)  Qa(y) if 0 < a < 1;

ra(y)  y0 if a  0;

ra(y)  y1 if a  1.

(a) Let p be known, with p < 1. Then sharp lower and upper bounds on the
–quantile of P(y z = 1) are Q(1p)(y) and Q[(1-p)+p](y). Sharp bounds on the
–quantile of P(y*) are r(p)(y) and r(+p)(y).
(b) For given  < 1, let it be known that p  . Then sharp lower and upper
bounds on the –quantile of P(y z = 1) are Q(1-)(y) and Q[(1)+](y). Sharp
a
bounds on the –quantile of P(y*) are r()(y) and r(+)(y).

68

4. Contaminated Outcomes

Proof: (a) Proposition 4.3 showed that the smallest and largest feasible
values for the –quantile of P(y z = 1) are the –quantiles of Lp and Up,
which are Q(1p)(y) and Q[(1p)+p](y). Proposition 4.3 showed that the
smallest and largest feasible values for the –quantile of P(y*) are the
–quantiles of [(1  p)Lp + p 0] and [(1  p)Up + p 1], which are r(p)(y)
and r(+p)(y).
(b) This is an immediate application of Proposition 4.3, part (b).
Q. E. D.

Means
Proposition 4.3 yields simple sharp lower and upper bounds on the
expectations E(y z = 1) and E(y*). Corollary 4.3.2 shows that the bounds on
E(y z = 1) are informative whenever the error probability is known to be
less than one. The lower bound on E(y*) is informative if y0 is finite, and the
upper bound is informative if y1 is finite. These results are immediate.
Corollary 4.3.2: Let Y be a subset of R that contains its lower and upper
bounds, y0 and y1.
(a) Let p be known, with p < 1. Then sharp lower and upper bounds on
E(y z = 1) are ,ydLp and ,ydUp. Sharp bounds on E(y*) are (1-p),ydLp + py0
and (1p),ydUp + py1.
(b) For given  < 1, let it be known that p  . Then sharp lower and upper
bounds on E(y z = 1) are ,ydL and ,ydU. Sharp bounds on E(y*) are
a
(1 - ),ydL + y0 and (1  ),ydU + y1.

Complement 4A. Contamination Through Imputation
Organizations conducting major surveys often impute values for missing
data and report statistics that mix real and imputed data. This practice may,
but need not, yield consistent estimates of population quantities of interest.
Consider an observer who sees the reported statistics but who does not
see the raw survey data and does not know the imputation rule used when
data are missing. To this observer, imputations are data errors that may be
analyzed using the findings of this chapter. I use income statistics published
by the U.S. Bureau of the Census to illustrate.

4A. Contamination Through Imputation

69

Income Distribution in the United States
Data on the household income distribution in the United States are collected
annually in the Current Population Survey (CPS). Summary statistics are
published by the U.S. Bureau of the Census (the Bureau) in Series P-60 of
its Current Population Reports. Two sampling problems identified by the
Bureau are interview nonresponse, wherein some households in the CPS
sampling frame are not interviewed, and item nonresponse, wherein some
of those interviewed do not provide complete income responses. Faced with
these nonresponse problems, the Bureau uses available information to
impute missing income data. The Bureau mixes actual and imputed data to
produce the household income statistics reported in its Series P-60
publications.
From the perspective of this chapter, y* is the income a household
selected for interview in the CPS would report if it were to complete the
survey, e is the income the Bureau would impute to the household if the
household were not to complete the survey, and z = 1 if a CPS household
actually completes the survey. P(y z = 1) is the distribution of income
reported by those CPS households who complete the survey, P(y*) is the
distribution of income that would be reported if all households in the CPS
sampling frame were to complete the survey, and P(y) is the distribution of
household income found in the Series P-60 publications. The error
probability p is the probability that a CPS household does not complete the
survey.
The Bureau’s imputation practice is valid if the distribution P(y z = 0)
of incomes imputed for persons who do not complete the survey coincides
with the distribution P(y* z = 0) that these persons would report if they were
to complete the survey; then P(y) = P(y*). However, P(y z = 0) and
P(y* z = 0) could differ markedly. The identification regions developed in
this chapter are agnostic about the quality of the Bureau imputation
practice.
Consider the year 1989. U.S. Bureau of the Census (1991, pp. 387–388)
states that in the March 1990 CPS, which provides data on incomes during
1989, approximately 4.5% of the 60,000 households in the sampling frame
were not interviewed and that incomplete income data were obtained from
approximately 8% of the persons in interviewed households. The Bureau's
publication does not report how the latter group are spread across households but we can be sure that no more than (0.08)(0.955) = 0.076 of the
households have item nonresponse, so  = 0.121 provides an upper bound
on p.
Now consider P(y). U.S. Bureau of the Census (1991, Table 5, p. 17)
provides findings for each of twenty-one income intervals (in thousands of
dollars):

70

4. Contaminated Outcomes
P[0, 5) = 0.053
P[5, 10) = 0.103
P[10, 15) = 0.097
P[15, 20) = 0.092
P[20, 25) = 0.087
P[25, 30) = 0.083
P[30, 35) = 0.076

P[35, 40) = 0.066
P[40, 45) = 0.060
P[45, 50) = 0.048
P[50, 55) = 0.043
P[55, 60) = 0.032
P[60, 65) = 0.028
P[65, 70) = 0.023

P[70, 75) = 0.018
P[75, 80) = 0.015
P[80, 85) = 0.013
P[85, 90) = 0.009
P[90, 95) = 0.008
P[95, 100) = 0.006
P[100, ) = 0.039

Let us “fill out” P(y) by imposing the auxiliary assumption that income is
distributed uniformly within each interval except the last. We may now
obtain bounds on features of P(y z = 1) and P(y*).
For example, consider the probability that household income is below
$30,000. We have P[0,30) = 0.515 and  = 0.121. Hence, the bound on
P(y  30 z = 1) is [0.448, 0.586] and the bound on P(y*  30) is [0.394,
0.636]. Now consider median household income. The median of P(y z = 1)
must lie between the 0.5(1  ) and [0.5(1  ) + ]–quantiles of P(y),
while the median of P(y*) must lie between the (0.5  ) and
(0.5 + )–quantiles of P(y). Invoking the auxiliary assumption that P(y) is
uniform within $5000 intervals, the sharp lower and upper bounds on the
median of P(y z = 1) are [25.482, 33.026], and the corresponding bounds
on the median of P(y1) are [21.954, 37.273].
These bounds illustrate what one can learn about the distribution of
income using the statistics reported in the Series P-60 publications. Tighter
inferences can be drawn if one has access to the raw CPS survey data,
which flag the cases in which income data are imputed. Access to the raw
data enables one to point-identify P(y z = 1) and also to learn the error
probability p. With this information, one faces a problem of missing
outcomes rather than the more severe problem of contaminated outcomes.

Complement 4B. Identification and Robust Inference
The mixture model of data errors has long been a central concern of
research on robust inference. Huber (1964) combined the mixture model
with knowledge of an upper bound on the error probability to develop
minimax estimators of location parameters. The literature on robust
inference that has developed out of Huber’s work has not sought to
determine identification regions for population parameters. Rather, it has
aimed to characterize how point estimates of population parameters behave
when data errors are generated in specified ways. The main objective has
been to find point estimates that are not greatly affected by errors. Huber
(1981) and Hampel, Ronchetti, Rousseeuw, and Stahel (1986) present

4B. Identification and Robust Inference

71

comprehensive treatments of robust inference.
The pre-occupation of robust inference with point estimation stands in
contrast to the perspective of this book. In general, I find it difficult to
motivate point estimation of parameters that are only partially identified.
It seems to me more natural to estimate the identification regions for such
parameters, or at least their sharp lower and upper bounds.
Although the literature on robust inference has focused on point
estimation, in other respects it has been more conservative than identification analysis. The practice in robustness studies has been to consider the
inference problem before data are collected. The objective is to guard
against the worst outcomes that errors in the data could conceivably
produce. But some outcomes that are possible ex ante can be ruled out ex
post—after the data have been collected. Identification analysis characterizes the inferences that can be made given knowledge of the empirical
distribution of the available data.
The problem of inferring the mean E(y z = 1) of the error-free data
provides a compelling example of the difference between identification
analysis and robust inference. It is well-known that E(y z = 1) is not robust
under the mixture model of data errors. Yet Corollary 4.3.2 of this chapter
has determined informative sharp bounds on E(y z = 1).
These findings are not contradictory. Identification analysis and robust
inference take different positions on the available empirical evidence.
Identification analysis shows what values of E(y z = 1) are feasible given
the empirical knowledge of P(y) revealed by the sampling process. Thus,
Corollary 4.3.2 showed that, given an upper bound  on the error probability, the range of feasible values of E(y z = 1) is [,ydL, ,ydU]. In contrast,
robust inference considers the ex ante situation in which P(y) is not yet
known because the sampling process has not yet been executed. In this
setting, robustness studies conjecture some value for E(y z = 1) and ask
what values of E(y) are consistent with this conjecture. Given  and a
conjecture for E(y z = 1), the set of feasible values for E(y) is

{(1  )E(y z = 1) + a, a  [y0, y1]}.
This set, which has the same structure as the identification region for E(y)
when outcome data are missing, has finite range only if y0 and y1 are finite.

72

4. Contaminated Outcomes

Endnotes
Sources and Historical Notes
The analysis in this chapter originally appeared in Horowitz and Manski
(1995). In particular, Propositions 4.1 through 4.3 here are based on
Propositions 1 and 4 there. Subsequently, Horowitz and Manski (1997)
summarized the main findings, explained how the bounds on certain
parameters can be estimated, obtained asymptotic confidence regions for the
bounds, and showed how to test hypotheses about unidentified population
parameters.
The mixture model studied in this chapter is one of two prominent
conceptualizations of data errors. The other is the convolution model, which
views the available data as realizations of a convolution of the outcome of
interest and of another random variable u; thus y  y* + u. The observable
outcome y is said to measure the unobservable y* with errors-in-variables.
Like the mixture model, the convolution model has no content per se but
may be informative when combined with distributional assumptions.
Researchers using the convolution model commonly maintain the assumption that u is statistically independent of y* and that P(u) is centered at zero
in some specified sense. The problem of identification of P(y*) given
knowledge of P(y) is then called the deconvolution problem.
In general, researchers using the mixture model and the convolution
model make non-nested assumptions about the nature of data errors. From
the perspective of the convolution model, the error probability of the
mixture model is p = P(u g 0). From the perspective of the mixture model,
the additive error of the convolution model is u  (e  y*)(1  z). Researchers using the convolution model generally do not place an a priori upper
bound on P(u g 0). Researchers using the mixture model generally do not
assume that the quantity (e  y*)(1  z) is statistically independent of y*.

5
Regressions, Short and Long
5.1. Ecological Inference
The ecological inference problem has long engaged social scientists who
aim to predict outcomes conditional on covariates. Let each member j of
population J have an outcome yj in a space Y and covariates (xj, zj) in a
space X × Z. Let the random variable (y, x, z): J  Y × X × Z have
distribution P(y, x, z). The general goal is to learn the conditional distributions P(y x, z)  {P(y x = x, z = z), (x, z)  X × Z}. When y is real-valued,
a particular objective may be to learn the mean regression E(y x, z) 
{E(y x = x, z = z), (x, z)  X × Z}.
Suppose that joint realizations of (y, x, z) are not observable. Instead,
data are available from two sampling processes. One process draws persons
at random from J and generates observable realizations of (y, x) but not z.
The other sampling process draws persons at random and generates
observable realizations of (x, z) but not y. The two sampling processes
reveal the distributions P(y, x) and P(x, z). Ecological inference is the use
of this empirical evidence to learn about P(y x, z).
A prominent example arises in the analysis of voting behavior. A
researcher may want to predict voting behavior (y) conditional on electoral
district (x) and demographic attributes (z). The available data may include
administrative records on voting by district and census data describing the
demographic attributes of persons in each district. Voting records reveal
P(y, x), and census data reveal P(z, x). However, it may be that no data
source reveals P(y, z, x).
This chapter studies the identification problem manifest in ecological
inference. To simplify the presentation, it is supposed throughout that X × Z
73

74

5. Regressions, Short and Long

is finite and that P(x = x, z = z) > 0 for all (x, z)  X × Z. This regularity
condition is maintained without further reference.

5.2. Anatomy of the Problem
The structure of the ecological inference problem is displayed by the Law
of Total Probability
P(y x) =  z  Z P(y x, z = z)P(z x).

(5.1)

The available empirical evidence reveals the short conditional distributions
P(y x) and P(z x), where short means that these distributions condition on
x but not on z. The objective is inference on the long conditional distributions P(y x, z), where long means that these distributions condition on (x, z).
Let x  X. Define P(y x = x, z)  [P(y x = x, z = z), z  Z]. Let Z be the
cardinality of Z. A Z -vector of distributions [z, z  Z]  ( Y) Z is a
feasible value for P(y x = x, z) if and only if it solves the finite mixture
problem
P(y x = x) =  z  Z z P(z = z x = x).

(5.2)

Hence, the identification region for P(y x = x, z) using the empirical
evidence alone is

[P(y x = x, z)] =

{(z, z  Z)  ( Y) Z : P(y x = x) =  z  Z z P(z = z x = x)}.
(5.3)
Moreover, the identification region for P(y x, z) is the Cartesian product
× xX [P(y x = x, z)]. This holds because the Law of Total Probability (5.1)
only restricts P(y x, z) across values of z, not across values of x.

Inference on One Long Conditional Distribution
The finite mixture problem stated in equation (5.2) generalizes the binary
mixture problem studied in Chapter 4. There the random variable z took the
values 0 and 1, indicating a data error and an error-free realization. Here the
random variable z takes values in the finite covariate space Z.
Proposition 5.1 shows that the generalization from binary to finite
mixtures is inconsequential if the objective is inference on P(y x = x, z = z)

5.2. Anatomy of the Problem

75

for any one specified covariate value (x, z)  X × Z. The present identification region for P(y x = x, z = z) has the same form as the region obtained for
the distribution of error-free data P(y z = 1) in part (a) of Proposition 4.1.
Proposition 5.1: Let (x, z)  X × Z. Let p  P(z g z x = x). Then the
identification region for P(y x = x, z = z) is

[P(y x = x, z = z)] =

Y

B {[P(y x = x)  p ]/(1p),



}. (5.4)
a

Y

Proof: By (5.3), (z, z  Z)  [P(y x = x, z)] if and only if (z, z  Z) 
( Y) Z and

z = [P(y x = x)   z1  Z, z1 g z z1 P(z = z1 x = x)]/(1  p)
= [P(y x = x)  p ]/(1  p),
where   z1  Z, z1 g z z1P(z = z1 x = x)/p. This shows that all elements of
the identification region for P(y x = x, z = z) belong to the set of distributions on the right side of (5.4).
To show that every member of this set of distributions is feasible, let z
belong to this set. Then there exists a  Y such that
P(y x = x) = (1  p)z + p .
Let z1 =

for all z1  Z, z1 g z. Then

(1  p)z + p

= (1  p)z +  z1  Z, z1 g z z1 P(z = z1 x = x).

Hence (z, z  Z)  [P(y x = x, z)].
Q. E. D.

Joint Inference on Long Conditional Distributions
The generalization from binary to finite mixtures is consequential if the
objective is joint inference on the vector of long conditional distributions
P(y x = x, z). This vector of distributions must solve the mixture problem
stated in equation (5.2). Hence the identification region [P(y x = x, z)]
stated in equation (5.3) necessarily is a proper subset of the Cartesian
product × z  Z [P(y x = x, z = z)].
Equation (5.3) is simple in form but is too abstract to communicate much
about the size and shape of [P(y x = x, z)]. Section 5.3 addresses an

76

5. Regressions, Short and Long

important aspect of this question, this being the structure of the identification region for long mean regressions. Section 5.4 examines the identifying
power of two distributional assumptions using instrumental variables.

5.3. Long Mean Regressions
Let x  X. Equation (5.3) implies that the feasible values of E(y x = x, z) are

[E(y x = x, z)] = {(,ydz, z  Z), (z, z  Z)  [P(y x = x, z)]}. (5.5)
This section characterizes [E(y x = x, z)] less abstractly than (5.5).

Some Immediate Properties
Some properties of [E(y x = x, z)] are immediate. First, observe that the
set [P(y x = x, z)] is convex and the expectation operator is linear. Hence
[E(y x = x, z)] is a convex set.
Second, observe that, for each z  Z, we already have sharp bounds on
E(y x = x, z = z). For a  [0, 1], let Qa(y x = x) denote the a–quantile of
distribution P(y x = x) and define
La[-, t]  P(y  t x = x)/(1  a)
 1
Ua[-, t]  0
 [P(y  t x = x)  a]/(1  a)

for t < Q(1-a)(y x = x)
for t  Q(1-a)(y x = x)

(5.6a)

for t < Qa(y x = x)
for t  Qa(y x = x).

(5.6b)

Let Lxz and Uxz be the distributions La and Ua for a = P(z g z x = x). By
Proposition 5.1 and Corollary 4.3.2, sharp bounds on E(y x = x, z = z) are
eLxz ,ydLxz and eUxz ,ydUxz. Hence, [E(y x = x, z)] is a convex subset
of the hyper-rectangle × z  Z [eLxz, eUxz].
Third, observe that, by the Law of Iterated Expectations, E(y x = x, z)
solves the linear equation
E(y x = x) =  z  Z E(y x = x, z)P(z = z x = x).

(5.7)

Hence [E(y x = x, z)] is a convex set that lies in the intersection of the
hyper-rectangle × z  Z [eLxz, eUxz] and the hyperplane that solves (5.7).
These immediate properties are useful but they do not completely
describe [E(y x = x, z)]. Proposition 5.2, presented later in this section,
goes much further by showing that [E(y x = x, z)] has finitely many

5.3. Long Mean Regressions

77

extreme points. These extreme points are the expectations of certain Z tuples of stacked distributions, defined below. Corollary 5.2.1 shows that
[E(y x = x, z)] is the convex hull of its extreme points when Y has finite
cardinality.

Stacked Distributions
Stacked distributions are sequences of Z distributions such that the entire
probability mass of the jth distribution lies weakly to the left of that of the
(j+1)st distribution. To describe these distribution sequences, let Z now be
the ordered set of integers (1, . . . , Z ). This set has Z ! permutations,
each of which generates a distinct Z -vector of stacked distributions. Label
these Z -vectors (Pmxj, j = 1, . . . , Z ), m = 1, . . . , Z !.
For each value of m, the elements of (Pmxj, j = 1, . . . , Z ) solve a
recursive set of minimization problems. What follows shows the construction of (P1xj, j = 1, . . . , Z ), which is based on the original ordering of Z.
The other ( Z ! - 1) vectors of distributions are generated in the same
manner after permuting Z, which alters the order in which the recursion is
performed.
For each j = 1, . . . , Z , P1xj is chosen to minimize its expectation subject
to the distributions earlier chosen for (P1xi, i < j) and subject to the global
condition that equation (5.2) must hold. The recursion is as follows. For j
= 1, . . . , Z , P1xj solves the problem
min ,yd5
5

(5.8)

Y

subject to
j1

P(y x = x) =

Z

 %xi P1xi + %xj 5 +  %xk 5k ,
i=1

(5.9)

k=j+1

where %xj  P(z = j x = x) and where 5k  Y, k = j + 1, . . . , Z are
unrestricted probability distributions.
This recursion yields a sequence of stacked distributions. For j = 1,
equation (5.9) reduces to
Z

P(y x = x) = %x15 +  %xk5k.

(5.10)

k=2

The distribution solving (5.8) subject to (5.10) is Lx1 defined in (5.6a),
which is a right-truncated version of P(y x = x). Thus P1x1 = Lx1.
For j = 2, equation (5.9) has the form

78

5. Regressions, Short and Long
Z

P(y x = x) = %x1Lx1 + %x25 +  %xk5k.

(5.11)

k=3

Let /x1 be the distribution that solves the equation
P(y x = x) = %x1Lx1 + (1  %x1)/x1.

(5.12)

Distribution /x1 is a left-truncated version of P(y x = x) that has all of its
mass to the right of Lx1. Combining (5.11) and (5.12) yields

%x2
Z
%xk
/x1 = ——— 5 +  ——— 5k.
k = 3 (1%x1)
(1%x1)

(5.13)

Equation (5.13) has the same form as (5.10), with /x1 replacing P(y x = x)
and %x(k+1) /(1  %x1) replacing %xk. Hence P1x2, the solution to (5.8) subject
to (5.13), is a right-truncated version of /x1. Distributions P1x1 and P1x2 are
stacked side-by-side, with all of the mass of the former distribution lying
weakly to the left of the mass of the latter distribution. Distributions {P1xj,
j = 3, . . . , Z } are similarly stacked. For each j, the mass of P1xj lies weakly
to the left of the mass of P1x(j+1).
Stacking implies that, for each value of j, the supremum of the support
of Px1j may equal the infimum of the support of P1x(j+1), but otherwise the
distributions are concentrated on disjoint intervals. If P(y x = x) has a mass
point, then P1xj and P1x(j+1) may share this mass point. However, if P(y x = x)
is continuous, then P1xj and P1x(j+1) are continuous and place their mass on
disjoint intervals.

The Extreme Points of the Identification Region
With the above as preliminary, Proposition 5.2 proves that the expectations
of the stacked distributions are the extreme points of [E(y x = x, z)]. Then
Corollary 5.2.1 shows that [E(y x = x, z)] is the convex hull of these
extreme points if Y has finite cardinality.
Proposition 5.2: Let emx  (,ydPmxj, j = 1, . . . , Z ). The extreme points of
[E(y x = x, z)] are {emx, m = 1, . . . , Z !}.
a
Proof: By construction, each vector in {emx, m = 1, . . . , Z !} is a feasible
value of E(y x = x, z). Step (i) of the proof shows that these vectors are
extreme points of [E(y x = x, z)]. Step (ii) shows that [E(y x = x, z)] has

5.3. Long Mean Regressions

79

no other extreme points. In this proof, the notation is simplified by
everywhere suppressing the conditioning on the covariate x; for example,
E(y x = x, z) and emx are abbreviated to E(y z) and em.
Step (i). It suffices to consider e1. Permuting Z does not alter the argument.
Suppose that e1 is not an extreme point of [E(y z)]. Then there exists
an   (0, 1) and distinct vectors (!1, !2)  [E(y z)] such that e1 = !1 +
(1  )!2. Suppose that e1, !1, and !2 differ in their first component. Then
either !11 < e11 < !21 or !21 < e11 < !11 . By construction, e11 = eL1, the global
minimum of E(y z = 1). Hence, it must be the case that !21 = !11 = e11.
Now, suppose that e1, !1, and !2 differ in their second component. Then
1
!2 < e12 < !22 or !22 < e12 < !21 . But e12 minimizes E(y z = 2) subject to the
previous minimization of E(y z = 1). Hence !22 = !21 = e12. Recursive
application of this reasoning shows that !2= !1 = e1, contrary to supposition.
Hence e1 is an extreme point of [E(y z)].
Step (ii). Let !  [E(y z)], with ! Õ {em, m = 1, . . . , Z !}. Then ! is the
expectation of some feasible Z -vector of non-stacked distributions. We
want to show that ! is not an extreme point of [E(y z)]. Thus, we must
show that there exists an   (0, 1) and distinct Z -vectors (!1, !2) 
[E(y z)] such that ! = !1 + (1  )!2.
Let the set-valued function S(5) denote the support of any probability
distribution 5 on the real line. Let (5j, j  Z)  ( Y) Z be any feasible Z vector of distributions with expectation !. This Z -vector is not stacked, so
there exist components 5i and 5k such that [inf S(5i), sup S(5i)] 
[inf S(5k), sup S(5k)] has positive length. Thus sup S(5i) > inf S(5k) and
sup S(5k) > inf S(5i). For ease of exposition, henceforth let aj  inf S(5j)
and bj  sup S(5j), for j = i, k.
Now, construct a feasible Z -vector of distributions that shifts mass, in
a particular balanced manner, between distributions 5i and 5k, while
leaving the other components of (5j, j  Z) unchanged. Let 0 <  <
½(bi - ak). Then 5k[ak, ak+] > 0, 5i[bi-, bi] > 0, and [ak, ak+]  [bi-, bi]
= L. Let

%k 5k[ak, ak + ]
  SSSSSSSSSSSSS .
%i 5i[bi  , bi]
Now, define the new Z -vector (51j , j  Z) as follows: Let 51j = 5j for
j g i, k. If   1, for A G Y let

80

5. Regressions, Short and Long

[51i (A), 5k1 (A)] =
[5i(A) + (%k/%i)5k(A), 0]

if A G [ak, ak + ]

[(1  )5i(A), 5k(A) + (%i/%k)5i(A)] if A G [bi  , bi]
[5i(A), 5k(A)]

elsewhere.

Alternatively, if  > 1, let
[51i (A), 5k1 (A)] =
[5i(A) + (%k/%i)5k(A), (1  1/)5k(A)] if A G [ak, ak + ]
[0, 5k(A) + (%i/%k)5i(A)]
[5i(A), 5k(A)]

if A G [bi  , bi]
elsewhere.

Thus, the new Z -vector shifts 5i mass leftward from the [bi  , bi]
interval to the [ak, ak + ] interval and compensates by shifting 5k mass
rightward to the [bi  , bi] interval from the [ak, ak + ] interval. The 
parameter ensures that we shift equal amounts of mass and that

%i51i + %k5k1 = %i5i + %k5k.
Hence (51j , j  Z) is a feasible Z -vector of distributions. The mean of (51j ,
j  Z) is related to the mean of (5j, j  Z) as follows: !1i < !i, !k1 > !k, and
!1j = !j for j g i, k.
An analogous operation switching the roles of i and k produces another
Z -vector (52j , jZ). Now let 0 <  < ½(bk  ai) and redefine  accordingly.
This construction shifts 5k mass leftward from the [bk  , bk] interval to the
[ai, ai + ] interval and shifts an equal amount of 5i mass rightward to the
[bk  , bk] interval from the [ai, ai + ] interval, while ensuring that

%i52i + %k52k = %i5i + %k5k.
The mean of this Z -vector is related to the mean of (5j, j  Z) as follows:
!2i > !i, !2k < !k, and !2j = !j for j gi, k.
It follows from the above that

%i!2i + %k!2k = %i!1i + %k!k1 = %i!i + %k!k.
Thus (!i, !k) lies on the line connecting (!1i , !k1 ) and (!2i , !2k). Moreover, !2i >
!i > !1i and !k1 > !k > !2k. Hence (!i, !k) is a strictly convex combination of

5.4. Instrumental Variables

81

(!1i , !k1 ) and (!2i , !2k). Finally, recall that !2j = !1j = !j for j g i, k. Hence ! is a
strictly convex combination of !1and !2. Thus ! is not an extreme point of
[E(y z)].
Q. E. D.
Corollary 5.2.1: Let Y have finite cardinality Y . Then [E(y x = x, z)] is
a
the convex hull of its extreme points {emx, m = 1, . . . , Z !}.
Proof: Minkowski’s Theorem shows that a compact convex set in R Z is the
convex hull of its extreme points.1 We already know that [E(y x = x, z)]
is a bounded convex set, so we need only show that this set is closed. For
(y, j)  Y × Z, let 1yj be a feasible value for P(y = y x = x, z = j). Then
equation (5.2) is this system of Y linear equations in the Y × Z
unknowns {1yj, (y, j)  Y × Z}:
P(y = y x = x) =  j  Z %xj1yj,

y  Y.

Let 0 denote the solutions to this system of equations. 0 forms a closed set
in R Y × Z . The identification region for E(y x = x, z) is a linear map from
0 to R Z , namely

[E(y x = x, z)] = {( y  Y y1yj, j  Z), 1  0}.
Hence [E(y x = x, z)] is closed.
Q. E. D.
Corollary 5.2.1 completely describes [E(y x = x, z)] when Y has finite
cardinality. It is topologically delicate to determine if [E(y x = x, z)] is
closed when Y has infinite cardinality. This question is not addressed here.

5.4. Instrumental Variables
Propositions 5.1 and 5.2 characterize the restrictions on E(y x, z) implied by
knowledge of P(y x) and P(z x), using the empirical evidence alone. Tighter
inferences may be feasible if distributional assumptions are imposed.
Let us first dispose of an assumption whose implications are so
immediate as barely to require comment. Suppose that y is known to be
mean-independent of z, conditional on x, so E(y x, z) = E(y x). Then
knowledge of P(y x) per se point-identifies E(y x, z).2
This section examines two assumptions that use components of x as
instrumental variables. Let x = (v, w) and X = V × W. One could assume

82

5. Regressions, Short and Long

that y is mean-independent of v, conditional on (w, z); that is,
E(y x, z) = E(y w, z).

(5.14)

Alternatively, one could assert that y is statistically independent of v,
conditional on (w, z); that is,
P(y x, z) = P(y w, z).

(5.15)

Both assumptions use v as an instrumental variable, with assumption (5.15)
being stronger than assumption (5.14).
Proposition 5.3 characterizes fully, albeit abstractly, the identifying
power of assumptions (5.14) and (5.15). Then Corollary 5.3.1 presents a
weaker, but much simpler, outer identification region that yields a
straightforward rank condition for point identification of E(y w, z).
Proposition 5.3: Let w  W. The identification regions for E(y w = w, z)
under assumptions (5.14) and (5.15) are respectively

w*  B [E(y v = v, w = w, z)]

(5.16)

vV

and

w* * = {(,ydz, z  Z), (z, z  Z)  B [P(y v = v, w = w, z)]}. (5.17)
vV

The corresponding identification regions for E(y w, z) are × w  W w* and
a
× w  W w* *.
Proof: Consider assumption (5.14). For (v, w)  V × W, (z, z  Z) 
[P(y v = v, w = w, z)] if and only if
P(y v = v, w = w) =  z  Z %(v, w)z z.
Let !  R Z . Under (5.14), ! is a feasible value for E(y w = w, z) if and only
if, for every v  V, there exists an element of [P(y v = v, w = w, z)] having
expectation !. w* comprises these feasible values of E(y w = w, z).
Consider (5.15). Under this assumption, (z, z  Z) is a feasible value
for P(y w = w, z) if and only if (z, z  Z) satisfies the system of equations
P(y v = v, w = w) =  z  Z %(v, w)z z,

~ v  V.

5.4. Instrumental Variables

83

Thus, the identification region for [P(y w = w, z), z  Z] is

B v  V [P(y v = v, w = w, z)].
w* * comprises the expectations of these feasible vectors of distributions.
Now, consider E(y w, z). Neither (5.14) nor (5.15) imposes a cross-w
restriction. Hence, the identification regions for E(y w, z) under these
assumptions are the Cartesian products of the respective regions for
E(y w = w, z), w  W.
Q. E. D.

A Simple Outer Identification Region
Proposition 5.3 is too abstract to convey a sense of the identifying power of
assumptions (5.14) and (5.15). Corollary 5.3.1 shows that a simple outer
identification region emerges if one exploits only the Law of Iterated
Expectations rather than the full force of the Law of Total Probability. The
corollary also shows that assumption (5.14) is a refutable hypothesis.
Corollary 5.3.1: (a) Let assumption (5.14) hold. Let w  W. Let V denote
the cardinality of V. Let $ denote the V × Z matrix whose zth column
is [%(v, w)z, v  V]. Let Cw* G R Z denote the set of solutions !  R Z to the
system of linear equations
E(y v = v, w = w) =  z  Z %(v, w)z !z,

~ v  V.

(5.18)

Then w* G Cw* . If $ has rank Z , then Cw* is a singleton and w* = Cw* .
(b) Let Cw* be empty. Then assumption (5.14) does not hold.

a

Proof: (a) The Law of Iterated Expectations and assumption (5.14) imply
that feasible values of E(y w = w, z) solve (5.18). Hence w* G Cw* . w* is
non-empty under assumption (5.14), so (5.18) must have at least one
solution. If $ has rank Z , then (5.18) has a unique solution, implying that
w* = Cw* .
(b) If Cw* is empty, then w* is empty. Hence (5.14) cannot hold.
Q. E. D.

84

5. Regressions, Short and Long

Complement 5A. Structural Prediction
Social scientists often want to predict how an observed mean outcome E(y)
would change if the covariate distribution were to change from P(x, z) to
some other distribution, say P*(x, z). It is common to address this prediction
problem under the assumption that the long mean regression E(y x, z) is
structural, in the sense that this regression would remain invariant under the
hypothesized change in the covariate distribution. Given this assumption,
the mean outcome under covariate distribution P*(x, z) would be
E*(y)   x  X  z  Z E(y x = x, z = z)P*(z = z x = x) P*(x = x).
To motivate the assumption that E(y x, z) is structural, social scientists
sometimes pose behavioral models of the form y = f(x, z, u), wherein a
person’s outcome y is some function f of the covariates (x, z) and of other
factors u. Then E(y x, z) is structural if u is statistically independent of (x, z)
and if the distribution of u remains unchanged under the hypothesized
change in the covariate distribution.
What can be said about E*(y) when E(y x, z) is not identified? The
findings of this chapter are applicable if the available data reveal P(y x) and
P(z x). For example, a well-known problem in poverty research is to predict
participation in social welfare programs under hypothesized changes in the
geographic distribution and demographic attributes of the population. Let
y indicate program participation, let x be a geographic unit such as a county,
and let z denote demographic attributes. One may be willing to assume that
E(y x, z) is structural in the sense defined above. Administrative records
may reveal program participation by county, and census data may reveal
demographic attributes by county; that is, P(y x) and P(z x).
In such settings, the findings of this chapter yield identification regions
for E(y x, z) and hence for E*(y). For example, using the empirical evidence
alone, one can conclude that E*(y) lies in the set

{ x  X z  Z !xz P*(z=z x=x)P*(x=x); (!xz, z  Z)  [E(y x=x, z)], x  X}.

Endnotes

85

Endnotes
Sources and Historical Notes
The analysis in this chapter originally appeared in Cross and Manski
(2002). In particular, Propositions 5.2 and 5.3 here are based on Propositions 1 and 3 there.
The early major contributions to analysis of the ecological inference
problem appeared in sociology in the 1950s. Robinson (1950) criticized the
then common practice of interpreting the ecological correlation, the cross-x
correlation of P(y x) and P(z x), as the correlation of y and z. Soon
afterwards two influential short papers were published in the same issue of
the American Sociological Review. Considering settings in which y and z
are binary random variables, Duncan and Davis (1953) and Goodman
(1953) performed informal partial analyses of the identification problems
that were eventually addressed in generality in Cross and Manski (2002).
Duncan and Davis used numerical illustrations to demonstrate that
knowledge of P(y x) and P(z x) implies a bound on P(y x, z). Goodman
showed that point identification of P(y x, z) may be possible if the available
data are combined with an assumption asserting that y is mean-independent
of an instrumental variable. In this chapter, Proposition 5.1 formalizes the
insight of Duncan and Davis, and Corollary 5.3.1 generalizes Goodman’s
finding.
The usage of the terms short and long in this chapter is borrowed from
Goldberger (1991, Section 17.2), who calls E(y x) a short regression and
E(y x, z) a long regression. A longstanding concern of the econometric
literature on linear regression exposited by Goldberger has been to compare
the parameter estimates obtained in a least squares fit of y to x with those
obtained in a least squares fit of y to (x, z). The expected difference between
the estimates obtained in the former and latter fits is sometimes called
“omitted variable bias.”
Comparison of short and long regressions has also been prominent in
statistics. Stimulated by Simpson (1951), statisticians have been intrigued
by the fact that the short regression E(y x) may be increasing in a scalar x
and yet the long regression E(y x, z = z) may be decreasing in x for all
z  Z. Studies of Simpson’s Paradox have sought to characterize the
circumstances in which this phenomenon occurs (see, for example, Lindley
and Novick, 1981; Zidek, 1984).

Text Notes
1. See Brøndsted (1983, Theorem 5.10).

86

5. Regressions, Short and Long

2. Less obvious distributional assumptions that point-identify P(y x, z) have
been studied by King (1997), who asserted achievement of “a solution to
the ecological inference problem” in a book of that name. However, his
assumptions immediately drew criticism, as evidenced in a dispute played
out in the Journal of the American Statistical Association (Freedman, Klein,
Ostland, and Roberts, 1998, 1999; King, 1999).

6
Response-Based Sampling
6.1. Reverse Regression
Consider once more the problem of prediction of outcomes conditional on
covariates. As before, the random variable (y, x): J  Y × X has distribution
P(y, x), and the objective is to learn the conditional distributions P(y*x).
For y  Y, let Jy denote the sub-population of persons who have outcome
value y. Researchers sometimes observe covariate realizations drawn at
random from the sub-populations Jy, y  Y. This sampling process is known
to epidemiologists studying the prevalence of disease as case-control, casereferent, or retrospective sampling. It is known to econometricians studying
choice behavior as choice-based sampling or response-based sampling. The
term response-based sampling will be used here.
Response-based sampling is often motivated by practical considerations.
For example, epidemiologists have found that random sampling can be a
costly way to gather data, so they have often turned to less expensive
stratified sampling designs, especially response-based designs. One divides
the population into ill (y = 1) and healthy (y = 0) response strata and
samples at random within each stratum. Response-based designs are
considered to be particularly cost-effective in generating observations of
serious diseases, as ill persons are clustered in hospitals and other treatment
centers.
Sampling at random from Jy, y  Y reveals the distributions P(x*y) of
covariates conditional on outcomes. The objective is to learn the distributions P(y*x) of outcomes conditional on covariates, so response-based
sampling poses this problem of inference from reverse regression: What
does knowledge of P(x*y) reveal about P(y*x)?
87

88

6. Response-Based Sampling

To simplify analysis, let the space Y × X be finite, with P(x = x) > 0, all
x  X. Then the identification problem is displayed by Bayes Theorem and
the Law of Total Probability, which give
P(x = x*y = y)P(y = y)
P(y = y*x = x) = ))))))))))))))))
P(x = x)
P(x = x*y = y)P(y = y)
= )))))))))))))))))))))) ,
 y1Y P(x = x*y = y1)P(y = y1)

(y, x)  Y × X.

(6.1)
Response-based sampling reveals P(x*y) but is uninformative about the
marginal outcome distribution P(y). Hence the identification region for
P(y*x) using the empirical evidence alone is

[P(y*x)] =
P(x = x*y = y) (y = y)

{[ )))))))))))))))))))))) ,
 y1Y P(x = x*y = y1) (y = y1)

(y, x)  Y × X],



}.

Y

(6.2)
Inspection of (6.2) shows that, for any given value (y, x)  Y × X, the
empirical evidence is uninformative about P(y = y*x = x); letting (y = y)
range over the interval [0, 1] yields [P(y = y*x = x)] = [0, 1]. Nevertheless, response-based sampling is informative about the manner in which
P(y = y*x = x) varies with x (see Section 6.4).
Econometricians and epidemiologists studying response-based sampling
have point-identified features of P(y*x) by combining the empirical
evidence with various other forms of information. Section 6.2 describes the
prevailing practice in econometrics, which has been to combine responsebased sampling data with auxiliary data on the marginal distribution of
outcomes or covariates. Section 6.3 describes the prevailing practice in
epidemiology, which has focused attention on binary response settings (i.e.,
Y contains two elements) and has studied inference under the rare-disease
assumption.
Sections 6.4 and 6.5 present findings on partial identification in binary
response settings. Section 6.4 analyzes the identification region [P(y*x)]

6.2. Auxiliary Data on Outcomes or Covariates

89

obtained using the empirical evidence alone and derives informative sharp
bounds on the relative risk and attributable risk statistics commonly used
in epidemiology. Section 6.5 examines inference when covariate data are
observed for only one of the two sub-populations Jy, y  Y.

6.2. Auxiliary Data on Outcomes or Covariates
Response-based sampling is problematic because the sampling process is
uninformative regarding the marginal outcome distribution P(y). The
econometrics literature on response-based sampling has recommended
solution of the problem by collection of auxiliary data that reveal P(y).
Administrative records on population outcomes or an auxiliary survey
of a random sample of the population may reveal P(y) directly. Alternatively, administrative records on population covariates or an auxiliary
random-sample survey may reveal the marginal covariate distribution P(x).
In the latter case, knowledge of P(x) and P(x y) may be used to solve the
Law of Total Probability
P(x = x) =  y  Y P(x = x*y = y)P(y = y),

xX

(6.3)

for feasible values of P(y).
Equation (6.3) generically has a unique solution if the cardinality of X
is at least as large as that of Y. For example, let x and y be binary random
variables, with X = {0, 1} and Y = {0, 1}. Then (6.3) reduces to
P(x = 1) = P(x = 1*y = 1)P(y = 1) + P(x = 1*y = 0)[1  P(y = 1)]. (6.4)
Response-based sampling identifies P(x = 1*y = 1) and P(x = 1*y = 0).
Hence, auxiliary data revealing P(x) enable solution for P(y), provided only
that P(x = 1*y = 1) g P(x = 1*y = 0).

6.3. The Rare-Disease Assumption
Epidemiologists often use response-based sampling to study the prevalence
of diseases that are known to occur infrequently in the population. Let y be
binary, with y = 1 if a person is ill with a specified disease and y = 0
otherwise. Epidemiologists use two statistics, relative risk and attributable
risk, to measure how prevalence varies with observable covariates.
The relative risk (RR) of illness for persons with different covariate
values, say x = k and x = j, is

90

6. Response-Based Sampling

RR  P(y = 1*x = k)/P(y = 1*x = j)
P(x = k*y = 1) P(x = j*y = 1)P(y = 1) + P(x = j*y = 0)P(y = 0)
= )))))))))) # )))))))))))))))))))))))))))))))))) .
P(x = k*y = 1)P(y = 1) + P(x = k*y = 0)P(y = 0)
P(x = j*y = 1)
(6.5)
The attributable risk (AR) is
AR  P(y = 1*x = k)  P(y = 1*x = j)
=

P(x = k*y = 1)P(y = 1)
))))))))))))))))))))))))))))))))))
P(x = k*y = 1)P(y = 1) + P(x = k*y = 0)P(y = 0)
P(x = j*y = 1)P(y = 1)



))))))))))))))))))))))))))))))))) .
P(x = j*y = 1)P(y = 1) + P(x = j*y = 0)P(y = 0)

(6.6)

In each case, the first identity defines the concept and the second equation
follows from (6.1).
For example, let y indicate the occurrence of heart disease and let x
indicate whether a person smokes cigarettes (yes = k, no = j). Then RR
gives the ratio of the probability of heart disease conditional on smoking to
the probability of heart disease conditional on not smoking, while AR gives
the difference between these probabilities.
Texts on epidemiology discuss both relative and attributable risk, but
empirical research has focused on relative risk. This focus is hard to justify
from the perspective of public health. The health impact of altering a risk
factor such as smoking presumably depends on the number of illnesses
averted; that is, on the attributable risk times the size of the population. The
relative risk statistic is uninformative about this quantity.
Yet relative risk continues to play a prominent role in epidemiological
research. The rationale, such as it is, is that relative risk is point-identified
under the rare-disease assumption, which lets the marginal probability of
illness approach zero. As P(y = 1)  0,
P(x = k*y = 1) P(x = j*y = 0)

lim
P(y=1)  0

RR

= )))))))))))))))))))))
P(x = j*y = 1) P(x = k*y = 0)

(6.7)

6.4. Bounds on Relative and Attributable Risk

91

The sampling process reveals the quantities on the right side of (6.7), so the
rare-disease assumption identifies relative risk. The expression on the rightside is called the odds ratio (OR) because it is the ratio of the odds of illness
for persons with covariates k and j; that is, equation (6.1) yields
P(x = k*y = 1) P(x = j*y = 0)

OR  )))))))))))))))))))))
P(x = j*y = 1) P(x = k*y = 0)

P(y = 1*x = k) P(y = 0*x = j)
= ))))))))))))))))))))) .
P(y = 0*x = k) P(y = 1*x = j)

(6.8)

Observe that the equality in (6.8) does not require the rare-disease
assumption. The odds ratio is identified using the empirical evidence alone.
The rare-disease assumption also point-identifies attributable risk, but
the result is unedifying. Letting P(y = 1)  0 yields
lim

AR = 0.

(6.9)

P(y=1)  0

Thus, the rare-disease assumption implies that the disease is inconsequential
from a public health perspective.

6.4. Bounds on Relative and Attributable Risk
This section shows what can be learned about relative and attributable risk
from the empirical evidence alone, without using the rare-disease assumption or any other information. It is shown that response-based sampling
partially identifies both quantities. As in Section 6.3, the present analysis
supposes that y is binary.

Relative Risk
Examine the expression for relative risk in (6.5). Response-based sampling
reveals all quantities on the right side except for P(y). Hence the feasible
values of RR may be determined by analyzing how the right side of (6.5)
varies as P(y = 1) ranges over the unit interval. The result, given in
Proposition 6.1, is that relative risk must lie between the odds ratio and the
value 1.

92

6. Response-Based Sampling

Proposition 6.1: Let P(x*y = 1) and P(x*y = 0) be known. Then the
identification region for RR is
OR  1 < (RR) = [OR, 1],
OR  1 < (RR) = [1, OR].

(6.10a)
(6.10b)
a

Proof: Relative risk is a differentiable, monotone function of P(y = 1), the
direction of change depending on whether OR is less than or greater than
one. To see this, let p  P(y = 1) and let Pim  P(x = i*y = m) for i = j, k, and
m = 0, 1. Write RR explicitly as a function of p. Thus, define
Pk1 (Pj1  Pj0)p + Pj0
RRp  )) # )))))))))))) .
Pj1 (Pk1  Pk0)p + Pk0
The derivative of RRp with respect to p is
Pk1

Pj1Pk0  Pk1Pj0

)) # ))))))))))))) .

Pj1 [(Pk1  Pk0)p + Pk0]2
This derivative is positive if OR < 1, zero if OR = 1, and negative if OR >
1. Hence, the extreme values of RRp occur when p equals its extreme values
of 0 and 1. Setting p = 0 makes RR = OR, and setting p = 1 makes RR = 1.
Intermediate values are feasible because RRp is continuous in p.
Q. E. D.
Recall that the rare-disease assumption makes relative risk equal the
odds ratio. Thus, this assumption always makes relative risks appear further
from one than they actually are. The magnitude of the bias depends on the
actual prevalence of the disease under study; the bias grows as P(y = 1)
moves away from zero.

Attributable Risk
Examine the expression for attributable risk in (6.6). Again, response-based
sampling reveals all quantities on the right side except for P(y). Hence the
feasible values of AR may be determined by analyzing how the right side
of (6.6) varies as P(y = 1) ranges over the unit interval. Define

6.4. Bounds on Relative and Attributable Risk

93

P(x = j*y = 1)P(x = j*y = 0)
ß 

[ )))))))))))))))))))) ]½

(6.11)

P(x = k*y = 1)P(x = k*y = 0)
and

% 
ßP(x = k*y = 0)  P(x = j*y = 0)
))))))))))))))))))))))))))))))))))))))))))))))))) .
[ßP(x = k*y = 0)  P(x = j*y = 0)]  [ßP(x = k*y = 1)  P(x = j*y = 1)]
(6.12)
Let
P(x = k*y = 1)%
AR% = ))))))))))))))))))))))))))))
P(x = k*y = 1)% + P(x = k*y = 0)(1  %)
P(x = j*y = 1)%
 )))))))))))))))))))))))))))
P(x = j*y = 1)% + P(x = j*y = 0)(1  %)

(6.13)

be the value that the attributable risk would take if P(y = 1) = %. The result,
given in Proposition 6.2, is that AR must lie between AR% and zero.
Proposition 6.2: Let P(x*y = 1) and P(x*y = 0) be known. Then the
identification region for AR is
OR  1 < (AR) = [AR%, 0]
OR  1 < (AR) = [0, AR%].

(6.14a)
(6.14b)
a

Proof: As P(y = 1) increases from 0 to 1, the attributable risk changes
parabolically, the orientation of the parabola depending on whether the odds
ratio is less than or greater than one. To see this, again let p  P(y = 1) and
let Pim  P(x = i*y = m) for i = j, k, and m = 0, 1. Write AR explicitly as a
function of p. Thus, define
Pk1p
Pj1p
ARp  )))))))))))  ))))))))))) .
(Pj1  Pj0)p + Pj0
(Pk1  Pk0)p + Pk0

94

6. Response-Based Sampling

The derivative of ARp with respect to p is
Pk1Pk0

Pj1Pj0

)))))))))))))  ))))))))))))) .

[(Pk1  Pk0)p + Pk0]2

[(Pj1  Pj0)p + Pj0]2

The derivative equals zero at
ßPk0  Pj0
% = )))))))))))))))))
(ßPk0  Pj0)  (ßPk1  Pj1)
and at
ßPk0 + Pj0
%* = ))))))))))))))))) ,
(ßPk0 + Pj0)  (ßPk1 + Pj1)
where ß  (Pj1Pj0/Pk1Pk0)1/2 was defined in (6.11). Examination of the two
roots reveals that % always lies between zero and one, but %* always lies
outside the unit interval; so % is the only relevant root. Thus ARp varies
parabolically as p rises from zero to one.
Observe that ARp = 0 at p = 0 and at p = 1. Examination of the derivative
of ARp at p = 0 and at p = 1 shows that the orientation of the parabola
depends on the magnitude of the odds ratio. If OR < 1, then as p rises from
zero to one, ARp falls continuously from zero to its minimum at % and then
rises back to zero. If OR > 1, then ARp rises continuously from zero to its
maximum at % and then falls back to zero. In the borderline case where OR
= 1, ARp does not vary with p.
Q. E. D.

6.5. Sampling from One Response Stratum
Methodological research on response-based sampling has concentrated on
situations in which one samples at random from all of the sub-populations
(Jy, y  Y) and so learns all of the conditional covariate distributions P(x*y).
Often, however, one is able to sample from only a subset of these subpopulations. For example, an epidemiologist studying the prevalence of a
disease may use hospital records to learn the distribution of covariates
among persons who are ill (y = 1), but have no comparable data on persons
who are healthy (y = 0). A social policy analyst studying participation in

6.5. Sampling from One Response Stratum

95

welfare programs may use the administrative records of the welfare system
to learn the backgrounds of welfare recipients (y = 1), but have no
comparable information on non-recipients (y = 0).
Suppose, as in these examples, that y is binary and that one can sample
from sub-population J1 but not from J0, so response-based sampling reveals
P(x*y = 1) but not P(x*y = 0). Inspection of (6.5) and (6.6) shows that
knowledge of P(x*y = 1) reveals nothing about relative and attributable
risks. However, inference is possible if knowledge of P(x*y=1) is combined
with auxiliary data on the marginal outcome and/or covariate distribution.
Consider first the case in which auxiliary data collection reveals both
marginal distributions, P(y) and P(x). Equation (6.1) shows that P(y = 1 x)
is point-identified. If y is binary, this means that P(y x) is point-identified.
Propositions 6.3 and 6.4 examine the cases in which one marginal
distribution is known, but not the other. The propositions give identification
regions for RR, AR, and the response probabilities themselves.
Proposition 6.3: Let P(x*y = 1) and P(y = 1) be known. Then the identification regions for RR and AR are

(RR) =
P(x = k*y = 1)P(y = 1)

[ )))))))))))))))))))))))) ,
P(x = k*y = 1)P(y = 1) + P(y = 0)

P(x = j*y = 1)P(y = 1) + P(y = 0)
))))))))))))))))))))))) ],
P(x = j*y = 1)P(y = 1)

(6.15)

(AR) =
P(y = 0)

P(y = 0)

[ )))))))))))))))))))))))) , ))))))))))))))))))))))) ].
P(x = k*y = 1)P(y = 1) + P(y = 0)

P(x = j*y = 1)P(y = 1) + P(y = 0)
(6.16)

For x  X, the identification region for P(y = 1*x = x) is
P(x = x*y = 1)P(y = 1)

[P(y = 1*x = x)] =

[ )))))))))))))))))))))))) , 1] .

(6.17)

P(x = x*y = 1)P(y = 1) + P(y = 0)

a

96

6. Response-Based Sampling

Proof: Sharp lower (upper) bounds on RR and AR are obtained by letting
P(x = j*y = 0) equal zero (one) and P(x = k*y = 0) equal one (zero) in (6.5)
and (6.6). Intermediate values are feasible because RR and AR vary
continuously with P(x = j*y = 0) and P(x = k*y = 0).
The sharp lower (upper) bound on P(y = 1*x = x) is obtained by letting
P(x = x*y = 0) equal one (zero) in (6.1). Intermediate values are feasible
because P(y = 1*x = x) varies continuously with P(x = x*y = 0).
Q. E. D.
Proposition 6.4: Let P(x*y = 1) and P(x) be known. Then RR is pointidentified, with
P(x = k*y = 1) P(x = j)
RR = )))))))))))))))) .
P(x = j*y = 1) P(x = k)

(6.18)

Let c  min i  X [P(x = i)/P(x = i*y = 1)]. The identification region for
P(y = 1*x = x) is the interval

[P(y = 1*x = x)] = [0, cP(x = x*y = 1)/P(x = x)].

(6.19)

The identification region for AR is
d  0 < (AR) = [cd, 0]
d  0 < (AR) = [0, cd],

(6.20a)
(6.20b)

where d  [P(x = k*y = 1)/P(x = k)  P(x = j*y = 1)/P(x = j)].

a

Proof: Recall equation (6.1), whose first equality (Bayes Theorem) gives
P(x = x*y = 1)P(y = 1)
P(y = 1*x = x) = ))))))))))))))))) ,
P(x = x)

x  X.

Equation (6.18) follows from this and the definition of relative risk.
Now, fix x and consider inference on P(y = 1*x = x). The quantities
P(x = x*y = 1) and P(x = x) are known by assumption. P(y = 1) is not
known but must lie in the interval [0, c]. To show this, let i be any element
of X, and write P(x = i) as
P(x = i) = P(x = i*y = 1)P(y = 1) + P(x = i*y = 0)[1  P(y = 1)].

6A. Smoking and Heart Disease

97

Solve for P(x = i*y = 0), yielding
P(x = i)  P(x = i*y = 1)P(y = 1)
P(x = i*y = 0) = ))))))))))))))))))))))) .
1 P(y = 1)
The probabilities {P(x = i*y = 0), i  X} must satisfy the inequalities {0 
P(x = i*y = 0), i  X} and must sum to one. These probabilities sum to one
for all values of P(y = 1), but the inequalities {0  P(x = i*y = 0), i  X}
hold if and only if P(y = 1)  c. This yields (6.19).
Finally, consider attributable risk. By definition of AR and d, AR =
P(y = 1)d. The empirical evidence reveals d, and it has been found that
P(y = 1)  [0, c]. Hence (6.20) holds.
Q. E. D.

Complement 6A. Smoking and Heart Disease
A numerical example concerning smoking and heart disease illustrates the
findings. This example is drawn from Manski (1995, Chapter 4).
Let y indicate the occurrence of heart disease. Let the probabilities of
heart disease conditional on smoking (x = k) and nonsmoking (x = j) be 0.12
and 0.08. Let the fraction of persons who smoke be 0.50. These values
imply that the marginal probability of heart disease is 0.10, and that the
probabilities of smoking conditional on being ill and healthy are 0.60 and
0.49. The implied odds ratio is 1.57, relative risk is 1.50, and attributable
risk is 0.04. Thus, the parameters of the example are
P(y = 1*x = k) = 0.12 P(y = 1*x = j) = 0.08 P(x = k) = P(x = j) = 0.50
P(y = 1) = 0.10
P(x = k*y = 1) = 0.60 P(x = k*y = 0) = 0.49
OR = 1.57
RR = 1.50
AR = 0.04.
The researcher does not know RR and AR. However, Proposition 6.1 shows
that RR lies between OR and one, so the empirical evidence reveals that RR
 [1, 1.57]. The quantities (ß, %, AR%) defined in Proposition 6.2 take the
values ß = 0.83, % = 0.51, and AR% = 0.11. Hence, the empirical evidence
reveals that AR  [0, 0.11].

Using Prior Information to Tighten the Bounds
The calculations above assume no prior information about the marginal
probability P(y = 1). Someone possessing information about the prevalence

98

6. Response-Based Sampling

of heart disease in the population can tighten the bounds. A simple way to
do this is to extend Propositions 6.1 and 6.2 to cases in which P(y = 1) is
permitted to vary over a restricted range rather than over the interval [0, 1].
King and Zeng (2002) extend the propositions in this manner and
reconsider the numerical example above when P(y = 1) is permitted to vary
over the range [0.05, 0.15]. Under the assumption that P(y = 1) lies in this
interval, they find that RR  [1.46, 1.53] and AR  [0.021, 0.056].

Sampling from One Response Stratum
Suppose that one has a random sample of ill persons but not of healthy
ones, so P(x*y = 1) is known but not P(x*y = 0). Consider Propositions 6.3
and 6.4 in this setting.
First, suppose that the outcome distribution is known. By Proposition
6.3, P(y = 1*x = k)  [0.06, 1], P(y = 1*x = j)  [0.04, 1], RR  [0.06, 23.5],
and AR  [0.94, 0.96]. Thus, knowing the marginal probability of heart
disease has little identifying power in this example.
Now, suppose that the covariate distribution is known. The parameters
of the example give c = 0.83, so Proposition 6.4 yields P(y = 1*x = k) 
[0, 1] and P(y = 1*x = j)  [0, 0.67]. The quantity d = 0.4, so AR  [0, 0.33].
Thus, knowing the prevalence of smoking in the population reveals little
about the magnitudes of the response probabilities but reveals a fair bit
about attributable risk and point-identifies relative risk.

Endnotes
Sources and Historical Notes
The analysis in Sections 6.4 and 6.5 originally appeared in Manski (1995,
2001). In particular, Propositions 6.1 and 6.2 are based on Manski (1995,
Chapter 4), and Propositions 6.3 and 6.4 on Manski (2001, Propositions 3
and 4). Manski and Lerman (1977) recommended collection of auxiliary
outcome data to learn P(y). Hsieh, Manski, and McFadden (1985) showed
that auxiliary covariate data can enable deduction of P(y).
Cornfield (1951) showed that the rare-disease assumption pointidentifies relative risk. The lack of relevance of relative risk to public health
has long been criticized; see, for example, Berkson (1958), Fleiss (1981,
Section 6.3), and Hsieh, Manski, and McFadden (1985).
The term reverse regression has been used in the literature on mean
regression to denote the conditional expectation E(x y), when the conditional expectation of interest is E(y x); see Goldberger (1984).

7
Analysis of Treatment Response
7.1. Anatomy of the Problem
The four remaining chapters of this book study a pervasive and distinctive
problem of missing outcomes. The problem is the non-observability of
counterfactual outcomes in empirical analysis of treatment response.
Studies of treatment response aim to predict the outcomes that would
occur if different treatment rules were applied to a population. Treatments
are mutually exclusive, so one cannot observe the outcomes that a person
would experience under all treatments. At most, one can observe the
outcome that a person experiences under the treatment actually received.
The counterfactual outcomes that a person would have experienced under
other treatments are logically unobservable.
For example, suppose that patients ill with a specified disease can be
treated by drugs or by surgery. The relevant outcome might be life span.
One may want to predict the life spans that would occur if patients with
specified risk factors were to receive each treatment. The available data may
be observations of the actual life spans of these patients, some of whom
were treated by drugs and the rest by surgery.

Predicting Outcomes Under Conjectural Treatment Rules
To formalize the inferential problem, let each member j of population J
have covariates xj  X and a response function yj($): T  Y mapping the
mutually exclusive and exhaustive treatments t  T into outcomes yj(t)  Y.
Let zj  T denote the treatment that person j receives and yj  yj(zj) be the
outcome that he experiences. Then yj(t), t =/ zj are counterfactual outcomes.
99

100

7. Analysis of Treatment Response

Let y($): J  Y T be the random variable mapping the population into
their response functions. Let z: J  T be the status quo treatment rule
mapping the members of J into the treatments that they actually receive.
Response functions are not observable, but covariates, realized treatments,
and realized outcomes may be observable. If so, random sampling from J
reveals the status quo (outcome, treatment) distributions P(y, z x) as well as
the covariate distribution P(x).
The distinctive problem of the analysis of treatment response is to
predict the outcomes that would occur under alternatives to the status quo
treatment rule. Let - : J  T be a conjectural treatment rule, the outcomes
of which one would like to predict. Thus, person j’s outcome under rule would be yj(-j). This outcome is counterfactual whenever -j g zj. Hence, the
sampling process does not reveal the conjectural outcome distributions
P[y(- ) x]. The problem is to combine empirical knowledge of P(y, z x) with
credible prior information to learn about P[y(- ) x].
To simplify the presentation, the analysis in this chapter supposes that
the covariate space X is finite and that P(x = x, z = t) > 0, (t, x)  T × X.
These regularity conditions are maintained without further reference.

The Selection Problem
Researchers studying treatment response often want to predict the outcomes
that would occur under conjectural treatment rules in which all persons with
the same covariates receive the same treatment. Consider, for example, the
medical setting described earlier. Let the relevant covariate be age. Then
one treatment rule might mandate that all patients receive drugs, another
that all patients receive surgery, and yet another that older patients receive
drugs and younger ones receive surgery.
By definition, P[y(t) x = x] is the distribution of outcomes that would
occur if all persons with covariate value x were to receive a specified
treatment t. Hence prediction of outcomes under rules mandating uniform
treatment conditional on covariates requires inference on the distributions
{P[y(t) x], t  T}. The problem of identification of these distributions from
knowledge of P(y, z x) is commonly called the selection problem.
The selection problem has the same structure as the missing-outcomes
problem of Chapter 1 and Section 3.2. To see this, write
P[y(t) x = x]
= P[y(t) x = x, z = t]P(z = t x = x) + P[y(t) x = x, z g t]P(z g t x = x)
= P(y x = x, z = t)P(z = t x = x) + P[y(t) x = x, z g t]P(z g t x = x). (7.1)

7.1. Anatomy of the Problem

101

The first equality is the Law of Total Probability. The second holds because
y(t) is the outcome experienced by persons who receive treatment t. The
sampling process reveals P(y x = x, z = t), P(z = t x = x), and P(z g t x = x),
but it is uninformative about P[y(t) x = x, z g t]. Hence, the identification
region for P[y(t) x = x] using the empirical evidence alone is

{P[y(t) x = x]} =

{P(y x = x, z = t)P(z = t x = x) + P(z g t x = x), 

}. (7.2)

Y

Now, consider the collection of conjectural outcome distributions
{P[y(t) x], t  T}. The sampling process is jointly uninformative about the
counterfactual outcome distributions {P[y(t) x = x, z g t], t  T, x  X},
which can take any value in × (t, x)  T × X Y. Hence the identification region
for {P[y(t) x], t  T} using the empirical evidence alone is the Cartesian
product

{P[y(t) x], t  T} = × (t, x)  T × X {P[y(t) x = x]}.

(7.3)

Observe that the empirical evidence cannot refute the hypothesis that all
treatments have the same outcome distribution, conditional on x. Consider
the hypothesis: P[y(t)*x] = P[y(t1)*x], all (t, t1)  T × T. Identification
region (7.3) necessarily contains distributions that satisfy this hypothesis.
The easiest way to see this is to observe that the empirical evidence cannot
refute the much stronger hypothesis {yj(t) = yj, all (t, j)  T × J}; that is,
each person’s counterfactual outcomes could in principle be the same as the
outcome that he actually experiences.

Random Treatment Selection
A familiar “solution” to the selection problem is to assume that the status
quo treatment rule makes realized treatments statistically independent of
response functions, conditional on x; that is,
P[y(#) x] = P[y(#) x, z].

(7.4)

This assumption implies that, for each t  T and x  X,
P[y(t) x = x] = P[y(t) x = x, z = t] = P(y x = x, z = t).

(7.5)

The sampling process reveals P(y x = x, z = t). Hence, assumption (7.4)
point-identifies P[y(t) x = x].

102

7. Analysis of Treatment Response

Assumption 7.4 is credible in a classical randomized experiment, in
which an explicit randomization mechanism has been used to assign
treatments and all persons comply with their treatment assignments. The
credibility of the assumption in other applied settings almost invariably is
a matter of controversy.

The Task Ahead
This opening section introduced the general problem of predicting outcomes
under conjectural treatment rules and then examined basic elements of the
selection problem. The remainder of this chapter uses a social-planning
problem to motivate rules mandating uniform treatment conditional on
covariates and to show how the selection problem affects treatment choice.
Chapters 8 and 9 study the identifying power of some monotonicity
assumptions that may be credible and useful in the analysis of treatment
response. Chapter 10 studies the mixing problem; that is, the problem of
predicting outcomes under conjectural treatment rules that do not mandate
uniform treatment conditional on covariates.

7.2. Treatment Choice in Heterogeneous Populations
An important practical objective of empirical studies of treatment response
is to provide decision makers with information useful in choosing treatments. Often the decision maker is a planner who must choose treatments
for a heterogeneous treatment population. The planner may want to choose
treatments whose outcomes maximize the welfare of the treatment
population.
For example, consider a physician choosing medical treatments for a
population of patients. The physician may observe each patient’s demographic attributes, medical history, and the results of diagnostic tests. He
may then choose a treatment rule that makes treatment a function of these
covariates. If the physician acts on behalf of his patients, the outcome of
interest may be a measure of patient health status and welfare may be this
measure of health status minus the cost of treatment, measured in comparable units.
As another example, consider a judge choosing sentences for a population of convicted offenders. The judge may observe each offender’s past
criminal record, demeanor in court, and other attributes. Subject to
legislated sentencing guidelines, she may consider these covariates when
choosing sentences. If the judge acts on behalf of society, the outcome of
interest may be a measure of recidivism, and welfare may be this measure

7.2. Treatment Choice in Heterogeneous Populations

103

of recidivism minus the cost of carrying out a sentence.
I present here a simple formulation of the planner’s problem that
motivates inference on the outcome distributions {P[y(t) x], t  T} in
general and the conditional mean outcomes {E[y(t) x], t  T} in particular.
I first specify the planner’s choice set and objective function. I then derive
the optimal treatment rule.

The Choice Set
Suppose that there is a finite set T of mutually exclusive and exhaustive
treatments. A planner must choose a treatment for each member of the
treatment population, denoted J*. Each member j of population J* has
observable covariates xj  X and an unobservable response function yj($):
T  Y mapping treatments into real-valued outcomes.
The treatment population J* is identical in distribution to the study
population J, in which treatments have already been selected and outcomes
have been realized. Thus (J*, 6, P) is a probability space whose probability
measure P coincides with that of (J, 6, P). The only difference between J
and J* is that the status quo treatment rule z has been applied in the former
population, whereas a treatment rule has yet to be chosen in the latter.
There are no budgetary or other constraints that make it infeasible to
choose some treatment rules. However, the planner cannot distinguish
among persons with the same observed covariates and so cannot implement
treatment rules that systematically differentiate among these persons.
Hence, the feasible non-randomized rules are functions mapping the
observed covariates into treatments.1 Thus, uniform treatment conditional
on covariates emerges naturally out of the planner’s problem.
Formally, let Z(X) be the space of all functions mapping X into T. Let
z(#)  Z(X). Then feasible treatment rules have the form -j = z(xj), j  J*.

The Objective Function
Suppose that the planner wants to maximize population mean welfare. Let
the welfare obtained from assigning treatment t to person j have the additive
form yj(t) + c(t, xj). Here c(#, #): T × X  R is a real-valued cost function
known to the planner at the time of treatment choice. For each z(#)  Z(X),
let E{y[z(x)] + c[z(x), x]} denote the mean welfare that would occur if the
planner were to choose treatment rule z(#). Then the planner wants to solve
the problem
max
z(#)  Z(X)

E{y[z(x)] + c[z(x), x]}.

(7.6)

104

7. Analysis of Treatment Response

In the case of a physician, for example, yj(t) may measure the health
status of patient j following receipt of treatment t, and c(t, xj) may be the
(negative-valued) cost of treatment. The physician may know the costs of
alternative treatments but not their health outcomes. Similarly, in the case
of a judge, yj(t) may measure the rate of recidivism of offender j following
receipt of sentence t, and c(t, xj) may be the cost of carrying out the
sentence. Again, the judge may know the costs of alternative sentences but
not their recidivism outcomes.
The assumption that the planner wants to maximize population mean
welfare has normative, analytical, and practical appeal. This criterion
function is standard in the public economics literature on social planning,
which assumes that the objective is to maximize a utilitarian social welfare
function. Linearity of the expectation operator yields substantial analytical
simplifications, particularly through use of the law of iterated expectations.
The practical appeal is that, as shown below, a planner choosing treatments
to maximize mean welfare will want to learn mean treatment response, the
main statistic reported in empirical studies of treatment response.

The Optimal Treatment Rule
The solution to optimization problem (7.6) is to assign to each member of
the population a treatment that maximizes mean welfare conditional on the
person’s observed covariates. To show this, let 1[#] be the indicator function
taking the value one if the logical condition in the brackets holds and the
value zero otherwise. For each z(#)  Z(X), use the Law of Iterated
Expectations to write
E{y[z(x)] + c[z(x), x]} = E{E{y[z(x)] + c[z(x), x]*x}}
=  x  X P(x = x){ t  T {E[y(t)*x] + c(t, x)}#1[z(x) = t]}.

(7.7)

For each x  X, the quantity t  T {E[y(t)*x = x] + c(t, x)}#1[z(x) = t] is
maximized by choosing z(x) to maximize E[y(t)*x] + c(t, x) on t  T.
Hence, a treatment rule z*(#) is optimal if , for each x  X, z*(x) solves the
problem
max t  T E[y(t)*x = x] + c(t, x).

(7.8)

A planner who knows the conditional mean outcomes E[y(#)*x] 
{E[y(t) x = x], t  T, x  X} can implement the optimal treatment rule.
However, the selection problem and other identification problems limit the
information that studies of treatment response provide. Sections 7.3 and 7.4

7.3. The Selection Problem and Treatment Choice

105

show how the selection problem affects treatment choice. Complements 7A
and 7C consider the implications of other identification problems.

7.3. The Selection Problem and Treatment Choice
Let Y contain its lower and upper bounds y0  inf y  Y and y1  sup y  Y. For
t  T and x  X, use the Law of Iterated Expectations to write
E[y(t)*x = x] =
E(y*x = x, z = t)#P(z = t*x = x) + E[y(t)*x = x, z =/ t]#P(z =/ t*x = x). (7.9)
The empirical evidence reveals E(y*x = x, z = t) and P(z*x = x), but it is
uninformative about E[y(t)*x = x, z =/ t]. Hence, the identification region for
E[y(t)*x = x] using the empirical evidence alone is the closed interval

{E[y(t)*x = x]}
= [E(y*x = x, z = t)#P(z = t*x = x) + y0#P(z =/ t*x = x),
E(y*x = x, z = t)#P(z = t*x = x) + y1#P(z =/ t*x = x)].

(7.10)

This result is a direct application of Proposition 1.1.
The object of interest is the collection of conditional mean outcomes
E[y(#)*x]. Its identification region {E[y(#)*x]} is the rectangle

{E[y(#)*x]}
= × (t, x)  T × X [E(y*x = x, z = t)#P(z = t*x = x) + y0#P(z =/ t*x = x),
E(y*x = x, z = t)#P(z = t*x = x) + y1#P(z =/ t*x = x)]. (7.11)
This is the identification region because the empirical evidence is uninformative about the set of counterfactual means {E[y(t)*x = x, z =/ t], (t, x) 
T × X}.
{E[y(#)*x]} is a bounded, proper subset of (Y T × X) if Y has bounded
range. Suppose that this is so. Then, without loss of generality, outcomes
may be scaled to take values in the unit interval. Setting y0 = 0 and y1 = 1
gives {E[y(#)*x]} the simpler form

106

7. Analysis of Treatment Response

{E[y(#)*x]} = × (t, x)  T × X [E(y*x = x, z = t)#P(z = t*x = x),
E(y*x = x, z = t)#P(z = t*x = x) + P(z =/ t*x = x)].
(7.12)
The analysis in the remainder of this chapter assumes that Y has bounded
range and that outcomes have been scaled to lie in the unit interval. Thus
(7.12) henceforth gives the identification region for E[y(#)*x] using the
empirical evidence alone.

Dominated Treatment Rules
In general, the empirical evidence alone is not sufficiently informative
about E[y(#)*x] to enable solution of optimization problem (7.8). What
should the planner do?
Clearly, the planner should not choose a dominated treatment rule. (A
treatment rule z(#) is dominated if there exists another feasible rule, say
z1(#), that necessarily yields at least the mean welfare of z(#) and that
performs strictly better than z(#) in some possible state of nature.) The
rectangular form of {E[y(#)*x]} makes it easy to determine what treatment
rules are dominated.
Let (t, x)  T × X, and consider any rule that assigns treatment t to
persons with covariate value x. By (7.12), the mean welfare yielded by this
treatment choice can take any value in the interval

[E(y*x = x, z = t)#P(z = t*x = x) + c(t, x),
E(y*x = x, z = t)#P(z = t*x = x) + P(z =/ t*x = x) + c(t, x)].
The mean welfare of another treatment choice, say t1, can take any value in
the interval

[E(y*x = x, z = t1)#P(z = t1*x = x) + c(t1, x),
E(y*x = x, z = t1)#P(z = t1*x = x) + P(z =/ t1*x = x) + c(t1, x)].
Treatment t is definitely inferior to t1 if the upper bound of the former
interval is no larger than the lower bound of the latter one. This gives
Proposition 7.1.
Proposition 7.1: Let (t, x)  T × X. Using the empirical evidence alone, a
treatment rule that assigns treatment t to persons with covariates x is
dominated if and only if there exists a treatment t1  T such that

7.3. The Selection Problem and Treatment Choice

107

E(y*x = x, z = t)#P(z = t*x = x) + P(z =/ t*x = x) + c(t, x)

 E(y*x = x, z = t1)#P(z = t1*x = x) + c(t1, x).

(7.13)
a

The special case in which all treatments have the same cost is noteworthy. Then there generically are no dominated treatments. To see this, let
c(t, x) = c(t1, x) for all treatments t and t1. Then inequality (7.13) reduces to
E(y*x = x, z = t)#P(z = t*x = x) + P(z =/ t*x = x)

 E(y*x = x, z = t1)#P(z = t1*x = x).
Observe that P(z =/ t*x = x)  P(z = t1*x = x), E(y*x = x, z = t)  [0, 1], and
E(y*x = x, z = t1)  [0, 1]. Hence, this inequality can never hold strictly.
Moreover, it holds weakly only when P(z =/ t*x = x) = P(z = t1*x = x),
E(y*x = x, z = t) = 0, and E(y*x = x, z = t1) = 1.

Choice Among Undominated Treatments
The fact that the empirical evidence does not enable determination of the
optimal treatment rule does not imply that a planner should be paralyzed,
unwilling and unable to choose a rule. It implies only that the planner
cannot assert optimality for whatever rule he does choose.
The planner could, for example, reasonably apply the maximin rule,
which calls for persons with covariates x to be assigned the treatment that
maximizes the lower bound on E[y(#)*x = x]. By (7.12), the maximin rule
solves the problem
max t  T E(y*x = x, z = t)#P(z = t*x = x) + c(t, x).

(7.14)

This rule is simple to apply and to comprehend. From the maximin
perspective, the desirability of treatment t increases with E(y*x = x, z = t),
the mean outcome experienced by persons who received this treatment, and
with P(z = t*x = x), the fraction of persons who received treatment t. The
second factor gives form to the conservatism of the maximin rule—the more
prevalent a treatment was in the study population, the more expedient it is
to choose this treatment in the treatment population.

108

7. Analysis of Treatment Response

7.4. Instrumental Variables
Section 7.3 considered a planner who confronts the selection problem using
the empirical evidence alone. Credible distributional assumptions may
enable the planner to shrink the identification region for E[y(#)*x] and hence
shrink the set of undominated treatment rules. Many distributional
assumptions make use of instrumental variables.

Treatment-Specific Assumptions
The selection problem is a matter of missing outcome data, so all of the
analysis of Chapter 2 may be brought to bear. Thus, suppose that person j
is characterized by an observable covariate vj in a finite space V. Let
P(y, z, x, v) denote the joint distribution of (y, z, x, v). The covariate v
serving as an instrumental variable need not be distinct from the covariate
x used to make treatment choices, but it simplifies analysis if v contains
information not conveyed by x. Hence, the presentation here assumes that
P(v = v, z = t x) > 0 for all (v, t)  V × T.
Let t  T and x  X. To help identify E[y(t) x = x], a planner could
impose any of the distributional assumptions studied in Chapter 2. This
section shows how Assumptions MAR, SI, MMAR, and MI may be applied
to the analysis of treatment response. Assumptions MM and MMM will be
considered separately in Chapter 9.
In the context of treatment response, Assumptions MAR, SI, MMAR,
and MI are as follows.
Outcomes Missing-at-Random (Assumption MAR):
P[y(t) x = x, v] = P[y(t) x = x, v, z = t] = P[y(t) x = x, v, z g t].

(7.15)

Statistical Independence of Outcomes and Instruments (Assumption SI):
P[y(t) x = x, v] = P[y(t) x = x].

(7.16)

Means Missing-at-Random (Assumption MMAR):
E[y(t) x = x, v] = E[y(t) x = x, v, z = t] = E[y(t) x = x, v, z g t].

(7.17)

Mean Independence of Outcomes and Instruments (Assumption MI):
E[y(t) x = x, v] = E[y(t) x = x].

(7.18)

7.4. Instrumental Variables

109

These assumptions restrict the distribution of outcomes under a specified
treatment t for persons with specified covariates x. A planner can consider
each value of (t, x) in turn and decide what assumption to assert.
Assumptions MAR and MMAR point-identify E[y(t) x = x]. The other
assumptions generally do not yield point identification but do shrink the
identification region. Propositions 7.2 through 7.5 give the results. These
propositions are immediate extensions of corresponding ones in Chapter 2,
so proofs are omitted.
Proposition 7.2: Let assumption MAR hold. Then P[y(t) x = x] is pointidentified with
P[y(t) x = x] =  v  V P(y x = x, v = v, z = t)P(v = v x = x).

(7.19)
a

Proposition 7.3: Let assumption SI hold. Then the identification region for
P[y(t) x = x] is

SI{P[y(t) x = x]} = B {P(y x = x, v = v, z = t)P(z = t x = x, v = v)
vV

+

#P(z g t x = x, v = v),

v

v



}. (7.20)

Y

a
Proposition 7.4: Let assumption MMAR hold. Then E[y(t) x = x] is pointidentified with
E[y(t) x = x] =  v  V E(y x = x, v = v, z = t)P(v = v x = x).

(7.21)
a

Proposition 7.5: Let assumption MI hold. Then the identification region for
E[y(t) x = x] is the closed interval

MI{E[y(t) x = x]} = [max v  V E{y#1[z = t] x = x, v = v},
min v  V E{y#1[z = t] + 1[z g t] x = x, v = v}].

(7.22)
a

Statistical Independence of Response Functions and Instruments
Whereas the assumptions considered above are treatment-specific, one
could instead have information that restricts the joint distribution of the
response function y(#). An especially prominent assumption is

110

7. Analysis of Treatment Response

Statistical Independence of Response Functions and Instruments (Assumption SI-RF):
P[y(#) x = x, v] = P[y(#) x = x].

(7.23)

Assumption SI-RF strengthens assumption SI. The latter assumption, when
applied to all treatments, asserts that each component [y(t), t  T] of the
outcome vector y(#) is statistically independent of v. Assumption SI-RF
asserts that [y(t), t  T] are jointly independent of v.
The prominence of assumption SI-RF derives from its credibility when
the study population are subjects in a randomized experiment. In a
randomized experiment, the instrumental variable v designates the treatment
group in which each subject has been placed; thus V = T. Randomization
implies that y($) is statistically independent of the designated treatment v, so
assumption SI-RF holds.
The classical theory of randomized experiments assumes that all subjects
comply with their designated treatments; that is, z = v. In this special case,
application of Proposition 7.3 to each treatment t shows that P[y(t) x = x]
is point-identified, with

SI{P[y(t) x = x]} = P(y x = x, z = t).

(7.24)

This finding only uses assumption SI; it does not require the full force of
assumption SI-RF.
When some subjects do not comply, randomization of designated
treatments generally does not point-identify P[y(t) x = x]. In this case,
assumption SI-RF may have identifying power beyond that obtained when
assumption SI is applied to all treatments. However, the form of identification regions under assumption SI-RF is largely an open question.2

Complement 7A. Identification and Ambiguity
The treatment-choice problem examined in this chapter is an instance of
choice under ambiguity. In general, a decision maker with a known choice
set who wants to maximize an unknown objective function is said to face
a problem of choice under ambiguity. A common source of ambiguity is
partial knowledge of a probability distribution describing a relevant
population—the decision maker may know only that the distribution of
interest is a member of some set of distributions. This is the generic
situation of a decision maker who seeks to learn a population distribution
empirically, but whose data and other information do not point identify the

7A. Identification and Ambiguity

111

distribution. Thus, identification problems in empirical analysis generate
problems of choice under ambiguity.
The term ambiguity appears to originate in Ellsberg (1961), which posed
thought experiments in which subjects were asked to draw a ball from either
of two urns, one with a known distribution of colors and the other with an
unknown distribution of colors. Keynes (1921) and Knight (1921) used the
term uncertainty, but uncertainty has since come to be used to describe
optimization problems in which the objective function depends on a known
probability distribution.

Dominated Treatment Rules
Manski (2000) shows that the social planner of Section 7.2 faces a problem
of treatment choice under ambiguity whenever identification problems
prevent the planner from knowing enough about mean treatment response
to be able to determine the optimal rule.
Considering the matter in abstraction, suppose that a planner learns from
the available studies that E[y(#)*x] lies in some identification region
{E[y(#)*x]}. This information may not suffice to solve problem (7.8) but
may suffice to determine that some treatment rules are dominated.
A feasible treatment rule z(#) is dominated if there exists another feasible
rule, say z1(#), that necessarily yields at least the social welfare of z(#) and
that performs strictly better than z(#) in some possible state of nature. Thus
z(#)  Z(X) is dominated if there exists a z1(#)  Z(X) such that

 x  X P(x = x){ t  T [(t, x) + c(t, x)]#1[z(x) = t]}
  x  X P(x = x){ t  T [(t, x) + c(t, x)]#1[z1(x) = t]}
for all   {E[y(#)*x]} and

 x  X P(x = x){ t  T [(t, x) + c(t, x)]#1[z(x) = t]}
<  x  X P(x = x){ t  T [(t, x) + c(t, x)]#1[z1(x) = t]}
for some   {E[y(#)*x]}, where (t, x) is a feasible value of E[y(t)*x].

Choice Among Undominated Rules
The central difficulty of choice under ambiguity is that there is no clearly
best way to choose among undominated actions. Two common suggestions
are application of the maximin rule or a Bayes decision rule.

112

7. Analysis of Treatment Response

A planner using the maximin rule selects a treatment rule that maximizes
the minimum mean welfare attainable under all possible states of nature.
This means solution of the optimization problem
max

min

 xX P(x = x){ t  T [(t, x) + c(t, x)]#1[z(x) = t]},

z(#)Z*(X)   {E[y(#)*x]}

where Z*(X) denotes the set of undominated treatment rules.
Bayesian decision theorists recommend that a decision maker facing
ambiguity place a subjective distribution on the states of nature and
maximize expected welfare with respect to this distribution. In the treatment
choice context, the planner would place a probability measure % on the set
{E[y(#)*x]}, where % expresses the decision maker’s personal beliefs
about where E[y(#)*x] may lie within {E[y(#)*x]}. The planner would then
solve the optimization problem
max

,  x  X P(x = x){ t  T [(t, x) + c(t, x)]#1[z(x) = t]} d%.

z(#)  Z*(X)

The maximin rule and Bayes rules are reasonable ways to make
decisions under ambiguity, but there is no optimal way to behave in the
absence of credible information on the location of E[y(#)*x] within
{E[y(#)*x]}. Wald (1950), who proposed and studied the maximin rule,
did not contend that the rule is optimal, only that it is reasonable. Considering the case in which the objective is to minimize rather than maximize an
objective function, he wrote (Wald, 1950, p.18): “a minimax solution
seems, in general, to be a reasonable solution of the decision problem.”
Bayesians often present procedural rationality arguments for use of
Bayes decision rules. Savage (1954) showed that a decision maker whose
choices are consistent with a certain set of axioms can be interpreted as
using a Bayes rule. Many decision theorists consider the Savage axioms, or
other sets of axioms, to be a priori appealing. Acting in a manner that is
consistent with these axioms does not, however, imply that chosen actions
yield good outcomes. Berger (1985, p. 121) calls attention to this, stating:
“A Bayesian analysis may be ‘rational’ in the weak axiomatic sense, yet be
terrible in a practical sense if an inappropriate prior distribution is used.”

Complement 7B: Sentencing and Recidivism
The question of how judges should sentence convicted juvenile offenders
has long been of interest to policy makers and criminologists. Manski and

7B. Sentencing and Recidivism

113

Nagin (1998) analyzed data on the sentencing of 13,197 juvenile offenders
in Utah and their subsequent recidivism. We compared recidivism under the
two main sentencing options available to judges: confinement in residential
facilities (t = 1) and sentences that do not involve confinement (t = 0).
Let the outcome take the value y = 1 if an offender is convicted of a
subsequent crime in the two-year period following sentencing, and the value
y = 0 otherwise. The empirical distribution of treatments and outcomes
among the observed offenders was found to be as follows:
residential treatment: P(z = 1) = 0.11,
recidivism given residential treatment: P(y = 1*z = 1) = 0.77,
recidivism given nonresidential treatment: P(y = 1*z = 0) = 0.59.
The problem is to use this empirical evidence to draw conclusions about the
response probabilities P[y(1) = 1] and P[y(0) = 1].
The empirical evidence alone reveals that
P[y(1) = 1]  [0.08, 0.97]

P[y(0) = 1]  [0.53, 0.64].

If one assumes that judges sentence offenders randomly, then
P[y(1) = 1] = 0.77

P[y(0) = 1] = 0.59.

Random sentencing did not seem a credible assumption, so we considered
two alternative models of treatment selection. The outcome optimization
model assumes that judges aim to minimize the chance of recidivism. The
empirical evidence plus this assumption can be shown to imply that
P[y(1) = 1]  [0.61, 0.97]

P[y(0) = 1]  [0.61, 0.64].

The skimming model assumes that judges classify offenders as “higher risk”
or “lower risk,” sentencing only the former to residential confinement. The
empirical evidence plus this assumption can be shown to imply that
P[y(1) = 1]  [0.08, 0.77]

P[y(0) = 1]  [0.59, 0.64].

Thus, conclusions about response to treatment depend critically on the
assumptions imposed.

114

7. Analysis of Treatment Response

Complement 7C. Missing Outcome and Covariate Data
Studies of treatment response may have missing data for reasons other than
the selection problem. Researchers performing randomized experiments
may encounter data collection problems at the beginning of a trial that
result in missing covariate or treatment data for some subjects. Subsequently, attrition of subjects may prevent observation of some outcome
realizations. Similar problems occur in observational studies, where data on
covariates, treatments, or outcomes may be missing due to survey
nonresponse.
The analysis of Sections 7.3 and 7.4 assumed that the empirical evidence
reveals the distribution P(y, z, x) of (outcomes, treatments, covariates) under
the status quo treatment rule. The empirical evidence only partly identifies
this distribution when data are missing. Hence, missing data exacerbate the
planner’s problem.
In principle, it is easy to see how the selection problem and other
missing-data problems combine to determine the identification region for
E[y(#) x]. Consider the situation using the empirical evidence alone; similar
considerations apply when distributional assumptions are imposed. Let
[P(y, z, x)] denote the identification region for P(y, z, x) when some data
are missing; as shown in Chapter 3, the particular form of this region
depends on the missing data pattern.3 By (7.11), each feasible distribution
  [P(y, z, x)] generates an identification region for E[y(#) x] that
recognizes only the selection problem; that is, a region computed under the
assumption that P(y, z, x) = . Call this region {E[y(#)*x]}. The actual
identification region for E[y(#) x] must recognize the selection problem and
other missing-data problems. This region is F   {P(y, z, x)} {E[y(#)*x]}.
In practice, determination of the identification region for E[y(#) x] when
data are missing may be easy or difficult, depending on the specifics of the
situation. I provide an empirical illustration in the relatively simple context
of a classical randomized experiment, where mean treatment response
would be point-identified if all realizations of (y, z, x) were observed.

Choosing Treatments for Hypertension
Physicians routinely face the problem of choosing treatments for hypertension. Medical research has sought to provide guidance through the conduct
of randomized trials comparing alternative treatments. Such trials inevitably
have missing data. I illustrate here how physicians might use the data from
a recent trial to inform treatment choice, without imposing assumptions
about the distribution of the missing data.
Materson et al. (1993) and Materson, Reda, and Cushman (1995) present

7C. Missing Outcome and Covariate Data

115

findings from a U.S. Department of Veterans Affairs (DVA) trial of
treatments for hypertension. Male veteran patients at 15 DVA hospitals
were randomly assigned to one of six antihypertensive drug treatments or
to a placebo: hydrochlorothiazide (t = 1), atenolol (t = 2), captopril (t = 3),
clonidine (t = 4), diltiazem (t = 5), prazosin (t = 6), placebo (t = 7). The trial
had two phases. In the first, the dosage that brought diastolic blood pressure
(DBP) below 90 mm Hg was determined. In the second, it was determined
whether DBP could be kept below 95 mm Hg for a long time. Treatment
was defined to be successful if DBP < 90 mm Hg on two consecutive
measurement occasions in the first phase and DBP  95 mm Hg in the
second. Treatment was unsuccessful otherwise. Thus, the outcome of
interest is binary, with y = 1 if the criterion for success is met and y = 0
otherwise. Materson et al. (1993) recommend that physicians making
treatment choices consider this medical outcome variable as well as a
patient’s quality of life and the cost of treatment.
Among the covariates measured at the time of randomization, one was
the biochemical indicator “renin response,” taking the values x = (low,
medium, high). This covariate had previously been studied as a factor that
might be related to the probability of successful treatment (Freis, Materson,
and Flamenbaum, 1983). Renin-response data were missing for some
patients in the trial. Moreover, some patients dropped out of the trial before
their outcomes could be determined. The pattern of missing covariate and
outcome data is shown in Table 1 of Horowitz and Manski (2000),
reproduced here.
Table 7C.1 Missing Data in the DVA Hypertension Trial
Number
Observed None Missing
Treatment Randomized Successes Missing Only y
1
188
100
173
4
2
178
106
158
11
3
188
96
169
6
4
178
110
159
5
5
185
130
164
6
6
188
97
164
12
7
187
57
178
3

Missing
Only x
11
9
13
13
14
10
6

Missing
y and x
0
0
0
1
1
2
0

For each value of x, Horowitz and Manski (2000) estimated the
identification region for {P[y(t) = 1 x = x], t = 1, . . . , 7} using the
empirical evidence alone. Rather than report the identification regions for
these success probabilities, we reported the implied regions for the average
treatment effects {P[y(t) = 1 x = x]  {P[y(7) = 1 x = x], t = 1, . . . , 6},
which measure the efficacy of each treatment relative to the placebo. This

116

7. Analysis of Treatment Response

reporting decision was motivated by the traditional research problem of
testing the hypothesis of zero average treatment effect. We did not
explicitly examine the implications for treatment choice.
Table 7C.2 reports the estimates of the identification regions for the
success probabilities themselves. To keep attention focused on the
identification problem, suppose that the estimates are the actual identification regions rather than finite sample estimates. Consider a physician who
accepts the DVA success criterion, observes renin response, and has no
prior information on mean treatment response or the distribution of missing
data. Suppose that all treatments have the same cost. How might this
physician choose treatments in a population similar to that studied in the
DVA trial?
Table 7C.2: Identification Regions for Success Probabilities
Conditional on Renin Response
Renin
Response
1
2
3
Low
[.54, .61] [.52, .62] [.43, .53]
Med
[.47, .62] [.60, .74] [.53, .68]
High
[.28, .50] [.64, .86] [.56, .75]

Treatment
4
5
6
7
[.58, .66] [.66, .76] [.54, .65] [.29, .32]
[.50, .69] [.68, .85] [.41, .65] [.27, .32]
[.63, .84] [.55, .78] [.34, .59] [.28, .40]

The physician should eliminate from consideration the dominated treatments. For patients with low renin response, treatments 1, 2, 3, 4, 6, and 7
are all dominated by treatment 5, which has the greatest lower bound (.66).
For patients with medium renin response, treatments 1, 3, 6, and 7 are
dominated by treatment 5, which again has the greatest lowest bound (.68).
For patients with high renin response, treatments 1, 6, and 7 are dominated
by treatment 2, which has the greatest lowest bound in this case (.64). Thus,
without imposing any distributional assumptions, the physician can reject
treatments 1, 6, and 7 for all patients, can reject treatment 3 for patients with
medium renin response, and can determine that treatment 5 is optimal for
patients with low renin response.
In the absence of assumptions about the distribution of missing data, it
is not possible to give the physician guidance on how to choose among
undominated treatments for patients with medium and high renin response.
A physician using the maximin rule would choose treatment 5 for patients
with medium renin response and treatment 2 for patients with high renin
response. This is a reasonable treatment rule, but one cannot say that it is an
optimal rule.

7D. Study and Treatment Populations

117

Complement 7D. Study and Treatment Populations
A longstanding issue in the analysis of treatment response concerns the
importance of correspondence between the study population and the
treatment population. This matter was downplayed in the influential work
of Donald Campbell, who argued that studies of treatment response should
be judged primarily by their internal validity and only secondarily by their
external validity (e.g., Campbell and Stanley, 1963; Campbell, 1984).
Campbell’s view has been endorsed by Rosenbaum (1999), who recommends that observational studies of human subjects aim to approximate the
conditions of laboratory experiments (p. 263):
“In a well-conducted laboratory experiment one of the rarest of things happens:
The effects caused by treatments are seen with clarity. Observational studies of
the effects of treatments on human populations lack this level of control but the
goal is the same. Broad theories are examined in narrow, focused, controlled
circumstances.”

Rosenbaum, like Campbell, downplays the importance of having the study
population be similar to the population of interest, writing (p.259):
“Studies of samples that are representative of populations may be quite useful
in describing those populations, but may be ill-suited to inferences about
treatment effects.”

From the perspective of treatment choice, the Campbell–Rosenbaum
position is well-grounded if treatment response is homogeneous across
persons. Then researchers can aim to learn about treatment response in
easy-to-analyze study populations and planners can be confident that
research findings can be extrapolated to populations of interest. In human
populations, however, homogeneity of treatment response may be the
exception rather than the rule. Whether the context be medical, educational
or social, it is common to find that people vary in their response to
treatment. To the degree that treatment response is heterogeneous, a planner
cannot readily extrapolate research findings from a study population to a
treatment population, as optimal treatments in the two may differ. Hence
correspondence between the study population and the treatment population
assumes considerable importance.
A specific instance of the general issue arises in research on partial
compliance in randomized experiments. Suppose that the study population
is formed by drawing experimental subjects at random from the treatment
population and randomly designating the treatments they should receive.
When some subjects do not comply with their designated treatments,

118

7. Analysis of Treatment Response

Imbens and Angrist (1994) and Angrist, Imbens, and Rubin (1996) have
proposed that treatment effects be reported for the sub-population of
“compliers,” persons who would comply with their designated experimental
treatments whatever they might be. A planner can extrapolate findings on
treatment effects for compliers to the treatment population if treatment
response is homogeneous but not to the degree that it is heterogeneous.
Indeed, a planner cannot even use findings for compliers to make treatment
choices in this particular subpopulation. The reason is that compliers are not
individually identifiable. Each subject in an experiment is placed in one of
a set of mutually exclusive treatment groups; hence it is not possible to
observe whether a given person would comply with all possible treatment
designations.
From the perspective of treatment choice in heterogeneous populations,
I see no reason to give internal validity primacy relative to external validity.
I am unable to motivate interest in the sub-population of compliers. To be
fair, researchers who have stressed internal validity and those who have
focused attention on compliers have not necessarily asserted that the
objective of their research is to inform treatment choice. For example,
Angrist, Imbens, and Rubin (1996) view their goal as the discovery of
“causal effects,” without reference to a treatment-choice problem.

Endnotes
Sources and Historical Notes
This chapter paraphrases and extends ideas introduced in Manski (1990,
2000, 2002).
A large and diverse literature on the evaluation of social programs seeks
to compare the outcomes that a population of interest would experience if
the members of the population were to receive alternative treatments. The
idea of a planner who must choose a treatment rule is implicit in much of
this literature, but explicit consideration of the planner’s decision problem
has been rare until recently. Stafford (1985, pp. 112–114) was an early
proponent of the idea.
This chapter views the matter from the perspective of welfare economics,
in which a planner aims to maximize a utilitarian social welfare function.
Some research on program evaluation adopts a different perspective, in
which the aim is to compare a specified “base” or “default” treatment rule
z1 with an alternative z2. The object of interest is P[y(z2)  y(z1)], which
measures the distribution of changes in outcomes that would be experienced
if the base treatment rule z1 were to be replaced by the proposed rule z2. For

Endnotes

119

example, Heckman, Smith, and Clements (1997) write: “Answers to many
interesting evaluation questions require knowledge of the distribution of
program gains.”

Text Notes
1. Although the planner cannot systematically differentiate among persons
with the same observed covariates, he can randomly assign different
treatments to such persons. Thus, the set of feasible treatment rules in
principle contains not only functions mapping covariates into treatments but
also probability mixtures of these functions. Explicit consideration of
randomized treatment rules would not substantively change the present
analysis, but would complicate the necessary notation. A simple implicit
way to permit randomized rules is to include in x a component whose value
is randomly drawn by the planner from some distribution. The planner can
then make the chosen treatment vary with this covariate component.
2. Analysis to date has been limited to the special case in which the
outcome is a binary random variable. Let Y = {0, 1}. Robins (1989) posed
the question in this case but only obtained an outer identification region for
{P[y(0)], P[y(1)]}. Balke and Pearl (1997) showed that the identification
region under assumption SI-RF is the set of solutions to a certain linear
programming problem. They presented numerical examples in which this
region sometimes (but not always) is smaller than the one obtained using
assumption SI, which is equivalent to assumption MI when response is
binary.
3. Chapter 3 covers cases in which outcome and/or covariate data are
missing but not ones in which treatment data are missing. Molinari (2002)
studies this problem.

8
Monotone Treatment Response
8.1. Shape Restrictions
Empirical researchers studying treatment response sometimes have credible
information about the shape of the response functions y(#). In particular, one
may have reason to believe that outcomes vary monotonically with the
intensity of the treatment. Let the set T of treatments be ordered in terms of
degree of intensity. The assumption of monotone treatment response asserts
that, for all persons j and all treatment pairs (s, t),
t  s < yj(t)  yj(s).

(8.1)

This chapter studies the selection problem when response functions are
assumed to be monotone in treatments (Section 8.2) or to obey the related
shape restrictions of semi-monotonicity (Section 8.3) or concave
monotonicity (Section 8.4). Nothing is assumed about the process of
treatment selection in the study population, and no cross-person restrictions
on response are imposed. The findings reported here may be useful to a
planner making treatment choices, as described in Chapter 7, but the present
analysis does not presume that the objective is to solve a planning problem.
The purpose may simply be to learn about the distribution of treatment
response in the study population.

Production Analysis
There are many applied settings in which a planner or researcher may be
confident that response is monotone, semi-monotone, or concave-monotone,
120

8.1. Shape Restrictions

121

but be wary of assuming anything else. Economic analysis of production
provides a good illustration.
Production analysis typically supposes that firms or other entities use
inputs to produce a scalar output; thus, the input is the treatment and the
output is the response. Firm j has a production function yj($) mapping inputs
into product output, so yj(t) is the output that firm j produces when t is the
input vector. The most basic tenet of the economic theory of production is
that output weakly increases with the level of the inputs. If there is a single
input (say labor), this means that treatment response is monotone. If there
is a vector of inputs (say labor and capital), treatment response is semimonotone.
Formally, suppose that there are K inputs, and let s  (s1, s2, . . . , sK) and
t  (t1, t2, . . . , tK) be two input vectors. Production theory predicts that yj(t)
 yj(s) if input vector t is at least as large, component-by-component, as
input vector s; that is, if tk  sk, all k = 1, . . . , K. Production theory does not
predict the ordering of yj(t) and yj(s) when the input vectors t and s are
unordered, each having some components larger than the other. Thus,
production functions are semi-monotone.
Consider, for example, the production of corn. The inputs include land
and seed. The output is bushels of corn. Production theory predicts that the
quantity of corn produced by a farm weakly increases with its input of land
and seed. Production theory does not predict the effect on corn production
of increasing one input component and decreasing the other.
Economists typically assume more than that production functions are
semi-monotone. They often assume that production functions exhibit
diminishing marginal returns, which means that the production function is
concave in each input component, holding the other components fixed. In
so-called short-run production analysis, researchers distinguish two types
of inputs: variable inputs whose values can be changed and fixed inputs
whose values cannot be changed. Short-run production analysis performs
thought experiments in which the variable inputs are varied and the fixed
inputs are held fixed at their realized values. Thus, the variable inputs are
considered to be treatments, the fixed inputs to be covariates, and the shortrun production function maps variable inputs into output. Suppose that there
is one variable input. Then it is common to assume that the short-run
production function yj($) is concave-monotone in this input. In the short-run
production of corn, for example, seed would usually be thought of as the
variable input and land as the fixed input. A researcher might find it
plausible to assume that, holding land fixed, output of corn rises with the
input of seed but with diminishing returns.
Economists studying production often find it difficult to justify
assumptions on the shapes of production functions that go beyond concave

122

8. Monotone Treatment Response

monotonicity. Empirical researchers may employ tight parametric models
of production, but they rarely can do more than assert on faith that such
models are adequate “approximations” to actual production functions.
Economists studying production also find it difficult to justify other
assumptions that may have identifying power, such as the assumptions
using instrumental variables that were discussed in Section 7.4. In
particular, it usually makes little economic sense to assume that the input
vectors chosen by firms are randomly selected.

D-Outcomes and D-Treatment Effects
The shape restrictions studied in this chapter have particular power to
identify parameters of outcome distributions that respect stochastic
dominance. Let D(#) be such a parameter and let t be a treatment. Propositions developed below give sharp lower and upper bounds for the Doutcome D[y(t)] when treatment response is assumed to be monotone, semimonotone, or concave-monotone. Corollaries apply the findings to specific
D-parameters, including quantiles and means of increasing functions of
outcomes. The sharp bounds obtained here are, by definition, the endpoints
of the identification regions for the parameters of interest under the
maintained shape restrictions. The propositions do not assert that an
identification region is the entire interval connecting its endpoints. This is
so for expectations but not necessarily for other D-parameters.
Bounds are also obtained for two types of D-treatment effects, namely
D[y(t)]  D[y(s)] and D[y(t)  y(s)], where t  T and s  T are specified
treatments. These two treatment effects coincide when D(#) is the expectation functional but otherwise may differ. To distinguish them,
D[y(t)]  D[y(s)] is henceforth called a D-treatment effect and D[y(t) 
y(s)] a D -treatment effect. In all cases but one (Proposition 8.9), the
reported bounds are sharp.
The propositions developed below apply immediately to inference on
conditional D-outcomes and D-treatment effects of the form D[y(t) x],
D[y(t) x] - D[y(s) x], and D[y(t)  y(s) x], where x is an observable
covariate. One simply needs to redefine the population of interest to be the
sub-population of persons who share a specified value of x. To simplify
notation, the analysis does not explicitly condition on x.
Throughout this chapter, the outcome space Y is a closed subset of the
extended real line. Whereas the set T of treatments was assumed to be finite
in most of Chapter 7, this cardinality assumption is not maintained here. In
Section 8.2, T is an ordered set of arbitrary cardinality. In Section 8.3, T is
a semi-ordered set of arbitrary cardinality. The treatment set has more
structure in Section 8.4, where it is a closed interval on the real line.

8.2. Monotonicity

123

8.2. Monotonicity
This section assumes that response is weakly increasing in treatments, as
stated in equation (8.1). With obvious modifications, the findings apply
when response is weakly decreasing in treatments. The analysis first
develops sharp bounds for D-outcomes and then for D-treatment effects.

D-Outcomes
Proposition 8.1 presents the sharp bound on D-outcomes under the
assumption of monotone treatment response.
Proposition 8.1: Let T be ordered. Let yj($), j  J be weakly increasing on
T. Define
y0j(t)  yj if t  zj
 y0 otherwise,

(8.2a)

y1j(t)  yj if t  zj
 y1 otherwise.

(8.2b)

Then, for every t  T,
D[y0(t)]  D[y(t)]  D[y1(t)].

(8.3)

a

This bound is sharp.
Proof: Monotonicity of yj($) implies this sharp bound on yj(t):
t < zj < y0  yj(t)  yj,
t = zj < yj(t) = yj,

(8.4)

t > zj < yj  yj(t)  y1.
Equivalently,
y0j(t)  yj(t)  y1j(t).

(8.5)

There are no cross-person restrictions, so the sharp bound on {yj(t), jJ} is
y0j(t)  yj(t)  y1j(t), j  J.

(8.6)

124

8. Monotone Treatment Response

Hence, the random variable y0(t) is stochastically dominated by y(t), which
in turn is stochastically dominated by y1(t). This shows that (8.3) is a bound
on D[y(t)].
The bound (8.3) is sharp because the bound (8.6) is sharp; that is, the
empirical evidence and prior information are consistent with the hypothesis
{yj(t) = y0j(t), j  J} and also with the hypothesis {yj(t) = y1j(t), j  J}.
Q. E. D.
Proposition 8.1 shows that the assumption of monotone treatment
response qualitatively reduces the severity of the selection problem. Using
the empirical evidence alone, an observed outcome realization yj is
informative about outcome yj(t) only if zj = t; then yj = yj(t). Using the
empirical evidence and the monotone-response assumption, observation of
yj always yields an informative lower or upper bound on yj(t), as shown in
equation (8.4).
Proposition 8.1 is simple to state and prove, but it is too abstract to give
a clear sense of the identifying power of the monotonicity assumption. This
emerges in Corollaries 8.1.1 through 8.1.3, which apply the proposition to
upper tail probabilities, the means of increasing functions, and quantiles. In
each case, the corollary is obtained by evaluating D[y0(t)] and D[y1(t)] for
the D-parameter of interest.
Corollary 8.1.1: Let f($): R  R be weakly increasing. Then
f(y0)P(t < z) + E[f(y)*t  z]$P(t  z)  E{f[y(t)]}

 f(y1)P(t > z) + E[f(y)*t  z]$P(t  z).

(8.7)
a

Corollary 8.1.2: Let   (0, 1). Let Q(u) denote the –quantile of a real
random variable u. Let 0  [  P(t < z)]/P(t  z) and 1  /P(t  z). Then
0 <   P(t < z) < y0  Q[y(t)]  Q1(y*t  z),
P(t < z) <   P(t  z) < Q0(y*t  z)  Q[y(t)]  Q1(y*t  z),

(8.8)

P(t  z) <  < 1 < Q0(y*t  z)  Q[y(t)]  y1.

a
It is revealing to compare Corollary 8.1.1, which exploits the assumption
of monotone response, with the bound on E{f[y(t)]} using the empirical

8.2. Monotonicity

125

evidence alone. The bound using the empirical evidence alone is
f(y0)P(t =/ z) + E[f(y)*t = z]$P(t = z)  E{f[y(t)]}

 f(y1)P(t =/ z) + E[f(y)*t = z]$P(t = z).

(8.9)

Whereas this bound draws information about E{f[y(t)]} only from the (y, z)
pairs with t = z, all (y, z) pairs are informative under the monotone-response
assumption. The lower bound in Corollary 8.1.1 draws information from the
persons with t  z, and the upper bound draws information from those with
t  z.
Corollary 8.1.1 may be used to obtain sharp bounds on tail probabilities.
Let r  (y0, y1]. The indicator function 1[y(t)  r] is an increasing function
of y(t) and E{1[y(t)  r]} = P[y(t)  r], so inequality (8.7) reduces to
P(t  z  y  r)  P[y(t)  r]  P(t > z F y  r).

(8.10)

The informativeness of this bound depends on the distribution P(y, z) of
realized treatments and outcomes. Suppose that P(t  z  y  r) = 0 and
P(t > z F y  r) = 1. Then (8.10) is the trivial bound 0  P[y(t)  r]  1.
Suppose however that P(t  z  y  r) = P(t > z F y  r). Then the assumption of monotone treatment response point-identifies P[y(t)  r].
Corollary 8.1.2 shows that the monotone response generically yields a
one-sided bound on quantiles of y(t). The upper bound is informative when
  P(t  z). The lower bound is informative when  > P(t < z). These cases
exhaust the possibilities if P(t = z) = 0. The lower and upper bounds are
both informative if P(t = z) > 0 and P(t < z) <   P(t  z).

D-Treatment Effects
Propositions 8.2 and 8.3 present the sharp bounds on the D-treatment effects
D[y(t)] - D[y(s)] and D[y(t)  y(s)].
Proposition 8.2: Let T be ordered. Let yj($), j  J be weakly increasing on
T. Then for every t  T and s  T with t > s,
0  D[y(t)]  D[y(s)]  D[y1(t)]  D[y0(s)].
This bound is sharp.

(8.11)

a

Proof: Monotonicity of response implies that y(t) stochastically dominates
y(s), so 0 is a lower bound on D[y(t)]  D[y(s)]. Proposition 8.1 implies that

126

8. Monotone Treatment Response

D[y1(t)]  D[y0(s)] is an upper bound. We need to prove that these bounds
are sharp.
Let j  J. Monotonicity of yj($) gives this sharp bound on {yj(t), yj(s)}:
s < t < zj < y0  yj(s)  yj(t)  yj,
s < t = zj < y0  yj(s)  yj(t) = yj,
s < zj < t < y0  yj(s)  yj  yj(t)  y1,

(8.12)

s = zj < t < yj = yj(s)  yj(t)  y1,
zj < s < t < yj  yj(s)  yj(t)  y1.
There are no cross-person restrictions, so the empirical evidence and prior
information are consistent with the hypothesis {yj(s) = yj(t), j  J} and also
with the hypothesis {yj(t) = y1j(t), yj(s) = y0j(s), j  J}. Hence (8.11) is sharp.
Q. E. D.
Proposition 8.3: Let T be ordered. Let yj($), j  J be weakly increasing on
T. Then for every t  T and s  T with t > s,
D(0)  D[y(t)  y(s)]  D[y1(t)  y0(s)].

(8.13)

a

This bound is sharp.

Proof: The proof to Proposition 8.2 showed that the sharp joint bound on
{yj(t) - yj(s), j  J} is
0  yj(t)  yj(s)  y1j(t)  y0j(s),

j  J.

(8.14)

Hence, the degenerate distribution with all mass at 0 is stochastically
dominated by y(t)  y(s), which in turn is dominated by y1(t)  y0(s). Thus,
(8.13) is a bound on D[y(t)  y(s)]. This bound is sharp because bound
(8.14) is sharp.
Q. E. D.
Observe that the lower bounds in Propositions 8.2 and 8.3, namely 0 and
D(0), are implied by the monotone-response assumption and do not depend
on the empirical evidence. The monotone-response assumption and the
empirical evidence together determine the upper bounds.
Propositions 8.2 and 8.3 generically give distinct bounds for distinct

8.3. Semi-monotonicity

127

treatment effects, but these bounds coincide when D($) is the expectation
functional. Proposition 8.2 and Corollary 8.1.1 yield the following.
Corollaries 8.2.1 and 8.3.1:
0  E[y(t)]  E[y(s)] = E[y(t)  y(s)]

 y1$P(t > z) + E(y*t  z)$P(t  z)  y0$P(s < z)  E(y*s  z)$P(s  z).
(8.15)
This bound is sharp.

a

This result takes a particularly simple form when outcomes are binary.
Let Y be the two-element set {0, 1}. Then y0 = 0, y1 = 1, and (8.15)
becomes
0  P[y(t) = 1]  P[y(s) = 1] = P[y(t)  y(s) = 1]

 P(y = 0, t > z) + P(y = 1, s < z).

(8.16)

8.3. Semi-Monotonicity
In this section, treatments are K-dimensional vectors and T is a semiordered set of treatment vectors. The notation s L t indicates that a pair (s, t)
is not ordered. Analysis of semi-monotone response uses much of the
structure developed in Section 8.2, and so is amenable to succinct presentation. When T is semi-ordered, the definitions of y0j(t) and y1j(t) in (8.2)
remain valid, the term “otherwise” now including the possibility that t L zj.

D-Outcomes
Proposition 8.4 gives the semi-monotone-response version of Proposition
8.1. Observe that the conclusion to Proposition 8.1 still holds and the proof
requires only slight modification.
Proposition 8.4: Let T be semi-ordered. Let yj($), j  J be weakly increasing
on the ordered pairs in T. Then, for every t  T,
D[y0(t)]  D[y(t)]  D[y1(t)].
This bound is sharp.

(8.17)

a

128

8. Monotone Treatment Response

Proof: Let j  J. Semi-monotonicity of yj($) implies this sharp bound on
yj(t):
t < zj < y0  yj(t)  yj,
t = zj < yj(t) = yj,
t > zj < yj  yj(t)  y1,

(8.18)

t L zj < y0  yj(t)  y1.
Thus (8.5) holds. The rest of the proof is the same as the proof to Proposition 8.1.
Q. E. D.
Although Propositions 8.1 and 8.4 have the same stated conclusion,
weakening the assumption of monotone response to semi-monotone
response is consequential. Suppose that an ordering of the treatment set T
is weakened to a semi-ordering. Each time that a pair (t, zj) with t > zj
becomes unordered, y0j(t) falls from yj to y0. Each time that a pair (t, zj) with
t < zj becomes unordered, y1j(t) rises from yj to y1. Hence, the ordered-T
version of y0(t) stochastically dominates its semi-ordered-T counterpart, and
the ordered-T version of y1(t) is stochastically dominated by its semiordered-T counterpart. Thus, weakening the assumption of monotone
response to semi-monotone response widens the bound on D[y(t)]. In the
extreme case where T is entirely unordered, Proposition 8.4 gives the bound
on D[y(t)] obtained using the empirical evidence alone.
Corollaries 8.4.1 and 8.4.2 give the semi-monotone-response versions of
Corollaries 8.1.1 and 8.1.2. The explicit forms for D[y0(t)] and D[y1(t)] in
these corollaries show clearly how weakening the assumption of monotone
response to semi-monotone response affects the bounds on D-outcomes.
Corollary 8.4.1: Let f($): R  R be weakly increasing. Then
f(y0)P(t < z F t L z) + E[f(y)*t  z]$P(t  z)  E{f[y(t)]}

 f(y1)P(t > z F t L z) + E[f(y)*t  z]$P(t  z).

(8.19)
a

Corollary 8.4.2: Let   (0, 1). Let 0  [  P(t < z F t L z)]/P(t  z) and
1  /P(t  z). Then

8.3. Semi-monotonicity

129

0 <   P(t < z F t L z) < y0  Q[y(t)],
P(t < z F t L z) <  < 1 < Q0(y*t  z)  Q[y(t)],
0 <   P(t  z) < Q[y(t)]  Q1(y*t  z),

(8.20)

P(t  z) <  < 1 < Q[y(t)]  y1.

a
Corollary 8.4.1 may be used to obtain sharp bounds on tail probabilities.
The result is
P(t  z  y  r)  P[y(t)  r]  P(t > z F t L z F y  r).

(8.21)

D-Treatment Effects
When T is semi-ordered, the conclusions to Propositions 8.2 and 8.3 still
hold if t > s. The upper bounds still hold if t L s, but the lower bounds need
to be modified. Propositions 8.5 and 8.6 give these extensions to the earlier
results.
Proposition 8.5: Let T be semi-ordered. Let t  T and s  T. Let yj($), j  J
be weakly increasing on the ordered pairs in T. For t > s, the sharp bound
on D[y(t)]  D[y(s)] is
0  D[y(t)]  D[y(s)]  D[y1(t)]  D[y0(s)].

(8.22)

For t L s, the sharp bound is
D[y0(t)]  D[y1(s)]  D[y(t)]  D[y(s)]  D[y1(t)]  D[y0(s)].

(8.23)
a

Proof: Let t > s. Semi-monotonicity of response implies that y(t)
stochastically dominates y(s), so 0 is a lower bound on D[y(t)]  D[y(s)].
Proposition 8.4 implies that D[y1(t)]  D[y0(s)] is an upper bound. To prove
that these bounds are sharp, consider j  J. If s, t, and zj are ordered, (8.12)
still gives the sharp joint bound on yj(t) and yj(s). If zj L t and/or zj L s, the
sharp joint bound on yj(t) and yj(s) is

130

8. Monotone Treatment Response
s < t  s < zj < y0  yj(s)  yj  yj(s)  yj(t)  y1,
s < t  zj < t < y0  yj(s)  yj(t)  yj  yj(t)  y1,

(8.24)

s < t < y0  yj(s)  yj(t)  y1.
The rest of the proof is the same as the proof to Proposition 8.2.
Let s L t. Proposition 8.4 implies that (8.23) is a bound on D[y(t)] D[y(s)]. For each j  J, the sharp joint bound on yj(t) and yj(s) is
y0j(t)  yj(t)  y1j(t),
y0j(s)  yj(s)  y1j(s).

(8.25)

There are no cross-person restrictions, so the empirical evidence and prior
information are consistent with the hypothesis {yj(t) = y0j(t) and yj(s) =
y1j(s), j  J} and with the hypothesis {yj(t) = y1j(t) and yj(s) = y0j(s), j  J}.
Hence (8.23) is sharp.
Q. E. D.
Proposition 8.6: Let T be semi-ordered. Let t  T and s  T. Let yj($), j  J
be weakly increasing on the ordered pairs in T. For t > s, the sharp bound
on D[y(t)  y(s)] is
D(0)  D[y(t)  y(s)]  D[y1(t)  y0(s)].

(8.26)

For s L t, the sharp bound is
D[y0(t)  y1(s)]  D[y(t)  y(s)]  D[y1(t)  y0(s)].

(8.27)
a

Proof: Let t > s. By (8.12) and (8.24), the sharp bound on {yj(t)  yj(s),
j  J} is
0  yj(t)  yj(s)  y1j(t)  y0j(s), j  J.

(8.28)

The rest of the proof is the same as the proof to Proposition 8.3.
Let s L t. By (8.25), the sharp joint bound on {yj(t)  yj(s), j  J} is
y0j(t)  y1j(s)  yj(t)  yj(s)  y1j(t)  y0j(s), j  J.

(8.29)

8.3. Semi-monotonicity

131

Hence (8.27) is the sharp bound on D[y(t)  y(s)].
Q. E. D.

Testing the Hypothesis of Semi-monotone Response
Whereas Propositions 8.4 through 8.6 take semi-monotone treatment
response as a maintained assumption, one may instead want to view it as a
hypothesis to be tested. It is easy to see that the hypothesis is not refutable
in isolation. For each j  J, only one point on the response function yj($) is
observable, namely yj(zj). Hence, the empirical evidence is necessarily
consistent with the hypothesis that yj($) is weakly increasing on the ordered
pairs in T. In particular, the empirical evidence is consistent with the
hypothesis that every response function is flat, with {yj(t) = yj, t  T, j  J}.
A researcher wanting to test the hypothesis of semi-monotone response
can do so only if this hypothesis is joined with other assumptions. Consider
assumption SI-RF, which states that z is statistically independent of y($);
that is,
P[y($)] = P[y($)*z].

(8.30)

The joint hypothesis of semi-monotone response and assumption SI-RF is
refutable. The key is Proposition 8.7.
Proposition 8.7: Let T be semi-ordered. Let t > s. Let yj($), j  J be weakly
increasing on the ordered pairs in T. Let z be statistically independent of
a
y($). Then P(y*z = t) stochastically dominates P(y*z = s).
Proof: Semi-monotonicity implies that y(t) stochastically dominates y(s).
Assumption SI-RF implies that P[y(s)] = P(y*z=s) and P[y(t)] = P(y*z = t).
Q. E. D.
Empirical knowledge of the distribution P(y, z) of realized treatments
and outcomes implies knowledge of P(y*z = s) and P(y*z = t) for s and t on
the support of P(z), so Proposition 8.7 yields this test:
Reject the joint hypothesis of semi-monotone treatment response and
assumption SI-RF if there exist s  T and t  T on the support of P(z)
such that t > s but P(y*z = t) does not stochastically dominate P(y*z = s).
In finite-sample practice, a researcher observing a random sample of (y, z)
pairs can estimate P(y*z = t) and P(y*z = s) and form an asymptotically
valid version of the test.

132

8. Monotone Treatment Response

There are three ways to interpret an empirical finding that P(y*z = t)
does not stochastically dominate P(y*z = s). Researchers who have
confidence in assumption SI-RF would conclude that response is not semimonotone. Researchers confident that response is semi-monotone would
conclude that assumption SI-RF does not hold. Other researchers would
conclude only that some part of the joint hypothesis is incorrect.

8.4. Concave Monotonicity
Whereas Section 8.3 weakened the assumption of monotone response to
semi-monotonicity, this section strengthens the assumption. In particular,
yj($), j  J now are concave-monotone functions. Moreover, T = [0, -] for
some -  (0, ] and Y = [0, ]. The important feature of this specification
of T and Y is that these sets are closed intervals with finite lower bounds.
Specifying that the lower bounds of T and Y are zero and that the upper
bound of Y is  merely permits some simplification in the analysis.1
The analysis uses this fact: Given three points (vm, wm)  [0, ]², m = 1,
2, 3, with 0 < v1 < v2 < v3, there exists a concave-monotone function
mapping [0, -]  [0, ] and passing through the three points if and only if
w1/v1  (w2  w1)/(v2  v1)  (w3  w2)/(v3  v2)  0.

(8.31)

Here w1/v1 is the slope of the line segment connecting the origin to (v1, w1),
and (wm  wm)1)/(vm  vm-1) is the slope of the line segment connecting
(vm1, wm1) to (vm, wm), m = 2, 3. In particular, the piecewise linear function
passing through the origin and the three points is concave-monotone if and
only if (8.31) holds.

D-Outcomes
Proposition 8.8 presents the sharp bound on D-outcomes under the
assumption of concave-monotone treatment response.
Proposition 8.8: Let T = [0, -] and Y = [0, ]. Let yj($), j  J be concave and
weakly increasing on T. Define
yc0j(t)  yj if t  zj
 yjt/zj otherwise,

(8.32a)

yc1j(t)  yj if t  zj
 yjt/zj otherwise.

(8.32b)

8.4. Concave Monotonicity

133

Then, for every t  T,
D[yc0(t)]  D[y(t)]  D[yc1(t)].

(8.33)

a

This bound is sharp.

Proof: For j  J, yj(#) is a concave-monotone function passing through
(zj, yj) and [t, yj(t)]. Application of (8.31) yields this sharp bound on yj(t):
t < zj < y(t)/t  [yj  y(t)]/(zj  t)  0
< yjt/zj  yj(t)  yj,
t = zj < yj(t) = yj,

(8.34)

t > zj < yj/zj  [yj(t)  yj]/(t  zj)  0
< yj  yj(t)  yjt/zj.
Equivalently,
yc0j(t)  yj(t)  yc1j(t).

(8.35)

The rest of the proof is the same as the proof to Proposition 8.1, with yc0j(t)
and yc1j(t) replacing y0j(t) and y1j(t).
Q. E. D.
Comparison of the bounds [yc0j(t), yc1j(t)] and [y0j(t), y1j(t)] shows that
strengthening the assumption of monotone response to concave-monotone
response has considerable identifying power. Monotonicity implies that
observation of the realized outcome yj yields either an informative lower or
upper bound on yj(t), as shown in equation (8.4). Concave monotonicity
implies that observation of yj yields both an informative lower and upper
bound on yj(t), as shown in equation (8.34). The present bound on yj(t) is
not only narrower than the earlier one, but its width varies with t in a
qualitatively different manner. The present bound has width yj$*(zj  t)/zj*,
whereas the earlier one has width yj$1[t < zj] + $1[t > zj]. Thus, the present
bound widens linearly from zero as t moves away from zj, whereas the
width of the earlier one varies discontinuously with t.
Corollaries 8.8.1 and 8.8.2 give the concave-monotone response versions
of Corollaries 8.1.1 and 8.1.2. Comparison of these corollaries with the
earlier ones shows clearly the additional identifying power of assuming that
response is concave. The earlier bounds on E{f [y(t)]} are uninformative if
f(y0) =  and f(y1) = , but the present bounds are essentially always

134

8. Monotone Treatment Response

informative. The earlier bounds on quantiles of y(t) are generically
informative only from one side or the other, but the present bounds are
informative both from above and below.
Corollary 8.8.1: Let f($): R  R be weakly increasing. Then
E[f(yt/z)*t < z]$P(t < z) + E[f(y)*t  z]$P(t  z)  E{f [y(t)]}

 E[f(yt/z)*t > z]$P(t > z) + E[f(y)*t  z]$P(t  z).

(8.36)
a

Corollary 8.8.2: Let   (0, 1). Then
Q{y $1[t  z] + yt/z $1[t < z]}  Q[y(t)]

 Q{y$1[t  z] + yt/z $1[t > z]}.

(8.37)
a

Corollary 8.8.1 implies sharp bounds on tail probabilities. The result is
P[(t  z  y  r) F (t < z  yt/z  r)]  P[y(t)  r]

 P[(t > z  yt/z  r) F (t  z  y  r)].

(8.38)

D-Treatment Effects
Bounds on D-treatment effects follow from the sharp bounds obtained for
{yj(t), yj(s)}. In Sections 8.2 and 8.3, we found these joint bounds to have
simple forms when response is monotone or semi-monotone. The joint
bounds assuming concave-monotone response are more complex. Application of (8.31) yields these bounds on {yj(t), yj(s)}:
s < t < zj < yj(s)/s  [yj(t)  yj(s)]/(t  s)  [yj  yj(t)]/(zj  t)  0,
s < t = zj < yj(s)/s  [yj(t)  yj(s)]/(t  s) = [yj  yj(s)]/(zj  s)  0,
s < zj < t < yj(s)/s  [yj  yj(s)]/(zj  s)  [yj(t)  yj]/(t  zj)  0,
s = zj < t < yj(s)/s = yj/zj  [yj(t)  yj]/(t  zj)  0,
zj < s < t < yj/zj [yj(s)  yj]/(s  zj)  [yj(t)  yj(s)]/(t  s)  0.

(8.39)

8.4. Concave Monotonicity

135

Proposition 8.10 uses (8.39) to derive the sharp bound on D[y(t)  y(s)].
Proposition 8.9 gives the sharp lower bound on D[y(t)]  D[y(s)] but only
a non-sharp upper bound.
Proposition 8.9: Let T = [0, -] and Y = [0, ]. Let yj($), j  J be concave and
weakly increasing on T. Then, for every t  T and s  T with t > s,
0  D[y(t)]  D[y(s)]  D[yc1(t)]  D[yc0(s)].
The lower bound is sharp, but the upper bound is not sharp.

(8.40)

a

Proof: Monotonicity of response implies that y(t) stochastically dominates
y(s), so 0 is a lower bound on D[y(t)]D[y(s)]. This lower bound is sharp
because the hypothesis {yj(t) = yj(s) = yj, j  J} satisfies (8.39).
Proposition 8.8 implies that D[yc1(t)]  D[yc0(s)] is an upper bound on
D[y(t)]  D[y(s)]. This upper bound is not sharp because the hypothesis
{yj(t) = yc1j(t), yj(s) = yc0j(s), j  J} does not satisfy (8.39). When s < t < zj,
setting {yj(t) = yj, yj(s) = yjs/zj} violates (8.39). Similarly, when zj < s < t,
setting {yj(t) = yjt/zj, yj(s) = yj} violates (8.39).
Q. E. D.
Proposition 8.10: Let T = [0, -]. Let yj($), j  J be concave and weakly
increasing on T. Let Y = [0, ]. For each t  T and s  T with t > s, define
yctj(s)  yjs/t if t < zj,
 yjs/zj otherwise.

(8.41)

D(0)  D[y(t)y(s)]  D[yc1(t)  yct(s)].

(8.42)

Then

This bound is sharp.

a

Proof: The lower bound holds because response is monotone. It is sharp
because the hypothesis {yj(t)  yj(s) = 0, j  J} satisfies (8.39).
To obtain the sharp upper bound, we need to determine the largest value
of yj(t)  yj(s) that satisfies (8.39). This can be accomplished in two steps.
First hold yj(t) fixed and minimize yj(s) subject to (8.39). This yields the
maximum of yj(t)  yj(s) as a function of yj(t). Then maximize this
expression over yj(t)  [yc0j(t), yc1j(t)].
When t < zj, setting {yj(t) = yj, yj(s) = yjs/t} yields the maximal value of
yj(t)  yj(s). When t  zj, setting {yj(t) = yjt/zj, yj(s) = yjs/zj} yields the

136

8. Monotone Treatment Response

maximal value of yj(t)  yj(s). It follows that the sharp bound on
{yj(t)  yj(s), j  J} is
0  yj(t)  yj(s)  yc1j(t)  yctj(s),

j  J.

(8.43)

Hence D[yc1(t)  yct(s)] is the sharp upper bound on D[y(t)  y(s)].
Q. E. D.
Proposition 8.10 may be applied to give the sharp bound for average
treatment effects. Write yc1(t)  yct(s) in the explicit form
yc1(t)  yct(s) = 1[t < z]$(t  s)$y/t + 1[t  z]$(t  s)$y/z.

(8.44)

The result is the following corollary.
Corollary 8.10.1:
0  E[y(t)]  E[y(s)] = E[y(t)  y(s)]

 (t  s)$[E(y/t*t < z)$P(t < z) + E(y/z*t  z)$P(t  z)].
This bound is sharp.

(8.45)

a

Complement 8A: Downward-Sloping Demand
Section 8.1 used the analysis of production to illustrate the shape restrictions studied in this chapter. Another economic illustration is the assumption that demand functions slope downward.
Economic analyses of market demand usually suppose that there is a set
of isolated markets for a given product. Each market is characterized by a
demand function, which gives the quantity of product that price-taking
consumers would purchase if the price were set at any specified level. In
each market, the interaction of consumers and firms determines the price at
which transactions actually take place.
In the language of the present chapter, markets are persons, prices are
treatments, and quantity demanded is an outcome. Thus T is the set of
logically possible prices. In each market j, transactions take place at some
realized price zj  T. The market demand function is yj($), and yj  yj(zj) is
the quantity actually transacted in market j. The empirical evidence is data
on the quantities, prices, and covariates (yi, zi, xi,), i = 1, . . . , N realized in
a random sample of N markets. The inferential problem is to combine this

8A. Downward-Sloping Demand

137

evidence with prior information to learn about the distribution P[y($)] of
demand functions across markets.
The one relatively firm conclusion of the theory of demand is that
market demand ordinarily is a downward-sloping function of price. This is
not a universal prediction. Introductory textbooks expositing consumer
theory distinguish between the substitution and income effects of price
changes. When income effects are sufficiently strong, consumer optimization implies the existence of Giffen goods, for which demand increases with
price over some domain. The modern theory of markets with imperfect
information emphasizes that price may convey information. If the informational content of price is sufficiently strong, demand functions need not
always slope downward. These exceptions notwithstanding, the ordinary
presumption of economists is that demand functions are downward-sloping.
Economic theory does not yield other conclusions about the shape of
demand functions. Nor does the theory of demand imply anything about
price determination. Conclusions about price determination can be drawn
only if assumptions about the structure of demand are combined with
assumptions about the behavior of the firms that produce the product in
question. Thus, demand analysis offers a good example of an inferential
problem in which the analyst can reasonably assert that response functions
are monotone but should be wary of imposing other assumptions.
Oddly, the classical econometric analysis of demand and supply as linear
simultaneous equations does not assume that market demand is downwardsloping. Instead, it imposes another assumption on the structure of demand
functions. Begun in the 1920s, brought to maturity in Hood and Koopmans
(1953), and exposited regularly in subsequent econometrics texts, the
classical analysis assumes that demand is a linear function of price, with the
same slope parameter in each market. Thus
yj(t) = ßt + uj,

(8A.1)

where ß is the common slope parameter and uj is a market-specific intercept.
Nothing is assumed about the sign or magnitude of ß.
Economic theory does not suggest that demand should be linear in price,
and applied researchers rarely motivate the assumption. The main appeal of
(8A.1) is that it reduces the problem of inference on the distribution P[y($)]
of demand functions to one of inference on the scalar parameter ß. The
central classical finding is that P[y($)] is point-identified if (8A.1) is
combined with the mean-independence assumption2
E(u*v = v0) = E(u*v = v1),
E(z*v = v0) =/ E(z*v = v1),

(8A.2a)
(8A.2b)

138

8. Monotone Treatment Response

where v is an instrumental variable taking the values v0 and v1. Here is a
simple proof taken from Manski (1995, p. 152).
Assumption (8A.1) implies that uj = yjßzj in each market j. This and
(8A.2a) imply that
E(y  ßz*v = v0) = E(y  ßz*v = v1).

(8A.3)

Solving (8A.3) for ß yields
E(y*v = v0)  E(y*v = v1)
ß = ))))))))))))))))))) ,
E(z*v = v0)  E(z*v = v1)

(8A.4)

provided that (8A.2b) holds. Empirical knowledge of P(y, z, v) identifies the
conditional expectations E(y*v) and E(z*v) on the right side of (8A.4), so
ß is point-identified. Knowledge of ß and P(y, z) implies knowledge of P(u)
and hence P[y($)].

Complement 8B. Econometric Response Models
This chapter has assumed only that the response functions yj($), j  J share
the common property of monotonicity, semi-monotonicity, or concave
monotonicity, as the case may be. In other respects, the members of the
population may have arbitrarily different response functions. The notation
yj($) gives succinct and convenient expression to the idea that response
functions may vary across the population.
Econometric analysis has a long tradition of expressing variation in
treatment response in terms of variation in covariates. This complement
interprets the assumption of monotone response from that perspective;
semi-monotonicity and concave monotonicity can be interpreted similarly.
Let each person j have a covariate vector uj  U. These covariates may
include the observable covariate x, but there is no need here to distinguish
observable from unobservable covariates. A standard econometric response
model expresses yj($) as
yj(t) = y*(t, uj).

(8B.1)

The function y*($, $) mapping T × U into Y is common to all j  J.
In terms of (8B.1), yj(t) is the outcome that person j would experience if
he were to receive treatment t while holding his covariates fixed at the
realized value uj. Monotonicity of yj($) is equivalent to monotonicity of

Endnotes

139

y*($, uj), with t  s < y*(t, uj)  y*(s, uj). Random variable y(t) expresses the
outcomes that would be experienced if all members of the population were
to receive treatment t while holding their covariates fixed at their realized
values uj, j  J. Treatment effects D[y(t)]D[y(s)] and D[y(t)y(s)] compare
the outcomes that would be experienced under treatments s and t if the
covariates were held fixed at their realized values.
An alternative interpretation of yj(t) becomes available if we generalize
the response model by supposing that variation in treatments induces
variation in covariates. Let the covariate response function uj($): T  U map
treatments into covariates, let uj  uj(zj), and replace (8B.1) by
yj(t) = y*[t, uj(t)].

(8B.2)

In this formulation, yj(t) is the outcome that person j would experience if he
were to receive treatment t and his covariates were to take the value uj(t).
Monotonicity of yj($) is equivalent to monotonicity of y*[$, uj($)] considered
as a function of t. Treatment effects D[y(t)]D[y(s)] and D[y(t)y(s)]
compare the outcomes experienced under treatments s and t taking account
of induced variation in covariates.
The two interpretations of yj(t) are not contradictory. Propositions 8.1
through 8.3 apply to the thought experiment with covariates held fixed at
realized values if y*($, uj) is monotone on T. The propositions apply to the
thought experiment with induced variation in covariates if y*[$, uj($)] is
monotone on T. The propositions apply to both thought experiments if y*
is monotone in both senses. In this last case, one should not conclude that
y*(t, uj) = y*[t, uj(t)], but one can conclude that y*(t, uj) and y*[t, uj(t)] both
lie within the common sharp bound [y0j(t), y1j(t)].

Endnotes
Sources and Historical Notes
The analysis in this chapter originally appeared in Manski (1997a).

Text Notes
1. If T and Y do not have finite lower bounds, assuming that response is
concave-monotone has no identifying power beyond assuming that response
is monotone. Even assuming that response is linear-monotone has no
additional identifying power. To see this, let Y = R and suppose that

140

8. Monotone Treatment Response
yj(t) = ßjt + uj,

where ßj  0 is a person-specific slope parameter and uj is a person-specific
intercept. Observation of (yj, zj) reveals that uj = yj  ßjzj, so
yj(t) = ßj(t  zj) + yj.
For s  T and t  T with t > s, the sharp bound on {yj(t), yj(s)} is
s < t < zj <   yj(s)  yj(t)  yj,
s < t = zj <   yj(s)  yj(t) = yj,
s < zj < t <   yj(s)  yj  yj(t)  ,
s = zj < t < yj = yj(s)  yj(t)  ,
zj < s < t < yj  yj(s)  yj(t)  .
This is the same as the bound (8.12) obtained when it was assumed only
that response is monotone. Hence, adding the linearity assumption leaves
unchanged the conclusions to Propositions 8.1 through 8.3.
2. The early econometrics literature using instrumental variables to pointidentify linear models of market demand (e.g., Wright, 1928; Reiersol,
1945) only assumed that u has zero covariance with v. However, the modern
literature has generally maintained at least assumption (8A.2). Manski
(1988, pp. 25–26 and Section 6.1) discusses the history and exposits the use
of these assumptions.

9
Monotone Instrumental Variables
9.1. Equalities and Inequalities
To cope with the selection problem, researchers studying treatment response
have long made use of distributional assumptions asserting forms of
independence between outcomes and instrumental variables. Section 7.4
described the identifying power of various such assumptions. Complement
8A showed that point identification can be achieved by combining mean
independence (assumption MI) with the assumption that all response
functions are linear in treatments and have the same slope parameter.
Although independence assumptions are applied widely, their credibility
in non-experimental settings often is a matter of considerable disagreement,
with empirical researchers frequently debating whether some covariate is
or is not a “valid instrument.” There is therefore good reason to consider
weaker assumptions that may be more credible. When the set V of
instrumental-variable values is ordered, a simple way to weaken independence assumptions is to replace equalities with weak inequalities.
This chapter studies identification of mean treatment response when the
equalities defining assumption MI are replaced with weak inequalities. Let
t  T. Assumption MI asserts that
E[y(t) v] = E[y(t)].

(9.1)

Replacement of the equality in (9.1) with a weak inequality yields the
assumption of mean monotonicity (MM):
Assumption MM: Let V be an ordered set. Let (v1, v2)  V × V. Then
141

142

9. Monotone Instrumental Variables
v2  v1 < E[y(t)*v = v2]  E[y(t)*v = v1].

(9.2)

Assumption MM was introduced in Chapter 2 in the context of prediction
with missing outcome data. Inequality (9.2) applies the assumption to the
analysis of treatment response.
A particularly interesting case occurs when the instrumental variable is
the realized treatment in the study population; that is, when v = z. Then
assumption MI becomes the assumption of Means Missing at Random
(MMAR):
E[y(t) z] = E[y(t)].

(9.3)

Assumption MM becomes the assumption of monotone treatment selection
(MTS):
Assumption MTS: Let T be an ordered set. Let (t1, t2)  T × T. Then
t2  t1 < E[y(t)*z = t2]  E[y(t)*z = t1].

(9.4)

Assumption MTS was introduced in Chapter 2, in the context of prediction
with missing outcome data, under the name Means Missing Monotonically
(MMM). The name MTS is more descriptive in the present setting.

The Returns to Schooling
Assumption MM is of applied interest to the extent that it is more credible
than assumption MI. Economic analysis of the returns to schooling
illustrates the potential for gains in credibility.
Labor economists studying the returns to schooling usually suppose that
each person j has a human-capital production function yj(t), giving the wage
that j would receive were he to have t years of schooling. Observing
realized wages and schooling, labor economists seek to learn about the
population distribution of these production functions.
Empirical researchers often impose assumption MMAR. However, this
assumption enjoys little credibility among economists. Perhaps the main
reason is that various models of schooling choice and wage determination
predict that persons with higher ability tend to have higher wage functions
and tend to choose more schooling than do persons with lower ability.
Assumption MMAR violates this prediction.
Assumption MTS is consistent with economic thinking about schooling
choice and wage determination. This assumption asserts that persons who
choose more schooling have weakly higher mean wage functions than do

9.2. Mean Monotonicity

143

those who select less schooling. Thus, when studying the returns to
schooling, assumption MTS is more credible than assumption MMAR.

Monotone Treatment Selection and Monotone Treatment Response
Assumption MTS is distinct from the assumption of monotone treatment
response (MTR) studied in Chapter 8. Assumption MTR asserts that
t  s < yj(t)  yj(s)

(9.5)

for all persons j and all treatment pairs (s, t). The inequalities in (9.4) and
(9.5) express distinct properties of response functions. In principle, both
assumptions could hold, or one, or neither.
To illustrate how Assumptions MTS and MTR differ, consider the
variation of wages with schooling. Labor economists often say that “wages
increase with schooling.” Assumptions MTS and MTR interpret this
statement in different ways. The MTS interpretation is that persons who
select higher levels of schooling have weakly higher mean wage functions
than do those who select lower levels of schooling. The MTR interpretation
is that each person’s wage function is weakly increasing in conjectured
years of schooling. As discussed above, assumption MTS is consistent with
economic models of schooling choice and wage determination that predict
that persons with higher ability tend to have higher wage functions and tend
to choose more schooling than do persons with lower ability. Assumption
MTR expresses the standard economic view that education is a production
process in which schooling is the input and wage is the output. Hence,
wages increase with conjectured schooling.
Perhaps the most interesting finding in this chapter is that, when imposed
together, Assumptions MTS and MTR have considerable identifying power.
This is shown in Section 9.3. As prelude, Section 9.2 studies the identifying
power of assumption MM alone, not combined with other assumptions.
The findings in this chapter apply immediately to sub-populations
indexed by values of an observable covariate x. To simplify notation, the
analysis does not explicitly condition on x.

9.2. Mean Monotonicity
Proposition 9.1 gives the identification region for E[y(t)] under assumption
MM. The proposition is an immediate extension of Proposition 2.6, so the
proof is omitted.

144

9. Monotone Instrumental Variables

Proposition 9.1: (a) Let V be an ordered set. Let assumption MM hold.
Then the identification region for E[y(t)] is the closed interval

MM{E[y(t)]} =

[  P(v = v)(max E{y(t)#1[z = t] + y0#1[z g t] v = v1}),
vV

v1 v

 P(v = v) (min E{y(t)#1[z = t] + y1#1[z g t] v = v1})].
vV

(9.6)

v1 v

(b) Let MM{E[y(t)]} be empty. Then assumption MM does not hold.

a

Suppose that the instrumental variable is the realized treatment z. Then
Proposition 9.1 implies this identification region under assumption MTS:
Corollary 9.1.1: Let T be an ordered set. Let assumption MTS hold. Then
the identification region for E[y(t)] is the closed interval

MTS{E[y(t)]} =
[P(z < t)y0 + P(z  t)E(y z = t), P(z > t)y1 + P(z  t)E(y z = t)]. (9.7)
a
Proof: Application of Proposition 9.1 with V = T and v = z gives

MTS{E[y(t)]} =

[  P(z = s)(max E{y(t)#1[z = t] + y0#1[z g t] z = s1}),
sT

s1 s

 P(z = s)(min E{y(t)#1[z = t] + y1#1[z g t] z = s1})].
sT

(9.8)

s1 s

The lower endpoint in (9.8) reduces to the one in (9.7). To show this,
observe that
s1 < t

< E{y(t)#1[z = t] + y0#1[z g t] z = s1} = y0,

s1 = t

< E{y(t)#1[z = t] + y0#1[z g t] z = s1} = E(y z = t),

s1 > t

< E{y(t)#1[z = t] + y0#1[z g t] z = s1} = y0.

9.3. Mean Monotonicity and Mean Treatment Response

145

Hence
s < t < max s1 s E{y(t)#1[z = t] + y0#1[z g t] z = s1} = y0,
s  t < max s1 s E{y(t)#1[z = t] + y0#1[z g t] z = s1} = E(y z = t).
This yields the lower endpoint in (9.7). The proof for the upper endpoint is
analogous.
Q. E. D.
It is revealing to compare Corollary 9.1.1 with the identification region
for E[y(t)] using the empirical evidence alone. The region using the
empirical evidence alone is

{E[y(t)]} = [P(z g t)y0 + P(z = t)E(y z = t), P(z g t)y1 + P(z = t)E(y z = t)].
(9.9)
The widths of intervals (9.7) and (9.9) are respectively

MTS{E[y(t)]} = [E(y z = t)y0] P(z < t) + [y1E(y z = t)]P(z > t)
(9.10)
and

{E[y(t)]} = (y1y0)P(z < t) + (y1y0)P(z > t).

(9.11)

The former region is narrower than the latter one. For example, if P(z < t)
= P(z > t), the former region has one-half the width of the latter one.

9.3. Mean Monotonicity and Mean Treatment Response
In this section, Assumptions MM and MTR both hold. Proposition 9.2 gives
the resulting sharp bound on E[y(t)].
Proposition 9.2: Let V and T be ordered sets. Let Assumptions MM and
MTR hold. Then

 P(v = v) ( max E{y#1[t  z] + y0#1[t < z] v = v1})
vV

v1 v

 E[y(t)] 

(9.12)

146

9. Monotone Instrumental Variables

 P(v = v) ( min E{y#1[t  z] + y1#1[t > z] v = v1}).
vV

v1  v

This bound is sharp.

a

Proof: Corollary 8.1.1 showed that, for each v  V, assumption MTR yields
this sharp bound on the conditional mean E[y(t) v = v]:
E{y#1[t  z] + y0#1[t < z] v = v}  E[y(t) v = v]

 E{y#1[t  z] + y1#1[t > z] v = v}.

(9.13)

Assumption MM implies that, for all (v1, v2)  V × V,
v1  v  v2 < E[y(t) v = v1]  E[y(t) v = v]  E[y(t) v = v2].

(9.14)

Combining (9.13) and (9.14) shows that E[y(t) v = v] is no smaller than
the MTR lower bound on E[y(t) v = v1] and no larger than the MTR upper
bound on E[y(t) v = v2]. This holds for all v1  v and all v2  v. There are
no other restrictions on E[y(t) v = v]. Thus, the sharp MM-MTR bound on
E[y(t) v = v] is
max v1 v E{y#1[t  z] + y0#1[t < z] v = v1}  E[y(t) v = v]

 min v1  v E{y#1[t  z] + y1#1[t > z] v = v1}.

(9.15)

Now, consider the marginal mean E[y(t)]. The Law of Iterated Expectations gives
E[y(t)] =  v  V P(v = v)E[y(t) v = v].

(9.16)

Inequality (9.15) shows that the sharp MM-MTR lower and upper bounds
on E[y(t) v = v] are weakly increasing in v. Hence, the sharp joint lower
(upper) bound on {E[y(t) v = v], v  V} is obtained by setting each of the
quantities E[y(t) v = v], v  V at its lower (upper) bound in (9.15). Inserting
these lower and upper bounds into the right side of (9.16) yields the result.
Q. E. D.
The lower (upper) bound in Proposition 9.2 is informative only if y0 (y1)
is finite. Suppose, however, that v is the realized treatment z, so assumption
MM becomes assumption MTS. Then the bound turns out to be informative

9.3. Mean Monotonicity and Mean Treatment Response

147

even if Y has infinite range. Corollary 9.2.1 gives the result.
Corollary 9.2.1: Let T be an ordered set. Let Assumptions MTS and MTR
hold. Then

 E(y z = s)$P(z = s) + E(y z = t)$P(z  t)  E[y(t)]
s<t

  E(y z = s)$P(z = s) + E(y z = t)$P(z  t).

(9.17)

s>t

a

This bound is sharp.
Proof: Application of Proposition 9.2 with V = T and v = z gives

 P(z = s) ( max E{y #1[t  z] + y0#1[t < z] z = s1})
sT

s1 s

 E[y(t)] 

(9.18)

 P(z = s) ( min E{y #1[t  z] + y1#1[t > z] z = s1}).
sT

s1  s

The lower bound in (9.18) reduces to the one in (9.17). To show this,
observe that
s1  t < E{y#1[t  z] + y0#1[t < z] z = s1} = E(y z = s1),
s1 > t < E{y#1[t  z] + y0#1[t < z] z = s1} = y0.
Hence
s < t < max s1 s E{y#1[t  z] + y0#1[t < z] z = s1}
= max s1 s E(y z = s1) = E(y z = s),
s  t < max s1 s E{y#1[t  z] + y0#1[t < z] z = s1}
= max s1 t E(y z = s1) = E(y z = t).
The final equalities hold because, by Assumptions MTS and MTR,

148

9. Monotone Instrumental Variables
s1 s < E(y z = s1) = E[y(s1) z = s1]

 E[y(s) z = s1]  E[y(s) z = s] = E(y z = s).

(9.19)

This yields the lower bound. The proof for the upper bound is analogous.
Q. E. D.
Inequality (9.19) suggests a test of assumption MTS-MTR. Under this
joint hypothesis, E(y z = s) must be a weakly increasing function of s.
Hence, the hypothesis is refuted if E(y z = s) is not weakly increasing in s.
This test is a weakened version of the stochastic dominance test proposed
in Section 8.3 for testing the joint hypothesis that treatment response is
monotone and that z is statistically independent of y($).

Bounds on Average Treatment Effects
Propositions 9.1 and 9.2 give sharp bounds on mean outcomes for specified
treatments. Let s and t be two such treatments, with s < t. Often the object
of interest is the average treatment effect E[y(t)]  E[y(s)].
As usual, a lower (upper) bound on E[y(t)]  E[y(s)] can be constructed
by subtracting the lower (upper) bound on E[y(t)] from the upper (lower)
bound on E[y(s)]. When the construction is based on assumption MM alone,
the resulting bound on E[y(t)]  E[y(s)] is sharp. This follows from the fact
that assumption MM imposes no joint restrictions on the response to
different treatments.
Analysis of sharpness is generally complex when the construction is
based on assumption MM-MTR but is possible in the special case when
Assumptions MTS and MTR are combined. Then the upper bound on
E[y(t)]  E[y(s)] is
E[y(t)]  E[y(s)]   E(y z = t1)$P(z = t1) + E(y z = t)$P(z  t)
t1 > t

  E(y z = s1)$P(z = s1)  E(y z = s)$P(z  s).

(9.20)

s1 < s

It follows from (9.19) that the right side of (9.20) is non-negative and no
smaller than E(y z = t)  E(y z = s), which is the value of E[y(t)]  E[y(s)]
under assumption MMAR. It is jointly feasible for E[y(t)] to take its
maximal value and E[y(s)] its minimal value, so inequality (9.20) is sharp.
A lower bound on E[y(t)]  E[y(s)] may be constructed in the same
manner as (9.20), but the result is always nonpositive and is generically

9.4. Variations on the Theme

149

negative. Assumption MTR implies that E[y(t)]  E[y(s)]  0, so the lower
bound generically is not sharp.

9.4. Variations on the Theme
It is easy to think of variations on the theme of this chapter that warrant
study and that may prove useful in empirical research. One would be to
begin with assumption SI and weaken it to an assumption of stochastic
dominance. Another would be to weaken assumption MI to some form of
“approximate” mean independence. A way to formalize this would be to
assert that, for (v, v1)  V × V,

E[y(t) v = v1]  E[y(t) v = v]  C,

(9.21)

where C > 0 is a specified constant. Yet another variation on the theme is
to assert that a distributional assumption such as mean independence holds
in part, but not all, of an observable population.1

Complement 9A. The Returns to Schooling
Manski and Pepper (2000) report an empirical analysis of the returns to
schooling under Assumptions MTS and MTR. As indicated in Section 9.1,
both assumptions are consistent with economic thinking about human
capital accumulation. Even if these assumptions do not warrant unquestioned acceptance, they certainly merit serious consideration.

Data
The analysis used data from the National Longitudinal Survey of Youth
(NLSY). In its base year of 1979, the NLSY interviewed 12,686 persons
who were between the ages of 14 and 22 at that time. Nearly half of the
respondents were randomly sampled, the remainder being selected to overrepresent certain demographic groups. We restricted attention to the 1257
randomly sampled white males who reported in 1994 that they were fulltime year-round workers with positive wages. The self-employed were
excluded. Thus, the empirical analysis concerned the sub-population of
persons who have the shared observable covariates
x = white males who reported in 1994 that they were full-time year-round
workers but not self-employed, and who reported their wages.

150

9. Monotone Instrumental Variables

The NLSY provides data on respondents’ realized years of schooling and
hourly wage in 1994. Thus z is realized years of schooling, the response
variable yj(t) is the log(wage) that person j would experience if he were to
have t years of schooling, and yj is the observed hourly log(wage). The
object of interest is the average treatment effect (s, t)  E[y(t)]  E[y(s)]
for specified values of s and t.
(Note: Use of log(wage) rather than wage to measure the production of
human capital follows the prevailing practice in labor economics. The
reasons are not so much substantive as historical. Early researchers of the
returns to schooling posed specific models for log(wages) that led to the
establishment of research conventions followed by later researchers.)

Statistical Considerations
The bounds obtained in Propositions 9.1 and 9.2 are continuous functions
of nonparametrically estimable conditional probabilities and mean
responses. In this application, it was necessary only to estimate the MTSMTR upper bound on (s, t) given in (9.20). Thus, we had to estimate the
probabilities P(z) of realizing z years of schooling and the expectations
E(y z) of log(wage) conditional on schooling. The empirical distribution of
schooling was used to estimate P(z) and the sample average log(wage) of
respondents with z years of schooling was used to estimate E(y z = z).
Estimation of the MTS-MTR upper bound was therefore a simple matter.
Asymptotically valid confidence intervals for the bounds may be
computed using the delta method or bootstrap approaches. We applied the
percentile bootstrap method. The bootstrap sampling distribution of an
estimate of the MTS-MTR upper bound (9.20) is its sampling distribution
under the assumption that the unknown distribution P(y, z) equals the
empirical distribution of these variables in the sample of 1257 randomly
sampled NLSY respondents. The 0.95–quantile of the bootstrap sampling
distribution is reported next to each upper-bound estimate.

Findings
Table 9.1 gives the estimates of E(y z) and P(z) used to estimate the MTSMTR bounds. The table shows that 41 percent of the NLSY respondents
have 12 years of schooling and 19 percent have 16 years, but the support of
the schooling distribution stretches from 8 years to 20 years. Hence, we
were able to report findings on (s, t) for t = 9 through 20 and 8  s < t.
Section 9.3 showed that assumption MTS-MTR is a testable hypothesis,
which should be rejected if E(y z = s) is not weakly increasing in s. The
estimate of E(y z = s) in Table 9.1 for the most part does increase with s,

9A. The Returns to Schooling

151

but there are occasional dips. Computing a uniform 95 percent confidence
band for the estimate of E(y z), we found that the band contains everywhere
monotone functions. Hence, we proceeded on the basis that assumption
MTS-MTR is consistent with the empirical evidence.
Table 9.1: Empirical Mean log(wage)
and Distribution of Years of Schooling
z

E(y|z)

P(z)

Sample Size

8
9
10
11
12
13
14
15
16
17
18
19
20

2.249
2.302
2.195
2.346
2.496
2.658
2.639
2.693
2.870
2.775
3.006
3.009
2.936

0.014
0.018
0.018
0.025
0.413
0.074
0.083
0.035
0.189
0.038
0.051
0.020
0.021

18
22
23
32
519
93
104
44
238
48
64
25
27

Total

1

1257

Table 9.2 reports the estimates and bootstrap 0.95–quantiles of the MTSMTR upper bounds on (t1, t), t = 9, . . . , 20 followed by the upper bound
on (12, 16), which compares high school completion with college
completion. Point estimates of these average treatment effects under
assumption MMAR may be obtained directly from the first column of Table
9.1. Under this assumption, (s, t) = E(y z = t)  E(y z = s).
To provide context for the results, it is useful to review the point
estimates of (t  1, t) reported in the empirical literature on the returns to
schooling. Most of the point estimates cited in the survey by Card (1994)
are between 0.07 and 0.09. Card (1993) reports a point estimate of 0.132.
Ashenfelter and Krueger (1994) report various estimates and conclude that
(p. 1171): “our best estimate is that increased schooling increases average
wage rates by about 12–16 percent per year completed.”

152

9. Monotone Instrumental Variables

Table 9.2: MTS-MTR Upper Bounds on Returns to Schooling
Upper Bound on (s, t)
Estimate
Bootstrap 0.95-Quantile

s

t

8
9
10
11
12
13
14
15
16
17
18
19

9
10
11
12
13
14
15
16
17
18
19
20

0.390
0.334
0.445
0.313
0.253
0.159
0.202
0.304
0.165
0.386
0.368
0.296

0.531
0.408
0.525
0.416
0.307
0.226
0.288
0.369
0.256
0.485
0.539
0.486

12

16

0.397

0.450

None of the estimates of upper bounds on (t  1, t) in Table 9.2 lies below
the point estimates reported in the literature. The smallest of the upperbound estimates are 0.159 for (13, 14) and 0.165 for (16, 17). These are
about equal to the largest of the available point estimates, namely those in
Ashenfelter and Krueger (1994). It may therefore appear that assumption
MTS-MTR does not, in this application, have sufficient identifying power
to affect current thinking about the magnitude of the returns to schooling.
A different conclusion emerges with consideration of the upper bound
on (12, 16). We estimate that completion of a four-year college yields at
most an increase of 0.397 in mean log(wage) relative to completion of high
school. This implies that the average value of the four year-by-year
treatment effects (12, 13), (13, 14), (14, 15), and (15, 16) is at most
0.099, which is well below the point estimates of Card (1993) and
Ashenfelter and Krueger (1994). This conclusion continues in force if,
acting conservatively, one uses the bootstrap 0.95–quantile of 0.450 to
estimate the upper bound on (12, 16). Then the implied upper bound on
the average value of the year-by-year treatment effects is 0.113. Thus, we
found that, under assumption MTS-MTR, the returns to college-level
schooling are smaller than some of the point estimates that have been
reported.

Endnotes

153

Endnotes
Sources and Historical Notes
The analysis in this chapter originally appeared in Manski and Pepper
(2000).

Text Notes
1. Hotz, Mullins, and Sanders (1997) study aspects of this last variation on
the theme. They suppose that assumption MI holds in a population of
interest. However, the observed population is a probability mixture of this
population and another in which the assumption does not hold. Their
analysis of contaminated instruments exploits the findings on contaminated
sampling in Chapter 4.

10
The Mixing Problem
10.1. Within-Group Treatment Variation
A broad concern of the analysis of treatment response is extrapolation from
one treatment rule to another. A planner or researcher observes the
distribution P(y, z, x) of (outcomes, treatments, covariates) realized under
some status quo treatment rule and wants to learn the distribution of
outcomes that would occur under a conjectural rule.
The planning problem of Chapter 7 motivated interest in predicting
outcomes under rules in which treatment may vary across persons with
different values of the observable covariate x but persons with the same
value of x receive the same treatment. Chapters 8 and 9 continued to focus
on rules that mandate uniform treatment of persons with the same observable covariates. Thus, these chapters studied identification of the outcome
distributions {P[y(t) x = x], t  T, x  X} and of treatment effects that
compare alternative mandated treatments.

Extrapolation from Randomized Experiments
This chapter studies prediction of outcomes when treatment may vary
within the group of persons who share the same value of the covariates x.
Within-group treatment variation may occur whenever treatment choices are
made not by the planner of Chapter 7 but rather by other decision makers
who can differentiate among persons with the same value of x. Withingroup variation is particularly common when treatment choice is decentralized, each member of the population selecting his own treatment. For
example, medical patients may choose among the several treatment options
154

10.1. Within-Group Treatment Variation

155

that physicians propose, youth may choose among a range of schooling
alternatives, and so on.
This chapter specifically studies extrapolation from classical randomized
experiments. As described in Chapter 7, the status quo rule in a classical
experiment randomly places subjects in designated treatment groups, and
all subjects comply with their designated treatments. A classical experiment
credibly point-identifies outcome distributions under rules that mandate
uniform treatment of persons with the same observable covariates, and
enables a planner to make optimal treatment choices. A classical experiment
does not point-identify outcome distributions under rules in which treatment
may vary within groups. The task is to characterize what an experiment
does reveal about outcomes under such treatment rules.

The Perry Preschool Project
An illustration helps to motivate the question under study and provides
some insight into the underlying issues. A notable early use of experiments
with random assignment of treatments to evaluate anti-poverty programs
was the Perry Preschool Project begun in the early 1960s. Intensive
educational and social services were provided to a random sample of about
sixty black children, aged three and four, living in a low-income neighborhood of Ypsilanti, Michigan. No special services were provided to a second
random sample of such children drawn to serve as a control group. The
treatment and control groups were subsequently followed into adulthood.
Among other things, it was found that 67 percent of the treatment group and
49 percent of the control group were high school graduates by age 19. This
and similar findings for other outcomes have been cited widely as evidence
that intensive early childhood educational interventions improve the
outcomes of children at risk.1
Let t = 1 be the educational and social services provided to children
participating in the project, and let t = 0 be the services available to children
in the control group. Let y(t) be a binary variable indicating high school
graduation by age 19. For purposes of this illustration, consider the Perry
Preschool Project to be a classical randomized experiment and ignore the
fact that the sample sizes were on the small side. With these idealizations,
the experimental data revealed that P[y(1) = 1] = 0.67 and P[y(0) = 1] =
0.49. Thus, the high school graduation probability would be 0.67 if all
children in the relevant population were to receive the services provided by
the Perry Preschool Project and would be 0.49 if none of them were to
receive these services.
The question is this: What does the experiment reveal about the
probability of high school graduation under a treatment rule in which some

156

10. The Mixing Problem

children receive the Perry Preschool services and the rest do not? For
example, what would be the probability of graduation if budget limitations
were to require rationing of services? What would it be if some parents
were to refuse to allow their children to receive the services?
It might be conjectured that, if some children were to receive the Perry
Preschool services and the rest were not, the high school graduation
probability would necessarily lie between those observed in the Perry
Preschool control and treatment groups, namely 0.49 and 0.67. This
conjecture is correct under certain assumptions but not in general.
The experiment alone reveals only that the graduation rate would lie
between 0.16 and 1. To see why, observe that each member of the
population has one of these four values for [y(1), y(0)]:
[y(1) = 0, y(0) = 0],
[y(1) = 1, y(0) = 0],

[y(1) = 0, y(0) = 1],
[y(1) = 1, y(0) = 1].

Treatment assignment has no impact on persons for whom y(1) = y(0) but
determines the outcomes of persons for whom y(1) =/ y(0). The highest
feasible graduation rate is attained by a treatment rule that always selects
the treatment with the better graduation outcome, and so gives treatment 1
to each person with [y(1) = 1, y(0) = 0] and treatment 0 to each person with
[y(1) = 0, y(0) = 1]. Then the only persons who do not graduate are those
with [y(1) = 0, y(0) = 0], so the graduation rate is 1  P[y(1) = 0, y(0) = 0].
Symmetrically, the lowest feasible graduation rate is attained by a rule that,
by design or error, gives treatment 0 to each person with [y(1) = 1, y(0) = 0]
and treatment 1 to each person with [y(1) = 0, y(0) = 1]. Then the only
persons who graduate are those with [y(1) = 1, y(0) = 1], so the graduation
rate is P[y(1) = 1, y(0) = 1].
The experiment cannot reveal the joint probabilities P[y(1) = 0, y(0) = 0]
and P[y(1) = 1, y(0) = 1] because treatments 1 and 0 are mutually exclusive.
The experiment does reveal the marginal probabilities P[y(1) = 1] = 0.67
and P[y(0) = 1] = 0.49. It can be shown that among all joint distributions
P[y(1), y(0)] that are consistent with these marginals, there is one that
minimizes both P[y(1) = 0, y(0) = 0] and P[y(1) = 1, y(0) = 1]. This is
P[y(1) = 0, y(0) = 0] = 0,
P[y(1) = 1, y(0) = 0] = 0.51,

P[y(1) = 0, y(0) = 1] = 0.33,
P[y(1) = 1, y(0) = 1] = 0.16.

Hence, the highest graduation rate consistent with the experimental
evidence is 1 and the lowest is 0.16.

10.2. Known Treatment Shares

157

From Marginals to Mixtures
Stripped to its essentials, extrapolation from a randomized experiment is a
problem of inference on a probability mixture given knowledge of its
marginals. This constitutes the mixing problem. The mixing problem should
not be confused with the converse problem, studied in Chapters 4 and 5, in
which one observes a probability mixture and wants to learn the distributions of the random variables that are mixed. Yet the two problems are
related, as will become evident presently.
Let - : JT be the treatment rule whose outcomes are to be predicted. Let
y(- )   t  T y(t)#1[- = t]

(10.1)

be the random variable describing outcomes under rule -. Thus y(- ) is a
probability mixture of [y(t), t  T], whose distribution is
P[y(- )] =  t  T P[y(t) - = t]#P(- = t).

(10.2)

A randomized experiment reveals the marginal outcome distributions
P[y(t)], t  T. For each value of t, the Law of Total Probability gives
P[y(t)] = P[y(t) - = t]#P(- = t) + P[y(t) - g t]#P(- g t).

(10.3)

Thus P[y(t)] is the sum of P[y(t) - = t]#P(- = t), which appears on the right
side of (10.2), and P[y(t) - g t]#P(- g t), which does not appear there.
The identification region for P[y(- )] depends on what one knows about
P[y(#)] and -. Section 10.2 supposes that one knows the treatment shares
[P(- = t), t  T] but has no other information. Section 10.3 studies
extrapolation from the experiment alone.2
The findings in this chapter apply immediately to sub-populations
indexed by values of an observable covariate x. To simplify notation, the
analysis does not explicitly condition on x.

10.2. Known Treatment Shares
Analysis of identification with known treatment shares is the key step en
route to study of extrapolation from the experiment alone. The case of
known treatment shares is also of substantive interest. For example,
resource constraints could limit implementation of the Perry Preschool
intervention to part of the eligible population. Knowledge of the budget
constraint and the cost per child of preschooling would suffice to determine

158

10. The Mixing Problem

the fraction of the population receiving the treatment. It may be more
difficult to predict how school officials, social workers, and parents would
interact to determine which children receive the treatment.
Suppose that the treatment shares under rule - are known. Then equation
(10.3) has the same structure as the contaminated sampling problem of
Chapter 4. Let p  [P(- = t), t  T] denote the vector of treatment shares
under rule -. For each t  T, application of Proposition 4.1, part (a), gives
the identification region for P[y(t) - = t]:

p{P[y(t) - = t]} 

Y

B {{P[y(t)]  (1  pt) }/pt,



}.

Y

(10.4)

The experimental evidence for each treatment is uninformative about
outcomes under other treatments. Hence, the joint identification region for
{P[y(t) - = t], t  T} is the Cartesian product × t  T p{P[y(t) - = t]}.
Evaluating the right side of equation (10.2) at all feasible values of
{P[y(t) - = t], t  T} yields the identification region for P[y(- )]:
Proposition 10.1: Let {P[y(t)], t  T} and p be known. Then the identification region for P[y(- )] is

p{P[y(- )]}  { t  T t#pt, t  p{P[y(t) - = t]}, t  T}.

(10.5)
a

Identification regions for event probabilities and for parameters that
respect stochastic dominance similarly follow from Propositions 4.2 and
4.3. Corollaries 10.1.1 and 10.1.2 give the results.
Corollary 10.1.1: Let B G Y. Then the identification region for P[y(- )  B]
is

p{P[y(-)  B]}  { t  T t(B),
t(B)  [max {0, P[y(t)  B]  (1  pt)}, min{pt, P[y(t)  B]}], t  T}.
(10.6)
a
Proof: By (10.2),
P[y(-)  B] =  t  T P[y(t)  B - = t]#pt.

(10.7)

Proposition 4.2, part (a), gives this identification region for P[y(t)  B - =t]:

10.2. Known Treatment Shares

159

p{P[y(t)  B - = t]} 
[0, 1] B [{P[y(t)  B]  (1  pt)}/pt, P[y(t)  B]/pt].

(10.8)

Hence, the identification region for P[y(t)  B - = t]pt is

p{P[y(t)  B - = t]pt}
 [0, pt]  [P[y(t)  B]  (1  pt), P[y(t)  B]]

(10.9)

= [max {0, P[y(t)  B]  (1  pt)}, min{pt, P[y(t)  B]}].
The identification region for {P[y(t)  B - = t]pt, t  T} is the Cartesian
product of the sets (10.9). Evaluating the right side of equation (10.7) at all
feasible values of these quantities yields (10.6).
Q. E. D.
Corollary 10.1.2: Let Y be a subset of R that contains its lower and upper
endpoints y0 and y1. For t  T, define the distributions L(p, t) and U(p, t) on R
as follows. For r  R,
L(p, t)[-, r]  P[y(t)  r]/pt
 1
U(p, t)[-, t]  0
 {P[y(t)  r]  (1  pt)}/pt

if r < Qpt[y(t)]
if r  Qpt[y(t)],
if r < Q1-pt[y(t)]
if r  Q1-pt[y(t)].

(10.10a)

(10.10b)

Let D(#) respect stochastic dominance. Then
D[ t  T L(p, t)#pt]  D{P[y(- )]}  D[ t  T U(p, t)#pt].
This bound is sharp.

(10.11)

a

Proof: The proof to Proposition 4.3, part (a) shows that L(p, t) and U(p, t) are
the smallest and largest elements of identification region p{P[y(t) - = t]},
with L(p, t) being stochastically dominated by every feasible distribution and
U(p, t) stochastically dominating every such distribution. Hence, evaluating
the right side of equation (10.2) at (L(p, t), t  T) and (U(p, t), t  T) yields the
smallest and largest feasible values for any parameter of P[y(- )] that
respects stochastic dominance.
Q. E. D.

160

10. The Mixing Problem

10.3. Extrapolation from the Experiment Alone
Now suppose that the treatment shares are unknown, so the only available
information is the empirical evidence from the randomized experiment. Let
S denote the unit simplex in R T . The treatment shares can take any value
in S. Hence, the identification region for P[y(- )] using the empirical
evidence alone is the union of the sets p{P[y(- )]} across all p  S.
Analogous findings hold for event probabilities and for parameters that
respect stochastic dominance. These findings are collected in Proposition
10.2.
Proposition 10.2: Let {P[y(t)], t  T} be known. Then the identification
region for P[y(- )] is

{P[y(- )]} = A p  S p{P[y(- )]}.

(10.12)

Let B G Y. Then the identification region for P[y(- )  B] is

{P[y(- )  B]} = A p  S p{P[y(- )  B]}.

(10.13)

Let D(#) respect stochastic dominance. Then
inf p  S D[ t  T L(p, t)#pt]  D{P[y(- )]}  sup p  S D[ t  T U(p, t)#pt].
(10.14)
This bound is sharp.

a

Proposition 10.2 is general but abstract. Consideration of the special case
of identification of event probabilities when there are two treatments yields
a result that is easy to grasp and apply. Corollary 10.2.1 gives this result.
Corollary 10.2.1: Let T = {0, 1}. Define C  P[y(1)  B] + P[y(0)  B].
Then the identification region for P[y(- )  B] is

{P[y(- )  B]} = [max(0, C  1), min(C, 1)].

(10.15)
a

Proof: The corollary can be proved directly, albeit laboriously, by
evaluation of equation (10.13) when T contains two elements. The
derivation below uses a simple direct argument to show that the endpoints
of the interval in (10.15) are sharp bounds on P[y(- )  B]. This derivation
formalizes the reasoning in the Perry Preschool illustration of Section 10.1.

10A. Experiments Without Covariate Data

161

The largest possible value of P[y(- )  B] is 1  P[y(1) Õ B B y(0) Õ B].
This is achieved by a rule that always chooses a treatment whose outcome
lies in B, when such a treatment exists. The smallest possible value of
P[y(-)  B] is P[y(1)  B  y(0)  B]. This is achieved by a rule that always
chooses a treatment whose outcome lies in the complement of B, when such
a treatment exists. Thus P[y(- )  B] must lie in the interval
P[y(1)  B  y(0)  B]  P[y(- )  B]  1  P[y(1) Õ B B y(0) Õ B].
(10.16)
If P[y(#)] were known, (10.16) would be the sharp bound on P[y(- )  B].
We are concerned, however, with the situation in which only the marginals
P[y(1)] and P[y(0)] are known. In this situation, the sharp lower bound on
P[y(- )  B] is the smallest value of P[y(1)  B  y(0)  B] that is consistent
with the known P[y(1)] and P[y(0)]. The sharp upper bound is one minus the
smallest feasible value of P[y(1) Õ B B y(0) Õ B].
Let A G Y. It can be shown that knowledge of the marginals P[y(1)] and
P[y(0)] implies this sharp bound on P[y(1)  A  y(0)  A]: 3
max{0, P[y(1)  A] + P[y(0)  A]  1}  P[y(1)  A  y(0)  A]

 min{P[y(1)  A], P[y(0)  A]}.

(10.17)

Application of (10.17) with A = B yields the lower bound on P[y(- )  B]
in (10.15). Application of (10.17) with A = Y  B yields the upper bound.
Q. E. D.
Observe that the identification region for P[y(- )  B] is informative
from the left or the right but not from both sides simultaneously. The width
of the region narrows toward 0 as C approaches 0 or 2 but widens toward
1 as C approaches 1. Thus, knowledge of the marginals may reveal a lot or
a little about the magnitude of P[y(- )  B], depending on the empirical
value of C. In the Perry Preschool illustration, B = {1}, P[y(0)  B] = 0.49,
P[y(1)  B] = 0.67, and C = 1.16.

Complement 10A. Experiments Without Covariate Data
An interesting manifestation of the mixing problem occurs when a planner
observes the treatments and outcomes realized in a classical randomized
experiment but does not observe covariates of the experimental subjects.
This informational situation is common in medical settings. Physicians

162

10. The Mixing Problem

often have extensive covariate information — medical histories, diagnostic
test findings, and demographic attributes — for the patients that they treat.
Physicians also often know the outcomes of randomized clinical trials
evaluating alternative treatments. However, the medical journal articles that
report the findings of clinical trials do not usually report much covariate
information for the subjects of the experiment. Articles reporting clinical
trials usually describe outcomes only for broad risk-factor groups.
To grasp the essence of the planner’s problem, it suffices to consider the
simplest non-trivial setting: that in which treatments, outcomes, and
covariates are all binary. Thus, suppose that there are two treatments, say
t = 0 and t = 1. The outcome y(t) is binary, with values y(t) = 0 and y(t) = 1;
hence E[y(t) x] = P[y(t) = 1 x]. The covariate x is also binary, taking the
values x = a and x = b.
Even in this simple setting, analysis of the planner’s problem turns out
to be complex. There are four feasible treatment rules. These rules and their
mean outcomes are
Treatment Rule - (0, 0): All persons receive t = 0. The mean outcome is
M(0, 0)  P[y(0) = 1].
Treatment Rule - (1, 1): All persons receive t = 1. The mean outcome is
M(1, 1)  P[y(1) = 1].
Treatment Rule - (0, 1): Persons with x = a receive t = 0, and persons with
x = b receive t = 1. The mean outcome is
M(0, 1)  P[y(0) = 1 x = a]$P(x = a) + P[y(1) = 1 x = b]$P(x = b).
Treatment Rule - (1, 0): Persons with x = a receive t = 1, and persons with
x = b receive t = 0. The mean outcome is
M(1, 0)  P[y(1) = 1 x = a]$P(x = a) + P[y(0) = 1 x = b]$P(x = b).

The Dominated Treatment Rules
Which of the four feasible treatment rules are dominated? The experiment
reveals M(0, 0) and M(1, 1). Thus, rule - (0, 0) is dominated if M(0, 0) <
M(1, 1), and rule - (1, 1) is dominated if M(1, 1) < M(0, 0). The planner is
indifferent between these two rules if M(0, 0) = M(1, 1).
The experiment does not reveal M(0, 1) and M(1, 0). However,
Corollary 10.1.1 shows that the experiment in the study population and
knowledge of the covariate distribution in the treatment population imply
sharp bounds on these quantities. The sharp bounds on M(0, 1) and M(1, 0)
are

10A. Experiments Without Covariate Data

163

max{0, P[y(1) = 1]  P(x = a)} + max{0, P[y(0) = 1]  P(x = b)}

 M(0, 1)  min{P(x = b), P[y(1) = 1]} + min{P(x = a), P[y(0) = 1]},
max{0, P[y(1) = 1]  P(x = b)} + max{0, P[y(0) = 1]  P(x = a)}

 M(1, 0)  min{P(x = a), P[y(1) = 1]} + min{P(x = b), P[y(0) = 1]}.
The form of these bounds determines which treatment rules are
dominated. Suppose that P[y(0) = 1]  P[y(1) = 1] and P(x = a)  P(x = b).
Then rule - (0, 0) is dominated by - (1, 1). The dominance relations among
the other rules depend on the ordering of P[y(0) = 1], P[y(1) = 1], P(x = a),
and P(x = b). There are six distinct orderings to be considered:
Case 1: P[y(0) = 1]  P[y(1) = 1]  P(x = a)  P(x = b).
0  M(0, 1)  P[y(1) = 1] + P[y(0) = 1].
0  M(1, 0)  P[y(1) = 1] + P[y(0) = 1].
Then rules - (0, 1), - (1, 0), and - (1, 1) are undominated.
Case 2: P[y(0) = 1]  P(x = a)  P[y(1) = 1]  P(x = b).
P[y(1) = 1]  P(x = a)  M(0, 1)  P[y(1) = 1] + P[y(0) = 1].
0  M(1, 0)  P(x = a) + P[y(0) = 1].
Then rules - (0, 1) and - (1, 1) are undominated. Rule - (1, 0) is dominated
by rule - (1, 1) if P(x = a) + P[y(0) = 1] < P[y(1) = 1].
Case 3: P[y(0) = 1]  P(x = a)  P(x = b)  P[y(1) = 1].
P[y(1) = 1]  P(x = a)  M(0, 1)  P(x = b) + P[y(0) = 1].
P[y(1) = 1]  P(x = b)  M(1, 0)  P(x = a) + P[y(0) = 1].
Then rule - (1, 1) is undominated. Rule - (0, 1) is dominated by rule
- (1, 1) if P(x = b) + P[y(0) = 1] < P[y(1) = 1]. Rule - (1, 0) is dominated by
rule - (1, 1) if P(x = a) + P[y(0) = 1] < P[y(1) = 1].
Case 4: P(x = a)  P[y(0) = 1]  P[y(1) = 1]  P(x = b).
P[y(1) = 1]  P(x = a)  M(0, 1)  P[y(1) = 1] + P(x = a).
P[y(0) = 1]  P(x = a)  M(1, 0)  P(x = a) + P[y(0) = 1].
Then rules - (1, 1) and - (0, 1) are undominated. Rule - (1, 0) is dominated
by rule - (1, 1) if P(x = a) + P[y(0) = 1] < P[y(1) = 1].
Case 5: P(x = a)  P[y(0) = 1]  P(x = b)  P[y(1) = 1].
P[y(1) = 1]  P(x = a)  M(0, 1)  1.
P[y(1) = 1] + P[y(0) = 1]  1  M(1, 0)  P(x = a) + P[y(0) = 1].
Then rules - (1, 1) and - (0, 1) are undominated. Rule - (1, 0) is dominated

164

10. The Mixing Problem

by rule - (1, 1) if P(x = a) + P[y(0) = 1] < P[y(1) = 1].
Case 6: P(x = a)  P(x = b)  P[y(0) = 1]  P[y(1) = 1].
P[y(1) = 1] + P[y(0) = 1]  1  M(0, 1)  1.
P[y(1) = 1] + P[y(0) = 1]  1  M(1, 0)  1.
Then rules - (0, 1), - (1, 0), and - (1, 1) are undominated.
Cases 1 through 6 show that as many as three or as few as zero treatment
rules are dominated, depending on the empirical values of P[y(0) = 1],
P[y(1) = 1], P(x = a), and P(x = b). The one constancy is that rule - (1, 1) is
always undominated. Indeed, - (1, 1) is always the maximin rule.

The Perry Preschool Project Revisited
To illustrate, consider the situation of a planner, perhaps a social worker,
who is charged with making preschool treatment choices for low-income
black children in Ypsilanti and whose objective is to maximize the high
school graduation rate. The planner can assign each child to the Perry
Preschool treatment or not. Suppose that the planner observes a binary
covariate that describes each member of the population. For the sake of
concreteness, let the covariate indicate the child’s family status, with x = a
if the child has an intact two-parent family and x = b otherwise.
The available outcome data reveal that rule - (0, 0), where no children
receive the Perry Preschool treatment, is dominated by rule - (1, 1), where
all children receive preschooling. The conclusions that the planner can draw
about rules - (0, 1) and - (1, 0) depend on the covariate distribution P(x).
Suppose that half the children have intact families, so P(x = a) = P(x=b)
= 0.5. Then Case 3 holds. The bounds on mean outcomes under rules
- (0, 1) and - (1, 0) are
0.17  M(0, 1)  0.99

0.17  M(1, 0)  0.99.

These bounds imply that rules - (0, 1) and - (1, 0), which reverse one
another’s treatment assignments, have an enormously wide range of
potential consequences for high school graduation. The best case for - (0, 1)
and the worst for - (1, 0) both occur if the (unknown) graduation probabilities conditional on covariates are
P[y(0) = 1 x = a] = 0.98,
P[y(0) = 1 x = b] = 0,

P[y(1) = 1 x = a] = 0.34,
P[y(1) = 1 x = b] = 1.

These graduation probabilities, which yield M(0, 1) = 0.99 and M(1, 0)

Endnotes

165

= 0.17, are consistent with the experimental evidence that P[y(0) = 1] =
0.49 and P[y(1) = 1] = 0.67. They describe a possible world in which
preschooling is necessary and sufficient for children in non-intact families
to complete high school but substantially hurts the graduation prospects of
children in intact families. There is another possible world with the reverse
graduation probabilities: one in which M(0, 1) = 0.17 and M(1, 0) = 0.99.
Hence, rules - (0, 1), - (1, 0), and - (1, 1) are all undominated.
The planner faces a much less ambiguous choice problem if most
children have non-intact families. Suppose that P(x = a) = 0.1 and P(x = b)
= 0.9. Then Case 4 holds. The bounds on mean outcomes under rules
- (0, 1) and - (1, 0) are
0.57  M(0, 1)  0.77

0.39  M(1, 0)  0.59.

These bounds are much narrower than those obtained when half of all
children have non-intact families. The upper bound on M(1, 0) is 0.59,
which is less than the known value of M(1, 1), namely 0.67. Hence, rule
- (1, 0) is dominated. Recall that rule - (0, 0) is also dominated. Thus,
although the planner does not observe graduation probabilities conditional
on covariates, he can nevertheless conclude that the 90 percent of children
who have non-intact families should receive preschooling. The only
ambiguity about treatment choice concerns the 10 percent of children who
have intact families. Treatment rules - (0, 1) and - (1, 1) are undominated.
Thus, in the absence of other information, the planner cannot determine
whether children in intact families should or should not receive
preschooling.

Endnotes
Sources and Historical Notes
This chapter extends analysis that originally appeared in Manski (1995,
1997b). Complement 10A is taken from Manski (2000).

Text Notes
1. See Berrueta-Clement et al. (1984) and Holden (1990).
2. Additional findings for other informational settings are reported in
Manski (1997b) and Pepper (2003).

166

10. The Mixing Problem

3. This was proved by Frechét (1951); see Ord (1972) for an exposition and
Ruschendorf (1981) for a thorough analysis. It is elementary to show that
P[y(1)  A  y(0)  A] must lie within the bound. The upper bound holds
because the event [y(1)  A  y(0)  A] implies each of its component
events [y(1)  A] and [y(0)  A]. The lower bound holds because
1  P[y(1)  A F y(0)  A]
= P[y(1)  A] + P[y(0)  A] - P[y(1)  A  y(0)  A].
Frechét’s general analysis of the problem of inference on a joint distribution
from knowledge of its marginals showed that bound (10.17) is sharp.

References
Angrist, J., G. Imbens, and D. Rubin (1996), “Identification of Causal Effects Using
Instrumental Variables,” Journal of the American Statistical Association, 91,
444–455.
Arabmazar, A. and P. Schmidt (1982), “An Investigation of the Robustness of the
Tobit Estimator to Non-Normality,” Econometrica, 50, 1055–1063.
Ashenfelter, O. and A. Krueger (1994), “Estimates of the Economic Returns to
Schooling from a New Sample of Twins,” American Economic Review, 84,
1157–1173.
Balke, A. and J. Pearl (1997), “Bounds on Treatment Effects from Studies with
Imperfect Compliance,” Journal of the American Statistical Association, 92,
1171–1177.
Bedford, T. and I. Meilijson (1997), “A Characterization of Marginal Distributions
of (Possibly Dependent) Lifetime Variables which Right Censor Each Other,” The
Annals of Statistics, 25, 1622–1645.
Berger, J. (1985), Statistical Decision Theory and Bayesian Analysis, New York:
Springer-Verlag.
Berkson, J. (1958), “Smoking and Lung Cancer: Some Observations on Two
Recent Reports,” Journal of the American Statistical Association, 53, 28–38.
Berrueta-Clement, J., L. Schweinhart, W. Barnett, A. Epstein, and D. Weikart
(1984), Changed Lives: The Effects of the Perry Preschool Program on Youths
Through Age 19, Ypsilanti, MI: High/Scope Press.
Brøndsted, A. (1983), An Introduction to Convex Polytopes, New York: SpringerVerlag.
Campbell, D. (1984), “Can We Be Scientific in Applied Social Science?,”
Evaluation Studies Review Annual, 9, 26–48.

167

168

References

Campbell, D. and R. Stanley (1963), Experimental and Quasi-Experimental
Designs for Research, Chicago: Rand McNally.
Card, D. (1993), “Using Geographic Variation in College Proximity to Estimate the
Return to Schooling,” Working Paper 4483, Cambridge, MA: National Bureau of
Economic Research.
Card, D. (1994), “Earnings, Schooling, and Ability Revisited,” Working Paper
4832, Cambridge, MA: National Bureau of Economic Research.
Center for Human Resource Research (1992), NLS Handbook 1992. The National
Longitudinal Surveys of Labor Market Experience, Columbus, OH: The Ohio State
University.
Cochran, W. (1977), Sampling Techniques, Third Edition. New York: Wiley.
Cochran, W., F. Mosteller, and J. Tukey (1954), Statistical Problems of the Kinsey
Report on Sexual Behavior in the Human Male, Washington, DC: American
Statistical Association.
Cornfield, J. (1951), “A Method of Estimating Comparative Rates from Clinical
Data. Applications to Cancer of the Lung, Breast, and Cervix,” Journal of the
National Cancer Institute, 11, 1269–1275.
Crowder, M. (1991), “On the Identifiability Crisis in Competing Risks Analysis,”
Scandinavian Journal of Statistics, 18, 223–233.
Cross, P. and C. Manski (2002), “Regressions, Short and Long,” Econometrica, 70,
357–368.
Duncan, O. and B. Davis (1953), “An Alternative to Ecological Correlation,”
American Sociological Review, 18, 665–666.
Ellsberg, D. (1961), “Risk, Ambiguity, and the Savage Axioms,” Quarterly Journal
of Economics, 75, 643–669.
Fitzgerald, J., P. Gottschalk, and R. Moffitt (1998), “An Analysis of Sample
Attrition in Panel Data,” Journal of Human Resources, 33, 251–299.
Fleiss, J. (1981), Statistical Methods for Rates and Proportions, New York: Wiley.
Frechét, M. (1951), “Sur Les Tableaux de Correlation Donte les Marges sont
Donnees,” Annals de l’Universite de Lyon A, Series 3, 14, 53-77.

References

169

Freedman, D., S. Klein, M. Ostland, and M. Roberts (1998), “Review of A Solution
to the Ecological Inference Problem, by G. King,” Journal of the American
Statistical Association, 93, 1518–1522.
Freedman, D., S. Klein, M. Ostland, and M. Roberts (1999), “Response to King’s
Comment,” Journal of the American Statistical Association, 94, 355–357.
Freis, E.D., Materson, B.J., and Flamenbaum, W. (1983), “Comparison of
Propranolol or Hydrochlorothiazide Alone for Treatment of Hypertension, III:
Evaluation of the Renin-Angiotensin System,” The American Journal of Medicine,
74, 1029–1041.
Frisch, R. (1934), Statistical Confluence Analysis by Means of Complete Regression
Systems, Oslo, Norway: University Institute for Economics.
Goldberger, A. (1972), “Structural Equation Methods in the Social Sciences,”
Econometrica, 40, 979–1001.
Goldberger, A. (1983), “Abnormal Selection Bias,” in T. Amemiya and I. Olkin
(eds.), Studies in Econometrics, Time Series, and Multivariate Statistics, Orlando:
Academic Press.
Goldberger, A. (1984), “Reverse Regression and Salary Discrimination,” Journal
of Human Resources, 19, 293-318.
Goldberger, A. (1991), A Course in Econometrics, Cambridge, MA: Harvard
University Press.
Goodman, L. (1953), “Ecological Regressions and Behavior of Individuals,”
American Sociological Review, 18, 663–664.
Gronau, R. (1974), “Wage Comparisons–a Selectivity Bias,” Journal of Political
Economy, 82, 1119–1143.
Hampel, F., E. Ronchetti, P. Rousseeuw, and W. Stahel (1986), Robust Statistics,
New York: Wiley.
Heckman, J. (1976), “The Common Structure of Statistical Models of Truncation,
Sample Selection, and Limited Dependent Variables and a Simple Estimator for
Such Models,” Annals of Economic and Social Measurement, 5, 479–492.
Heckman, J., J. Smith, and N. Clements (1997), “Making the Most out of
Programme Evaluations and Social Experiments: Accounting for Heterogeneity in
Programme Impacts,” Review of Economic Studies, 64, 487–535.

170

References

Hirano, K., G. Imbens, G. Ridder, and D. Rubin (2001), “Combining Panel Data
Sets with Attrition and Refreshment Samples,” Econometrica, 69, 1645–1659.
Holden, C. (1990), “Head Start Enters Adulthood,” Science, 247, 1400–1402.
Hood, W. and T. Koopmans (eds.) (1953), Studies in Econometric Method, New
York: Wiley.
Horowitz, J. and C. Manski (1995), “Identification and Robustness with Contaminated and Corrupted Data,” Econometrica, 63, 281–302.
Horowitz, J. and C. Manski (1997), “What Can Be Learned About Population
Parameters when the Data Are Contaminated?,” in C. R. Rao and G. S. Maddala
(eds.), Handbook of Statistics, Vol. 15: Robust Statistics, Amsterdam: NorthHolland, pp.439–466.
Horowitz, J. and C. Manski (1998), “Censoring of Outcomes and Regressors due
to Survey Nonresponse: Identification and Estimation Using Weights and
Imputations,” Journal of Econometrics, 84, 37–58.
Horowitz, J. and C. Manski (2000), “Nonparametric Analysis of Randomized
Experiments with Missing Covariate and Outcome Data,” Journal of the American
Statistical Association, 95, 77–84.
Horowitz, J. and C. Manski (2001), “Imprecise Identification from Incomplete
Data,” Proceedings of the 2nd International Symposium on Imprecise Probabilities
and Their Applications, http://ippserv.rug.ac.be/~isipta01/proceedings/index.html.
Hotz, J., C. Mullins, and S. Sanders (1997), “Bounding Causal Effects Using Data
from a Contaminated Natural Experiment: Analyzing the Effects of Teenage
Childbearing,” Review of Economic Studies, 64, 575–603.
Hsieh, D., C. Manski, and D. McFadden (1985), “Estimation of Response
Probabilities from Augmented Retrospective Observations,” Journal of the
American Statistical Association, 80, 651-662.
Huber, P. (1964), “Robust Estimation of a Location Parameter,” Annals of
Mathematical Statistics, 35, 73–101.
Huber, P. (1981), Robust Statistics, New York: Wiley.
Hurd, M. (1979), “Estimation in Truncated Samples when There Is Heteroskedasticity,” Journal of Econometrics, 11, 247–258.
Imbens, G. and J. Angrist (1994), “Identification and Estimation of Local Average
Treatment Effects,” Econometrica, 62, 467–476.

References

171

Keynes, J. (1921), A Treatise on Probability, London: MacMillan.
King, G. (1997), A Solution to the Ecological Inference Problem: Reconstructing
Individual Behavior from Aggregate Data, Princeton: Princeton University Press.
King, G. (1999), “The Future of Ecological Inference Research: A Comment on
Freedman et al.,” Journal of the American Statistical Association, 94, 352–355.
King, G. and L. Zeng (2002), “Estimating Risk and Rate Levels, Ratios and
Differences in Case-Control Studies,” Statistics in Medicine, 21, 1409–1427.
Klepper, S. and E. Leamer (1984), “Consistent Sets of Estimates for Regressions
with Errors in All Variables,” Econometrica, 52, 163–183.
Knight, F. (1921), Risk, Uncertainty, and Profit, Boston: Houghton-Mifflin.
Koopmans, T. (1949), “Identification Problems in Economic Model Construction,”
Econometrica, 17, 125–144.
Lindley, D. and M. Novick (1981), “The Role of Exchangeability in Inference,”
Annals of Statistics, 9, 45–58.
Little, R. (1992), “Regression with Missing X’s: A Review,” Journal of the
American Statistical Association, 87, 1227–1237.
Little, R. and D. Rubin (1987), Statistical Analysis with Missing Data, New York:
Wiley.
Maddala, G. S. (1983), Limited-Dependent and Qualitative Variables in Econometrics, Cambridge, UK: Cambridge University Press.
Manski, C. (1988), Analog Estimation Methods in Econometrics, London: Chapman
& Hall.
Manski, C. (1989), “Anatomy of the Selection Problem,” Journal of Human
Resources, 24, 343–360.
Manski, C. (1990), “Nonparametric Bounds on Treatment Effects,” American
Economic Review Papers and Proceedings, 80, 319–323.
Manski, C. (1994), “The Selection Problem,” in C. Sims (ed.), Advances in
Econometrics, Sixth World Congress, Cambridge, UK: Cambridge University Press,
pp.143–170.
Manski, C. (1995), Identification Problems in the Social Sciences, Cambridge, MA:
Harvard University Press.

172

References

Manski, C. (1997a), “Monotone Treatment Response,” Econometrica, 65,
1311–1334.
Manski, C. (1997b), “The Mixing Problem in Programme Evaluation,” Review of
Economic Studies, 64, 537–553.
Manski, C. (2000), “Identification Problems and Decisions Under Ambiguity:
Empirical Analysis of Treatment Response and Normative Analysis of Treatment
Choice,” Journal of Econometrics, 95, 415–442.
Manski, C. (2001), “Nonparametric Identification Under Response-Based
Sampling,” in C. Hsiao, K. Morimune, and J. Powell (eds.), Nonlinear Statistical
Inference: Essays in Honor of Takeshi Amemiya, New York: Cambridge University
Press.
Manski, C. (2002), “Treatment Choice Under Ambiguity Induced by Inferential
Problems,” Journal of Statistical Planning and Inference, 105, 67–82.
Manski, C. (2003), “Social Learning from Private Experiences: The Dynamics of
the Selection Problem,” Review of Economic Studies, forthcoming.
Manski, C. and S. Lerman (1977), “The Estimation of Choice Probabilities from
Choice-Based Samples,” Econometrica, 45, 1977-1988.
Manski, C. and D. Nagin (1998), “Bounding Disagreements About Treatment
Effects: A Case Study of Sentencing and Recidivism,” Sociological Methodology,
28, 99–137.
Manski, C. and J. Pepper (2000), “Monotone Instrumental Variables: With an
Application to the Returns to Schooling,” Econometrica, 68, 997–1010.
Manski, C. and E. Tamer (2002), “Inference on Regressions with Interval Data on
a Regressor or Outcome,” Econometrica, 70, 519–546.
Materson, B., D. Reda, and W. Cushman (1995), “Department of Veterans Affairs
Single-Drug Therapy of Hypertension Study: Revised Figures and New Data,”
American Journal of Hypertension, 8, 189–192.
Materson, B., D. Reda, W. Cushman, B. Massie, E. Freis, M. Kochar, R.
Hamburger, C. Fye, R. Lakshman, J. Gottdiener, E. Ramirez, and W. Henderson
(1993), “Single-Drug Therapy for Hypertension in Men: A Comparison of Six
Antihypertensive Agents with Placebo,” The New England Journal of Medicine,
328, 914–921.
Molinari, F. (2002), “Missing Treatments,” Evanston, IL: Department of Economics, Northwestern University.

References

173

Ord, J. (1972), Families of Frequency Distributions, Griffin's Statistical Monographs & Courses No. 30, New York: Hafner.
Pepper, J. (2003), “Using Experiments to Evaluate Performance Standards: What
Do Welfare-to-Work Demonstrations Reveal to Welfare Reformers?” Journal of
Human Resources, forthcoming.
Peterson, A. (1976), “Bounds for a Joint Distribution Function with Fixed
Subdistribution Functions: Application to Competing Risks,” Proceedings of the
National Academy of Sciences U.S.A., 73, 11–13.
Reiersol, O. (1945), “Confluence Analysis by Means of Instrumental Sets of
Variables,”Arkiv fur Matematik, Astronomi Och Fysik, 32A, No.4, 1–119.
Robins, J. (1989), “The Analysis of Randomized and Non-Randomized AIDS
Treatment Trials Using a New Approach to Causal Inference in Longitudinal
Studies,” in L. Sechrest, H. Freeman, and A. Mulley. (eds.), Health Service
Research Methodology: A Focus on AIDS, Washington, DC: NCHSR, U.S. Public
Health Service.
Robins, J., A. Rotnitzky, and L. Zhao (1994), “Estimation of Regression Coefficients when Some Regressors Are Not Always Observed,” Journal of the American
Statistical Association, 89, 846–866.
Robinson, W. (1950), “Ecological Correlation and the Behavior of Individuals,”
American Sociological Review, 15, 351–357.
Rosenbaum, P. (1995), Observational Studies, New York: Springer-Verlag.
Rosenbaum, P. (1999), “Choice as an Alternative to Control in Observational
Studies,” Statistical Science, 14, 259–304.
Rubin, D. (1976), “Inference and Missing Data,” Biometrika, 63, 581–590.
Ruschendorf, L. (1981), “Sharpness of Frechet-Bounds,” Zeitschrift fur
Wahrscheinlichkeitstheorie und Verwandte Gebiete, 57, 293–302.
Savage, L. (1954), The Foundations of Statistics, New York: Wiley.
Scharfstein, D., A. Rotnitzky, and J. Robins (1999), “Adjusting for Nonignorable
Drop-Out Using Semiparametric Nonresponse Models,” Journal of the American
Statistical Association, 94, 1096–1120.
Simpson E. (1951), “The Interpretation of Interaction in Contingency Tables,”
Journal of the Royal Statistical Society B, 13, 238–241.

174

References

Stafford, F. (1985), “Income-Maintenance Policy and Work Effort: Learning from
Experiments and Labor-Market Studies,” in J. Hausman and D. Wise (eds.), Social
Experimentation, Chicago: University of Chicago Press.
U.S. Bureau of the Census (1991), “Money Income of Households, Families, and
Persons in the United States: 1988 and 1989,” in Current Population Reports,
Series P-60, No. 172. Washington, DC: U.S. Government Printing Office.
Wald, A. (1950), Statistical Decision Functions, New York: Wiley.
Wang, C., S. Wang, L. Zhao, and S. Ou (1997), “Weighted Semiparametric
Estimation in Regression Analysis with Missing Covariate Data,” Journal of the
American Statistical Association, 92, 512–525.
Wright, S. (1928), Appendix B to Wright, P. The Tariff on Animal and Vegetable
Oils, New York: McMillan.
Zaffalon, M. (2002), “Exact Credal Treatment of Missing Data,” Journal of
Statistical Planning and Inference, 105, 105–122.
Zidek, J. (1984), “Maximal Simpson-Disaggregations of 2 × 2 Tables,” Biometrika,
71, 187–190.

Index
ambiguity 110-112, 165, 168, 172
analysis of treatment response 2, 3,
99, 100, 102, 108, 117, 142,
154, 172
Angrist, J. 118, 167, 170
Arabmazar, A. 38, 167
Ashenfelter, O. 151, 152, 167
assumption MAR 27, 29, 46, 48,
108, 109
assumption MI 28, 32-34, 36,
108-110, 141, 142, 149
assumption MM 28, 34-36,
141-144, 146, 148
assumption MMM 28, 34, 35
assumption MTR 143, 146, 149
assumption MTS 142-144, 146,
148, 150-152
assumption SI 28, 30, 31, 34, 36,
45, 46, 48, 108-110, 131, 132,
149
assumption SI-RF 110, 131, 132
attributable risk 89-93, 97, 98
auxiliary data 88, 89, 95
Balke, A. 110, 167
Barnett, W. 167
Bayes decision rule 111
Bayes Theorem 3, 42, 47, 49, 88,
96
Bedford, T. 167
Berger, J. 112, 167
Berkson, J. 98, 167
Berrueta-Clement, J. 155, 167
Brøndsted, A. 81, 167
Campbell, D. 117, 167, 168

Card, D. 151, 152, 168
case-control sampling 87, 171
case-referent sampling 87
causal effects 118, 167, 170
Center for Human Resource Research 18, 168
choice-based sampling 87
Clements, N. 119, 169
Cochran, W. 5, 23, 168
competing risk model 5
compliers 118
concave monotonicity 120, 122,
132, 133, 138
conditional prediction 40, 50
conjectural treatment rule 100
contaminated instruments 149
contaminated outcomes 60, 62, 70
convolution model 72
Cornfield, J. 98, 168
counterfactual outcomes 2, 99, 101
covariate response function 139
credible inference 1, 26
Cross, P. 5, 85, 168
Crowder, M. 5, 168
Cushman, W. 114, 172
data errors 60, 61, 68, 70-72
Davis, B. 5, 85, 168
deconvolution problem 72
design weights 37
diminishing marginal returns 121
dominated treatment rules 106,
111, 162
downward-sloping demand 136
Duncan, O. 5, 85, 168

175

176
D-outcomes 122, 123, 127, 128,
132
D-parameter 11, 16, 124
D-treatment effects 122, 123, 125,
129, 134
ecological correlation 85, 168, 173
ecological inference 3, 5, 62, 73,
74, 81, 85, 169, 171
econometric response model 138
Ellsberg, D. 111, 168
Epstein, A. 167
error probability 61-63, 67-72
errors-in-variables 72
event probabilities 62, 63, 158, 160
experiment 102, 110, 114, 117,
118, 139, 155-157, 160-162,
170
external validity 117, 118
Fitzgerald, J. 29, 168
Flamenbaum, W. 115, 169
Fleiss, J. 98, 168
Frechét, M. 5, 161, 168
Freedman, D. 81, 169, 171
Freis, E. 115, 169, 172
Frisch, R. 5, 169
Fye, C. 172
Goldberger, A. 38, 85, 98, 169
Goodman, L. 85, 169
Gottdiener, J. 172
Gottschalk, P. 29, 168
Gronau, R. 38, 169
Hamburger, R. 172
Hampel, F. 70, 169
Heckman, J. 38, 119, 169
Henderson, W. 172
Hirano, K. 13, 170
Holden, C. 155, 170
Hood, W. 137, 170
Horowitz, J. 5, 18, 20, 23, 53, 58,
72, 115, 170
Hotz, J. 149, 170
Hsieh, D. 13, 98, 170
Huber, P. 70, 170
Hurd, M. 38, 170
hypothesis(es) 10, 29, 32, 63, 72,
83, 101, 116, 124, 126,

Index
130-132, 135, 148, 150
identical in distribution 103
identification, defined 4-7
identification region, defined 6
Imbens, G. 13, 118, 167, 170
imputation 11, 68, 69
instrumental variable(s) 8, 26-31,
34, 36-38, 45, 54, 76, 81, 82,
85, 108, 110, 122, 137, 138,
141, 142, 144, 167, 172
internal validity 117, 118
interval data 2, 172
interval measurement 8, 17, 18, 41,
48
interview nonresponse 69
item nonresponse 69
John Godfrey Saxe 21
joint inference 41, 53, 54, 75
Keynes, J. 111, 171
King, G. 81, 98, 169, 171
Klein, S. 81, 169
Klepper, S. 5, 171
Knight, F. 111, 171
Kochar, M. 172
Koopmans, T. 4, 137, 170, 171
Krueger, A. 151, 152, 167
Lakshman, R. 172
Law of Decreasing Credibility 1,
26
Law of Total Probability 3, 6, 14,
29, 42, 47, 49, 54, 61, 74, 83,
88, 89, 101, 157
Leamer, E. 5, 171
Lerman, S. 98, 172
Lindley, D. 85, 171
Little, R. 23, 58, 171
long regression 76, 85
Maddala, G. S. 38, 170, 171
Manski, C. 4, 5, 13, 18, 20, 23, 38,
48, 53, 58, 72, 85, 97, 98, 111,
112, 115, 118, 137-139, 149,
153, 157, 165, 168, 170-172
Massie, B. 172
Materson, B. 114, 115, 169, 172
maximin rule 107, 111, 112, 116,
164

Index
McFadden, D. 13, 98, 170
mean(s) 2, 5, 7-9, 11, 28, 32, 34,
36, 37, 48, 58, 71, 73, 76,
80-82, 84, 85, 98, 103-108, 111,
112, 114, 116, 137, 141-143,
145, 146, 148-152, 162, 164,
165
mean independence 28, 32, 34, 36,
48, 108, 141, 149
mean monotonicity 28, 32, 34, 141,
143, 145
Meilijson, I. 5, 167
meta-analysis 13
Minkowski’s Theorem 81
missing covariates 46, 48
missing outcome(s) 2, 3, 5-7, 17,
18, 23, 26, 27, 38, 40-42, 45,
48, 53-55, 58, 70, 99, 108, 114,
142
missing-at-random 27-29, 108
mixing problem 62, 102, 154, 157,
161, 172
mixture model 60, 70-72
Moffitt, R. 29, 168
Molinari, F. 114, 172
monotone treatment response 120,
123-125, 131, 132, 143, 172
monotonicity 3, 28, 32, 34, 36, 38,
48, 102, 120, 122-129, 131-133,
135, 138, 139, 141, 143, 145
Mosteller, F. 5, 23, 168
Mullins, C. 149, 170
multiple sampling processes 8, 13,
14, 16, 21, 27, 30, 31, 44, 54
Nagin, D. 113, 172
nonresponse weights 37
Novick, M. 85, 171
odds ratio 91-94, 97
Ord, J. 161, 173
Ostland, M. 81, 169
Ou, S. 58, 174
outer bound 13
outer identification region 82, 83,
110
parameters that respect stochastic
dominance 8, 11, 12, 16, 18, 23,

177
48, 62, 65, 158, 160
parametric prediction 56-58
partial identification 1-5, 13, 23, 88
Pearl, J. 110, 167
Pepper, J. 38, 149, 153, 157, 172,
173
Perry Preschool Project 155, 164
perturbation analysis 5
Peterson, A. 5, 173
planner 102-104, 106-109, 111,
112, 117, 118, 120, 154, 155,
161, 162, 164, 165
point estimation 1, 10, 71
point identification 1-3, 13, 23, 27,
30, 31, 34, 38, 82, 85, 109, 141
population of interest 1, 8, 14, 41,
62, 117, 118, 122, 149
prediction 3, 40, 46, 50, 53, 56-58,
84, 87, 100, 137, 142, 154
procedural rationality 112
production analysis 120, 121
production function 121, 142
quantile(s) 2, 11-13, 67, 122, 124,
125, 134, 152
Ramirez, E. 172
random treatment selection 101
rank condition 82
rare-disease assumption 88-92, 98
Reda, D. 114, 172
regression 1-3, 5, 58, 73, 84, 85,
87, 98, 169, 171, 173, 174
Reiersol, O. 38, 137, 173
relative risk 89-92, 96-98
response function 99, 103, 109,
131, 139
response-based sampling 3, 87-89,
91, 92, 94, 95, 172
retrospective sampling 87
returns to schooling 142, 143,
149-152, 167, 172
reverse regression 87, 98, 169
Ridder, G. 13, 170
Roberts, M. 81, 169
Robins, J. 5, 58, 110, 173
Robinson, W. 85, 173
robust inference 61, 70, 71

178
robustness 5, 71, 167, 170
Ronchetti, E. 70, 169
Rosenbaum, P. 5, 117, 173
Rotnitzky, A. 5, 58, 173
Rousseeuw, P. 70, 169
Rubin, D. 13, 23, 29, 118, 167,
170, 171, 173
Ruschendorf, L. 5, 161, 173
sample analog 8, 10, 57, 58
sample augmentation 13
sample data 1, 11, 18
sampling process 1, 3, 6, 7, 9, 13,
14, 16, 17, 26, 27, 29, 33, 40,
43, 45, 47, 49, 54, 60, 61, 63,
64, 71, 73, 87, 89, 91, 100, 101
Sanders, S. 149, 170
Savage, L. 112, 168, 173
Scharfstein, D. 5, 173
Schmidt, P. 38, 167
Schweinhart, L. 167
selection on observables 29
selection problem 100-102, 104,
105, 108, 114, 120, 124, 141,
171, 172
semi-monotone response 127, 128,
131
semi-monotonicity 120, 127-129,
131, 132, 138
sensitivity analysis 5
sharp bound 123, 126, 128-133,
135, 136, 139, 145, 146, 161
short regression 85
Simpson, E. 85, 173, 174
Simpson’s Paradox 85
Smith, J. 119, 169
stacked distributions 77-79
Stafford, F. 118, 174
Stahel, W. 70, 169
Stanley, R. 117, 168
statistical independence 3, 27, 28,
30, 32, 34, 108-110
statistical inference 1, 4, 5, 7, 10,
172
statistical theory 1, 2, 5
status quo treatment rule 100, 101,
103, 114, 154

Index
structural prediction 84
study population 103, 107, 110,
117, 120, 142, 162
Tamer, E. 23, 48, 172
The Blind Men and the Elephant 21
treatment choice 2, 102, 103, 105,
106, 111, 112, 114, 116-118,
154, 165, 172
treatment population 102, 103, 107,
117, 118, 162
treatment shares 157, 158, 160
Tukey, J. 5, 23, 168
U.S. Bureau of the Census 68, 69,
174
valid instrument 28, 141
Wald, A. 112, 174
Wang, C. 58, 174
Wang, S. 58, 174
Weikart, D. 167
Wright, S. 38, 137, 174
Zaffalon, M. 50, 174
Zeng, L. 98, 171
Zhao, L. 58, 173, 174
Zidek, J. 85, 174

