Nonparametric estimation of causal heterogeneity under
high-dimensional confounding

arXiv:1908.08779v1 [econ.EM] 23 Aug 2019

Michael Zimmert† and Michael Lechner‡
SEW-HSG
Swiss Institute for Empirical Economic Research
University of St.Gallen, Switzerland

Abstract
This paper considers the practically important case of nonparametrically estimating heterogeneous
average treatment effects that vary with a limited number of discrete and continuous covariates in
a selection-on-observables framework where the number of possible confounders is very large. We
propose a two-step estimator for which the first step is estimated by machine learning. We show
that this estimator has desirable statistical properties like consistency, asymptotic normality and
rate double robustness. In particular, we derive the coupled convergence conditions between the
nonparametric and the machine learning steps. We also show that estimating population average
treatment effects by averaging the estimated heterogeneous effects is semi-parametrically efficient.
The new estimator is an empirical example of the effects of mothers smoking during pregnancy on
the resulting birth weight.

JEL classification: C14, C21
Keywords: causal machine learning, effect heterogeneity, group average treatment effects, semiparametric efficiency, ensemble learning
† Email:

michael.zimmert@unisg.ch
Lechner is also affiliated with CEPR, London, CESIfo, Munich, IAB, Nuremberg, IZA, Bonn, and RWI, Essen.
Email: michael.lechner@unisg.ch, www.michael-lechner.eu
‡ Michael

1

Introduction

Recently, new machine learning based estimators showed immense potential to systematically uncovering
causal effect heterogeneity so that there is now a rapidly growing literature on this topic (e.g., see the
overviews in Athey and Imbens, 2019; Athey and Imbens, 2017 and Knaus, Lechner, and Strittmatter,
2018). In the context of heterogeneity, the respective aggregation levels for which the heterogeneity is
estimated is playing an important role. Most papers of this literature focus on a selection-on-observable
framework and investigate estimators for the heterogeneity at the lowest aggregation level to uncover
possible heterogeneities to the largest extent possible. While this finest level of causal granularity is obviously of interest, Chernozhukov, Fernández-Val, and Luo (2018b) and Lechner (2018) argue to analyse
heterogeneity at higher levels, so called ‘Group Average Treatment Effects’ (GATEs). Such aggregates
can be estimated more precisely, may be far more easily interpretable by researchers in substantive terms,
and are more useful for decision makers. In particular, some subgroup heterogeneities are of limited value
per se because it is hard to justify a decision or policy based on certain characteristics (race, gender etc.).
Therefore, decision-makers are often only interested in effect heterogeneities based on a rather small subset of available covariates. This paper suggests an approach that is based on statistical-learning assisted
estimation of the GATEs for the various discrete and continuous variables of interest, and subsequent
non-parametric aggregation of the GATEs to obtain ‘Average Treatment Effects’ (ATEs).1
More technically speaking, in effect heterogeneity analysis covariates do not (only) serve the purpose of
making identifying assumptions credible. They become part of the outcome analysis by discriminating
different subgroups of units for which the effect is of interest. Further, whenever new observations enter
the sample the covariate realizations could be used to predict a causal effect. The set of covariates to be
included in the statistical model to explore effect heterogeneity is therefore not a statistical but rather a
substantive decision.
The estimation of subgroup specific effects is a tedious task when there is confounding. In such settings,
causal effects are typically only identified if the researcher includes the confounding covariates in the
statistical model as well. Hence, the identifying assumptions dictate the inclusion of the set of covariates
required. In empirical research based on selection-on-observables, the credibility of causal effects estimation often depends on a very large set of possible covariates with very many possible functional forms.
Qualitatively assessing which covariates should ultimately enter the model in which specific form in a
non-systematic fashion is prone to be flawed.
1 Lechner (2018) also proposed this aggregation idea. However, that paper considered only a version of a Causal Forest
while here we are in principle agnostic with respect to the machine learning method used. Furthermore, it considered only
GATEs based on discrete variables and thus GATEs were obtained as unweighted within-cell means.

1

There are currently several suggestions to estimate heterogeneous effects when there is confounding.
The general concept of estimating causal effects conditional on covariates2 already dates back to Hahn
(1998). He suggested estimating a nonparametric outcome regression on the set of covariates that needs
to be controlled for. Averaging over the conditional means leads to estimators of ATE that attain the
semiparametric efficiency bound. In practice, however, nonparametric regression with many covariates
is hardly feasible because the convergence rate of nonparametric methods exponentially decreases with
the number of covariates included. Recently, Wager and Athey (2018) follow the same ideas as in Hahn
(1998) but use Causal Forests instead of standard nonparametric regression. Athey, Tibshirani, and
Wager (2019) and Lechner (2018) modify the Random Forest algorithm to better adjust for confounding
and improve precision. Outcome-based models adjust for confounding and infer heterogeneous effects in
a single estimation step. Therefore, in all of these approaches inference for effect heterogeneity relies on a
dimension of the covariate space that is fixed. Given the previous discussion, this might be a very strong
assumption.
In this paper we follow an alternative approach in the literature. The two distinct roles of the covariates –
adjusting for confounding and estimating heterogeneous effects – are explicitly reflected in a two-step estimation procedure for the GATEs. This idea is conceptionally not new in the literature.3 In the context
of difference-in-differences estimation, Abadie (2005) shows that propensity scores weighted outcomes
can be used as a dependent variable in a second stage regression on the covariates that are of interest for
heterogeneous effects. Abrevaya, Hsu, and Lieli (2015) use a similar idea in the standard selection-onobservables setting. They provide inferential results for nonparametric and parametric propensity score
first stages with nonparametric second stages. In line with other results in the literature on average effects
(Hirano, Imbens, and Ridder, 2003, Robins, Rotnitzky, and Zhao, 1994, Lunceford and Davidian, 2004),
they show that the variance for Inverse Probability Weighting (IPW) estimators can be substantially
decreased when the propensity score is estimated nonparametrically. Since their second stage also relies
on nonparametric regression, the validity of their asymptotic results requires jointly choosing two kernel
bandwidths which have to be in a rather small feasible interval. Lee, Okui, and Whang (2016) augment
the model of Abrevaya et al. (2015) by including outcome projections (Augmented IPW, AIPW) and
show that when both the propensity score and the outcome projections are estimated parametrically,
one can treat the nuisance parameters as if they were known. Their asymptotic results with parametric
first stages are then equivalent4 to those of Abrevaya et al. (2015) with nonparametric propensity score
2 We

avoid the imprecise term ‘Conditional Average Treatment Effect’ because it is unclear which conditioning set is
actually meant.
3 A few days before this work appeared first on arXiv, Fan, Hsu, Lieli, and Zhang (2019) published their independent
work on arXiv (up to that moment unknown to us) that uses similar ideas about aggregation and machine learning.
4 While Abrevaya et al. (2015) use local constant nonparametric regression, Lee et al. (2016) show their results with local
linear nonparametric regression.

2

estimation. Still the parametric nuisance models used could lead to substantial misspecification bias if
the functional form does not coincide with the unknown data generating process (DGP). Moreover, if
there are more potential regressors than observations even parametric estimators collapse.
In their contributions to ATE estimation Belloni, Chernozhukov, and Hansen (2014) and Chernozhukov,
Chetverikov, Demirer, Duflo, Hansen, Newey, and Robins (2018a) show how AIPW type estimators can
be adapted to settings where the dimension of the relevant confounders grows with the sample size. They
use various machine learning methods to estimate the propensity score and the outcome projections.
Recently, Chernozhukov and Semenova (2018) use their framework to estimate effect heterogeneity based
on linear models. They provide conditions under which their second-stage linear model can become increasingly flexible.
Postulating nonparametric second stages, we do not assume any specific functional form of the GATE.
We contribute to the literature on GATE estimation by allowing for flexible functional forms as well as a
high-dimensional5 confounder space. This enables our proposed estimator to be robust against functional
form misspecification and to remain consistent even if the number of covariates relative to the sample
size is large. In particular, we provide a generic statistical framework such that the convergence rate
requirements of the first stage nuisance estimation are coupled with the kernel bandwidth second stage
nonparametric convergence.
Additionally, we link our identification and estimation result to semiparametric efficiency theory by providing a new estimator for the ATE that can be estimated as a by-product of the GATEs. The estimator
aggregates over all point estimates of the GATEs. We show that under certain convergence conditions for
the kernel bandwidth, asymptotically it hits the variance lower bound of the semiparametric estimation
problem. We therefore also contribute to the small literature on three-step semiparametric ATE estimation. Specifically, Hahn and Ridder (2013) (for an alternative theoretical development see also Mammen,
Rothe, and Schienle, 2012) investigate a related set-up showing that nonparametric regression on an
estimated propensity score can lead to efficient estimation of ATE. To the best of our knowledge, this
paper is, however, the first that analyses the asymptotic properties of averaging a transformed outcome
projection instead of the outcome projection on the propensity score. Like propensity score matching,
our three-step estimator might have better finite-sample properties than two-step estimators that share
the same first-order asymptotic properties (Robins et al., 1994, Hirano et al., 2003, Chernozhukov et al.,
2018a) because the propensity score weights are subject to an additional smoothing step. Unlike propensity score matching, IPW or Hahn’s (1998) estimator, the proposed ATE estimator remains feasible when
the dimension of the confounders entering the model is high.
5 In general the term ‘high-dimensional’ refers to the fact that the dimension of the model can grow with the sample size.
We will provide specific rate conditions in the main part of the paper.

3

After providing some more information on the theoretical background in Section 2, we present the details
of our main asymptotic results in Section 3. An empirical example in Section 4 compares different alternative estimators for GATE and ATE and illustrates the applicability and usefulness of the new methods.
The last section concludes. The formal proofs of our theorems as well as some details on the empirical
implementation are relegated to the Appendix.

2

Methodology

2.1

Notation

Suppose that we observe an independent and identically distributed random sample {wi }N
i=1 with sample
size N where wi = (yi , di , xi , zi ). Denote with uppercase letters a variable and with lowercase letters its
realizations. Then Y is the outcome variable and D is the binary treatment of interest. To describe causal
effects, we use Rubin’s (1974) potential outcome notation such that Y d is the outcome that would have
been observed under treatment D = d. Further, X is a matrix of observed covariates with support X
and Z ⊆ X as a set of predefined variables where the researcher is interested in effect heterogeneity with
support Z. Also let X ∈ Rdim X and Z ∈ Rdim Z and denote λX = dim X and λZ = dim Z. Potentially we
have that λX → ∞ when N → ∞ whereas λZ is fixed.6 Hence, we explicitly allow for models where the
dimension of X is high-dimensional but the dimension of the subset of covariates that is of interest for
the heterogeneity analysis does not grow with the sample size. We remain agnostic about the underlying
cumulative distribution from which the sample of W = (Y, D, X, Z) is drawn F = F (W ) and just assume
that it exists with density f = f (W ).

2.2

Semiparametric efficiency theory

The main parameter of interest in this study is the GATE defined as



τ (z) = E Y 1 − Y 0 |Z = z .

Since we want to avoid usually unrealistic parametric assumptions on the underlying DGP, we allow for
a flexible function ψ(W, ·) such that GATE is identified as

τ (z) = E [ψ(W, ·)|Z = z]
6 The

concrete growth rates of λX in relation to N will be discussed in Section 3.

4

using the variables observed or functions of them. Many possible transformations of the outcome exists
(see the references mentioned in the Introduction). It is, however, a priori unclear which one is a ‘good’
transformation in the sense that it achieves the variance lower bound for the problem. Hence, ideally
our exposition would start by deriving the semiparametric efficiency bound for the problem at hand
and then use the moment condition implied as an estimand for GATE. However, since parameters using
‘last stage’ nonparametric projections are not pathwise differentiable, standard semiparametric efficiency
bounds cannot be derived following established theory (e.g. Bickel, Klaassen, Ritov, and Wellner, 1993,
Newey, 1994b, Hahn, 1998, Tsiatis, 2006). This was also noted for different problems in Rubin and van
der Laan (2007) and Kennedy, Ma, McHugh, and Small (2017). We follow their approaches. Instead of
directly relying on an efficiency result for GATE, we use that

ATE = θ = E [E [ψ(W, ·)|Z = z]]

implying Hahn’s (1998) efficient score function for ATE

ψ(W, p, m0 , m1 ) =

D(Y − m1 (X)) (1 − D)(Y − m0 (X))
−
+ m1 (X) − m0 (X)
p(X)
1 − p(X)

where p(X) = E [D|X] denotes the propensity score and md (X) = E [Y |X, D = d] for d ∈ {0, 1} denotes
the conditional expectations of the outcome in the treatment-specific subpopulations.

2.3

Parameter identification

For identification of GATE and ATE we make the following assumptions.

Assumption 1 (Conditional independence).

Y 0 , Y 1 ⊥ D|X = x

∀x ∈ X

Assumption 2 (Stable Unit Treatment Value Assumption (SUTVA)).

Y = DY 1 + (1 − D)Y 0

5

Assumption 3 (Exogeneity of confounders).

X1 = X0

Assumption 4 (Common support).

c < p(X) < 1 − c

for some small positive constant c.

Assuming that appropriate moments exist, then for GATE we have




τ (z) =E E Y 1 − Y 0 |X |Z = z


D(Y − m1 (X)) (1 − D)(Y − m0 (X))
−
+ m1 (X) − m0 (X) Z = z
=E
p(X)
1 − p(X)


DY
(1 − D)Y
=E
−
Z=z
p(X)
1 − p(X)
=E [m1 (X) − m0 (X)|Z = z] .

The exposition shows that IPW and outcome based estimands are embedded in the estimand based on
ψ(W, p, m0 , m1 ). Finally, by noticing that

θ = E [τ (Z)]

identification of ATE trivially follows from these considerations.

3
3.1
3.1.1

Main results
GATE estimation
Proposed estimator

The identification results from the preceding section suggest a two-step estimation strategy. The details
of our proposed estimator are described in Procedure 1. In a first step a sample plug-in versions of

6

Procedure 1. GATE estimation
Introduce the subsample index l = 1, ..., L and denote the corresponding information set by
Il as well as its complement by IlC .
1. Randomly split the sample in equally sized subsamples 1, ..., L.
2. for l = 1 to L do
Estimate the propensity score p(x) and the outcome projections m0 (x) and m1 (x) in the
sample with IlC using any suitable machine learning method or an ensemble of them.
Predict p̂(x), m̂0 (x) and m̂1 (x) in the sample with Il .
end
3. Denote p̂ = p̂l=1,...,L , m̂0 = m̂0,l=1,...,L and m̂1 = m̂1,l=1,...,L . Then construct the vector with
elements ψ̂ = ψ(Wi , p̂, m̂0 , m̂1 ) for i = 1, ..., N and estimate GATE as
τ̂ (z) =

N
X
K

zi −z
h



ψ(Wi , p̂, m̂0 , m̂1 )

zi −z
i=1 K
h

PN

i=1

where K = K(u) is some kernel function that depends on a bandwidth h.

ψ(W, p, m0 , m1 ) can be obtained by estimating the nuisance parameters. In a second step the ψ-vector
can be projected on Z. Our goal is to estimate both stages as flexible as possible and to avoid parametric
assumptions. Further, our estimator can cope with settings where λX is very large which precludes
classical nonparametric and parametric methods to estimate the first stage nuisances p(x), m0 (x) and
m1 (x). However, we can use a large class of supervised machine learning algorithms that have been
shown to be very effective predictors for such types of tasks. Following the suggestions of Chernozhukov
et al. (2018a) we apply a cross-fitting algorithm for the nuisance parameter estimation step in order
to guarantee that the resulting estimator of ψ(W, p, m0 , m1 ) consists of independent observations. The
requirements for the second stage estimation step are more sophisticated as this estimator should allow
for valid inference. To estimate GATE flexibly, we apply nonparametric local constant regression in the
second step.

3.1.2

Asymptotic results

We now investigate the theoretical properties of our proposed estimation procedure. To ease the notational burden, we start with some definitions.

Definition 1 (Norms). Denote by kg(X)kp the Lp norm of the generic function g(·). Further denote the
supremum norm by supX∈X |g(X)| = kg(X)k∞ .

7

Definition 2 (Rates). The nuisance parameter first stage estimates p̂, m̂0 and m̂1 obtained by the sample
splitting procedure described above belong to the realization sets P, M0 and M1 with probability 1 − o(1).
For any realization p∗ , m∗0 and m∗1 in the sets define the rates

md =

sup km∗d (X) − md (X)k2

m∗
d ∈Md

p = sup kp∗ (X) − p(X)k2
p∗ ∈P

max = max{m0 , m1 , p }.

Definition 3 (Scaling factor). For any function g define a scaling parameter δg that determines g =

O N −δg .

We then make the following standard assumptions on the kernel regression step (see for example Pagan
and Ullah, 1999).

Assumption 5 (Kernel regression).
1. Z = z is a point in the interior of the support Z.
2. The density function estimator is uniformly bounded away from zero such that inf z∈Z fˆ(z) ≥ C
where C > 0 is a generic constant.
3. The Kernel function K(u) is r times continuously differentiable, symmetric and of order r in the
R
R
sense ur−1 K(u)du = 0 and ur K(u)du = O(1) for r ∈ N.
4. f (z) and E (ψ(W, p, m0 , m1 )|Z = z) are r times continuously differentiable.
5. Further the Kernel function satisfies (i)

R

|K(u)|2+C du = O(1) for any C > 0,
R
= O(1) and (v) K 2 (u)du = O(1).

K(u)du = 1, (ii)

(iii) |u||K(u)| → 0 as |u| → ∞, (iv) kK(u)k∞

R

Assumption 5 comprises the standard nonparametric local constant regression assumptions allowing for
multiple covariates and higher-order kernels. For illustrative purposes multivariate regression results are
derived assuming the same bandwidth for every regressor. Further, we have to make boundedness assumptions on the second moment of the sample error of the outcome model and on the nuisance prediction
errors.

8

Assumption 6 (Boundedness of conditional variances).

The conditional variances of the outcome

models are bounded such that they obey
h
i
2
E (DY − m1 (X)) |X = O(1)

h
i
2
E ((1 − D)Y − m0 (X)) |X = O(1).

and

Assumption 7 (Boundedness of convergence rates).

The nuisance parameter prediction errors are

bounded such that they obey

sup km∗d (X) − md (X)k∞ = O(1)

for d ∈ {0, 1} and

m∗
d ∈Md

sup kp∗ (X) − p(X)k∞ = O(1).

p∗ ∈P

Additionally, the convergence rates of our first stage nuisance parameter prediction and the second stage
nonparametric regression are assumed to be as follows:

Assumption 8 (Coupled convergence (GATE)).

The bandwidth h and the sample size N jointly

converge such that
(i) h = o(1), N hλZ → ∞ as N → ∞ and
1

1

(ii) N 2 h 2 λZ hr = o(1).
Further, N and h satisfy the joint convergence conditions with the nuisance parameter convergence rates
1

(iii) h− 2 λZ max = o(1)
1

1

1

1

(iv) N 2 h− 2 λZ m0 p + N 2 h− 2 λZ m1 p = o(1).

Assumption 8 comprises the coupled convergence rate assumptions that are at the centre of our theoretical
results. Conditions (i) and (ii) quantify how the bandwidth has to converge to zero in relation to the
sample size N and the number of regressors λZ . As usual, the bandwidth has to go to zero but slower
than the sample size grows to infinity. Also the bandwidth has to be chosen such that the asymptotic bias
term vanishes faster than the variance. This allows to apply the Central Limit Theorem and makes the
estimator asymptotically unbiased. In particular, condition (ii) requires undersmoothing in the sense that
the bandwidth has to be below the mean squared error (MSE) optimal rate. As discussed for example in
Pagan and Ullah (1999), choosing a higher order kernel mitigates the problem.

9

Conditions (iii) and (iv) state that h has to be chosen such that first stage convergence rates vanish fast
enough. In particular, by condition (iv) the joint convergence rates from propensity score and outcome
√
projection estimation have to vanish faster than N scaled with the kernel bandwidth. Since h → 0,
this is a more restrictive assumption compared to the rate conditions usually required for average effects
estimation (see for example Chernozhukov et al., 2018a). In contrast to average effects, one is only
interested in estimating the effect at a prespecified point Z = z. Thus, since sample observations enter
the estimator in a weighted form, the prediction precision needed is for the lower effective sample size
around Z = z. Hence, the first stage prediction guarantees need to adapt to this smaller sample conditions
and therefore achieve a faster joint rate of convergence in terms of the sample size N . Condition (iii)
additionally prevents the worst rate from becoming arbitrarily slow especially when λZ is larger than
one. Still, our estimator has a ‘rate’ double robustness feature in the sense that joint rates can vanish
relatively slowly but all single first stage rates are restricted from converging very slowly. L2 convergence
rates of many supervised machine learning methods satisfy these properties under sparsity conditions.
For example Belloni and Chernozhukov (2013) show that the predictive error of the Lasso is of order
q

s log max(λX ,N )
O
where s the unknown number of true coefficients in the oracle model. Suppose that
N
s and λX are equal in the outcome and the propensity score models then we require

s2 log2 max(λX ,N )
N h λZ

→ 0.

Hence the dimension of the confounding variables λX can grow with the effective sample size N hλZ .
Similar rates can be shown for L2 boosting (Luo and Spindler, 2016) and nonlinear models like Random
Forests (Wager and Walther, 2016) or forms of Deep Neural Nets (Farrell, Liang, and Misra, 2018).7
A natural question is then if a bandwidth exists that satisfies the rate conditions in Assumption 8. Indeed,
one can show (for more details see Appendix B) that the theoretical range of possible bandwidth choices
can be described by
2(δp + δmd ) − 1
1
< δh <
λZ + 2r
λZ
and we achieve a condition for the order of the kernel

r > λZ

1 − (δp + δmd )
2(δp + δmd ) − 1

.

For example if we restrict ourselves on second order kernel functions then for λZ = 1 we require
δp + δmd =

3
5.

Similarly, for λZ = 2 and λZ = 3, δp + δmd =

2
3

and δp + δmd =

5
7

are required

respectively. Thus, for a growing dimension λZ the joint rate condition for the first stage nuisance parameters approaches the parametric rate.
7 For

the concrete dependence of sparsity conditions on the parameters of the predictors see the references mentioned.

10

The discussion indicates that given one has chosen an appropriate order of the kernel function, the researcher can choose the bandwidth somehow below but not too much below the MSE optimal rate. One
could therefore simply use a certain fraction (e.g. 0.9) of the cross-validation bandwidth choice. Hence,
from a practical perspective our bandwidth choice problem is equivalent to nonparametric regression with
undersmoothing.
Given these assumptions we can then derive the first main theoretical result.

Theorem 1. Under Assumptions 1-8 our proposed estimation procedure for GATE obeys
√

N hλZ (τ̂ − τ ) = √

N
X

1
N hλZ

1
i=1 N hλZ

K zih−z
PN
i=1 K


zi −z
h

 (ψ(Wi , p, m0 , m1 ) − τ ) + o(1)

and
√

2
with σGATE
=

R

2
N hλZ (τ̂ − τ ) →d N (0, σGAT
E)

K(u)2 du×E[(ψ(Wi ,p,m0 ,m1 )−τ )2 |Z=z ]
.
f (z)

Theorem 1 shows that under the assumptions discussed above the speed of convergence is determined only
by the nonparametric regression step. In particular, it does not depend on the first stage estimation steps.
An equivalent result can also be achieved by using IPW with nonparametric first stages (see Abrevaya
et al., 2015). However, this requires an additional bandwidth choice for the first stage propensity score
regression and is limited to the case when also λX is very small. The dimension of X can be increased
under functional form assumptions for the first stage. However, as shown by the authors the price to pay
is an increase in the asymptotic variance. This is not the case for the estimator proposed in this paper.
Also Theorem 1 is valid under generally weaker conditions compared to the results in Lee et al. (2016) for
√
parametric first stages. Heuristically8 , if the first stage estimators converge at N then our conditions
on the bandwidth are satisfied and our asymptotic results continue to apply. To this extent, our results
comprise the result of Lee et al. (2016) as a special case.
8 In contrast, to Lee et al. (2016) we use local constant instead of local linear regression and introduce cross-fitting for
nuisance parameter estimation. This should, however, not be a concern for the intuitive argument made.

11

3.2

Joint estimation of GATE and ATE

3.2.1

Proposed estimator

Given the considerations so far, it appears ‘naturally’ to estimate ATE in three steps as an average
of GATEs in the sample. The details of the proposed estimator are described in Procedure 2. As
Procedure 2. ATE estimation

1. Follow steps 1-3 of Procedure 1.
2. Predict GATE at every observation in the sample as τ̂ (zj ).
3. Estimate ATE as
θ̂ =

N
1 X
τ̂ (zj ).
N j=1

suggested in Chernozhukov et al. (2018a) one could also directly estimate ATE as the average of the
vector ψ(W, p, m0 , m1 ) using the first stage nuisance parameter predictions. However, as a by-product of
GATE estimation, using an additional kernel smoothing step may lead to an ATE estimator with better
finite sample properties. In particular, the propensity score weights do not enter the last step of our
estimator directly and our hope is that small misspecification errors of propensity scores close to zero
or one are therefore smoothed out. The sensitivity of estimators incorporating inverse propensity score
weights directly is the subject of many Monte Carlo experiments (e.g. Huber, Lechner, and Wunsch,
2013, and Frlich, 2004). We notice that similar reasoning is also behind three-step estimators that apply
nonparametric regression on an estimated propensity score often used in practice.

3.2.2

Asymptotic results

To obtain our theoretical results we have to modify Assumption 8 slightly.

Assumption 80 (Coupled convergence (ATE)). The bandwidth h and the sample size N jointly converge
such that
(i) h = o(1), N hλZ → ∞ as N → ∞,
1

1

(ii) N 2 h 2 λZ hr = o(1) and
(iii) N h4r = o(1) and N h2λZ → ∞.

12

Further, N and h satisfy the joint convergence conditions with the nuisance parameter convergence rates
(iv) h−λZ max = o(1) and
1

1

(v) N 2 h−λZ m0 p + N 2 h−λZ m1 p = o(1).

We notice that averaging over the estimated projection E [ψ(W, p, m0 , m1 )|X] is a partial mean problem in
the sense of Newey (1994a,b). While parts (i) and (ii) of Assumption 8 remain unchanged, the additional
condition (iii) is necessary in order to guarantee that the MSE of the kernel regression estimator scaled
1

with N 4 converges to zero. In this way we guarantee the applicability of Newey’s (1994b) framework.
We could have also assumed uniform convergence rates for the kernel regression step. However, this
would involve a unnecessarily strong condition (for a discussion see Newey, 1994b, pp. 1364-1368 and
also Newey and McFadden, 1994, p. 2205).
Since we want ATE to converge with a rate of

√

N , the requirements on the first stage convergence rates

in condition (v) are more restrictive than those in the respective condition of Assumption 8. The range
of theoretically feasible bandwidth choices reduces to

max

Assuming

1
4r

<

1
λZ +2r

1
1
,
4r λZ + 2r


< δh <

(δp + δmd ) −
λZ

1
2

.

we get a modified condition for the order of the kernel function

r > λZ

3
2

− (δp + δmd )

2(δp + δmd ) − 1

.

In general this result indicates that one relies on a higher-order kernel function whenever λZ > 1 when
GATE and ATE are estimated jointly.
Under the stronger Assumption 80 we can then derive the following efficiency result.

Theorem 2. Under Assumptions 1-7 and 80 and the regularity conditions on the nonparametric second
step as in Newey (1994b, pp. 1364-1368) our proposed estimation procedure for ATE has the influence
function
D(Y − m1 (X)) (1 − D)(Y − m0 (X))
−
+ m1 (X) − m0 (X) − θ
p(X)
1 − p(X)
and therefore obeys
√

2
N (θ̂ − θ) →d N (0, σAT
E)

13

2
where σAT
E is the semiparametric efficiency bound of Hahn (1998).

Conceptually, Theorem 2 underpins our intuition from semiparametric theory outlined in Section 2.2.
The result shows that indeed every estimator that involves a nonparametric projection of the AIPW
modified outcome on any low-dimensional subset of X is consistent, asymptotically normal and achieves
the semiparametric efficiency bound. This asymptotic result has also been shown for other estimators
already discussed in Section 1. In contrast to Hirano, Imbens, and Ridder’s (2003) estimator, Hahn’s
(1998) estimator and matching on the propensity score (Hahn and Ridder, 2013), we do not rely on
nonparametrically estimated first stages. Due to the fact that λX → ∞ these estimators are of no
practical use in our setting. Further, unlike AIPW with machine learning nuisance parameter estimation
(Chernozhukov et al., 2018a), our estimator involves an additional step. Therefore the inverse propensity
score does not directly enter our estimator but is smoothed through the additional nonparametric step.
Asymptotically, this does not make any difference as the result in Theorem 2 shows. However, in finite
sample this could be a major advantage over the usual AIPW estimator.

4

Illustrative example

We investigate the applicability of our methods using Cattaneo’s (2010) dataset on the effect of cigarette
smoking on birthweight available from the Stata website.9 The dataset contains the outcome variable
birthweight in grams (Y ), whether the mother smoked during pregnancy (D = 1) and several covariates
on the mother’s health and socio-economic background (X). A detailed description of all covariates in the
dataset can be found in Appendix C. Applied studies with different estimation approaches unambiguously
find negative average effects (see Abrevaya, 2006, da Veiga and Wilder, 2008, Walker, Tekin, and Wallace,
2009). Conditional average treatment effects were investigated by Abrevaya et al. (2015) and Lee et al.
(2016) who find that mother’s age is associated with increasingly negative effects of smoking. We replicate
their results and compare their estimators with ours. Clearly, this type of analysis is limited in its scope
since the true DGP remains unknown. However, the dataset has the particular advantage that some
strong hypothesis about the estimation results are plausible. (i) The effect of smoking on birthweight
should be either negative or zero. (ii) The effect should be increasingly negative with mother’s age.
As a second example we consider how the effect changes with the number of prenatal care visits. On
the one hand a very low number of care visits could indicate the mother’s insufficient access to medical
infrastructure and therefore could be associated with particularly negative effects. On the other hand a
9 The

original dataset can be retrieved from here.

14

very high number of care visits could indicate a poor health situation. Hence, it is a priori unclear how
the treatment effect and health care visits are exactly related.

4.1

Empirical results

Figure 1 depicts the main results of our empirical analysis. We estimate the GATEs as described in
Procedure 1 using an ensemble learner comprising Lasso, Ridge, Elastic Net and a Random Forest. The
weights of the ensemble are obtained by cross-validating the out-of-sample MSE of the procedure. X in
our specification is an extended variable set (‘alldata’) and is exactly documented in Appendix C. For
example in contrast to Lee et al. (2016) we also include the available characteristics for the father of the
child, since they could be a good predictor for the smoking behaviour of the mother. The covariates enter
our model very flexibly. For the penalized regression predictors we allow for polynomials up to order four
and all two way interactions. The Random Forest has the particular advantage of being an ensemble
of trees itself and is therefore very flexible by construction. The results are generally in line with the
hypothesis made. In particular, the effect of smoking is unambiguously negative over the whole support
of mother’s age and prenatal care visits. As expected the effect increases with age. Interestingly, a higher
number of prenatal care visits seems to be associated with higher negative effects.
We estimate all our results with second-order Gaussian kernel functions. The same analysis using higher
order Gaussian kernel functions as proposed by Li and Racine (2007) yields similar results (see Appendix
C). In practice the biggest challenge is to determine the bandwidth for the nonparametric regression.
To achieve undersmoothing, we multiply the bandwidth obtained by leave-one-out cross-validation with
0.9. Since this choice is arbitrary, the stability of our results towards this choice is a particular concern.
Figure 2 shows that our estimator is relatively robust regarding this choice. A major change in the shape
of the function only appears for massive oversmoothing. An equivalent analysis for prenatal care visits
yielding the same conclusion is relegated to Appendix C.
Table 1: Smoothed ATE estimators
Smoothed AIPW (age)

Smoothed AIPW (care visits)

Smoothed AIPW (age, care visits)

-238.937
(27.257)

-235.672
(27.257)

-236.904
(27.257)

Results for smoothed AIPW ATE estimation as in Procedure 2 using Z = age, Z = care visits and Z =
(age, care visits). Results were obtained with a second-order Gaussian kernel function and a 0.9×LOOCV
bandwidth choice. Nuisance parameters were estimated using an ensemble learner comprising Lasso,
Elastic Net, Ridge and Random Forest. For Lasso, Ridge and Elastic Net the penalty term was chosen
such that the cross-validation criterion was minimized. The ensemble weights were chosen by minimizing
out-of-sample MSE. Asymptotic standard errors are in parenthesis.

Finally, Table 1 shows the results for ATE estimation as described in Procedure 2. In line with the

15

Figure 1: AIPW GATE estimator with ensemble first stages
0

0

−100

−100

−200

−200

−300

−300

−400

−400

−500

−500
20

30
mother's age

40

0

10

20
care visits

30

40

effe
vis
its

ct
m

r's

re

he

ca

ot

ag

e

Results were obtained as described in Procedure 1 with a second-order Gaussian kernel function and a 0.9×LOOCV
bandwidth choice. Nuisance parameters were estimated using an ensemble learner comprising Lasso, Elastic Net, Ridge
and Random Forest. For Lasso, Ridge and Elastic Net the penalty term was chosen such that the cross-validation criterion
was minimized. The ensemble weights were chosen by minimizing out-of-sample MSE. Asymptotic confidence bands are at
the 95% level.

16

Figure 2: Sensitivity to bandwidth choice (age)
(a) 0.5 × CV choice

(b) 0.7 × CV choice

(c) 0.8 × CV choice

0

0

0

−100

−100

−100

−200

−200

−200

−300

−300

−300

−400

−400

−400

−500

−500
20

30
mother's age

40

−500
20

(d) 0.9 × CV choice

30
mother's age

40

20

(e) 1.0 × CV choice
0

0

−100

−100

−100

−200

−200

−200

−300

−300

−300

−400

−400

−400

−500
20

30
mother's age

40

40

(f) 1.5 × CV choice

0

−500

30
mother's age

−500
20

30
mother's age

40

20

30
mother's age

40

Results were obtained as described in Procedure 1 with a second-order Gaussian kernel function and different multiples of the
LOOCV bandwidth choice. Nuisance parameters were estimated using an ensemble learner comprising Lasso, Elastic Net,
Ridge and Random Forest. For Lasso, Ridge and Elastic Net the penalty term was chosen such that the cross-validation
criterion was minimized. The ensemble weights were chosen by minimizing out-of-sample MSE. Asymptotic confidence
bands are at the 95% level.

previous literature mentioned above, the average effect of smoking is estimated to be negative. Crucially,
the estimated effect turns out to be very robust regarding the choice of the smoothing variable.

4.2

Comparison with other estimators

A ‘fair’ comparison with other estimators is hardly feasible because our approach does not require specific
functional form assumptions.10 In other words, the related estimators of Abrevaya et al. (2015) and Lee
et al. (2016) suppose that they know the true propensity score or outcome projection specifications. Since
we cannot compare our estimator against every possible parametric specification, we use the specification
selected by Lee et al. (2016) as a benchmark. Figure 3 depicts GATE estimation results using the
benchmark models. Strikingly, the IPW based estimator gives implausible results. For mother’s age
positive effects of smoking can almost nowhere be excluded. For care visits we do not obtain a GATE
10 We do not consider nonparametric propensity score estimation as suggested in Abrevaya et al. (2015) because it most
likely does not allow to include all potential confounders in order to make Assumption 1 credible.

17

Figure 3: Other GATE estimators
(a) AIPW linear first stages (age)

(b) AIPW linear first stages (care visits)

0

0

−100

−100

−200

−200

−300

−300

−400

−400

−500

−500
20

30
mother's age

40

0

(c) IPW linear first stages (age)

10

20
care visits

30

40

(d) IPW linear first stages (care visits)

1000

1000

500

500

0

0

−500

−500

−1000

−1000
20

30
mother's age

40

0

10

20
care visits

30

40

Results were obtained following the procedures in Abrevaya et al. (2015) and Lee et al. (2016) with a second-order Gaussian
kernel function and a 0.9×LOOCV bandwidth choice. Nuisance parameters were estimated using Logit for the propensity
score and OLS for the outcome projections. Asymptotic confidence bands are at the 95% level.

18

estimation result for our bandwidth choice. In fact, ATE is estimated indicating that the bandwidth
is too large in order to obtain GATE estimates. In line with the theoretical result, standard errors are
inflated compared to our estimation procedure. The AIPW based estimator with parametric models
for the propensity score and the outcome projections gives plausible results for GATE with respect to
mother’s age. Slight differences arise when comparing the GATE curves regarding the number of prenatal
care visits.
Table 2: Averaged ATE estimators
Averaged AIPW (ensemble)

Averaged AIPW (parametric)

Averaged IPW (parametric)

-234.826
(27.257)

-242.990
(25.885)

-295.388
(45.110)

Results for ATE estimation using Inverse Probability Weighting (IPW) and Augmented IPW (AIPW).
For the ensemble learner nuisance parameters were estimated using an ensemble learner comprising
Lasso, Elastic Net, Ridge and Random Forest. For Lasso, Ridge and Elastic Net the penalty term was
chosen such that the cross-validation criterion was minimized. The ensemble weights were chosen by
minimizing out-of-sample MSE. For the parametric specifications nuisance parameters were estimated
using Logit for the propensity score and OLS for the outcome projections. Asymptotic standard errors
are in parenthesis.

Table 2 shows the results for ATE estimation. As expected the results of Procedure 2 in Table 1 are
roughly in line with the standard AIPW based ATE estimator with ensemble first stages.11 The relative
bad performance of IPW based estimation for the GATEs is also reflected in the estimation of ATE. In
particular, the standard error nearly doubles compared to AIPW based estimators and point estimates
are reduced. Interestingly, for average effects there seems to be only little value-added for the flexible
machine learning based estimators compared to the parametric specification.

5

Conclusion

In this study we propose new estimators for specific conditional and average causal effects when the
dimension of the covariate space is high. In particular, by discriminating the different roles of covariates
(adjusting for confounding vs. measuring causal heterogeneity of interest) in our approach, they can
be included very flexibly – not relying on any functional form assumptions. Rather, we show coupled
convergence conditions for the different steps involved. The procedures suggested are based on semiparametric efficiency theory. In this sense, our proposed three-step estimator for ATE estimation is shown to
reach the semiparametric efficiency bound. A widely used empirical example shows that our estimators
are useful in practice. Compared to other estimators their desirable theoretical properties and increased
11 This might also be seen as a implicit test for the credibility of the stronger conditions required for the smoothed
estimator compared to the averaged efficient score as in Chernozhukov et al. (2018a).

19

flexibility could lead to divergent empirical results.
The specific structure of the GATE problem should be easily applicable to related settings. For example
efficient score based estimation can also be used for instrumental variables problems (Chernozhukov et al.,
2018a), difference-in-differences estimation (Zimmert, 2018) and continuous treatment settings (Kennedy
et al., 2017).
Some other interesting problems and refinements are beyond the scope of this study and have to be left
for further research as well. For example, the nonparametric regression estimator could be refined to
the extent that its bandwidth is chosen in a data-adaptive manner. As an alternative to classical nonparametric regression, one could also investigate using methods from the toolbox of supervised machine
learning. This might help to get reliable estimators even for cases when the dimension of Z is moderately
higher than considered in this paper, while sacrificing only little flexibility.
Finally, it might be worth to investigate the finite sample properties of the proposed three-step estimators
for ATE compared to averaging the efficient score vector directly. Here, we consider our ATE estimator as a by-product of the GATE procedure underpinning the theoretical motivation of our framework.
While the smoothed three-step estimator is first order asymptotically equivalent to directly averaging
the efficient score vector, it might posses better finite sample properties since it does not directly rely on
propensity score weights. However, the finite sample performance may crucially rely on the bandwidth
choice and the set of covariates in Z. We regard this as yet another interesting direction for further
research.

20

References
Abadie, A. (2005). Semiparametric difference-in-differences estimators. Review of Economic Studies 72 (1),
1–19.
Abrevaya, J. (2006). Estimating the effect of smoking on birth outcomes using a matched panel data
approach. Journal of Applied Econometrics 21 (4), 489–519.
Abrevaya, J., Hsu, Y.-C., & Lieli, R. P. (2015). Estimating conditional average treatment effects. Journal
of Business and Economic Statistics 33 (4), 485–505.
Athey, S. & Imbens, G. (2019). Machine Learning Methods Economists Should Know About. Version 1.
arXiv: 1903.10075v1.
Athey, S. & Imbens, G. W. (2017). The state of applied econometrics: causality and policy evaluation.
The Journal of Economic Perspectives 31 (2), 3–32.
Athey, S., Tibshirani, J., & Wager, S. (2019). Generalized random forests. The Annals of Statistics 47 (2),
1148–1178.
Belloni, A. & Chernozhukov, V. (2013). Least squares after model selection in high-dimensional sparse
models. Bernouille 19 (2), 521–547.
Belloni, A., Chernozhukov, V., & Hansen, C. (2014). Inference on treatment effects after selection among
high-dimensional controls. Review of Economic Studies 81 (2), 608–650.
Bickel, P. J., Klaassen, C. A., Ritov, Y., & Wellner, J. A. (1993). Efficient and Adaptive Estimation for
Semiparametric Models. Springer.
Cattaneo, M. D. (2010). Efficient semiparametric estimation of multi-valued treatment effects under
ignorability. Journal of Econometrics 155 (2), 138–154.
Chernozhukov, V. & Semenova, V. (2018). Simultaneous Inference for Best Linear Predictor of the Conditional Average Treatment Effect and Other Structural Functions. Version 2. arXiv: 1702.06240v2.
Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., & Robins, J. (2018a).
Double/debiased machine learning for treatment and structural parameters. The Econometrics Journal 21 (1), C1–C68.
Chernozhukov, V., Fernández-Val, I., & Luo, Y. (2018b). The sorted effects method: discovering heterogeneous effects beyond their averages. Econometrica 86 (6), 1911–1938.
da Veiga, P. V. & Wilder, R. P. (2008). Maternal smoking during pregnancy and birthweight: a propensity
score matching approach. Maternal and Child Health Journal 12 (2), 194–203.
Fan, Q., Hsu, Y.-C., Lieli, R. P., & Zhang, Y. (2019). Estimation of Conditional Average Treatment
Effects with High-Dimensional Data. Version 1. arXiv: 1908.02399v1.

21

Farrell, M. H., Liang, T., & Misra, S. (2018). Deep Neural Networks for Estimation and Inference:
Application to Causal Effects and Other Semiparametric Estimands. Version 2. arXiv: 1809.09953v2.
Frlich, M. (2004). Finite-sample properties of propensity-score matching and weighting estimators. The
Review of Economics and Statistics 86 (1), 77–90.
Hahn, J. (1998). On the role of the propensity score in efficient semiparametric estimation of average
treatment effects. Econometrica 66 (2), 315–331.
Hahn, J. & Ridder, G. (2013). Asymptotic variance of semiparametric estimators with generated regressors. Econometrica 81 (1), 315–340.
Hirano, K., Imbens, G. W., & Ridder, G. (2003). Efficient estimation of average treatment effects using
the estimated propensity score. Econometrica 71 (4), 1161–1189.
Huber, M., Lechner, M., & Wunsch, C. (2013). The performance of estimators based on the propensity
score. Journal of Econometrics 175 (1), 1–21.
Kennedy, E. H., Ma, Z., McHugh, M. D., & Small, D. S. (2017). Non-parametric methods for doubly
robust estimation of continuous treatment effects. Journal of the Royal Statistical Society Series B
Statistical Methodology 79 (4), 1229–1245.
Knaus, M. C., Lechner, M., & Strittmatter, A. (2018). Machine Learning Estimation of Heterogeneous
Causal Effects: Empirical Monte Carlo Evidence. Version 2. arXiv: 1810.13237v2.
Lechner, M. (2018). Modified Causal Forests for Estimating Heterogeneous Causal Effects. Version 2.
arXiv: 1812.09487v2.
Lee, S., Okui, R., & Whang, Y.-J. (2016). Doubly robust uniform confidence band for the conditional
average treatment effect function. Journal of Applied Econometrics 32 (7), 1207–1225.
Li, Q. & Racine, J. (2007). Nonparametric Econometrics: Theory and Practice. Princeton University
Press.
Lunceford, J. & Davidian, M. (2004). Stratification and weighting via the propensity score in estimation
of causal treatment effects: a comparative study. Statistics in Medicine 23 (19), 29372960.
Luo, Y. & Spindler, M. (2016). High-Dimensional L2 Boosting: Rate of Convergence. Version 2. arXiv:
1602.08927v2.
Mammen, E., Rothe, C., & Schienle, M. (2012). Nonparametric regression with nonparametrically generated covariates. The Annals of Statistics 40 (2), 1132–1170.
Newey, W. K. (1994a). Kernel estimation of partial means and a general variance estimator. Econometric
Theory 10 (2), 233–253.
— (1994b). The asymptotic variance of semiparametric estimators. Econometrica 62 (6), 1349–1382.

22

Newey, W. K. & McFadden, D. (1994). “Large sample estimation and hypothesis testing.” Handbook of
Econometrics. Vol. 4. Elsevier Science B.V. Chap. 36, 2113–2245.
Pagan, A. & Ullah, A. (1999). Nonparametric Econometrics. Cambridge: Cambridge University Press.
Robins, J. M., Rotnitzky, A., & Zhao, P. (1994). Estimation of regression coefficients when some regressors
are not always observed. Journal of the American Statistical Association 89 (427), 846–866.
Rubin, D. & van der Laan, M. J. (2007). A doubly robust censoring unbiased transformation. The
International Journal of Biostatistics 3 (1), Article 4.
Rubin, D. B. (1974). Estimating causal effects of treatments in randomized and nonrandomized studies.
Journal of Educational Psychology 66 (5), 688–701.
Tsiatis, A. A. (2006). Semiparametric Theory and Missing Data. Springer Series in Statistics. Springer
Science+Business Media.
Wager, S. & Athey, S. (2018). Estimation and inference of heterogeneous treatment effects using random
forests. Journal of the American Statistical Association 113 (523), 1228–1242.
Wager, S. & Walther, G. (2016). Adaptive Concentration of Regression Trees, with Application to Random
Forests. Version 3. arXiv: 1503.06388v3.
Walker, M., Tekin, E., & Wallace, S. (2009). Teen smoking and birth outcomes. Southern Economic
Journal 75 (3), 892–907.
Zimmert, M. (2018). Efficient Difference-in-Differences Estimation with High-Dimensional Common Trend
Confounding. Version 4. arXiv: 1809.01643v4.

23

A
A.1

Proof of Theorems
Proof of Theorem 1

We can write

τ̂ − τ =

PN

1
N h λZ

1
N h λZ

=

zi −z
h



(ψ(Wi , p̂, m̂0 , m̂1 )) − τ

PN
zi −z
1
i=1 K
h
N hλZ

PN
zi −z
(ψ(Wi , p, m0 , m1 ) − τ )
i=1 K
h

PN
zi −z
1
i=1 K
h
N h λZ
i=1

K

|

{z

}

i

1
N h λZ

+

PN

i=1

K

zi −z
h



(ψ(Wi , p̂, m̂0 , m̂1 ) − ψ(Wi , p, m0 , m1 ))

PN
zi −z
1
i=1 K
h
N h λZ
{z
}

|

ii

Influence function
Denote ψ̄i = ψ(Wi , p̂, m̂0 , m̂1 ) − ψ(Wi , p, m0 , m1 ). Then the second term can be further expanded as

ii =

1
N h λZ

PN

1
N h λZ

|


K

i=1 E
PN
i=1

K

Z−z
ψ̄
h

zi −z
h

1
N h λZ

 

{z

+
}

iia

PN

i=1

zi −z
ψ̄i − E K
h

PN
zi −z
i=1 K
h



K

1
N hλZ

|



{z

iib

Z−z
h

 
ψ̄
}

and therefore
√

N hλZ ii ≤

√

N hλZ iia +

√

N hλZ iib .

Bounding iia
We first of all notice that
√

N hλZ iia

√
 


N
Z −z
−1
ˆ
(ψ(W, p̂, m̂0 , m̂1 ) − ψ(W, p, m0 , m1 ))
≤ f (z)
E K
× √
h
hλZ

and

fˆ(z)−1 ≤ sup fˆ(z)−1 =
z∈Z

24

1
inf z∈Z fˆ(z)

≤

1
= O(1)
C

by Assumption 5.
Further, under the sample splitting procedure used
 


Z −z
E K
(ψ(W, p̂, m̂0 , m̂1 ) − ψ(W, p, m0 , m1 ))
h
 


Z −z
=E K
(ψ(W, p̂, m̂0 , m̂1 ) − ψ(W, p, m0 , m1 )) |Wi∈Ilc
h
 


Z −z
≤
sup
E K
(ψ(W, p∗ , m∗0 , m∗1 ) − ψ(W, p, m0 , m1 ))
∗
h
p∗ ∈P,m∗
0 ∈M0 ,m1 ∈M1
Define the Gâteaux derivative of the generic function g in the direction [p∗ − p, m∗0 − m0 , m∗1 − m1 ] by
∂[p∗ −p,m∗0 −m0 ,m∗1 −m1 ] g. Then using Taylor’s expansion we can write


 
Z −z
(ψ(W, p∗ , m∗0 , m∗1 ) − ψ(W, p, m0 , m1 ))
E K
h
 


Z −z
= ∂[p∗ −p,m∗0 −m0 ,m∗1 −m1 ] E K
ψ(W, p, m0 , m1 )
h


 
Z −z
1 2
E
K
ψ(W,
p,
m
,
m
)
+ ...
+ ∂[p
∗
∗
∗ −p,m −m ,m −m ]
0
1
0
1
0
1
2
h
For the first order term we get
 


Z −z
∂[p∗ −p,m∗0 −m0 ,m∗1 −m1 ] E K
ψ(W, p, m0 , m1 )
h
" 



D(Y − m1 (X)) (1 − D)(Y − m0 (X))
Z −z
−
+
(p∗ (X) − p(X))
=E K
h
p(X)2
(1 − p(X))2
!#




(1 − D)
D
∗
∗
+
− 1 (m0 (X) − m0 (X)) + 1 −
(m1 (X) − m1 (X))
1 − p(X)
p(X)
=0

by the Law of Iterated Expectation and using the fact that Z ⊆ X. For the second order term we get
 


1 2
Z −z
∂ ∗
E K
ψ(W, p, m0 , m1 )
∗
∗
2 [p −p,m0 −m0 ,m1 −m1 ]
h
" 
 

Z −z
D(Y − m1 (X)) (1 − D)(Y − m0 (X))
2
=E K
−
(p∗ (X) − p(X))
h
p(X)3
(1 − p(X))3
1−D
(p∗ (X) − p(X)) (m∗0 (X) − m0 (X))
(1 − p(X))2
!#
D
+
(p∗ (X) − p(X)) (m∗1 (X) − m1 (X))
p(X)2
" 

Z −z
1
=E K
(p∗ (X) − p(X)) (m∗0 (X) − m0 (X))
h
(1 − p(X))
+

25

!#
1
∗
∗
+
(p (X) − p(X)) (m1 (X) − m1 (X))
p(X)
"
1
≤ kK(u)k∞ × E
(p∗ (X) − p(X)) (m∗0 (X) − m0 (X))
(1 − p(X))
#
1
∗
∗
+
(p (X) − p(X)) (m1 (X) − m1 (X)) Z
p(X)
1

1
1
≤C
(p∗ (X) − p(X)) (m∗0 (X) − m0 (X)) +
(p∗ (X) − p(X)) (m∗1 (X) − m1 (X))
(1 − p(X))
p(X)

1

≤ C × kp∗ (X) − p(X)k2 × (km∗0 (X) − m0 (X)k2 + km∗1 (X) − m1 (X)k2 )
which follows from Hlder’s and Jensen’s inequality, kK(u)k∞ = O(1) in Assumption 5 and Assumption
4. All higher order terms can be shown to be dominated by the second order term under the boundedness
Assumption 7. Therefore


 
Z −z
E K
(ψ(W, p̂, m̂0 , m̂1 ) − ψ(W, p, m0 , m1 )) = O(p m0 + p m1 )
h
and
√

 1 1

N hλZ iia = O N 2 h− 2 λZ × (p m0 + p m1 ) .

Bounding iib
We can write
√

N hλZ iia =

√ 1
N h λZ

≤√

≤√

1
hλZ
1
hλZ

PN

zi −z
h
PN
i=1


 
ψ̄i − E K Z−z
ψ̄
h

z
−z
1
K ih
N h λZ


 
 
N 
1 X
zi − z
Z −z
fˆ(z)−1 √
K
ψ̄i − E K
ψ̄
h
h
N i=1


 
 
N 
1
1 X
zi − z
Z −z
√
K
ψ̄i − E K
ψ̄
C
h
h
N i=1
i=1

K



which follows again from Assumption 5. The convergence of the last factor term remains to show. Since
L is a fixed integer that is independent of N , it suffices to show that for any l ∈ [L] the term converges.
More formally


 
 


 
 
N 
n 
1 X
zi − z
Z −z
1 X
zi − z
Z −z
√
ψ̄i − E K
ψ̄
≤ max √
ψ̄i − E K
ψ̄
K
K
h
h
h
h
l∈[L]
N i=1
nL i∈Il
26

where Il is the set of observation in subsample l and Ilc is the set of observations not in subsample l.
Under the sample splitting procedure we have





 
  2
n 
X
1
z
−
z
Z
−
z
i

E √
K
ψ̄i − E K
ψ̄
h
h
n
i∈Il




 
  2
n 
X
1
z
−
z
Z
−
z
i
Wi∈Ilc 
= E √
K
ψ̄i − E K
ψ̄
h
h
n
i∈Il
" 
#

2
zi − z
∗
∗
∗
≤
sup
E K
(ψ(W, p , m0 , m1 ) − ψ(W, p, m0 , m1 ))
∗
h
p∗ ∈P,m∗
0 ∈M0 ,m1 ∈M1
h
i
2
E (ψ(W, p∗ , m∗0 , m∗1 ) − ψ(W, p, m0 , m1 )) |Z
sup
≤ sup K(u)2 ×
∗
p∗ ∈P,m∗
0 ∈M0 ,m1 ∈M1

u

2

≤ kK(u)k∞

h

sup
∗
p∗ ∈P,m∗
0 ∈M0 ,m1 ∈M1

≤C

sup
∗
p∗ ∈P,m∗
0 ∈M0 ,m1 ∈M1

2

E |ψ(W, p∗ , m∗0 , m∗1 ) − ψ(W, p, m0 , m1 )|

1

i

h
i
2
E |ψ(W, p∗ , m∗0 , m∗1 ) − ψ(W, p, m0 , m1 )|

by Hlder’s inequality, Jensen’s inequality and Assumption 5. Now

sup
∗
p∗ ∈P,m∗
0 ∈M0 ,m1 ∈M1

 h
i 21
2
E |ψ(W, p∗ , m∗0 , m∗1 ) − ψ(W, p, m0 , m1 )|
"

=

sup
∗
p∗ ∈P,m∗
0 ∈M0 ,m1 ∈M1

≤

sup
∗
p∗ ∈P,m∗
0 ∈M0 ,m1 ∈M1

sup
∗
p∗ ∈P,m∗
0 ∈M0 ,m1 ∈M1

+

D(Y − m∗1 (X)) (1 − D)(Y − m∗0 (X))
D(Y − m1 (X))
−
+ m∗1 (X) − m∗0 (X) −
∗
∗
p (X)
1 − p (X)
p(X)
#! 21

(1 − D)(Y − m0 (X))
− m1 (X) + m0 (X)
1 − p(X)

+

+

E

sup
∗
p∗ ∈P,m∗
0 ∈M0 ,m1 ∈M1

2

km∗1 (X) − m1 (X)k2 +

sup
∗
p∗ ∈P,m∗
0 ∈M0 ,m1 ∈M1

D(Y − m∗1 (X)) D(Y − m1 (X))
−
p∗ (X)
p(X)

km∗0 (X) − m0 (X)k2

2

(1 − D)(Y − m∗0 (X)) (1 − D)(Y − m0 (X))
−
1 − p∗ (X)
1 − p(X)

2

and by defining U = DY − m1 (X)
D(Y − m∗1 (X)) D(Y − m1 (X))
−
p∗ (X)
p(X)

=
2

≤c

1
(D(Y − m∗1 (X))p(X) − D(Y − m1 (X))p∗ (X))
p(X)p∗ (X)
−2

kD(Y −

m∗1 (X))p(X)

∗

− D(Y − m1 (X))p (X)k2

=c−2 kp(X)(m1 (X) − m∗1 (X)) + U (p(X) − p∗ (X))k2
≤c−2 km1 (X) − m∗1 (X)k2 + c−2 kU (p(X) − p∗ (X))k2 .

27

2

Since

kU (p(X) − p∗ (X))k2 =
=

r h
i
2
E (U (p(X) − p∗ (X)))
p

E [E [U 2 |X] (p(X) − p∗ (X))2 ]

and a similar argument for the other term by Assumption 6 we get

sup
∗
p∗ ∈P,m∗
0 ∈M0 ,m1 ∈M1

sup
∗
p∗ ∈P,m∗
0 ∈M0 ,m1 ∈M1

sup
∗
p∗ ∈P,m∗
0 ∈M0 ,m1 ∈M1

sup
∗
p∗ ∈P,m∗
0 ∈M0 ,m1 ∈M1

km∗1 (X) − m1 (X)k2 = m1
km∗0 (X) − m0 (X)k2 = m0
D(Y − m∗1 (X)) D(Y − m1 (X))
−
p∗ (X)
p(X)

= m 1 + p
2

(1 − D)(Y − m∗0 (X)) (1 − D)(Y − m0 (X))
−
1 − p∗ (X)
1 − p(X)

= m0 + p .
2

It follows that

sup
∗
p∗ ∈P,m∗
0 ∈M0 ,m1 ∈M1

h
i
2
E |ψ(W, p∗ , m∗0 , m∗1 ) − ψ(W, p, m0 , m1 )| ≤ max(p , m0 , m1 )2 = 2max .

By Markov’s inequality and the fact that if L is a constant independent of N it follows that


 
 
N 
zi − z
1 X
Z −z
√
K
≤ C × max
ψ̄i − E K
ψ̄
h
h
N i=1
and therefore
√

1

N hλZ iia = O(h− 2 λZ max ).

Collecting terms, we can write
√

N hλZ (τ̂ − τ ) =

√ 1
N hλZ

PN

zi −z
(ψ(Wi , p, m0 , m1 )
h

P
N
zi −z
1
i=1 K
h
N h λZ

i=1

K



− τ)

 1 1

1
+ O N 2 h− 2 λZ × (p m0 + p m1 ) + h− 2 λZ max .

Under the convergence conditions in Assumption 8 the first claim of the theorem is verified.

28

Asymptotic normality
Notice that under the standard conditions provided in Assumptions 5 and 8 on the nonparametric regression (see for example Pagan and Ullah, 1999, chapter 2)


N
1 X
zi − z
K
→p f (z).
N hλZ i=1
h
Therefore, we can rewrite the influence function as
√

N hλZ (τ̂ − τ ) = √

N

1
N hλZ

=√




zi − z
(ψ(Wi , p, m0 , m1 ) − τ ) + o(1)
h


N
1 X
zi − z
K
(ψ(Wi , p, m0 , m1 ) − E [ψ(Wi , p, m0 , m1 )|Z = zi ])
f (z) i=1
h
{z
}

1 X
K
f (z) i=1

1
N hλZ

|

ia

+√

1
1
N hλZ f (z)

N
X


K

i=1

zi − z
h



|

(E [ψ(Wi , p, m0 , m1 )|Z = zi ] − τ )
{z
ib

}

+ o(1).

The second term is the bias of the nonparametric regression estimator scaled with the convergence rate.
1

1

Thus, Assumption 8 implies ib = O(N 2 h 2 λZ hr ) = o(1). Under the usual assumptions on the existence
of higher order moments in Assumption 5, we can apply the Lyapunov Central Limit Theorem on ia as
in Pagan and Ullah (1999, chapter 3.4). Then
√



R

N hλZ (τ̂ − τ ) →d N 0,

h
i
2
K(u)2 du × E (ψ(Wi , p, m0 , m1 ) − τ ) |Z = z
.
f (z)
q.e.d.

A.2

Proof of Theorem 2

Similar to the proof in Theorem 1 we can write
N N
1 XX
θ̂ − θ =
N i=1 j=1

1
N h λZ

K



zj −zi
h

1
N hλZ



ψ (Wj , p̂, m̂0 , m̂1 )


−θ
zj −zi
j=1 K
h

PN

29

N N
1 XX
=
N i=1 j=1
|

1
N hλZ



K

zj −zi
h



ψ (Wj , p, m0 , m1 )


−θ
PN
zj −zi
1
K
λ
j=1
Z
h
Nh
{z
}
i


zj −zi
1
N N
(ψ (Wj , p̂, m̂0 , m̂1 ) − ψ (Wj , p, m0 , m1 ))
h
1 X X N hλZ K


+
.
PN
zj −zi
1
N i=1 j=1
K
λ
j=1
h
Nh Z
|
{z
}
ii

Bounding ii
Using the notation from the proof of Theorem 1 again leads to
N N
1 XX
ii =
N i=1 j=1
|

1
i
E K Z−z
ψ̄
h
N h λZ


P
zj −zi
N
1
K
λ
j=1
h
Nh Z



 

{z

}

iia

N N
1 XX
+
N i=1 j=1
|

1
N h λZ




 
i
ψ̄j − E K Z−z
ψ̄
h


.
P
zj −zi
N
1
K
λ
j=1
h
Nh Z
{z
}

K



zj −zi
h



iib

Then
N
1 X
|iia| =
N i=1

≤ sup
i

≤

1
hλZ

Z−zi
ψ̄
h


zj −zi
j=1 K
h

1
hλZ
1
N h λZ


E K
PN

1
hλZ
1
N h λZ


E K
PN

 

Z−zi
ψ̄
h


zj −zi
j=1 K
h

 

 
 
Z − zi
−1
ˆ
sup f (z)
× sup E K
ψ̄ .
h
i
z∈Z

By the same steps as in the proof of Theorem 1 we obtain
 


Z −z
E K
(ψ(W, p̂, m̂0 , m̂1 ) − ψ(W, p, m0 , m1 )) = O(p m0 + p m1 )
h
and therefore
√

√
N iia = O

N

hλZ

!
p × (m0 + m1 ) .

Also for iib we find that
√

N N
1 XX
N iib =
N i=1 j=1

√ 1λ
Nh Z



K




 
i
ψ̄i − E K Z−z
ψ̄
h


PN
zj −zi
j=1 K
h

zj −zi
h

1
N h λZ



30

≤

1
hλZ

≤C



 
 
N
1 X
zj − zi
Z − zi
−1
ˆ
× sup √
sup f (z)
K
ψ̄i − E K
ψ̄
h
h
N j=1
i
z∈Z

max
.
hλZ

Hence, for the overall term we have
√

√
N ii = O

N

hλZ

p × (m0

max
+ m1 ) + λZ
h

!
= o(1),

under the coupled convergence conditions of Assumption 80 .

Bounding i
For i notice that
√

√
Ni =



N θ̃ − θ ,

where
N N
1 XX
θ̃ =
N i=1 j=1

1
N h λZ

K



zj −zi
h

1
N hλZ



ψ (Wj , p, m0 , m1 )


.
PN
zj −zi
j=1 K
h

Thus, term i gives the contribution of estimating the nonparametric projection of the vector ψ =
ψ(W, p, m0 , m1 ) with population nuisance parameters on Z ∈ Z. To derive the influence function of
the estimator θ̃, we follow Newey’s (1994b) Proposition 4 which holds under the condition that the first
stage nonparametric estimator is bounded by any norm (and some further regularity conditions). For
example, using Assumption 80 we have

1

N4

N
X

1
N hλZ

j=1

K



zj −z
h

1
N h λZ



ψ (Wj , p, m0 , m1 )


− τ (z)
zj −z
K
j=1
h

= o(1),

PN

2

such that Assumption 5.1 in Newey (1994b) is satisfied for the L2 norm. In particular, we notice that
the influence function φ is composed of the moment condition and an adjustment term. The moment
condition of the problem is given by

E [τ̃ (Z) − θ] = 0

31

with τ̃ (Z) = E [ψ|Z].
Denote the general family of distributions of W = (Y, D, X, Z) as F = {F (W )}. Further, denote
Fβ (W ) ∈ F a subfamily of F that is a path in F indexed by β. Also let F0 be the true distribution
of W . Accordingly, W realizes with density fβ (W ) when β = 0. Additionally, define Eβ [g(W )] =
R
g(W )fβ (W )dw for the generic function g(·) and τ̃ (Z, β) = Eβ [ψ|Z]. Following the steps of Proposition
4 in Newey (1994b) indicates that one should evaluate the derivative
∂
E [τ̃ (Z, β)]
∂β
at β = 0. By the Chain Rule we have
∂
∂
∂
Eβ [τ̃ (Z, β)] =
Eβ [τ̃ (Z)] +
E [τ̃ (Z, β)]
∂β
∂β
∂β
at β = 0. Furthermore, for any τ̄ (Z) and for some path β we have the mean-square projection optimization
problem
"
τ̃ (Z, β) = arg max Eβ
τ̄

2 #
D(Y − m1 (X)) (1 − D)(Y − m0 (X))
−
+ m1 (X) − m0 (X) − τ̄ (Z)
,
p(X)
1 − p(X)

giving the first order condition

Eβ

Define S(W ) =


D(Y − m1 (X)) (1 − D)(Y − m0 (X))
−
+ m1 (X) − m0 (X) − τ̃ (Z, β) = 0.
p(X)
1 − p(X)

∂
∂β

ln fβ (W ) at β = 0. Then combining the two previous result gives

∂
∂
∂
E [τ̃ (Z, β)] = Eβ [τ̃ (Z, β)] −
Eβ [τ̃ (Z)]
∂β
∂β
∂β


∂
D(Y − m1 (X)) (1 − D)(Y − m0 (X))
= Eβ
−
+ m1 (X) − m0 (X) − τ̃ (Z)
∂β
p(X)
1 − p(X)



D(Y − m1 (X)) (1 − D)(Y − m0 (X))
=E
−
+ m1 (X) − m0 (X) − τ̃ (Z) S(W )
p(X)
1 − p(X)
at β = 0. It follows that the adjustment term is given by ψ(W, p, m0 , m1 ) − τ̃ (Z) and the influence
function has the form

φ = τ̃ (Z) − θ + ψ(W, p, m0 , m1 ) − τ̃ (Z)
=

D(Y − m1 (X)) (1 − D)(Y − m0 (X))
−
+ m1 (X) − m0 (X) − θ.
p(X)
1 − p(X)

32

Hence, combining the results for terms i and ii gives
√


N 
1 X di (yi − m1 (xi )) (1 − di )(yi − m0 (xi ))
√
N (θ̂ − θ) =
−
+ m1 (xi ) − m0 (xi ) − θ + o(1)
p(xi )
1 − p(xi )
N i=1

such that by the Central Limit Theorem we obtain
√


N (θ̂ − θ) →d N



V ar(Y |D = 1, X) V ar(Y |D = 0, X)
0, E
+
+ (m1 (X) − m0 (X) − θ)2
p(X)
1 − p(X)


.

q.e.d.

33

B

Details on the bandwidth ranges

Using the notation implied by Definition 2, notice that the convergence conditions in Assumption 8 imply
the following system of inequalities.
1
λZ δh − δmax < 0
2
1 1
+ λZ δh − (δp + δmd ) < 0
2 2
1 1
− λZ δh − rδh < 0
2 2
−δh < 0
1 − δh λZ > 0

The third and fourth inequality imply δh >
2(δp +δmd )−1
λZ

described by

<

2δmax
λZ

1
λZ +2r

<

1
λZ .

< δh <

1
λZ +2r

> 0. Further, the other inequalities imply δh <

It therefore follows that the possible range of the bandwidth can be

2(δp +δmd )−1
.
λZ

The range for ATE follows similarly from the convergence conditions in Assumption 80 .

34

C
C.1

Details on the empirical example
Covariates in the dataset
Table 3: Description of covariates in the dataset

variable

description

newly created

smalldata

alldata

Y
D
mmarried
mhisp
fhisp
foreign
alcohol
deadkids
mage
medu
fage
fedu
nprenatal
mrace
frace
prenatal1
prenatal2
prenatal3
order1
order2
order3
birthmonth1
birthmonth2
birthmonth3
birthmonth4
birthmonth5
birthmonth6
birthmonth7
birthmonth8
birthmonth9
birthmonth10
birthmonth11
birthmonth12

infant birth weight in grams
=1 if mother smoked during pregnancy
=1 if mother is married
=1 if mother is hispanic
=1 if father is hispanic
=1 if mother born abroad
=1 if alcohol consumed during pregnancy
=1 if previous birth were newborn died
mother’s age
mother’s educational attainment
father’s age
father’s educational attainment
number of prenatal care visits
=1 if mother is white
=1 if father is white
=1 if first prenatal visit in first trimester
=1 if first prenatal visit in second trimester
=1 if first prenatal visit in third trimester
=1 if first infant
=1 if second infant
=1 if j th infant with j ≥ 3
=1 if birth in January
=1 if birth in February
=1 if birth in March
=1 if birth in April
=1 if birth in May
=1 if birth in June
=1 if birth in July
=1 if birth in August
=1 if birth in September
=1 if birth in October
=1 if birth in November
=1 if birth in December

no
no
no
no
no
no
no
no
no
no
no
no
no
no
no
no
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes

yes
yes
no
yes
no
no
yes
yes
yes
yes
no
no
yes
yes
no
yes
no
no
yes
no
no
no
no
no
no
no
no
no
no
no
no
no
no

yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes

mean
3361.68
0.19
0.70
0.03
0.04
0.05
0.03
0.26
26.50
12.69
27.27
12.31
10.76
0.84
0.81
0.80
0.15
0.05
0.44
0.34
0.22
0.07
0.08
0.08
0.08
0.08
0.10
0.08
0.10
0.09
0.08
0.08
0.08

Sample with N = 4642 observations with 864 treated and 3778 non-treated. ‘smalldata’ indicates that the variable was
also used in Lee et al. (2016). ‘newly created’ indicates that the variable was additionally created from the original dataset
by the authors. ‘alldata’ contains the specification used for the estimation results in Section 4.

35

C.2

Additional sensitivity analysis
Figure 4: Sensitivity to kernel order (age)
(a) Fourth order kernel function

(b) Sixth order kernel function

0

0

−100

−100

−200

−200

−300

−300

−400

−400

−500

−500
20

30
mother's age

40

20

30
mother's age

40

Results were obtained as described in Procedure 1 with a fourth and sixth order Gaussian kernel function and 0.9×LOOCV
bandwidth choice. Nuisance parameters were estimated using an ensemble learner comprising Lasso, Elastic Net, Ridge
and Random Forest. For Lasso, Ridge and Elastic Net the penalty term was chosen such that the cross-validation criterion
was minimized. The ensemble weights were chosen by minimizing out-of-sample MSE. Asymptotic confidence bands are at
the 95% level.

36

Figure 5: Sensitivity to bandwidth choice (care visits)
(b) 0.7 × CV choice

(a) 0.5 × CV choice

(c) 0.8 × CV choice

0

0

0

−100

−100

−100

−200

−200

−200

−300

−300

−300

−400

−400

−400

−500

−500
0

10

20
care visits

30

40

−500
0

(d) 0.9 × CV choice

10

20
care visits

30

40

0

(e) 1.0 × CV choice

0

0

0

−100

−100

−100

−200

−200

−200

−300

−300

−300

−400

−400

−400

−500

−500
0

10

20
care visits

30

40

10

20
care visits

30

40

(f) 1.5 × CV choice

−500
0

10

20
care visits

30

40

0

10

20
care visits

30

40

Results were obtained as described in Procedure 1 with a second-order Gaussian kernel function and different multiples of the
LOOCV bandwidth choice. Nuisance parameters were estimated using an ensemble learner comprising Lasso, Elastic Net,
Ridge and Random Forest. For Lasso, Ridge and Elastic Net the penalty term was chosen such that the cross-validation
criterion was minimized. The ensemble weights were chosen by minimizing out-of-sample MSE. Asymptotic confidence
bands are at the 95% level.

37

