The Fitting of Straight Lines if Both Variables are Subject to Error
Author(s): Abraham Wald
Source: The Annals of Mathematical Statistics, Vol. 11, No. 3 (Sep., 1940), pp. 284-300
Published by: Institute of Mathematical Statistics
Stable URL: https://www.jstor.org/stable/2235677
Accessed: 24-10-2019 21:50 UTC
JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide
range of content in a trusted digital archive. We use information technology and tools to increase productivity and
facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org.
Your use of the JSTOR archive indicates your acceptance of the Terms & Conditions of Use, available at
https://about.jstor.org/terms

Institute of Mathematical Statistics is collaborating with JSTOR to digitize, preserve and
extend access to The Annals of Mathematical Statistics

This content downloaded from 206.253.207.235 on Thu, 24 Oct 2019 21:50:04 UTC
All use subject to https://about.jstor.org/terms

THE FITTING OF STRAIGHT LINES IF BOTH VARIABLES ARE

SUBJECT TO ERROR
BY ABRAHAM WALD

1. Introduction. The problem of fitting straight lines if both variables x

and y are subject to error, has been treated by many authors. If we have N > 2

observed points (xi, yi) (i = 1, * * , N), the usually employed method of least
squares for determining the coefficients a, b, of the straight line y = ax + b
is that of choosing values of a and b which minimize the sum of the squares of

the residuals of the y's, i.e. 2(axi + b - yi)2 is a minimum. It is well known
that treating y as an independent variable and minimizing the sum of the
squares of the residuals of the x's, we get a different straight line as best fit. It

has been pointed out' that if both variables are subject to error there is no
reason to prefer one of the regression lines described above to the other. For

obtaining the "best fit," which is not necessarily equal to one of the two lines
mentioned, new criteria have to be found. This problem was treated by R. J.
Adcock as early as 1877.2
He defines the line of best fit as the one for which the sum of the squares of
the normal deviates of the N observed points from the line becomes a minimum.
(Another early attempt to solve this problem by minimizing the sum of squares
of the normal deviates was made by Karl Pearson.3)

Many objections can be raised against this method. First, there is no justifi-

cation for minimizing the sum of the squares of the normal deviates, and not
the deviations in some other direction. Second, the straight line obtained by
that method is not invariant under transformation of the coordinate system.

It is clear that a satisfactory method should give results which do not depend
on the choice of a particular coordinate system. This point has been empha-

sized by C. F. Roos. He gives4 a good summary of the different methods and
then proposes a general formula for fitting lines (and planes in case of more than
two variables) which do not depend on the choice of the coordinate system.
I See for instance Henry Schultz' "The Statistical Law of Demand," Jour. of Political
Economy, Vol. 33, Dec. (1925).

2 Analyst, Vol. IV, p. 183 and Vol. V, p. 53.
3 "On Lines and Planes of Closest Fit to Systems of Points in Space" Phil. Mag. 6th
Ser. Vol. II (1901).
4 "A General Invariant Criterion of Fit for Lines and Planes where all Variates are
Subject to Error," Metron, February 1937. See also Oppenheim and Roos Bulletin of the
American Mathematical Society, Vol. 34 (1928), pp. 140-141.

284

This content downloaded from 206.253.207.235 on Thu, 24 Oct 2019 21:50:04 UTC
All use subject to https://about.jstor.org/terms

FITTING OF STRAIGHT LINES 285

Roos' formula includes many previo
gives an interesting geometric interpretation of Roos' general formula.
It is a common feature of Roos' general formula and of all other methods
proposed in recent years that the fitted straight line cannot be determined
without a priori assumptions (independent of the observations) regarding the
weights of the errors in the variables x and y. That is to say, either the standard
deviations of the errors in x and in y are involved (or at least their ratio is
included) in the formula of the fitted straight line and there is no method given

by which those standard deviations can be estimated by means of the observed
values of x and y.

R. Frisch7 has developed a new general theory of linear regression analysis,
when all variables are subject to error. His very interesting theory employs
quite new methods and is not based on probability concepts. Also on the basis

of Frisch's discussion it seems that there is no way of determining the "true"
regression without a priori assumptions about the disturbing intensities.
T. Koopmans8 combined Frisch's regression theory with the classical one in
a new general theory based on probability concepts. Also, according to his

theory, the regression line can be determined only if the ratio of the standard
deviations of the errors is known.

In a recent paper R. G. D. Allen9 gives a new interesting method for determining the fitted straight line in case of two variables x and y. Denoting by a.

the standard deviation of the errors in x, by a, the standard deviation of the
errors in y and by p the correlation coefficient between the errors in the two
variables, Allen emphasizes (p. 194)9 that the fitted line can be determined only

if the values of two of the three quantities OE, o-,, p are given a priori.
Finally I should like to mention a paper by C. Eisenhart,10 which contains
many interesting remarks related to the subject treated here.
In the present paper I shall deal with the case of two variables x and y in

which the errors are uncorrelated. It will be shown that under certain conditions:

(1) The fitted straight line can be determined without making a priori assumptions (independent of the observed values x and y) regarding the standard
deviations of the errors.

(2) The standard deviation of the errors can be well estimated by means of
5 For instance also Corrado Gini's method described in his paper, "Sull' Interpolazione
di una Retta Quando i Valori della Variable Independente sono Affecti da Errori Accidentalis," Metron, Vol. I, No. 3 (1921), pp. 63-82.

6 "Some Geometrical Considerations in the General Theory of Fitting Lines and Planes,"
Metron, February 1937.

7Statistical Confluence Analysis by Means of Complete Regression Systems, Oslo, 1934.
8 Linear Regression Analysis of Economic Time Series, Haarlem, 1937.
9 "The Assumptions of Linear Regression," Economica, May 1939.

l0 "The interpretation of certain regression methods and their use in biological and
industrial research," Annals of Math. Stat., Vol. 10 (1939), pp. 162-186.

This content downloaded from 206.253.207.235 on Thu, 24 Oct 2019 21:50:04 UTC
All use subject to https://about.jstor.org/terms

286

ABRAHAM

WALD

the observed values of x and y. The precision of the estimate increases with
the number of the observations and would give the exact values if the number
of observations were infinite. (See in this connection also condition V in
section 3.)

2. Formulation of the Problem. Let us begin with a precise formulation of
the problem. We consider two sets of random variables"
XI , .. * XZN ; Yl;X-'' * YX8N -

Denote the expected value E(xi) of xi by Xi and the expected value E(yi) of
yi by Yi (i= 1,= * - *, N). We shall call Xi the true value of xi, Y1 the true
value of yi , xi -Xi = e the error in the i-th term of the x-set, and yi -Yi =
the error in the i-th term of the y-set.

The following assumptipns will be made:

I. The random variables 61, ***, eN each have the same distribution and they
are uncorrelated, i.e. E(ejei) = 0 for i # j. The variance of e- is finite.
II. The random variables r, * * *, rlN each have the same distribution and are

uncorrelated, i.e. E(,jirxs) = 0 for i 0 j. The variance of -lt is finite.
III. The random variables e$ and rx (i = 1, ,N; j = 1, .. ,N) are uncorrelated, i.e. E(eiq,) = 0.
IV. A single linear relation holds between the true values X and Y, that is to
say Yi = alXi- + (i = 1) ** , N).
Denote by e a random variable having the same probability distribution as

possessed by each of the random variables C1, *.., CN, and by X7 a random
variable having the same distribution as X1, *. . , )'N .
The problem to be solved can be formulated as follows:
We know only two sets of observations: xl, * *, XN; Yi . * *YN where xi

denotes the observed value of xi and y' denotes the observed value of yi. We
know neither the true values X1, * * *, XN; Y1, * * * , YN , nor the coefficients
a and i3 of the linear relation between them. We have to estimate by means
of the observations xl, * * , XN ; Yi, * * YN, (1) the values of at and ,B, (2) the

standard deviation a- of e, and (3) the standard deviation a, of q.
Problems of this kind occur often in Economics, where we are dealing with
time series. For example, denote by xi the price of a certain good G in the
period t , and by yi the quantity of G demanded in t; . In each time period ti
there exists a normal price Xi and a normal demand Yi which would obtain if
the influence of some accidental disturbances could be eliminated. If we have
reason to assume that there exists between the normal price and the normal
demand a linear relationship we have to deal with a problem of the kind dR
scribed above.
In the following discussions we shall use the notations xi and yi also for their
11 A random or stochastic variable is a real variable associated with a probability
distribution.

This content downloaded from 206.253.207.235 on Thu, 24 Oct 2019 21:50:04 UTC
All use subject to https://about.jstor.org/terms

FITTING OF STRAIGHT LINES 287

observed values xi and y' since it will be clear in which sense they are meant
and no confusion can arise.

3. Consistent Estimates of the Parameters- a, fly E, Y o . For the sake of

simplicity we assume that N is even. We consider the expression

(xl + - - . + Xm) (Xm+l + * * + XN).

a, = ~N

(1)= (Yl + - - - + Ym) (Ym+1 + * * * + YN)

a2

=N

where m = N/2. As an estimate of a we shall use the expression

(2) a-= a2 _ (Yl + - - - + Ym) (Ympl + * + * + YN)
a() Xi + - -* + Xm) - (Xm+l + * - - + XN)

We make the assumption
V. The limit inferior of

(X1 + + Xm) - (Xm+l + + XN) (NI2,3,...ad.inf.
N

is positive.

We shall prove that a is a consistent estimate of a, i.e. a converges stochas-

tically to a with N -- oo, if the assumptions I-V hold. Denote the expected
value of a, by al and the expected value of a2 by a2 . It is obvious that
(X1 + - - + Xm) - (Xm+l + + XN)

al=

N

a

(3) y +(Y ++ Ym) N
(Ym+l + + YN)
a2=
On account of the condition IV we have

(4) a2= aai, or a2 = a.

The variance of a, - a1 is equal to or /N
to 2I /N. Hence a, and a2 converge stochas

From that and assumption V it follows t
a2~~~~~~~~~~~a
towards 2 = a. The intercept ,B of the regression line will be estimated by

(5) b =y-ax, where Xl + N+ XN and y= + + VN
Denote by X the arithmetic mean of X1, i , XN and by Y the arithmetic
mean of Y1, * * , YN . Since y ctonverges stochastically towards Y, x towards

This content downloaded from 206.253.207.235 on Thu, 24 Oct 2019 21:50:04 UTC
All use subject to https://about.jstor.org/terms

288

ABRAHAM

WALD

X, anid a towards a, b
tion IV it follows that Y - aX F ,B. Hence b converges stochastically

con

towards ,8.
Let us introduce the following notations:

s, = ( - N), = sample standard deviation of the x-observations,
8y = (y 2 )2 = sample standard deviation of the y-observations,
SZU = ; (X'i 2t) (yi Y)= sample covariance between the x-set and y-set.
SX, 8y and sxy denote the same expressions of the true values X1, *i * , XN;
It is obvious that

(6)

)E(2 = 82 + N
2N -1
(S.)=sx+o

( )~ ~E(a=n+,
~~~~. E(,S2) = s2 + ,2N
N-1
(7)
N
(8)

E(8X,)

where

=

siy,

E(s2),

E(Q2),

and

E(sy)

deno

Since Y, = aXi + 13, we have
(9)
(10)

sy

=

sxy

=

as1,
as4.

From (8), (9) and (10) we get

(11) S2 = E(s,%,)
(12) 82 = aE(sxy)If we substitute in (6) and (7) for S and S their values in (11) and (12),
we get

(13) o2 [E(s2) - E( )]N/(N - 1),

(14) 2 = [E(s) -aE(s)]N(N -1).
12 I observe that the equations (6), (7) and (8) are
gated by R. Frisch, Statistical Confluence Analysis pp. 51-52. See also Allen's equations
(4) 1.c. p. 194.

This content downloaded from 206.253.207.235 on Thu, 24 Oct 2019 21:50:04 UTC
All use subject to https://about.jstor.org/terms

FITTING OF STRAIGHT LINES 289

Since s2, s82, s8v converge stochastically towa
converges stochastically towards a, the expressions

(15)

-82

-

1)

and

(16) [s2 - as8,1N/(N - 1)

are consistent estimates of ao and ac respectively

4. Confidence Interval for a. In this section, as
only the assumptions I-IV are assumed to hold. In other words, all statements made in these sections are valid independently of Assumption V, except
where the contrary is explicitly stated.
Let us introduce the following notation:
X_XI + . . . + X.. yl + ***+ Y.

m + Y/1 m

Xtmn+ + * * * + XZN - =tn+l + . + 1/N

m
=

2

=

12

m

(Xi

Nim+I

m

2

'

N

-X

N

I .-1
(, - (l)2ii+l
+ ( (y x _ 2 )2
=

N

(Xi;-X1)
(Yi - 1) + E: ($jXi (yj-g2)
t i-1 j~~~~~~i-m+l
Xl, X2, Y1, Y2, (s~r)2, (sr)2 and sxy denote the same fuactions of the true

values X1, * . ., XN, Y1, * * YN . The expressions s,, sv, and s,v are

slightly different from the corresponding expressions s8, .5, and s8. The
reason for introducing these new expressions is that the distributions of s8,

sv,, and s,, are not independent of the slope a = a2 of the sample regression
a,

line, but s', sv and 4, are distributed independently from a (assuming that e
and q are normally distributed). The latter statement follows easily from the

fact that according to (1) and (2) a - Y/ -2 and s8, sI, 4 are distributed
Xi - X2

independently of xl, x2, g1 and g2.

This content downloaded from 206.253.207.235 on Thu, 24 Oct 2019 21:50:04 UTC
All use subject to https://about.jstor.org/terms

290

ABRAHAM

WALD

In the same way as we derived (13) and (14), we get

(13') a t =E(s _ ( )]/(N - 2),
(14') a2= [E(sv)2- aE(s')]N/(N-2).
These formulae differ from the corresponding formulae (13) and (14) only in
the denominator of the second factor, having there N - 2 instead of N - 1.

This is due to the fact that the estimates sr, sY, s,y are based on N - 1 deg
of freedom whereas s', sy and s'v are based only on N - 2 degrees of free
From (13') and (14') we get the following estimates"3 for o. and a,, :

(17) [(8s)2 - xyjN/(N - 2),

(18) [(Rs)2 - as',]N/(N - 2).
Hence we get as an estimate of a,, + a 2Oa the expression:

82 = [(s8)2 + a2(s8)2 - 2as' IN/(N - 2)

(19) E [(yi - aXi) - (l - aI)]2 + E [(y -aX) -(2 a- 2 ]2

NN 2f,
-

j=m+l
N) aX)]

2

Now we shall show that

(20)

(N - 2)82

a2)s

has the x2-distribution with N -2 degrees of freedom, provided that e and v
are normally distributed. In fact,

(Yi -axi) - (91- a-t) = IN - aci - (Xi- aii) (i=1 ,m)
and

(y acX )(g2 - at2) = 17 a e -(t2-C a2) (j = m + 1,***XN))
where
=
1

61+
m

*

*Em+l

62

m

-

+

+

-

m

EN
m

)71 + * * * + )7 2 - + + + 'ON

Since the variance of 1k - aek is equal to a2 + a 2au and s
correlated with s - aej (k $ 1) (k, 1 = 1, * * *, N), the expression (20) has the
x2-distribution with N - 2 degrees of freedom.
13 An "estimate" is usually a function of the observations not involving any unknown
parameters. We designate here as estimates also some functions involving the parameter a.

This content downloaded from 206.253.207.235 on Thu, 24 Oct 2019 21:50:04 UTC
All use subject to https://about.jstor.org/terms

FITTING OF STRAIGHT LINES 291

Now we shall show that

(21)

v'N

a,(a

-

a)

is normally distributed with zero mean and unit variance. In fact from the
equations (1)-(4) it follows that
ai(a-a)=2+ ' 2 al(2)

= a2 + l 72,(al+E1-2)Q2)

71-f2 I - E1-62

-i - af2Z
2

Since

2

the

latter
2~ + a2

expr

normally distributed) with zero mean and variance a N e ou
about (21) is proved.

Obviously (20) and (21) are independently distributed, hence V/N -2 times
the ratio of (21) to the square root of (20), namely,

(22) t =VN 2N al(a-cr) ai(a-a)VN

V/N -2 s V/(S)2 + a2(S.)2 - 2

has the Student distribution with N - 2 degrees of freedom. Denote by to the,
critical value of t corresponding to a chosen probability level. The deviation
of a from an assumed population value a is significant if

ai(a-_a)V/N-_2 >

I\/('S)2 + a2(s')2 - 2as1

The confidence interval for a can be obtained by solving the equation in a,

(23) a2(a - a)2 = [(81)2 + a 2(S1)2- 2a8sI N Now we shall show that if the relation

(24)

a2

holds,

>

the

[alad2.

(s)2'
roots

From

a,

and

(19)

it

a2

are

rea

follows

(s) + a2(sx)2 - 2as > 0
for all values of a. Hence, for a = a the left hand side of (23) is smaller than
the right hand side. On account of (24) there exists a value a' > a and a

This content downloaded from 206.253.207.235 on Thu, 24 Oct 2019 21:50:04 UTC
All use subject to https://about.jstor.org/terms

292

ABRAHAM

WALD

value a" < a such that the left hand side of (23) is greater than the right hand

side for a = a' and a = a". Hence one root must lie between a and a' and the
other root between a" and a. This proves our statement. The relation (24)
always holds for sufficiently large N if Assumption V is fulfilled. The confidence interval of ca is the interval [al , a21. For very small N (24) may not hold.
Finally I should like to remark that no essentially better estimate of the
variance of v - ae can be given than the expression S2 in (19). In fact, we

have 2N observations xl, ... , XN; YI * * *, YN . For the estimation of the
variance of ? - ace we must eliminate the unknowns X1, * * *, XN and B. (The

unknowns Y1, ''' , YN are determined by the relations Yi = aXi + ,B and a is
involved in the expression whose variance is to be determined.) Hence we have
at most N - 1 degrees of freedom and the estimate in (19) is based on N - 2
degrees of freedom.

5. Confidence Interval for $ if a is Given. In this case the best estimate of 8
is given by the expression:

ba= at where =XI +N XN and y= + + YN
We have

ba-/ = (a- Y) -a(U - ) = fl-a
where

C-1 + ***+ eN X11 + ***+ nN

N ,and = N

Hence,

(25)

V/N (ba-t3

o(+b

Y

is normally distributed with zero mean and unit variance. It is obvious that
the expressions (20) and (25) are independently distributed. Hence -/N - 2
times the ratio of (25) to the square root of (20), i.e.

t= -VN 2\N (bax- 3) = IN- 2 (ba-)

IN2 - 2s \/(s8)2 + a2(s')2- 2as'ly

has the Student distribution with N - 2 degrees of freedom. Denoting by to
the critical value of t according to the chosen probability level, the confidence
interval for ,B is given by the interval:

r \/(SV)2 + Ct%(g2 2 /(s)2 + at2(s)2 - 2a84 1

I ba + VIN -_ 2 ' ba VN-2 J

This content downloaded from 206.253.207.235 on Thu, 24 Oct 2019 21:50:04 UTC
All use subject to https://about.jstor.org/terms

FITTING OF STRAIGHT LINES 293

6. Confidence Region for a and 1 Jointly. In most practical cases we want to

know confidence limits for a and 6B jointly. A pair of values a, 1 can be represented in the plane by the point with the coordinates a, 13. A region R of this
plane is called confidence region of the true point (a, 13) corresponding to the
probability level P if the following two conditions are fulfilled.

(1) The region R is a function of the observations xi, * , x * , YN,
i.e. it is uniquely determined by the observations.

(2) Before performing the experiment the probability that we shall obtain
observed values such that (a, 1) will be contained in R, is exactly equal to P.
P is usually chosen to be equal to .95 or .99.

We have shown that the expressions (21) and (25), i.e.

v N a, (a -a) -\N (ba - 3)

.V/F2
2 0.,
v/'r2
2 2
.I/ a
a +
o.
are normally distributed with zero mean and unit variance. Now we shall
show that these two quantities are independently distributed. For this purpose

we have only to show that x, y, a, and a2 are independently distributed (a, and a2
are defined in (1)), but since

a, - E(a1) = (i1- -/2
a2- E(a2) = (171 - i72)/2

- E(X) = i

- E(BY) = -7,

we have only to show that I, f, it - , !12 - #2are independently d
We obviously have
_ 1++ 1 &1 + f12

2

'

2

It is evident that ii, 2, fi and 12 are independently distributed. Hence,
- 2)] = (Ee-E12)/2 = 0 and also E[17(1l -12)] = (Et - E 2)/2 = 0.
Since e1- 2, I - 12, and e and 17 are normally distributed, the independence
of this set of variables is proved, and therefore also (21) and (25) are independently distributed. It is obvious that the expression (20) is distributed
independently of (21) and (25). From this it follows that

N-2 N[a2(a-aa)2 + (-ax - )2I
2

(N-2)s2

(26) (N-2)[a2(a _ a)2 + (g- _ a- 1)2]
- 2[(sI)2
+ a2('1)2
2as8
2 [(s&)
a cz(X)- -2ts,
has the F-distribution (analysis of variance distribution) with 2 and N - 2
degrees of freedom. The F-distribution is tabulated in Snedecor's book: Calcu-

This content downloaded from 206.253.207.235 on Thu, 24 Oct 2019 21:50:04 UTC
All use subject to https://about.jstor.org/terms

294

ABRAHAM

WALD

lation and Interpretation of Analysis of Variance, Collegiate Press, Ames, Iowa,
1934. The distribution of I log F = z is tabulated in R. A. Fisher's book:
Statistical Methods for Research Workers, London, 1936. Denote by Fo the
critical value of F corresponding to the chosen probability level P. Then the

confidence region R is the set of points (a, P8) which satisfy the inequality
N-2 a2(a_-a)2 + (y-a 2

(27) 2 (1)2 + 2( I)2 - 2Fs
The boundary of the region is given by the equation

2 aNa-2(a
_(a-)2 + (- _))2 a
= 2FO
[( 0 )2 + 2 2(
2
(28)
2SX)2
2as'
I.
This is the equation of an ellipse. Hence the region R is the interior of the

ellipse defined by the equation (28). If Assumption V holds, the length of the
axes of the ellipse are of the order 1/V/N, hence with increasing N the ellipse
reduces to a point.

7. The Grouping of the Observations. We have divided the observations in

two equal groups G1 and G2, GI containing the first half (xi, y'), * * *, (x" , yi)
and G2 the second half (Xm+lm yYml)y * * * , (XN, YN) of the observations. All
the formulas and statements of the previous sections remain exactly valid for
any arbitrary subdivision of the observations in two equal groups, provided
that the subdivision is defined independently of the errors el, * **, EN;

711 , - * - , N . The question of which is the most advantageous grouping arises,

i.e. for which grouping will a be the most efficient estimate of a (will lead to

the shortest confidence interval for a). It is easy to see that the greater I a, I
the more efficient is the estimate a of a. The expression j a, I becomes a maxi-

mum if we order the observations such that x1 ? X2 < ... < XN. That is to

say I a, I becomes a maximum if we group the observations according to the

following:

RULE I. The point (xi, Yi) belongs to the group G1 if the number of elements
xi (j $ i) of the series xi, * , XN for which xi < xi is less than m = N/2. The
point (xi, yi) belongs to G2 if the number of elements xi (j - i) for which xi < xi
is greater than or equal to m.

This grouping, however, depends on the observed values xi, * , XN and is
therefore in general not entirely independent of the errors el, * **, EN. Let us
now consider the grouping according to the following:
RULE II. The point (xi, yi) belongs to the group G1 if the number of elements

Xi of the series X1, i , XN for which Xi < Xi (j $ i) is less than m. The
point (xi, yi) belongs to G2 if the number of elements Xi for which Xi ?< Xi (j # i)
is equal to or greater than m.

This content downloaded from 206.253.207.235 on Thu, 24 Oct 2019 21:50:04 UTC
All use subject to https://about.jstor.org/terms

FITTING OF STRAIGHT LINES 295

The grouping according to Rule II is entirely independent of the errors

el E ... * eN; 71 - - *, 7'N . It is identical with the grouping according to Ru
in the following case: Denote by x the median of xi, * * *, XN; assume that
can take values only within the finite interval [-c, +c] and that all the values

Xl1, * * , XNC fall outside the interval [x - c, x -j c]. It is easy to see that in
this case xi < x(i= 1, * *i, N) holds if and only if Xi < X, where X denotes

the median of X1, *l* , XN . Hence the grouping according to Rule II is
identical to that according to Rule I and therefore the grouping according to

Rule I is independent of the errors el, * * *, eN . In such cases we gpt the best
estimate of a by grouping the observations according to Rule I. Practically,
we can use the grouping according to Rule I and regard it as independent of the

errors el, ... I, eN; fl, * * * , nN if there exists a positive value c for which the
probability that I e i 2 c is negligibly small and the number of observations
contained in [x - c, x + c] is also very small.
Denote by a' the value of a which we obtain by grouping the observations
according to Rule I and by a" the value of a if we group the observations
according to Rule II. The value a" is in general unknown, since the values

X ... , XN are unknown, except in the special case considered above, when
we have a" = a'. We will now show that an upper and a lower limit for a"
can always be given. First, we have to determine a positive value c such that

the probability that I e I > c is negligibly small. The value of c may often be

determined before we make the observations having some a priori knowledge

about the possible range of the errors. If this is not the case, we can estimate

the value of c from the data. It is well known that if we have errors in both
variables and fit a straight line by the method of least squares minimizing in

the x-direction, the sum of the squared deviations divided by the number of
degrees of freedom will overestimate oa, Hence, if e is normally distributed,

we can consider the interval [-3v, 3v] as the possible range of C, i.e. c = 3v,
where v2 denotes the sum of the squared residuals divided by the number of
degrees of freedom. If the distribution of e is unknown, we shall have to take
for c a somewhat larger value, for instance c = 5v. After having determined c,

upper and lower limits for a" can be given as follows: we consider the system S
of all possible groupings satisfying the conditions:

(1) If xi x - c the point (xi, yi) belongs to the group G1.
(2) If xi > x + c the point (xi, yi) belongs to the group G2.
We calculate the value of a according to each grouping of the system S and
denote the minimum of these values by a*, and the maximum by a**. Since
the grouping according to Rule II is contained in the system S, a* is a lower
and a** an upper limit of a".

Let g be a grouping contained in S and denote by I4 the confidence interval
for a which we obtain from formula (23) using the grouping g. Denote further
by I the smallest interval which contains the intervals 1 for all elements g
of S. Then I contains also the confidence interval corresponding to the grouping
according to Rule II. If we denote by P the chosen probability level (say

This content downloaded from 206.253.207.235 on Thu, 24 Oct 2019 21:50:04 UTC
All use subject to https://about.jstor.org/terms

296

ABRAHAM

WALD

P = .95), then we can say: If we were to draw a sample consisting of N pairs

of observations (xi, yi), . .. , (XN , YN), the probability is greater than or equal
to P that we shall obtain a system of observations such that the interval I will
include the true slope a.
The computing work for the determination of I may be considerable if the
number of observations within the interval [x - c, x + c] is not small. We

can get a good approximation to I by less computation work as follows: First
we calculate the slope a' using the grouping according to Rule I and determine

the confidence interval [a' - 6, a' + A] according to formula (23). Denote by

a(g) - ~2,corresponding to a grouping
a(g) the value
of the slope, i.e. the value of -1 _ Y2, corresponding to a groupin

g of the system S, and by [a(g) - 8,, a(g)
interval calculated from (23). ,Neglecting
we obtain for I the interval [a* - 8, a** + A].

If the difference a** - a* is small, we can consider I = [a* - 6, a** + A] as
the correct confidence interval of a corresponding to the chosen probability
level P. If, however, a** - a* is large, the interval I is unnecessarily large.
In such cases we may get a much shorter confidence interval by using some

other grouping defined independently of the errors E1, ... , N ; I ) ... *, 'N-

For instance if we see that the values xl, * * *, XN considered in the order as

they have been observed, show a monotonically increasing (or decreasing) tendency, we shali define the group GI as the first half, and the group G2 as the
second half of the observations. Though we decide to make this grouping after

having observed that the values xi, * , XN show a clear trend, the grouping
can be considered as independent of the errors (1, ***, EN. In fact, if the
range of the error e is small in comparison to the true part X, the trend tendency

of the value xl, X , ZN will not be affected by the size of the errors ei e N, ENe
We may use for the grouping also any other property of the data which is
independent of the errors.

The results of the preceding considerations can be summarized as follows:

We use first the grouping according to Rule I, calculate the slope a' = -i _ V
and the corresponding confidence interval [a' - 6, a' + Al (formula (23)). This
confidence interval cannot be considered as exact since the grouping according
to Rule I is not completely independent of the errors. In order to take account
of this fact, we calculate a* and a**. If a** - a* is small, we consider I =
[a* - 5, a** + A] with practical approximation as the correct confidence interval.

If, however, a** - a* is large, the interval I is unnecessarily large. We can
only say that I is a confidence interval corresponding to a probability level
greater than or equal to the chosen one. In such cases we should try to use
some other grouping defined independently of the errors, which eventually will
lead to a considerably shorter confidence interval.
Analogous considerations hold regarding the joint confidence region for a
and A. We use the grouping according to Rule I and calculate from (27) the

This content downloaded from 206.253.207.235 on Thu, 24 Oct 2019 21:50:04 UTC
All use subject to https://about.jstor.org/terms

FITTING OF STRAIGHT LINES 297

corresponding confidence region R

(b* = - a*x and b** = - a**x) we enlarge R to a region R corresponding
to the fact that a and b may take any values within the intervals [a**, a*] and

[b**, b*] respectively. The region R? can be considered with practical approxi-

mation as the correct confidence region. If I a** - a* I or I b** -b* is large,

we may try some other grouping defined independently of the errors, which

may lead to a smaller confidence region. In any case R represents a confidence
region corresponding to a probability level greater than or equal to the
chosen one.

8. Some Remarks on the Consistency of the Estimates of a, i, c-, . We
have shown in section 3 that the given estimates of a, B, 0- and a,7 are consistent
if condition V is satisfied.

If the values xl, ' , XN are not obtained by random sampling, it will in
general be possible to define a grouping which is independent of the errors and
for which condition V is satisfied. We can sometimes arrange the experiments

such that no values of the series xI, * * *, XN should be within the interval
[x - c, x + c] where x denotes the median of xl, ... , XN and c the range of
the error e. In such cases, as we saw, the grouping according to Rule I is
independent of the errors. Condition V is certainly satisfied if we group the
data according to Rule I.

Let us now consider the case that X1, * , XN are random variables inde-

pendently distributed, each having the same distribution. Denote by X a
random variable having the same probability distribution as possessed by each

of the random variables X1, * * *, XN. Assuming that X has a finite second
moment, the expression in condition V will approach zero stochastically with

N - oo for any grouping defined independently of the values X1, * * *, XN .
It is possible, however, to define a grouping independent of the errors (but not

independent of X1, * * *, XN) for which the expression in V does not approach

zero, provided that X has the following property: There exists a real value X

such that the probability that X will lie within the interval [X - c, X + cl
(c denotes the range of the error e) is zero, the probability that X > X + c
is positive, and the probability that X <X - c is positive. The grouping can
be defined, for instance, as follows:

The i-th observation (xi, yi) belongs to the group G1 if xi < X and to G2 if
xi > X. We continue the grouping according to this rule up to a value i for

which one of the groups GI, G2 contains already N/2 elements. All further observations belong to the other group.

It is easy to see that the probability is equal to 1 that the relation xi < X
is equivalent to the relation X, < X - c and the relation xi > X is equivalent to
the relation Xi > X + c. Hence this grouping is independent of the errors.
Since for this grouping condition V is satisfied, our statement is proved.
If X has not the property described above, it may happen that for every
grouping defined independently of the errors, the expression in condition V con-

This content downloaded from 206.253.207.235 on Thu, 24 Oct 2019 21:50:04 UTC
All use subject to https://about.jstor.org/terms

298 ABRAHAM WALD

verges always to zero stochastically. Such a case arises for instance if X, e and

a are normally distributed.'4 It can be sho,wn that in this case no consistent

estimates of the parameters a and ft can be given, unless we have some additional information not contained in the data (for instance we know a priori the
ratio o?/c,,).

9. Structural Relationship and Prediction.'5 The problem discussed in this
paper was the question as to how to estimate the relationship between the true
parts X and Y. We shall call the relationship between the true parts the structural relationship. The problem of finding the structural relationship must not
be confused with the problem of prediction of one variable by means of the
other. The problem of prediction can be formulated as follows: We have ob-

served N pairs of values (xi, yr), * * *, (XN, YN). A new observation on x is
given and we have to estimate the corresponding value of y by means of our

previous observations (xi , y1), . .. , (XN, YN). One might think that if we have
estimated the structural relationship between X and Y, we may estimate y by
the same relationship. That is to say, if the estimated structural relationship
is given by Y = aX + b, we may estimate y from x by the same formula:
y = ax + b. This procedure may lead, however, to a biased estimate of y.

This is, for instance, the case if X, e and t7 are normally distributed. It can
easily be shown in this case that for any given x the conditional expectation of
y is a linear function of x, that the slope of this function is different from the
slope of the structural relationship, and that among all unbiased estimates of
y which are linear functions of x, the estimate obtained by the method of least

squares has the smallest variance. Hence in this case we have to use the least
square estimate for purposes of prediction. Even if we would know exactly the

structural relationship Y = aX + ,B, we would get a biased estimate of y by
putting y = ax + P.
Let us consider now the following example: X is a random variable having
a rectangular distribution with the range [0, 1]. The random variable e has a
rectangular distribution with the range [-0.1, + 0.1]. For any given x let us
denote the conditional expectation of y by E(y I x) and the conditional expectation of X by E(X I x). Then we obviously have

E(y I x) = aE(X I x) +

Now let us calculate E(X I x). It is obvious that the joint distribution of X an
e is given by the density function:
5 dX de,
14 I wish to thank Professor Hotelling for drawing my attention to this case.
15 I should like to express my thanks to Professor Hotelling for many interesting suggestions and remarks on this subject.

This content downloaded from 206.253.207.235 on Thu, 24 Oct 2019 21:50:04 UTC
All use subject to https://about.jstor.org/terms

FITTING OF STRAIGHT LINES 299

where X can take any value within the i
within [-0.1, + 0.1]. From this we obtain easily that the joint distribution of
x and X is given by the density function
5 dx dX,

where x can take any value within the interval [-0.1, 1.11 and X can take any
value lying in both intervals [0, 1] and [x - 0.1, x + 0.11 simultaneously. Denote by I. the common part of these two intervals. Then for any fixed x the
relative distribution of X is given by the probability density
dX
IdX
Hence, we have

JXdX

E(XIx) = -'_
fdX
We have to consider 3 cases:

(1)

0.1

<x

<0.9.

In this case I. = [x - 0.1, x + 0.11 and
XdX

E(Xix) = .1- = x
dX

(2) -0.1 < x <0.1. Then I. = [0, x + 0.11 and
XdX

E(XIx)- ?}1 = .5x + .05.
j dX

(3) 0.9 <x < 1.1. Then I. [x -0.1, 11 and
|XdX

E(X l x) - I,x = .5x + .45.

f0dX

This content downloaded from 206.253.207.235 on Thu, 24 Oct 2019 21:50:04 UTC
All use subject to https://about.jstor.org/terms

300

ABRAHAM

WALD

Since

E(y I x) = aE(X I x) + ,
we see that the structural relationship gives an unbiased prediction of y from x

if 0.1 < x < 0.9, but not in the other cases.
The problem of cases for which the structural relationship is appropriate also
for purposes of prediction, needs further investigation. I should like to mention

a class of cases where the structural relationship has to be used also for prediction.

Assume that we have observed N values (xi , yi), * * *, (XN , YN) of the variables
x and y for which the conditions I-IV of section 2 hold. Then we make a new
observation on x obtaining the value x'. We assume that the last observation

on x has been made under changed conditions such that we are sure that x' does
not contain error, i.e. x' is equal to the true part X'. Such a situation may arise
for instance if the error e is due to errors of measurement and the last observation has been made with an instrument of great precision for which the error of
measurement can be neglected. In such cases the prediction of the corresponding y' has to be made by means of the estimated structural relationship, i.e. we
have to put y' = ax' + b.
The knowledge of the structural relationship is essential for constructing any
theory in the empirical sciences. The laws of the empirical sciences mostly
express relationships among a limited number of variables which would prevail
exactly if the disturbing influence of a great number of other variables could
be eliminated. In our experiments we never succeed in eliminating completely
these disturbances. Hence in deducing laws from observations, we have the
task of estimating structural relationships.
COLUMBIA UNIVERSITY,
NEw YORK, N. Y.

This content downloaded from 206.253.207.235 on Thu, 24 Oct 2019 21:50:04 UTC
All use subject to https://about.jstor.org/terms

