CHAPTER 4

Decision Theoretic Approaches to
Experiment Design and External
Validitya
A.V. Banerjee*, jj, S. Chassangx, 1, E. Snowberg{, jj
*Massachusetts Institute of Technology, Cambridge, MA, United States
x
New York University, New York, NY, United States
{
California Institute of Technology, Pasadena, CA, United States
jj
NBER (National Bureau of Economic Research), Cambridge, MA, United States
1
Corresponding author: E-mail: chassang@nyu.edu

Contents
1. Introduction
1.1 Motivation
1.2 Overview
1.3 A brief history
2. The Framework
3. Perspectives on Experimental Design
3.1 Bayesian experimentation
3.1.1 Example: the logic of Bayesian experimentation

3.2 Ambiguity or an audience
3.2.1 A theory of experimenters

4. Rerandomization, Registration, and Preanalysis
4.1 Rerandomization
4.2 Registration
4.2.1 Good commitment
4.2.2 Bad commitment
4.2.3 Examples

4.3 Preanalysis plans
4.3.1 Preanalysis and bounded rationality
4.3.2 Caveats
4.3.3 Theory

5. External Validity
6. Structured Speculation
6.1 The Value of Structured Speculation
6.2 Examples
6.2.1 A Post hoc evaluation

7. Issues of Particular Interest

a

142
142
143
144
145
147
147
148

149
151

153
153
155
155
156
157

157
157
158
159

159
161
161
162
164

165

We thank Esther Duﬂo for her leadership on the handbook and for extensive comments on earlier drafts. Chassang
and Snowberg gratefully acknowledge the support of NSF grant SES-1156154.

Handbook of Economic Field Experiments, Volume 1
ISSN 2214-658X, http://dx.doi.org/10.1016/bs.hefe.2016.08.005

© 2017 Elsevier B.V.
All rights reserved.

141

142

Handbook of Field Experiments

7.1 Scalability
7.2 Effect on other populations
7.3 Same population, different circumstances
7.4 Formats for structured speculation
8. Conclusion
References

166
167
168
170
171
171

Abstract
A modern, decision-theoretic framework can help clarify important practical questions of experimental
design. Building on our recent work, this chapter begins by summarizing our framework for understanding the goals of experimenters and applying this to rerandomization. We then use this framework
to shed light on questions related to experimental registries, preanalysis plans, and most importantly,
external validity. Our framework implies that even when large samples can be collected, external decision-making remains inherently subjective. We embrace this conclusion and argue that in order to
improve external validity, experimental research needs to create a space for structured speculation.

Keywords
Ambiguity aversion; External validity; Non-Bayesian decision-making; Randomization; Self-selection

JEL Codes
C93; D70; D80

1. INTRODUCTION
1.1 Motivation
In the last couple of decades, two of the most successful areas of economic research
have been decision theorydand its close cousins, behavioral, and experimental
economicsdand empirical microeconomics. Despite the fact that both emphasize experimentation as a method of investigation, there is almost no connection between the two
literature.1 Indeed, there are good reasons why such a dialog is difﬁcult: an experiment
designed according to the prescriptions of mainstream economic theory would
get rejected by even the most benevolent referees; conversely, experimentation as it is
practiced fails the standard axioms of subjective rationality.
Building on our work in Banerjee et al. (2014), this chapter seeks to establish such
a dialog. We believe that modern decision theory can provide a much needed
framework for experiment design, at a time when experimenters seek to codify their
practice. In turn, we believe that the issues facing the experimental community present
1

See Chassang et al. (2012), Kasy (2013), and Banerjee et al. (2014) for recent exceptions. This lack of connection
despite the fact that economic theorists have extensively studied experimentation (Grossman and Stiglitz, 1980;
Milgrom, 1981; Banerjee, 1992; Persico, 2000; Bergemann and V€alim€aki, 2002). Bandit problems have been a
particular focus of this literature (Robbins, 1952; Bellman, 1956; Rothschild, 1974; Gittins, 1979; Aghion et al., 1991;
Bergemann and V€alim€aki, 1996, 2006).

Decision Theoretic Approaches to Experiment Design and External Validity

a rich and useful set of challenges for decision theory. It is a rare opportunity for theorists
to write models that could impact the practice of their colleagues down the hall.

1.2 Overview
We believe the main difﬁculty in ﬁnding a good theoretical framework for understanding
experimental design stems from inconsistencies between the preferences of experimenters as individuals and as a group. As individuals, experimenters behave more or
less like Bayesians. As a group however, experimenters behave like extremely ambiguity
averse decision makers, believing it is unwise to settle on a speciﬁc prior by which to evaluate new information.
Our framework considers the problem of a decision maker choosing both an
experimental design and a decision ruledthat is a mapping from experimental results
into policydwho seeks to maximize her own subjective utility, while also satisfying an
adversarial audience who may be able to veto her choices. We describe this
framework and then summarize the results in Banerjee et al. (2014): ﬁrst, it uniﬁes the
Bayesian and frequentist perspectives. For small sample sizes, or if the decision maker
places little weight on convincing her audience, optimal experimentation is deterministic
and maximizes subjective utility. If instead the sample size is large, then randomized
experiments allowing for prior-free inference become optimal. Second, the framework
sheds light on the tradeoffs involved in rerandomization. It always improves the
subjective value of experiments but reduces the robustness of policy inferences. However,
provided the number of rerandomizations is not terribly large (compared to the sample
size), the robustness cost of rerandomization is negligible.
Having a model of experimenters also provides a useful perspective on preregistration
and preanalysis. Bayesian decision makers do not need or desire either. On the other
hand, a decision maker worried about an adversarial audience will value both. The
important observation is that there is no need for the two perspectives to be seen as in
opposition. Provided ex ante hypotheses are clearly labeled, there is no reason to
constrain the dynamic updating of experiments as they are being run. Some decision
makers will value knowing the ex ante hypotheses formulated by the experimenter,
while Bayesian decision makers, who care only about the data collected, will value getting the most informative experiment possible. Reporting both, as “ex ante questions of
interest,” and “interim questions of interest” can satisfy both types.
The ﬁnal sections are dedicated to the question of external validity. While there are
ways to satisfy both the Bayesian and adversarial perspective in (policy) decision
problems internal to the experimental environment, we argue that decision-making
in external environments is necessarily subjectivedthings may just be different in
different circumstances. However, this does not mean that external inferences need
to be vague or uninformative. We embrace the idea that external inference is necessarily

143

144

Handbook of Field Experiments

speculative and that it should be thought of and reported as such as part of experimental
research.
We formulate a framework for structured speculation that builds on two main observations. First, the manner of speculation, whether it is through a structural model or a
reduced-form set of empirical predictions, is unimportant. What is important is for speculation to be stated as crisp hypotheses that can be falsiﬁed by further data. The advantage
of structural modeling is that it automatically leads to a fully speciﬁed set of falsiﬁable predictions. However, model parameters are no less speculative than hypotheses formulated
in natural language by experienced ﬁeld researchers. While models have value in systematizing and clarifying thought, there is no formal reason to rule out any format of speculation experimenters are comfortable with, provided that predictions are made is a
precise, falsiﬁable way.
The second observation is that creating space for structured speculation may have an
important effect on how experiments are designed, run, and reported. Indeed, we believe
it may result in a more effective and informative process of experimentation. We argue
that the need for “better” speculation will lead experimenters to collect data that are
ignored, unreported, or viewed as unconstructive to reported research: for instance,
data on participant preferences and beliefs, the participants’ place in a broader economic
system, the role that values and norms play in the outcomes we measure, and so on. We
illustrate this point by providing explicit examples of interesting topics for structured
speculation.
The rest of this section very brieﬂy discusses the history of experimental design, highlighting the interplay of theory and practice.

1.3 A brief history
The ﬁrst documented controlled experiment is found in the biblical book of Daniel, a
story set around 605 BC comparing the health effects of a vegetarian diet with the
Babylon court diet of meat and wine.
Then Daniel asked the guard whom the palace master had appointed over Daniel, Hananiah,
Mishael, and Azariah: “Please test your servants for 10 days. Let us be given vegetables to eat
and water to drink. You can then compare our appearance with the appearance of the young
men who eat the royal rations, and deal with your servants according to what you observe.” So
he agreed to this proposal and tested them for 10 days. At the end of 10 days, it was observed
that they appeared better and fatter than all the young men who had been eating the royal
rations (Daniel 1:11e14, NRSV).

Despite the early emergence of controlled trials, it took millennia for randomization
to be inserted into the processdby statistical theorists well versed in ﬁeld experiments.
Simpson and Pearson (1904) argues for a crude form of randomization in the testing
of inoculants (while at the same time performing the ﬁrst meta-analysis, see

Decision Theoretic Approaches to Experiment Design and External Validity

Egger et al., 2001) in order to establish a true control group. Over the years that followed,
Pearson would formulate stronger and stronger defenses of randomization, emphasizing
the need to draw controls from the same population as those that are treated (culminating
in Maynard, 1909). Fisher (1926) was the ﬁrst to provide a detailed program for randomization, which he expanded into his classic text on experimental design (Fisher, 1935).
Randomization became a mainstay of experimental design thanks to two factors. The
ﬁrst was medical practitioners looking for a way to evaluate treatments in a way that
would prevent manipulation from the manufacturers of those treatments. Randomization alone proved insufﬁcient to this task, which leads to the development of many tools,
such as preregistration and preanalysis plans for trials, that we discuss in this chapter. These
tools have had success in medicine, but their costs and beneﬁts are likely to vary by ﬁeld.
As such, we have tried to identify, as abstractly as possible the factors that may make them
more or less appealing, depending on the circumstances.
The second factor was a desire in many other ﬁelds of social science to identify the
causal effects of interventions. Randomization was put at the center of frameworks for
causal analysis leading, after some delay, to an explosion of randomized controlled ﬁeld
trials in several disciplines of the social sciences (Rubin, 1974; Pearl, 2000). Once again,
however, randomization alone has not been sufﬁcient to the task. Practical difﬁculties,
such as treated participants being unwilling to receive treatment, have interfered. A number of statistical tools have been created to address these issues. However, as decision theory has little to say about the choice of statistical techniques, we do not discuss them here.
Finally, there is also work on experimental design that takes a Bayesian, rather than
classical, perspective. However, like in econometrics, its presence is somewhat marginal.
Even the proponents of Bayesian experimental design note that despite its strong normative appeal, it remains rarely, if ever, used (Chaloner and Verdinelli, 1995).

2. THE FRAMEWORK
We take the point of view of a decision maker who can inform her policy choice by
running an experiment. She could be a scholar who is trying to come up with a policy
recommendation or a political entrepreneur trying to shape policy for the better. The
decision problem can be internal, if the ultimate policy decision affects the population
targeted by the experiment, or external, if it applies to a population different from that
involved in the experiment (hence external validity).2
Our discussion and modeling follows Banerjee et al. (2014) but is more informal. The
interested reader may consult the original paper for more details.
2

Note that the decision problem may differ because the population has changedde.g., it consists of different people, or
the same people with different beliefs, or in a different contextdor because the treatment differs in some wayde.g., it
is delivered at a different time, through a different distribution channel.

145

146

Handbook of Field Experiments

Actions and preferences. A decision maker needs to decide whether to implement
some policy a ˛f0; 1g, that provides a treatment s ˛ f0; 1g to a unit mass populationd
which may be composed of people, districts, cities, schools, and so ondindexed by
i ˛ ½0; 1 for individuals.3 To inform her judgment, the decision maker is able to run experiments assigning a given number N of subjects to treatment or control.
Potential outcomes for subject i, given treatment s, are denoted by Yis ˛f0; 1g. Y ¼ 1
is referred to as a success. Each individual i is associated with covariates xi ˛ X, where the
set X is ﬁnite. Covariates x ˛X are observable and affect the distribution of outcomes Y.
The distribution q ˛ DðXÞ of covariates in the population is known and has full support.
Outcomes Yi are i.i.d. conditional on covariates. The success probabilities, conditional on
treatment s, and covariates x are denoted by psx h probðYis ¼ 1jxi ¼ xÞ.
Environments and decision problems. To specify the decision problem, and the
distinction between internal and external problems, we deﬁne environments z, which are
described by the ﬁnite-dimensional vector p of success probabilities conditional on covariates and treatment status



X
p ¼ p0x ; p1x x˛X ˛ ½0; 12 hP :
For the ﬁrst half of this chapter, we consider internal decision problems in which the
environment is the same in both the experimental and policy-relevant population. The
second half puts more attention on external decision problems and external validity, in
which the two environments may differ.
Given a known environment p and a policy decision a ˛ f0; 1g, the decision maker’s
payoff uða; pÞ can be written as
X
uða; pÞhEp Y a ¼
qðxÞpax :
x˛X

This formulation does not explicitly recognize unobservables, although it allows psx to
vary in arbitrary ways as x varies, which is effectively the consequence of unobservables.
Experiments and decision rules. An experiment is a realized assignment of treatment to individuals represented by a tuple e ¼ ðxi ; si Þi˛f1;.;Ng ˛ðX  f0; 1gÞN hE.
Experiments generate outcome data y ¼ ðyi Þi˛f1;.;Ng ˛f0; 1gN hY , with each yi an
independent realization of Yisi given ðxi ; si Þ.
The decision maker’s strategy consists of both a (possibly randomized) experimental
design E ˛ DðEÞ and a decision rule a : E  Y /Dðf0; 1gÞ which maps experimental
datadincluding the realized design e and outcomes ydto a policy decision a. We denote
by A the set of possible decision rules. Since E is the set of possible probability

3

For simplicity, we focus on policies that assign the same treatment status to all i˛½0; 1.

Decision Theoretic Approaches to Experiment Design and External Validity

distributions over the realized assignments of treatment, this framework allows for randomized experiments.
We assume that subjects are exchangeable conditional on covariates, so that experiments identical up to a permutation of labels are equivalent from the perspective of the
experimenter (De Finetti, 1937).4

3. PERSPECTIVES ON EXPERIMENTAL DESIGN
3.1 Bayesian experimentation
Much of economic theory proceeds under the assumption that decision makers are subjective expected utility maximizers. As this implies Bayesian updating, we refer to such
decision makers as Bayesians. While subjective expected utility maximization has been
an incredibly useful framework, it leads to theoretical prescriptions at odds with experimental practice.5
Formally, let the decision maker start from a prior h0 ˛DðP Þ over treatment effects.
In the context of our experimentation problem, optimal experiments E and decision
rules a must solve,
max Eh0 ½uðaðe; yÞ; pÞ:
E ;a

(1)

An immediate implication of the subjective expected utility framework is that
randomization is never strictly optimal and for generic priors it is strictly suboptimal.
Proposition 1 (Banerjee et al. (2014), Bayesians do not Randomize).
Assume that the decision maker is Bayesian, i.e., designs experiments according to (1).
Then, there exist deterministic solutions e ˛ E to (1). A mixed strategy (randomization)
E ˛DðEÞ solves (1) if and only if for all e ˛ supp E , e solves (1).6
The intuition of the result is straightforward. Mixed strategies are never strictly
optimal for subjective expected utility maximizers when a pure strategy equilibrium exists, and an randomized controlled trial (RCT) is a mixed strategy in the decision problem
described above. Kasy (2013) uses a result similar to Proposition 1 to argue that randomized controlled trials are suboptimal. Speciﬁcally, it emphasizes that if the goal is to
achieve balance between the treatment and control samples, this is more efﬁciently
4

5

6

The framework here is not particularly general. The goal is to provide us with just enough ﬂexibility to illustrate
speciﬁc issues. For example, we consider coarse policy decisions between treating the entire population or no one. In
practice, one may consider more sophisticated policy decisions indexed on observable covariates. We also assume that
the number of treatment and control observations are freely chosen under an aggregate constraint. In practice, the cost
of treatment and control data points may differ. These simpliﬁcations do not affect our results.
It is normatively appealing as well, and the “as if” axiomatization proposed by Savage (1954) seems so natural that
subjective expected utility maximization is sometimes considered an expression of rationality.
See Banerjee et al. (2014) for precise deﬁnitions and a proof.

147

148

Handbook of Field Experiments

done by purposefully assigning participants to treatment and control based on their
observables, so as to eliminate any chance of ending up with an unbalanced sample purely
because of bad luck in the randomization process.
Proposition 1 is obviously at odds with experimental practice. Real-life experimenters go through nontrivial expense in order to assign treatment and control
randomly. We interpret this mismatch as an indication that the Bayesian paradigm
provides a poor description of the objectives of actual experimenters. However, we
also believe there is insight into experimental practice that can be gained by carefully
considering Proposition 1. We do this in the following example, before turning to the
adversarial perspective discussed in the introduction.
3.1.1 Example: the logic of Bayesian experimentation
Consider an experiment evaluating educational vouchers. This experiment will inﬂuence
a school superintendent’s decision of whether or not to introduce vouchers in her district.
The superintendent has dismissed vouchers in the past, believing that by far the most
important determinant of academic outcomes is whether a student is from a poor or privileged background. She has used this belief to explain the superior performance of private
schools in her district, as they are a bastion for privileged students. However, in recent
years, she has become open to the radical opposite of her belief: Schooling is the sole
determinant of academic success. That is, even a poor student would do better at a private
school. To test this hypothesis, she has convinced a private school to let her assign, however she likes, a single student to enroll there.
Faced with an experiment with a single observation, most academic experimenters
would give up. How could anyone ever learn from such an experiment? What is the
comparison group? Yet designing an informative experiment is easy: a Bayesian decision
maker always has a prior, and she can compare the outcome of the child to that. Suppose
the superintendent believes that a poor child can never score higher than the 70th
percentile on a standardized test. She would then clearly ﬁnd it informative if a poor child
were given the lone spot in the private school and then scored in the 90th percentile.
Adding a second child to the experiment brings new questions and new insights. In
particular, suppose that a slot in a public school is also allocated to this experiment.
Should the child in the public school have an identical or different background to the
student assigned to the private school? Should we allocate the private-school spot by
lottery?
Once we recognize the role of the prior in setting the benchmark, these questions
become easy to answer. Our superintendent starts from the theory that only background
matters. Under that theory, the most surprising outcome, and therefore the one likely to
move her prior the most, is one in which a poor child who goes to a private school significantly outperforms a privileged child who goes to a public school. If this occurs, she
would strongly update toward the alternative explanation that schooling is all that

Decision Theoretic Approaches to Experiment Design and External Validity

matters. Thus, the optimal design involves giving the private school slot to a poor child
and sending a privileged child to a public school. In particular, she is more likely to be
impressed by the outcome of this experiment than one where both students are from
the same background.
Strikingly, this example falsiﬁes the idea that balanced treatment and control groups
are intrinsically appealing. Moreover, we are arguing for a deterministic, rather than
random, assignment of the students. Indeed, a lottery only moves us away from the ideal
design: If the privileged child is assigned to the private school, very little can be learned.
Proposition 1 shows that this result applies for all sample sizes. The limits of this line of
reasoning are only met if multiple decision makers with different priors (or a single decision maker unable to commit to a single prior) are involved. Introduce another school
ofﬁcial with a slightly different prior beliefs about the effect of economic background: she
believes that while a poor student would not beneﬁt from a move to a private school, a
privileged student would be harmed by moving to a public school. In this case, the design
suggested previously is much less attractive. If we observe that the poor child does better,
it could be either because the private school helps him to do better or because the public
school hurts the richer child (or both!).
When the experimenter wants to convince other decision makers, she will design an
experiment that not only informs her but also informs members of her audience with
arbitrary priors. This is the perspective that Banerjee et al. (2014) seeks to capture. In
this setting, randomized experiments emerge as the only ones that successfully defend
against all priors, i.e., the only experiments whose interpretation cannot be challenged
even by a devil’s advocate.

3.2 Ambiguity or an audience
Although Bayesian decision-making is the default framework of economic theory, it is by
no means a consensus. First, a decision maker may not trust her prior, exhibiting
ambiguity aversion (Ellsberg, 1961; Schmeidler, 1989; Gilboa and Schmeidler, 1989;
Klibanoff et al., 2005). Second, she may simply not be able to think through all possible
implications of holding a particular prior, in effect violating Savage’s completeness axiom
(Gilboa et al., 2009; Bewley, 1998). Third, she may recognize that she needs to convince
others whose priors may diverge from her own.7
The model we propose in Banerjee et al. (2014) takes seriously the idea that experimenters care about convincing such an audience. This “audience” may actually reﬂect
the experimenter’s own self-doubts and internal critics, or a real audience of stakeholders
7

A related concern is that she may be accused of fraudulent manipulation of the evidence by those who disagree with
her a priori. However, if outright fraud is a concern, veriﬁable procedures, more than randomization, become
necessary.

149

150

Handbook of Field Experiments

with veto power (e.g., referees).8 The decision maker chooses the experimental design E
and decision rule a that solve
max UðE ; aÞ h l Eh0 ;E ½uðaðe; yÞ; pÞ þ ð1  lÞ min Eh;E ½uðaðe; yÞ; pÞ
h˛H
E ;a
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
subjective effectiveness

(2)

robust effectiveness

where l ˛ ½0; 1. Here, h0 is a ﬁxed reference prior, while H is a convex set of alternative
priors h ˛ DðPÞ. A decision maker with these preferences can be interpreted as
maximizing its usefulness under reference prior h0 , while also satisfying an adversarial
audience with priors h ˛H.9 The ﬁrst term captures a desire for informativeness from
the point of view of the experimenter, and the second captures a desire for robustness.
Ambiguity averse experimentation. Banerjee et al. (2014) study optimal experimentation by ambiguity-averse decision makers under one additional assumption.
Assumption 1
We assume that there exists n > 0 such that, for all X0 3X with jX0  N=2j, there
exists a prior h ˛arg minh˛H Eh ðmaxa˛f0;1g pa Þ such that for almost every
pX0 hðp0x ; p1x Þx˛X0 ,
 
 



a
0
a
1


min Eh max p  p pX0 ; Eh max p  p pX0
> n:
a˛f0;1g

a˛f0;1g

The condition says that even if an experiment was to reveal the probability of success
at every value of the covariate x in X0 , there is still at least one prior in the set H under
which the conditional likelihood of making a wrong policy decision is bounded away
from zero.10
Proposition 2. For l ˛ð0; 1Þ:
1. Take sample size N as given. For generically every prior h0 , there exists l ˛ð0; 1Þ such
that for all l  l , the solution E  to (2) is unique, deterministic, and Bayesianoptimal for l ¼ 1.
2. Take weight l as given. There exists N such that for all N  N , the optimal experiment E  is randomized. As N goes to inﬁnity, the optimal experiment allows for
correct policy decisions with probability going to one, uniformly over priors h ˛H.

8
9

10

The model belongs to the class of maximin preferences axiomatized in Gilboa and Schmeidler (1989).
Note that if l ¼ 1, we recover (1), so that this model nests standard Bayesian expected utility maximization. If
satisfying audience members was introduced as a hard constraint, then the weight ratio 1l
l would be interpreted as an
appropriate Lagrange multiplier for that constraint.
The way this condition is speciﬁed implies that N < 2jXj. It also implies the existence of priors in H that place some
weight on the possibility that the function psx is not smooth with respect to x. This lack of smoothness conditional on
observables can be microfounded using unobservables although success rates conditional on observed and unobserved variables are smooth, sharp changes in variation of unobserved variables correlated with small changes in the
value of observables x generates the lack of smoothness in x.

Decision Theoretic Approaches to Experiment Design and External Validity

Proposition 2 shows that the optimal experimental design depends on the number of
available data points (or participants), and the weight the decision maker puts on her own
prior versus those of the audience. Part (1) of the result shows that when sample points are
scarce, or when the decision maker does not put much weight on satisfying anyone else
(l close to 1), optimal experimentation will be Bayesian. That is, the experimenter will
focus on assigning treatment and control observations to the subjects from whom she expects to learn the most. Part (2) shows that when sample points are plentiful and/or the
decision maker cares about satisfying an adversarial audience, she will use randomized trials that allow for prior-free identiﬁcation of correct policies.11
To build intuition for the result, and Assumption 1, it is useful to think of the decision
maker as playing a zero-sum game against nature (with probably 1  l). After the decision maker picks an experiment, nature picks the prior which maximizes the chance of
picking the wrong policy, given that experimental design. If there is any clear pattern in
the decision maker’s assignment of treatment, nature can exploit these due to Assumption
1. Randomization eliminates patterns for nature to exploit.
3.2.1 A theory of experimenters
Although randomization prevents nature from exploiting patterns in an experimental
design, it is not always the optimal solution. There are two possible reasons for this. First,
the decision maker may care so little about the audience (l is close to 1), that preparing
for the worst is of little use. Second, with small samples, the loss of power from randomization (relative to the optimal deterministic experiment) is so large that it offsets the
beneﬁt of reducing nature’s ability to exploit a deterministic assignment. As the sample
becomes large, the loss of power from randomizing shrinks to nothing, while the gains
from robustness against challenging priors remain positive and bounded away from
zero.12
Fig. 1 maps out implications of Proposition 2 for practical experiment design. In scientiﬁc research, when an experimenter faces a skeptical audience, she randomizes. In
contrast, a ﬁrm implementing a costly new process in a handful of production sites
will not try it on random teams. The ﬁrm will focus on a few teams where it can learn
11

12

Kasy (2013) reports a result that seems to directly contradict ours: that randomized experiments can never do better
than a deterministic one, even with a maximin objective. The difference in the results comes from the fact that in
Kasy’s framework the audience sets its prior after randomization occurs, rather than between the revelation of the
design and the actual randomization, as in our framework. In Kasy’s framework, the audience will obviously pick a
prior that means, in effect, that they can learn nothing from the actual treatment assignment.Taking journal referees
as an example of a skeptical audience, we believe our assumption is more realistic: Referees do show a fair amount of
forbearance, even when faced with imbalance in covariates generated by a randomized control trial, although there
are instances where they are sufﬁciently troubled by a particular case of imbalance to recommend rejection.
As pointed out by Kasy (2013), the decision maker may also be able to limit the set of possible interpretations by
deterministically choosing the right set of xs if there is enough continuity in px . Too much continuity is ruled out by
Assumption 1.

151

152

Handbook of Field Experiments

Figure 1 Different modes of experimentation.

the most.13 Yet when the available sample is large, ﬁrms do randomize. This is the case for
ﬁrms dealing online with many end users: although the ﬁrm only needs to convince itself
of the effectiveness of a particular ad or user interface design, observations are plentiful
and randomization is cheap and used.
The logic of Proposition 2 applies at all stages of the decision-making tree that leads to
the evaluation of a particular technology. When scientists want to convince others, they
run detailed randomized experiments. At earlier stages however, when a scientist decides
what to experiment on, they do not just randomly pick a hypothesis. Instead, they
develop a subjective prior on the technologies most likely to be worth exploring in detail.
This ﬁts well with our result: the number of experiments a scientist can run is limited, and
each one of them is very costly, so it makes sense to subjectively reﬁne the set under
consideration. In online marketing, where experiments can be run at very little cost,
there is much less need to use a subjective prior to reﬁne the set of possible ads with
which to experiment.
Additional implications of Proposition 2 reﬁne our understanding of experimental
practice. Part (2) implies that a decision maker who randomizes even without
understanding all its ramiﬁcationsdwhy she is randomizing, what audience the experiment is meant to satisfydwill nevertheless produce an almost-optimal experiment for
large values of N. Even if someone (or her own doubts) produces a particularly
challenging prior, the decision rule is still likely to be close to optimal. In this sense,

13

Similarly, a politician trying out platforms will do so at a few carefully chosen venues in front of carefully chosen
audiences.

Decision Theoretic Approaches to Experiment Design and External Validity

our approach addresses the concern that decision makers may violate Savage’s
completeness axiom.14
Proposition 2 also highlights the importance of actually randomizing. An experiment
that adopts a protocol where assignment is only “nearly” random, such as assignment
based on time of day of an experimental session (see Green and Tusicisny, 2012; for a
critique), or the ﬁrst letter of an experimental subject’s name (as was the case in the
deworming study of Miguel and Kremer, 2004; see Deaton, 2010 for a critique), will
tend to ﬁnd a skeptical prior in its audience. Randomization provides a defense against
the most skeptical priors, but near-randomization offers no such protection.

4. RERANDOMIZATION, REGISTRATION, AND PREANALYSIS
Proposition 2 suggests that the adversarial experimentation framework described by (2)
may be useful for capturing the objectives of real-life experimenters. We now highlight
ways in which having such a model can shed light on questions of current importance to
the experimental community.

4.1 Rerandomization
Banerjee et al. (2014) brings the adversarial framework of (2) to bear on the question of
rerandomization. A well-known problem with randomization is that it sometimes results
in observable characteristics being poorly balanced across treatment and control groups
(see Morgan and Rubin, 2012, and references therein).15 Of course, stratiﬁcation, blocking, and matching methods can be used to improve balance while maintaining randomization.16 However, as any researcher who has tried to simultaneously stratify on multiple
continuous variables knows, this can be quite difﬁcult in practice. Moreover, these techniques have issues of their own (Athey and Imbens, 2016).
Rerandomization is a simple and intuitively attractive alternative: if a sample “looks”
unbalanced, simply randomize again, and keep doing so until the sample looks balanced.
While many authors caution against the use of rerandomization because it may have large
14

15

16

A similar results hold for more complex policies that vary treatment with covariate x, provided the complexity of
possible policies is limited. See Vapnik (1999) for operational deﬁnitions of “complexity.”
Balance is important because it limits the possible set of alternative interpretations of the evidence, as described
previously. It also seems to serve as a rule of thumb for the experiment being competently executed, although this
may not be warranted.
Stratifying on several continuous variables is usually impractical for reasons related to the “curse of dimensionality.
” Consider an experiment with one treatment, one control, and four (continuous) variables describing participant
heterogeneity. The natural strategy would be to bin subject characterizations along each dimension. In this example,
we suppose each variable is split into ﬁve bins. Then there are 45 ¼ 1; 024 cells in which to stratify, with each cell
requiring two observations: one treatment, and one control. Unless the sample size is signiﬁcantly greater than 2,048,
with high likelihood there will be many cells with only one, unmatched, observation.

153

154

Handbook of Field Experiments

statistical and internal validity costs (see Bruhn and McKenzie, 2009, and references
therein), our framework can be used to precisely those costs.
From a purely Bayesian perspective, rerandomization does not create any
concerns and, indeed, may be beneﬁcial because it may select an experiment closer to
the optimal deterministic experiment from a particular subjective point of view. That
is, why should a Bayesian learn differently from the same balanced sample if it is reached
by a single lucky randomization, or by choosing among many?
In Banerjee et al. (2014), we show that the concerns brought up by Bruhn and
McKenzie (2009) make sense in our adversarial framework. Rerandomization does
have a cost in terms of robustness. Indeed, sufﬁciently many rerandomizations lead to
an essentially deterministic allocation, which, we show, results in losses bounded away
from zero for the adversarial audience. However, we also show that this cost is negligible,
provided the number of rerandomizations is not exponential in the sample size.
We can make these costs and beneﬁts precise, if K randomizations occur (K ¼ 1 being a standard RCT), frequentist decision-makingdi.e., assigning the treatment that perqﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
forms best empiricallydis optimal up to a loss bounded by maxf1;logðKÞg
. Importantly,
N
pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
logðNÞ is a number between 1.5 and 3 for sample sizes between 10 and 10,000, which
suggests that setting K  N results in minimal losses of robustness. In turn, K randomizations guarantee that the ﬁnal sample will be within the group of 5% most balanced
samples with probability 1  0:95K . Observing that 1  0:95100 > 0:99, this suggests
the following rule of thumb for rerandomization.
Rule of Thumb:
Use the most balanced sample out of K randomizations, where K ¼ minfN; 100g.
Note that the balance criteria need not be deﬁned ex ante. That is, the researcher can
rerandomize K times and select the assignment of treatment and control, however, they
like even after seeing the set of possible assignments.17
We believe our proposal for rerandomization has several beneﬁts. First, it provides
simple, effective guidelines under which rerandomization is not problematic. Second,
by doing so, it may help bring rerandomization out in the open. As discussed in Bruhn
and McKenzie (2009), many authors who employ rerandomization fail to disclose it,
possibly because of the stigma attached to the practice. However, as long as rerandomization is done in a way that explicitly takes into account its costs and beneﬁts, there is
no reason for such a stigma.

17

Two important notes are in order here. First, when clustered randomization is done, e.g., at the village level, then the
number of rerandomizations should equal the number of clusters, not observations. Second, one can both stratify and
rerandomize. That is, an experimenter can choose simple variables on which to stratify, and then rerandomize to
achieve better balance on the more complex or continuous variables.

Decision Theoretic Approaches to Experiment Design and External Validity

Finally, rerandomization may help experimenters ﬁnd compromises with governments or research partners uncomfortable with randomization. In some cases, experimenters negotiate a near-random treatment assignment scheme, as in the deworming
example discussed previously. Our proposal is a middle ground: experimenters could
produce a list of K randomizations to give to their implementation partner, and the partner could choose from that list. The criteria the implementing partner uses to choose a
particular randomization could be anything they like it to be: from the one that “looks”
the fairest to them, to more cynical ones that values having a particular village or person
in the treatment group. Hybrids are possible as well: an experimenter could generate 100
randomization schemes and allow the implementing partner to choose, however they
want, from among the ﬁve most balanced.

4.2 Registration
Registration, enabled by platforms such as the American Economic Association’s Randomized Controlled Trials Registry, is being embraced by a growing proportion of
the experimental community. It has two effects. First, it creates a centralized and thorough database of experimental designs and outcomes that does not suffer from publication bias, ﬁle drawer bias, and so on. Second, it often leads researchers to commit to a
particular experiment and not change the experimental design during the course of
the experiment. It should be noted that the latter is not a primary intention of
registries or their designers.
Within the framework described by (2), the ﬁrst aspect of registration is unambiguously good. More information is always beneﬁcial, simply because it can be ignored.18
The commitment value of registration is much less obvious. In a dynamic setting,
where experimental designs can be updated after the arrival of new information, Bayesians have no value for commitment, as they are time consistent. Indeed, if the decision
maker is limited in her ability to specify complex contingency plans, then commitment
has negative value. The value is even more negative when one considers the fact that
updating a design may produce more useful information.
4.2.1 Good commitment
Although registries are imperfect commitment devices, they are often used that way by
experimenters. Commitment is valuable for the ambiguity-averse decision maker
described by (2). Indeed, as Machina (1989) highlights, nonexpected utility maximizers
are not dynamically consistent. In other words, an ambiguity-averse decision maker who
18

Note that decision makers exhibiting self-control problems (Gul and Pesendorfer, 2001), or decision makers with
preferences over the revelation of uncertainty (Kreps and Porteus, 1978), may prefer to restrict the information
available. Players involved in a strategic game may also have this preference.

155

156

Handbook of Field Experiments

likes a particular design ex ante may be unsatisﬁed with the resulting experiment ex
post and try to alter its design.
The kind of temptation against which a decision maker may want to commit amounts
to either: (1) tampering with realized random assignments or (2) reneging from implementing a policy proven to be effective according to a burden of proof speciﬁed ex
ante. Indeed, once a random assignment is drawn, there always exist priors under which
the realized sample assignment and/or the policy conclusions are unsatisfactory.
Commitment allows the experimenter follow through with the original plan. A plan,
it should be remembered, that was ex ante satisfactory to both the experimenter and
her adversarial audience.
The idea that registries allow various parties to commit to both an experiment and an
action plan is plausible. Research partners may sometimes want to redraw assignments,
shut down all or part of the experiment, or suppress parts of the data because they
ﬁnd the results misleading. Such hiding of information is likely to be undesirable in itself,
in addition to its potentially harmful effects on the incentives of the experimenter
(Aghion and Tirole, 1994). Registration can reduce this risk.
4.2.2 Bad commitment
There is, however, scope for excessive commitment. Indeed, while it is important for experimenters to commit to a randomized assignment, they need not commit to a speciﬁc
treatment to guarantee robust inferences. For instance, after gaining experience with a
treatment A, the experimenter may subjectively decide that a variant A0 is likely to be
much more useful. Experimenting with A0 does not preclude robust inference about
the value of A0 versus the default alternative. In fact, data from experimenting with A
and A0 can be aggregated, corresponding to a mixture treatment A=A0 .
In principle, if there are ﬁnite possible treatments, an ambiguity-averse decision maker
may wish to randomize the treatments with which she experiments. In practice, however,
experimenters do not randomize the treatments they evaluate. The space of possible
treatments is simply too large for such random exploration to be useful. Instead, the experimenter’s subjective prior ends up driving the choice of intervention to evaluate. Randomized assignment after the treatment is chosen allows the experimenter to convince her
audience to take the data seriously, although this may create a loss of valuable information.
If experimenters are bounded in terms of the number of possibilities they can imagine
(as we deﬁnitely were), committing to a very detailed design once and for all makes little
sense. It is costly to do, and it limits ﬂexibility in ways that do not improve robustness.
There is little reason not to update experiments, provided that these updates are registered, as is allowed (and tracked) by most registration platforms.19
19

Of course, experimenters should not be allowed to ﬁrst collect data and then register a design that speaks only to a
selected portion of this data.

Decision Theoretic Approaches to Experiment Design and External Validity

4.2.3 Examples
An insider’s perspective into Alatas et al. (2012) illustrates the cost of excessive commitment. Alatas et al. (2012) describe a ﬁeld experiment in Indonesia where communities
were asked to rank their members from poor to rich. The order in which households
were considered for ranking was chosen randomly, driven by some concern for fairness.
Households that were ranked earlier were ranked much more accurately, presumably
because the rankers got tired or bored as the ranking meeting progressed. This was not
something the authors had set out to learn. However, it might have made sense to change
the protocol to guard against the inefﬁciency of late rankingsdperhaps the ranking could
have been done in batches, with breaks in between. But, the fact that the experiment was
registered gave us a false sense that we were could not alter the design, although such an
update could have been reﬂected in the registry and may have allowed for more learning.
Another example can be found in Andreoni et al. (2016), which used incentived
choices to estimate time-discounting parameters of polio vaccinators in Pakistan. These
parameters were then used to construct optimal contracts, which were tested against a
standard piece rate. Unfortunately, the authors had preregistered and thus felt committed
to, a model of time preferences that the data showed to be misspeciﬁed. This was a potentially fatal decision as the paper is a “proof of concept” of using preference estimation to
design personalized contracts and had the misspeciﬁcation been severe enough, it would
have resulted in a failure to generate a signiﬁcant improvement. Luckily, this was not the
case, but it illustrates the dangers of “too much” commitment.

4.3 Preanalysis plans
A preanalysis plan lists all outcomes of interest, and the ways in which data will be
analyzed when the experiment is complete. Formally, it may be thought of as a subset
of statistics S of the data.
4.3.1 Preanalysis and bounded rationality
Interestingly, neither Bayesian nor ambiguity-averse decision makers ﬁnd it beneﬁcial to
register a preanalysis plan, nor would her audience care if she did. This follows from two
implicit assumptions: (1) all data are disclosed and (2) the decision maker and audience
members have unbounded cognitive capacity. If an audience member is suspicious
that the experimenter cherry-picked results, she can just run her own analyses. This seems
appropriate when the experimenter faces a sophisticated professional audience (i.e., referees, editors, seminar participants). However, in practice, there is demand for preanalysis
and thus, a careful, decision-theoretic foundation for preanalysis plans is likely
worthwhile.
While such a foundation is beyond the scope of this chapter, we can hint at a setup in
which preanalysis, i.e., preselecting a subset S of statistics to be reported, becomes relevant. We believe this reﬂects the bounded rationality constraints of the decision maker or

157

158

Handbook of Field Experiments

audience members. Indeed, if the decision maker can only process a subset of information
S, she may be rightfully concerned about the way this set is selected. Formulating a preanalysis plan can reassure the stakeholders and facilitate actionable inference. Of course, if
cognitive capacity is the issue, then preanalysis plans cannot be excessively complicated:
The goal is not for authors to anticipate all possible interesting inquiries into the data. This
would defeat the purpose of preanalysis plans by making them inaccessible to time-constrained decision makers.
In practice, experimenters are likely to speak to various audiences, each warranting
different attitudes toward preanalysis plans. A scholarly audience might reason that by
demanding robustness checks, it is, in effect, forcing the reporting of all relevant dimensions of the data. Such an audience may prefer to ignore preanalysis plans. However, an
audience of time-constrained policymakers may behave differently, and only update
from experiments with simple, clearly stated preanalysis plans.
We see no need to view these perspectives as oppositional. Given the variety of audiences, the best response to us seems to allow for both ex ante and ex post analyses of the
data within clearly deﬁned “ex ante analysis” and “ex post analysis” sections. Ex antee
speciﬁed hypotheses will be useful to time-constrained audiences lacking the desire to
really delve into the data. Ex post analysis of the data will allow experimenters to report
insights that were hard to anticipate without the help of data.
4.3.2 Caveats
The previous discussion does not touch on moral hazard concerns.20 In this respect, two
questions seem relevant: Is misbehavior by experimenters is prevalent in economics? Are
the mechanisms of registration and preanalysis a long-term solution to this potential issue?
The data at this point suggest that the answers are respectively “not very” and “maybe
not”. In particular, Brodeur et al. (2016), ﬁnd very little evidence of nefarious conduct
in articles in top economics journals and detect none in the reporting of results from randomized experiments. Moreover, in medicine, where norms of preregistration and preanalysis are often enforced by journals, a recent study by the Center for Evidence Based
Medicine at Oxford University found that 58/67 of the articles examined contain
misreportingdi.e., failure to report prespeciﬁed outcomes.21 Response to these results
has been quite varied, with at least one prestigious journal issuing corrections to all implicated articles, and another releasing an editorial defending aspects of misreporting.
20

21

Humphreys et al. (2013) also emphasizes a communication role for preanalysis plans. However, this should not
detract from the very real commitment dimensions of registration and preanalysis plans, and the fact that in order to
make them successful, one needs to pay attention to how this commitment gives authors incentives to comply, or
not.
See http://compare-trials.org/blog/post-hoc-pre-speciﬁcation-and-undeclared-separation-of-results-a-brokenrecord-in-the-making/.

Decision Theoretic Approaches to Experiment Design and External Validity

A valuable aspect of preanalysis plans that we do not account for is that they serve as
contractual devices with research partners heavily invested in the outcome of an experiment (Casey et al., 2012; Olken, 2015). In these environments, a preanalysis plan may
prevent a research partner from shifting deﬁnitions after the data are collected. In addition, specifying table formats, and the analyses therein, ahead of time, is useful in identifying and eliminating disagreement between co-authors and translating intentions into
clear instructions for research assistants.
4.3.3 Theory
Prespeciﬁed theories are potentially a way to protect against the accusation (and temptation) of motivated choices in analysis, while still preserving some analytical ﬂexibility.
This is in contrast with current common practice, which is to announce the theory in
the same paper that shows the results of empirical analyses. Instead, a prespeciﬁed theory
should be “published” prior to empirical analysis, and, ideally prior to running the
experiment.
An explicit, prespeciﬁed theory preserves some ﬂexibility of analysis, while restricting
interpretation. It can be used to justify running regressions that were not prespeciﬁedd
those that are natural implications of the theorydwithout opening the door to full-scale
speciﬁcation searchers. Moreover, prespecifying theory has the effect of making the experimenter’s priors public, thereby allowing the audience to challenge an experimenter’s
interpretation of data on the grounds that it is inconsistent with the experimenter’s prior
theory. Of course some elements of an analysis cannot be derived from an abstract theory,
e.g., the level of clustering. Therefore, it may make sense to combine some ex ante restrictions on speciﬁcations with a prespeciﬁed theory.
It is worth emphasizing that even a theory that is developed ex post can impose useful
restrictions on the analysis and reporting of results. In particular, given the implications of
a theory, an audience can enquire why some are tested and others are not. While, in some
cases, an ex post theory may turn out to be fully jury-rigged, it at least makes clear the
author’s assumptions, which then can be challenged.
In a sense, a preanalysis plan is often just a reﬂection of an implicit theory, thus a prespeciﬁed theory has some of the same drawbacks as a preanalysis plan, but additional beneﬁts. Like preanalysis plans, theory will not change the beliefs of a skeptical audience that
wants to examine the entire data set. In addition, if the theory turns out, ex post, to be
irrelevant it may distract from useful features of the data (just as with preanalysis plans).
However, theory has the added beneﬁt of making external extrapolation easier and
more transparent, as we develop further in the next section.

5. EXTERNAL VALIDITY
So far we have focused on internal decision problems, where treatment effects in the
population enrolled in the experiment are the same as in the population a policy will

159

160

Handbook of Field Experiments

be implemented upon. We now bring our framework to bear on external decision problems, in which treatment effects may differ between these two populations.
Formally, we allow the effectiveness of the treatment, described by vector p, to vary
with the environment, denoted z ˛fze ; zp g (for experimental and policy-relevant):

X
pz ¼ p0x;z ; p1x;z
˛ ½0; 12 hP :
x˛X

While randomization is robustly optimal in internal decision problems (ze ¼ zp )d
provided the sample size is large enoughdwe now show that policy advice for external
environments remains Bayesian even for arbitrarily large sample sizes. Under plausible assumptions, the best guess about which policy to choose in an environment or population
that has not been studied is the experimenter’s posterior after seeing experimental results
in a related setting.
Let Hjpz denote the set of marginal distributions hjpz over treatment effects pz for
priors h ˛H entertained by the audience. While information about environment ze
will likely affect the posterior over pzp for any given prior, it need not restrict the set
of possible priors over pzp . This is captured by the following formal assumption.
Assumption 2
H

pzp

 Hjpz 3H:22
e

External validity can be thought of as the following problem: after running an experiment in environment ze , the experimenter is asked to make a recommendation for the
external environment zp . She thus chooses E and a to solve

h
i
h
i
max lEhe u a; pzp þ ð1  lÞ min Eh u a; pzp
:
(3)
E wpze ;a

h˛H

Proposition 3 (external policy advice is Bayesian).
The optimal recommendation rule a in (3) depends only on the experimenter’s posterior belief he ð pzp jeÞ given experimental realization e. The optimal experiment E  is
Bayesian optimal under prior he .
That is, external recommendations only reﬂect the beliefs held by the experimenter,
not by the audience. This occurs because, under Assumption 2, evidence accumulated in
environment ze does not change the set of priors entertained by the audience in environment zp zp , i.e., it does not reduce the ambiguity in environment zp . This further implies
that the most information one can hope to obtain is the experimenter’s subjective posterior belief over state pzp .

22

While this assumption is clearly stylized, our results generalize, provided there remains sufﬁcient ambiguity about
environment zp , even conditional on knowing environment ze very well.

Decision Theoretic Approaches to Experiment Design and External Validity

6. STRUCTURED SPECULATION
Proposition 3 formalizes the natural intuition that external policy advice is unavoidably
subjective. This does not mean that it needs to be uninformed by experimental evidence,
rather, judgment will unavoidably color it.
This also does not imply that subjective recommendations by experimenters cannot
be used to inform policymakers. In many (most?) cases, the policymaker will have to
make a call without a randomized controlled trial tailored to the particular environment.
As such, the decision maker’s most useful repository of information is likely to be the
experimenter, because she is likely to deeply understand the experimental environment,
previous results and evaluations, and how a policy environment may differ from experimental environments.
Proposition 3 also does not mean that external policy advice is cheap talk. Indeed,
further evidence may be collected, and, provided that advice is precise, it may be proven
to be right or wrong. What we should aim to do is extract the experimenter’s honest beliefs about the efﬁcacy of treatment in different environments. While this is not an
entirely obvious exercise, we know from the literature on incentivizing experts that it
is possible (see, e.g., Olszewski and Peski, 2011; Chassang, 2013).
Practically, we do not think formal incentives are necessary to ensure truthful revelation. Instead, we believe a clear set of systematic guidelines for structured speculation
may go a long way.
Guidelines for structured speculation:
1. Experimenters should systematically speculate about the external validity of their
ﬁndings.
2. Such speculation should be clearly and cleanly separated from the rest of the paper;
maybe in a section called “Speculation”.
3. Speculation should be precise and falsiﬁable.
The core requirements here are for speculative statements to be labeled as such and be
falsiﬁable. Practically, this means predictions need to be sufﬁciently precise that the
experiment to validate or falsify them is unambiguous. This will allow testing by subsequent experimenters. By a reputational argument, this implies that speculative statements
will not be cheap talk.

6.1 The Value of Structured Speculation
We believe that creating space for structured speculation is important and useful for
several reasons.
First, providing a dedicated space for speculation will produce information that
would not otherwise be transmitted. When assessing external questions, experimenters
will bring to bear the full range of their practical knowledge built in the ﬁeld. This includes an intuitive understanding of the mechanisms at work, of the underlying

161

162

Handbook of Field Experiments

heterogeneity in treatment effects, how these correlates with observable characteristics,
and so on.
Second, enforcing the format of speculative statementsdi.e., ensuring statements are
precise and falsiﬁabledwill facilitate and encourage follow-up tests, as well as interaction
with closely related work.
Finally, to us, the most important side effect of asking experimenters to speculate
about external validity is the creation of incentives to produce experimental designs
that maximize the ability to address external questions. To address scalability, experimenters may structure local pilot studies for easy comparison with their main experiments. To identify the right subpopulations for generalizing to other environments,
experimenters can identify ahead of time the characteristics of groups that can be
generalized and stratify on those. To extend the results to populations with a different
distribution of unobserved characteristics, experimenters may elicit the former using
the selective trial techniques discussed in Chassang et al. (2012) and run the experiment
separately for each of the groups so identiﬁed.
While these beneﬁts are speculative (and difﬁcult to falsify!), it is our belief that
creating a rigorous framework for external validity is an important step in completing
an ecosystem for social science ﬁeld experiments and a complement to many other aspects of experimentation.
In the next subsections, we describe an operational framework for structured speculation that can be used today. We begin by providing concrete examples of what structured speculation may look like, and how it may be useful. We then propose a baseline
set of external validity issues that should be systematically addressed. We conclude by discussing possible formats for structured speculation: qualitative, reduced form, and
structural.

6.2 Examples
To ﬂesh out what we mean by structured speculation, we describe the form it may take in
the context of a few papers.
Dupas (2014). Dupas (2014) studies the effect of short-term subsidies on long-run
adoption and reports that short-term subsidies had a signiﬁcant impact on the adoption
of a more effective and comfortable class of bed nets. In its Section 5, the paper provides
an extraordinary discussion of external validity.
It ﬁrst spells out a simple and transparent argument relating the effectiveness of shortrun subsidies to: (1) the speed at which various forms of uncertainty are resolved and (2)
the timing of user’s costs and beneﬁts. If the uncertainty over beneﬁts is resolved quickly,
short-run subsidies can have a long-term effect. If uncertainty over beneﬁts is resolved
slowly, and adoption costs are incurred early on, short-run subsidies are unlikely to
have a long-term effect.

Decision Theoretic Approaches to Experiment Design and External Validity

It then answers the question, “For what types of health products and contexts would we
expect the same results to obtain?” It does so by classifying potential technologies into three
categories based on how short-run (or one-time) subsidies would change adoption patterns:
1. Increased: cookstoves, water ﬁlters;
2. Unaffected: water disinfectant;
3. Decreased: deworming drugs.
While very simple, these statements are perfect examples of what structured speculation might look like. They attack a relevant policy questiondthe extension of one-time
subsidies to other technologiesdand make clear predictions that could be falsiﬁed
through new experiments.
Banerjee et al. (2015a). This paper does not engage in speculation but can illustrate
the potential value of structured speculation for experimenters and their audiences. In
particular, it reports on seven separate ﬁeld trials, in seven different countries, of a program designed to help the ultra poor. The basic intervention was the same in all countries, was funded out of the same pool, and the evaluations were all coordinated by Dean
Karlan of Yale University.
Within the study, there were two options for external speculation. First, different
countries were evaluated at different times. Second, there were multiple rounds of results
for each location. Results from countries evaluated early in the experiment could have
been used to speculate about the results from those evaluated later. Within a country,
earlier rounds could have been used to speculate about later rounds. But what would
have been the beneﬁt of doing so? And how would we go about it, in hindsight?
There were many common questions that came up about this research: How long did
we expect the effects to last? Was there any point in carrying out this program in rich or
middle-income countries? Formally speculating about these questions in earlier rounds
and countries would have provided a structure for answering those multiple
queries and justiﬁed elements of our experimental design that readers and reviewers
had some reason to criticize. In addition, making public predictions would have provided
an opportunity for the authorsdand other scholarsdto learn about what kinds of predictions tend to be trustworthy.
Even directional predictionsda speculation that this effect will be larger than that
one, or that it will be bigger or smaller than some number, possibly zerodwould
have been of some use. The point estimates of the program impact are smaller in richer
countries. Does this mean the program needs to be rethought for richer countries? We
could have informed this decision by aggregating all we knew about the program,
including the quantile results and results for certain subpopulations, to declare whether
we believe the effects shrink with a country’s gross domestic product. We could have
done this at different points in time, as the results came in from different countries and
rounds, to see how good we are at making these sorts of predictions, and thus, how
strongly we should advocate for our predictions at the end of the study. A similar exercise

163

164

Handbook of Field Experiments

could have also been carried to predict the change in impact over time, which is key to
understanding whether the intervention actually frees people from a poverty trap.
Banerjee et al. (2015b). Directional predictions would have also been very useful in
maximizing the information from a series of so-called Teaching at the Right Level (TaRL)
interventions described in Banerjee et al. (2015b). These interventions seek to teach children basic skills they lack, even when they are in a grade that presumes they have mastered
those skills. This does not happen as a matter of course in most schools in the developing
world (Banerjee and Duﬂo, 2011). As this intervention had already been shown to work
on the margins of the school system, each experiment (RCT) focused on a different way of
integrating this practice into government schools. The interventions varied from training
teachers, to giving them the materials to use for TaRL, to implementing TaRL during the
summer break (when teachers are not required to follow the set curriculum), to integrating
TaRL directly into the curriculum, and so on. Each intervention built on the successes and
failures of the previous interventions, culminating in two different, but successful, models.
Yet without recording the predictions made along the way, this would look like an ex post
rationalization of shooting in the dark. Even minimal public predictionsdthis approach is
likely to work better than thatdwould have helped at lot.
Duﬂo et al. (2008). Another innovation that would have been useful is our call to
record structured speculation at the end of each paper (in addition to in a repository, as
we describe in the following section). This would allow for a clear demarcation of results
that are speculativedwhich will tend to arise in papers with preregistration and preanalysis
plansdand those that are not. Such a demarcation would have clearly helped in
dealing with Deaton’s (2010, pp. 441e442) critique of the ﬁrst TaRL paper (Banerjee
et al., 2007).
When two independent but identical randomized controlled trials in two cities in India ﬁnd that
children’s scores improved less in Mumbai than in Vadodara, the authors state “this is likely
related to the fact that over 80 percent of the children in Mumbai had already mastered the basic
language skills the program was covering” (Duﬂo et al., 2008). It is not clear how “likely” is established here, and there is certainly no evidence that conforms to the “gold standard” that is seen as
one of the central justiﬁcations for (randomized controlled trials). For the same reason, repeated
successful replications of a “what works” experiment, i.e., one that is unrelated to some underlying
or guiding mechanism, are both unlikely and unlikely to be persuasive.

Our proposal would have helped with such criticism by establishing a place within the
paper where it was clear that this assertion was based on our own knowledge and intuitions, rather than a part of the experimental design.
6.2.1 A Post hoc evaluation
In summary, our proposal would have helped with criticisms of prior research in three
ways. First, it would establish a place within research where such speculation is both expected and encouraged. Second, by attaching reputational incentives to such speculation,

Decision Theoretic Approaches to Experiment Design and External Validity

the reader can be assured that it is not just idle chatter intended to explain away an uncomfortable discrepancy. Third, because experimenters will be encouraged to speculate
about the outcomes of replications before they happen, replications that are close to their
predictions should increase, at least slightly, the credibility of the experimenter’s preferred
underlying mechanism.
An alternative approach, being pioneered by Stefano DellaVigna, of the University of
California, and Devin Pope, of the University of Chicago, is to elicit priors on a speciﬁc
research question from a wide range of experts (Della Vigna and Pope, 2016a,b). This
has the beneﬁt of forcing the audience to think about their priors before research is carried
out, and identifying the places in which research can make the largest contribution by shifting the average prior, or collapsing the distribution of priors. However, it is unlikely to protect against the most skeptical members of an audience, who may not be in any given
surveyed panel of experts. Moreover, it lacks many of the side beneﬁts of our proposal.
On the other hand, the DellaVigna and Pope approach is being implemented today,
while, with the exception of Dupas (2014), none of the papers above contained structured speculation. Why not? This was, of course, in part because it was not on the agenda.
But there are deeper reasons: we, like many other researchers, focused on the reduced
form local average treatment effect estimates, which tell us very little directly about
how they would translate to other environments. A more natural basis for speculation
would be to estimate a structural model and use the estimated parametersdwhich can
be made to directly depend on features of the environmentdto predict out of sample.
But, we must recognize that the choice of a model is itself subjective, so providing a
model that rationalizes some prediction is not, in itself, completely reassuring.
However, the alternative may be worse. With different (Bayesian) readers having
different priors and models of the world, even well-structured speculation without a model
could be interpreted in multiple ways. The model serves as a currency for reducing, to a
single number, the many disparate pieces of information that the author has. Without the
prop of a model that exercise seems too hard to carry out with any accuracy.
To reduce the space of possible models, it would be helpful to demarcate the set of
environments where structured speculation would be particularly useful, and the challenges likely to be encountered there. This is what the next subsection attempts to do.

7. ISSUES OF PARTICULAR INTEREST
While our proposal could apply to any element of external validity, it is perhaps useful to
outline a number of external validity issues that are focal for economists.
Focal external validity issues:
1. How scalable is the intervention?
2. What are treatment effects on a different population?
3. What are treatment effects on the same population in different circumstances?

165

166

Handbook of Field Experiments

Another important question that we do not discuss further is the one addressed by
Dupas (2014): What is the effect of a different, but related, technology?

7.1 Scalability
A central concern in many development environments is how an intervention might
scale, i.e., how might the treatment effects measured in an experiment change if the intervention was rolled out across a province, country, or region? This concern is often
composed of two interrelated issues: how spillover effects might enhance or reduce
the beneﬁts of a particular treatment, and how the incentives of an organization capable
of implementing large-scale interventions might affect outcomes.
Spillovers. Spillovers encompass both general equilibrium effects and externalities.
Consider an intervention that gives scholarships for top-performing students in local
schools to attend provincial schools. As an experimental intervention, this policy may
have large positive effects on a locality because several students from the local school
would be able to get an improved education. However, if rolled out nationally, the
returns on human capital may diminish, possibly diminishing the treatment effect on
outcomes such as wealth, savings, and consumption. There may, however, be positive
general equilibrium effects. For instance, a more educated available workforce may increase foreign direct investment and lead to the creation of new types of jobs. General
equilibrium effects are difﬁcult to apprehend through purely experimental methods,
but it is possible to draw on different sources of information to inform speculation.
For instance, one may use regional heterogeneity in average human capital to map
out what the effect may be should the program get rolled out.
Direct externalities (e.g., physical interaction, information, contagion, and so on)
may be more easily captured, as they tend to be more localized. Experimental designs
that at least partially capture local externalities are now quite standard (Baird et al., 2014;
CrYPER et al., 2013; Muralidharan and Sundararaman, 2015). The difﬁcult external
validity question relates to the effect of scaling on adoption rates. In some cases, such
as those of deworming drugs or vaccines, private returns are a diminishing function
of aggregate adoption. This may be addressed by variation in the intensity of the program across locations. While this variation is likely to not result in sufﬁcient power to
become a main ﬁnding of a paper, it would be useful for guiding speculation about
external validity.
Implementation by others. Informed speculation about implementing agencies is
inherently difﬁcult. Three environments seem relevant: implementations by other researchers, implementations by non-governmental organizations (NGOs) or international
agencies, and implementations by provincial or country governments (Bold et al., 2013).
The difﬁculty is that in order to make her speculation meaningful, the experimenter
would need to specify the precise governments or NGOs that her projections apply

Decision Theoretic Approaches to Experiment Design and External Validity

to. This might expose the experimenter to political risk and hamper her ability to
conduct future experiments.23 At the very least, it should be possible for the experimenter to highlight the speciﬁc aspects of the intervention that may make it difﬁcult
to be implemented by others.
One aspect of implementation that can possibly be controlled by experimenters is the
reputational capital they have when they interact with the target population. They may
be able to control for this by running initial perception surveys regarding their potential
implementation partners, as well as by varying the way they present themselves. Having
an ofﬁcial present at a meeting may signiﬁcantly affect the way people engage with an
experiment.
Again, an experiment may not be sufﬁciently well powered for variation in implementation to lead to signiﬁcant ﬁndings. However, that data would clearly help informed
speculation. In some cases, the experimenter may just have an intuitive understanding of
how things would play out in different settings. Such intuitive understandings would be
of great value to the next experimenter(s) who tried similar experiments. As such, it would
be a useful contribution to speculate about the role of implementing agencies on outcomes.

7.2 Effect on other populations
If a program is effective in one region, or one country, is it effective in another? If a program is effective for a speciﬁc social group, would it be effective for different groups in
the same country? For comparable groups in a different country? Answering such questions is inherently a speculative exercise, and yet it is useful for experimenters to do so.
Experimenters have detailed intuitive knowledge of local mechanisms that can help
clarify what matters for results to extend or not.
For example, suppose a program was found to be effective in India, and the experimenter tried to speculate about its effectiveness in Kenya. The experimenter may ﬁrst
assess the underlying heterogeneity in treatment effects and decide the program is principally effective in helping members of Scheduled Castes. If this is the case, one may
reason that the program could be effective for historically discriminated populations of
Kenya, say Muslims. However, by spelling this hypothesis out clearly, another experimenter may question its relevance if she believed afﬁrmative action for Scheduled Castes
appears essential for the treatment to be effective.
Subgroups and selective trials. We believe that subgroup analysis, which is often
instructive but poorly identiﬁed, has an important role to play in formulating successful
speculative hypotheses. Reweighting treatment effects by subgroups provides a natural
way to project ﬁndings to different environments. This obviously includes groups formed
23

This concern may be mitigated in practice by the fact that the employees of many organizations are more aware of
their limited implementation abilities than researchers themselves.

167

168

Handbook of Field Experiments

on observable characteristics, say income, education, religion, and so on. Interestingly,
this also includes unobservable characteristics elicited through mechanisms.
A recent strand of the experimental literature, illustrated by Ashraf et al. (2010), Berry
et al. (2012), Cohen and Dupas (2010), Jack et al. (2013), and Karlan and Zinman
(2009) and formalized in Chassang et al. (2012), combines randomization with self-selection in order to “observe unobservables.” The idea is as follows: randomized trials are lotteries over treatment. Many trials consist of a single lottery. By having multiple lotteries
with different winning rates and assigning costs to these lotteries, it becomes possible to
elicit participants’ values for the treatment, and estimate treatment effects conditional on
values. This provides additional information helpful to project treatment effects on
different populations.
For instance, as selective trials recover marginal treatment effects, Heckman and
Vytlacil (2005), they allow the experimenter to ﬁgure out the effect of the program
on populations selected through prices, by reduced availability, and so on. An experimenter will also make very different predictions about external treatment effects
depending on whether it is effective for everybody, or only for highly motivated
participants.
It is important to note that the “cost” of a lottery does not need to be monetary.
Indeed, effort, more than money, seems to be a metric more easily comparable across locations. Alatas et al. (2016) varies whether a participant has to travel for an interview, or
can stay at home, to see if they qualify for a cash-transfer program for the poor. They ﬁnd
that those who travel are signiﬁcantly more likely to actually be qualiﬁed for the
program and that interviewer coding of them is signiﬁcantly more reliable. Randomizing
the treatment (the cash-transfer program) conditional on whether or not the participant is
judged to be qualiﬁed would have allowed this work to estimate returns for the motivated and for the less motivated. More generally, the variety of information that can
be elicited through mechanisms is very large, and it frequently comes with a natural structural interpretation. We believe that collecting such information will prove helpful in
formulating speculative hypotheses.

7.3 Same population, different circumstances
The same population may react differently to treatment in different circumstances. For
instance, if an intervention helps people save more, one may ask whether it will continue
to be effective as people accumulate savings. Similarly, one may ask about the effectiveness of subsidies for technology adoption as information about the new technology
spreads, and so on.
As before, subgroup analysis is likely to be helpful in forming opinions about the way
effects will pan out as the population evolves. Richer participants or more informed communities may be used to proxy for future populations. As such, innovations in

Decision Theoretic Approaches to Experiment Design and External Validity

Figure 2 A two-by-two blind trial. The ﬁgure shows the two stages of randomization, with participants
ﬁrst allocated to either a high- or low-probability treatment group, then informed of this probability
(thus generating the corresponding placebo effect) and then receiving either treatment or nontreatment in a standard, blinded manner. (Reproduced from Chassang, S., Snowberg, E., Seymour, B. and
Bowles, C., 2015. Accounting for behavior in treatment effects: new applications for blind trials, PLoS One
10 (6), e0127227.)

experimental design may also be helpful in this respect. Chassang et al. (2012) emphasizes
that by either varying incentives or by varying the participants’ beliefs that they are being
treated, it is possible to identify purely behavioral dimensions of treatment effects, i.e.,
treatment effects that are due to participant’s behavioral changes accompanying (the
expectation of) treatment. For instance, a small business owner participating in an experiment in which she receives accounting training may decide to work longer hours
because she believes the training is making her more productive. This could result in
ﬁnding positive effects to training even when accounting itself is not useful. These effects,
however, may not persist, the treatment effect for an informed participant, aware that accounting is of limited use, may be much smaller.
This observation is useful in medicine, where isolating treatment effects due to the
interaction of a new drug with patient behavior is essential for understanding the true
value add of that drug. In the context of medical trials, Chassang et al. (2015) proposes
2  2 blind trials (Fig. 2) able to isolate both the pure effect of treatment and the interaction of treatment and behavior. In a 2  2 trial, participants are randomly split into two
arms. In one arm, the participants are told they will have a high probability of treatment,
and in the other, they are told they will have a low probability of treatment. The trial
within each arm is then run accordingly. Under the assumption that participant behavior
will change with the probability of treatment, the trial independently randomizes both
behavior and treatment. This is sufﬁcient to isolate the pure effect of treatment, the
pure effect of behavior, and the interaction of treatment and behavior.
Bulte et al. (2014) implements a version of a 2  2 trial in a development context
using seed varieties as its technology. Its ﬁndings suggest that purely behavioral responses,
i.e., mediated by expectations of changedto treatment are signiﬁcant. This, in turn, suggests that participants may change their response over time, as they learn the true effectiveness of a treatment. Practically, running the complex mechanisms described in
Chassang et al. (2012), or using blind treatments as in Bulte et al. (2014) may not always
be feasible. However, it should always be possible to survey participants about their

169

170

Handbook of Field Experiments

expectations for the technology, and about how they are changing their practice in
response to treatment. These survey measures, which in many cases would not naturally
be reported as hard evidence, may prove quite useful in shaping speculative hypotheses.

7.4 Formats for structured speculation
We conclude our discussion of structured speculation with a brief discussion of the formats in which structured speculation may be expressed. We argue that, at this stage, there
is no wrong way to formulate hypotheses about external validity, provided the hypotheses are formulated in a clear and falsiﬁable way.
Qualitative predictions. Simple qualitative statements are not necessarily less
rigorous than analytical ones. For instance, Dupas (2014) describes environments in
which treatment effects are likely to larger or smaller. These descriptions are simple,
yet precise and falsiﬁable.
Experimenters often produce reduced form estimates for multiple subpopulations
and/or quantile treatment effectsdalthough they may not report all of themdand these,
along with some intuitive understanding of which environments are similar, make it
possible for them to predict the direction of change in the treatment effect, although
perhaps not the magnitude of the change. Such speculation is naturally expressed qualitatively, as in, “This treatment effect is likely to be larger than that.”
Predictive models. If sufﬁcient data about subgroups are available, experimenters
may feel comfortable producing statistical models predicting treatment effects in other
environments conditional on observables. Multiperiod, multicountry trials such as Alatas
et al. (2012), or multitrial meta-analyses offer natural starting points. The main advantage
of producing a fully speciﬁed, predictive model is that it is unambigous, and by construction, clear and falsiﬁable. It is therefore a better starting point for further analysis than
purely qualitative predictions. Note that the model need not necessarily predict point estimates. A model predicting ranges of estimates would be equally well speciﬁed.
Theory and structural models. Theory has an important role to play in formulating useful speculative hypotheses. If an experimenter lays out a theoretical model,
she thinks best, summarizes the facts she sees, and the theory is rich enough to cover environments beyond what is in her experiment, she is effectively making a directional prediction for other environments.24
This is already happening to some degree. For example, Karlan et al. (2012)
evaluate two interventions to improve business for tailors in Ghana. In one intervention,
the tailors were provided with a cash grant; in the other, they were given training. Both
changed the way the tailors practiced business (at least brieﬂy) but neither increased
proﬁts over the long term. The authors develop a model in which this occurs because
24

Some predictions may be ambiguous, which is both a beneﬁt and a drawback of formal models.

Decision Theoretic Approaches to Experiment Design and External Validity

tailors treat these interventions as opportunities to explore new opportunities. While
most of these fail, the option value of experimentation is likely still positive. This implies
that there should be some tailors who experience very large gains from these interventions, but that, on average, the effect will be small and difﬁcult to detect. To test this prediction, the authors look at other studies of similar interventions that are powered to
detect differential changes in the right tail of the distribution. They ﬁnd some support
for their theory.
Identiﬁed structural models, just as predictive statistical models, are attractive because
they make fully-speciﬁed predictions in external environments. An advantage they have
over purely statistical models is that they can make the process of external extrapolation
more transparent. We emphasize, however, the cautionary implications of Proposition 3.
In all external decision-making problems, inference is unavoidably subjective. In structural modeling, the source of subjectivity is the model itself.

8. CONCLUSION
This chapter has ranged from models of experimentation, to prescriptions for experimental design, all the way to external validity. Hopefully, the wide range of (potential)
applications of decision theory to experimental practice is enough to convince theorists
and practitioners alike that this is a fruitful area for further discovery.

REFERENCES
Aghion, P., Tirole, J., 1994. The management of innovation. Q. J. Econ. 109 (4), 1185e1209.
Aghion, P., Bolton, P., Harris, C., Jullien, B., 1991. Optimal learning by experimentation. Rev. Econ. Stud.
58 (4), 621e654.
Alatas, V., Abhijit, B., Hanna, R., Olken, B.A., Tobias, J., 2012. Targeting the poor: evidence from a ﬁeld
experiment in Indonesia. Am. Econ. Rev. 102 (4), 1206e1240.
Alatas, V., Purnamasari, R., Wai-Poi, M., Banerjee, A., Olken, B.A., Hanna, R., 2016. Self-targeting:
evidence from a ﬁeld experiment in Indonesia. J. Polit. Econ. 124 (2), 371e427.
Andreoni, J., Callen, M., Khan, Y., Jaffar, K., Sprenger, C., 2016. Using Preference Estimates to Customize Incentives: An Application to Polio Vaccination Drives in Pakistan. NBER Working Paper Series # 22019.
Ashraf, N., Berry, J., Shapiro, J.M., December 2010. Can higher prices stimulate product use? Evidence from
a ﬁeld experiment in Zambia. Am. Econ. Rev. 100 (6), 2383e2413.
Athey, S., Imbens, G.W., 2016. The econometrics of randomized experiments. In: Esther, D., Banerjee, A.
(Eds.), Handbook of Field Experiments, vol.1, pp. 73e140.
€ 2014. Designing Experiments to Measure Spillover
Baird, S., Aislinn Bohren, J., McIntosh, C., Berk, O.,
Effects. PIER Working Paper #14-032.
Banerjee, A., August 1992. A simple model of herd behavior. Q. J. Econ. 107 (3), 797e817.
Banerjee, A., Duﬂo, E., 2011. Poor Economics: A Radical Rethinking of the Way to Fight Global Poverty.
PublicAffairs.
Banerjee, A., Duﬂo, E., Goldberg, N., Karlan, D., Osei, R., Pariente, W., Shapiro, J., Thuysbaert, B.,
Udry, C., 2015a. A multifaceted program causes lasting progress for the very poor: evidence from six
countries. Science 348 (6236), 1260799.
Banerjee, A., Banerji, R., Berry, J., Duﬂo, E., Kannan, H., Mukherji, S., Walton, M., 2015b. Teaching at the
Right Level: Evidence From Randomized Evaluations in India. MIT Working paper.

171

172

Handbook of Field Experiments

Banerjee, A., Cole, S., Duﬂo, E., Linden, L., August 2007. Remedying education: evidence from two randomized experiments in India. Q. J. Econ. 122 (3), 1236e1264.
Banerjee, A., Chassang, S., Montero, S., Snowberg, E., 2014. A Theory of Experimenters. Princeton University, Mimeo.
Bellman, R., April 1956. A problem in the sequential design of experiments. Sankhya Indian J. Statistics 16
(3/4), 221e229.
Bergemann, D., V€alim€aki, J., September 1996. Learning and strategic pricing. Econometrica 64 (5),
1125e1149.
Bergemann, D., V€alim€aki, J., 2002. Information acquisition and efﬁcient mechanism design. Econometrica
70 (3), 1007e1033.
Bergemann, D., V€alim€aki, J., 2006. Bandit Problems. Cowles Foundation discussion paper.
Berry, J., Fischer, G., Guiteras, R., 2012. Eliciting and Utilizing Willingness to Pay: Evidence from Field
Trials in Northern Ghana. Cornell University, mimeo.
Bewley, T.F., 1998. Knightian uncertainty. In: Jacobs, D.P., Kalai, E., Kamien, M.I. (Eds.), Frontiers of
Research in Economic Theory: The Nancy L. Schwartz Memorial Lectures 1983e1997. Cambridge
University Press: Econometric Society Monographs, pp. 71e81.
Bold, T., Kimenyi, M., Mwabu, G., Ng’ang’a, A., Sandefur, J., 2013. Scaling up What Works: Experimental
Evidence on External Validity in Kenyan Education. Center for Global Development Working Paper
#321.
Brodeur, A., Le, M., Sangnier, M., Zylberberg, Y., 2016. Star wars: the empirics strike back. Am. Econ. J.
Appl. Econ. 8 (1), 1e32.
Bruhn, M., McKenzie, D., 2009. In pursuit of balance: randomization in practice in development ﬁeld
experiments. Am. Econ. J. Appl. Econ. 1 (4), 200e232.
Bulte, E., Beekman, G., Di Falco, S., Hella, J., Lei, P., 2014. Behavioral responses and the impact of new
agricultural technologies: evidence from a double-blind ﬁeld experiment in Tanzania. Am. J. Agric.
Econ. 96 (3), 813e830.
Casey, K., Glennerster, R., Miguel, E., 2012. Reashaping institutions: evidence on aid impacts using a preanalysis plan. Q. J. Econ. 127 (4), 1755e1812.
Chaloner, K., Verdinelli, I., August 1995. Bayesian experimental design: a review. Stat. Sci. 10 (3), 273e304.
Chassang, S., 2013. Calibrated incentive contracts. Econometrica 81 (5), 1935e1971.
Chassang, S., Snowberg, E., Seymour, B., Bowles, C., 2015. Accounting for behavior in treatment effects:
new applications for blind trials. PLoS One 10 (6), e0127227.
Chassang, S., Padr
o i Miquel, G., Snowberg, E., June 2012. Selective trials: a principal-agent approach to
randomized controlled experiments. Am. Econ. Rev. 102 (4), 1279e1309.
Cohen, J., Dupas, P., 2010. Free distribution or cost-sharing? Evidence from a randomized malaria prevention experiment. Q. J. Econ. 125 (1), 1e45.
Crepon, B., Duﬂo, E., Gurgand, M., Rathelot, R., Zamora, P., 2013. Do labor market policies have
displacement effects? Evidence from a clustered randomized experiment. Q. J. Econ. 128 (2), 531e580.
De Finetti, B., 1937. La Prevision: ses lois Logiques, ses sources subjectives. Ann. l’institut Henri
Poincare 7 (1), 1e68.
Deaton, A., June 2010. Instruments, randomization, and learning about development. J. Econ. Literature
48 (2), 424e455.
Della Vigna, S., Pope, D., 2016a. What Motivates Effort? Evidence and Expert Forecasts. University of
California, mimeo.
Della Vigna, S., Pope, D., 2016b. Run This Treament, Not that: What Experts Know. University of
California, Mimeo.
Duﬂo, E., Glennerster, R., Kremer, M., 2008. Using randomization in development economics research: a
tool kit. In: Paul Schultz, T., Strauss, J. (Eds.), Handbook of Development Economics, vol. 4. Elsevier,
Amsterdam, pp. 3895e3962.
Dupas, P., 2014. Short-run subsidies and long-run adoption of new health products: evidence from a ﬁeld
experiment. Econometrica 82 (1), 197e228.

Decision Theoretic Approaches to Experiment Design and External Validity

Egger, M., Davey Smith, G., Sterne, J.A.C., 2001. Uses and abuses of meta-analysis. Clin. Med. 1 (6),
478e484.
Ellsberg, D., 1961. Risk, ambiguity, and the savage axioms. Q. J. Econ. 75 (4), 643e669.
Fisher, R.A., 1926. The arrangement of ﬁeld experiments. J. Ministry Agric. G. B. 33, 503e513.
Fisher, R.A., 1935. The Design of Experiments. Oliver & Boyd, Edinburgh and London.
Gilboa, I., Schmeidler, D., 1989. Maxmin expected utility with a non-unique prior. J. Math. Econ. 18 (2),
141e153.
Gilboa, I., Postlewaite, A., Schmeidler, D., 2009. Is it always rational to satisfy Savage’s axioms? Econ. Philosophy 25 (3), 285e296.
Gittins, J.C., 1979. Bandit processes and dynamic allocation indices. J. R. Stat. Soc. Ser. B Methodol. 41 (2),
148e177.
Green, D.P., Tusicisny, A., 2012. Statistical Analysis of Results from Laboratory Studies in Experimental
Economics: A Critique of Current Practice. Columbia University, mimeo.
Grossman, S.J., Stiglitz, J.E., June 1980. On the impossibility of informationally efﬁcient markets. Am. Econ.
Rev. 70 (3), 393e408.
Gul, F., Pesendorfer, W., 2001. Temptation and self-control. Econometrica 69 (6), 1403e1435.
Heckman, J.J., Vytlacil, E., May 2005. Structural equations, treatment effects, and econometric policy
evaluation. Econometrica 73 (3), 669e738.
Humphreys, M., de la Sierra, R.S., Van der Windt, P., 2013. Fishing, commitment, and communication: a
proposal for comprehensive nonbinding research registration. Polit. Anal. 21 (1), 1e20.
Jack, B.K., et al., 2013. Private information and the allocation of land use subsidies in Malawi. Am. Econ. J.
Appl. Econ. 5 (3), 113e135.
Karlan, D., Knight, R., Udry, C., 2012. Hoping to Win, Expected to Lose: Theory and Lessons on Micro
Enterprise Development. NBER Working Paper Series # 18325.
Karlan, D.S., Zinman, J., 2009. Observing unobservables: identifying information asymmetries with a consumer credit ﬁeld experiment. Econometrica 77 (6), 1993e2008.
Kasy, M., 2013. Why Experimenters Should Not Randomize, and What They Should Do Instead. Harvard
University, Mimeo.
Klibanoff, P., Marinacci, M., Mukerji, S., 2005. A smooth model of decision making under ambiguity.
Econometrica 73 (6), 1849e1892.
Kreps, D.M., Porteus, E.L., 1978. Temporal resolution of uncertainty and dynamic choice theory. Econometrica 46 (1), 185e200.
Machina, M.J., 1989. Dynamic consistency and non-expected utility models of choice under uncertainty. J.
Econ. Literature 27 (4), 1622e1668.
Maynard, G.D., March 1909. Statistical study of anti-typhoid inoculation. Biometrika 6 (4), 366e375.
Miguel, E., Kremer, M., January 2004. Worms: identifying impacts on education and health in the presence
of treatment externalities. Econometrica 72 (1), 159e217.
Milgrom, P.R., July 1981. Rational expectations, information acquisition, and competitive bidding. Econometrica 89 (4), 921e943.
Morgan, K.L., Rubin, D.B., 2012. Rerandomization to improve covariate balance in experiments. Ann. Statistics 40 (2), 1263e1282.
Muralidharan, K., Sundararaman, V., 2015. The aggregate effects of school choice: evidence from a twostage experiment. Q. J. Econ. 130 (3), 1011e1066.
Olken, B.A., 2015. Promises and perils of pre-analysis plans. J. Econ. Perspect. 29 (3), 61e80.
Olszewski, W., Peski, M., 2011. The principal-agent approach to testing experts. Am. Econ. J. Microeconomics 3 (2), 89e113.
Pearl, J., 2000. Causality: Models, Reasoning, and Inference. Cambridge University Press, New York.
Persico, N., 2000. Information acquisition in auctions. Econometrica 68 (1), 135e148.
Robbins, H., September 1952. Some aspects of the sequential design of experiments. Bull. Am. Math. Soc.
58 (5), 527e535.
Rothschild, M., 1974. A two-armed bandit theory of market pricing. J. Econ. Theory 9 (2), 185e202.

173

174

Handbook of Field Experiments

Rubin, D.B., 1974. Estimating causal effects of treatments in randomized and nonrandomized studies.
J. Educ. Psychol. 66 (5), 688e701.
Savage, L.J., 1954. The Foundations of Statistics. Courier Corporation.
Schmeidler, D., July 1989. Subjective probability and expected utility without additivity. Econometrica 57
(3), 571e587.
Simpson, R.J.S., Pearson, K., 1904. Report on certain enteric fever inoculation statistics. Br. Med. J.
2 (2288), 1243e1246.
Vapnik, V., 1999. The Nature of Statistical Learning Theory, second ed. Springer.

