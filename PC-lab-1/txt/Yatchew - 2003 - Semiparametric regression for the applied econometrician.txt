This page intentionally left blank

Semiparametric Regression for the Applied Econometrician
This book provides an accessible collection of techniques for analyzing nonparametric and semiparametric regression models. Worked examples include estimation
of Engel curves and equivalence scales; scale economies; semiparametric Cobb–
Douglas, translog, and CES cost functions; household gasoline consumption; hedonic housing prices; and, option prices and state price density estimation. The book
should be of interest to a broad range of economists, including those working in
industrial organization, labor, development, urban, energy, and ﬁnancial economics.
A variety of testing procedures are covered such as simple goodness-of-ﬁt tests
and residual regression tests. These procedures can be used to test hypotheses such
as parametric and semiparametric speciﬁcations, signiﬁcance, monotonicity, and
additive separability. Other topics include endogeneity of parametric and nonparametric effects as well as heteroskedasticity and autocorrelation in the residuals.
Bootstrap procedures are provided.
Adonis Yatchew teaches economics at the University of Toronto. His principal areas
of research are theoretical and applied econometrics. In addition, he has a strong
interest in regulatory and energy economics and is Joint Editor of the Energy
Journal. He has received the social science undergraduate teaching award at the
University of Toronto and has taught at the University of Chicago.

i

Further Praise for Semiparametric Regression for the Applied Econometrician
“This ﬂuent book is an excellent source for learning, or updating one’s knowledge of semi- and nonparametric methods and their applications. It is a valuable
addition to the existent books on these topics.”
– Rosa Matzkin, Northwestern University
“Yatchew’s book is an excellent account of semiparametric regression. The
material is nicely integrated by using a simple set of ideas which exploit the
impact of differencing and weighting operations on the data. The empirical
applications are attractive and will be extremely helpful for those encountering
this material for the ﬁrst time.”
– Adrian Pagan, Australian National University
“At the University of Toronto Adonis Yatchew is known for excellence in teaching. The key to this excellence is the succinct transparency of his exposition. At
its best such exposition transcends the medium of presentation (either lecture or
text). This monograph reﬂects the clarity of the author’s thinking on the rapidly
expanding ﬁelds of semiparametric and nonparametric analysis. Both students
and researchers will appreciate the mix of theory and empirical application.”
– Dale Poirier, University of California, Irvine

ii

Themes in Modern Econometrics
Managing editor
peter c.b. phillips, Yale University
Series editors
richard j. smith, University of Warwick
eric ghysels, University of North Carolina, Chapel Hill
Themes in Modern Econometrics is designed to service the large and growing
need for explicit teaching tools in econometrics. It will provide an organized
sequence of textbooks in econometrics aimed squarely at the student population and will be the ﬁrst series in the discipline to have this as its express aim.
Written at a level accessible to students with an introductory course in econometrics behind them, each book will address topics or themes that students and
researchers encounter daily. Although each book will be designed to stand alone
as an authoritative survey in its own right, the distinct emphasis throughout will
be on pedagogic excellence.

Titles in the series
Statistics and Econometric Models: Volumes 1 and 2
christian gourieroux and alain monfort
Translated by quang vuong
Time Series and Dynamic Models
christian gourieroux and alain monfort
Translated and edited by giampiero gallo
Unit Roots, Cointegration, and Structural Change
g.s. maddala and in-moo kim
Generalized Method of Moments Estimation
Edited by lászló mátyás
Nonparametric Econometrics
adrian pagan and aman ullah
Econometrics of Qualitative Dependent Variables
christian gourieroux
Translated by paul b. klassen
The Econometric Analysis of Seasonal Time Series
eric ghysels and denise r. osborn

iii

iv

SEMIPARAMETRIC
REGRESSION FOR THE
APPLIED ECONOMETRICIAN
ADONIS YATCHEW
University of Toronto

v

  
Cambridge, New York, Melbourne, Madrid, Cape Town, Singapore, São Paulo
Cambridge University Press
The Edinburgh Building, Cambridge  , United Kingdom
Published in the United States of America by Cambridge University Press, New York
www.cambridge.org
Information on this title: www.cambridge.org/9780521812832
© Adonis Yatchew 2003
This book is in copyright. Subject to statutory exception and to the provision of
relevant collective licensing agreements, no reproduction of any part may take place
without the written permission of Cambridge University Press.
First published in print format 2003
-
isbn-13 978-0-511-07313-7 eBook (EBL)
-
isbn-10 0-511-07313-5 eBook (EBL)
-
isbn-13 978-0-521-81283-2 hardback
-
isbn-10 0-521-81283-6 hardback
-
isbn-13 978-0-521-01226-3 paperback
-
 paperback
isbn-10 0-521-01226-0
Cambridge University Press has no responsibility for the persistence or accuracy of
s for external or third-party internet websites referred to in this book, and does not
guarantee that any content on such websites is, or will remain, accurate or appropriate.

To Marta, Tamara and Mark.
Your smiles are sunlight,
your laughter, the twinkling of stars.

vii

viii

Contents

List of Figures and Tables
Preface
1 Introduction to Differencing
1.1 A Simple Idea
1.2 Estimation of the Residual Variance
1.3 The Partial Linear Model
1.4 Speciﬁcation Test
1.5 Test of Equality of Regression Functions
1.6 Empirical Application: Scale Economies in Electricity
Distribution
1.7 Why Differencing?
1.8 Empirical Applications
1.9 Notational Conventions
1.10 Exercises
2 Background and Overview
2.1 Categorization of Models
2.2 The Curse of Dimensionality and the Need for Large
Data Sets
2.2.1
Dimension Matters
2.2.2
Restrictions That Mitigate the Curse
2.3 Local Averaging Versus Optimization
2.3.1
Local Averaging
2.3.2
Bias-Variance Trade-Off
2.3.3
Naive Optimization
2.4 A Bird’s-Eye View of Important Theoretical Results
2.4.1
Computability of Estimators
2.4.2
Consistency
2.4.3
Rate of Convergence

page xv
xvii
1
1
2
2
4
4
7
8
11
12
12
15
15
17
17
17
19
19
19
22
23
23
23
23
ix

x

Contents

2.4.4
2.4.5
2.4.6
2.4.7

Bias-Variance Trade-Off
Asymptotic Distributions of Estimators
How Much to Smooth
Testing Procedures

3 Introduction to Smoothing
3.1 A Simple Smoother
3.1.1
The Moving Average Smoother
3.1.2
A Basic Approximation
3.1.3
Consistency and Rate of Convergence
3.1.4
Asymptotic Normality and Conﬁdence Intervals
3.1.5
Smoothing Matrix
3.1.6
Empirical Application: Engel Curve Estimation
3.2 Kernel Smoothers
3.2.1
Estimator
3.2.2
Asymptotic Normality
3.2.3
Comparison to Moving Average Smoother
3.2.4
Conﬁdence Intervals
3.2.5
Uniform Conﬁdence Bands
3.2.6
Empirical Application: Engel Curve Estimation
3.3 Nonparametric Least-Squares and Spline Smoothers
3.3.1
Estimation
3.3.2
Properties
3.3.3
Spline Smoothers
3.4 Local Polynomial Smoothers
3.4.1
Local Linear Regression
3.4.2
Properties
3.4.3
Empirical Application: Engel Curve Estimation
3.5 Selection of Smoothing Parameter
3.5.1
Kernel Estimation
3.5.2
Nonparametric Least Squares
3.5.3
Implementation
3.6 Partial Linear Model
3.6.1
Kernel Estimation
3.6.2
Nonparametric Least Squares
3.6.3
The General Case
3.6.4
Heteroskedasticity
3.6.5
Heteroskedasticity and Autocorrelation
3.7 Derivative Estimation
3.7.1
Point Estimates
3.7.2
Average Derivative Estimation
3.8 Exercises

25
26
26
26
27
27
27
28
29
29
30
30
32
32
34
35
35
36
37
37
37
39
40
40
40
41
42
43
43
44
46
47
47
48
48
50
51
52
52
53
54

Contents

4 Higher-Order Differencing Procedures
4.1 Differencing Matrices
4.1.1
Deﬁnitions
4.1.2
Basic Properties of Differencing and Related
Matrices
4.2 Variance Estimation
4.2.1
The mth-Order Differencing Estimator
4.2.2
Properties
4.2.3
Optimal Differencing Coefﬁcients
4.2.4
Moving Average Differencing Coefﬁcients
4.2.5
Asymptotic Normality
4.3 Speciﬁcation Test
4.3.1
A Simple Statistic
4.3.2
Heteroskedasticity
4.3.3
Empirical Application: Log-Linearity of
Engel Curves
4.4 Test of Equality of Regression Functions
4.4.1
A Simpliﬁed Test Procedure
4.4.2
The Differencing Estimator Applied to the
Pooled Data
4.4.3
Properties
4.4.4
Empirical Application: Testing Equality of Engel
Curves
4.5 Partial Linear Model
4.5.1
Estimator
4.5.2
Heteroskedasticity
4.6 Empirical Applications
4.6.1
Household Gasoline Demand in Canada
4.6.2
Scale Economies in Electricity Distribution
4.6.3
Weather and Electricity Demand
4.7 Partial Parametric Model
4.7.1
Estimator
4.7.2
Empirical Application: CES Cost Function
4.8 Endogenous Parametric Variables in the Partial Linear Model
4.8.1
Instrumental Variables
4.8.2
Hausman Test
4.9 Endogenous Nonparametric Variable
4.9.1
Estimation
4.9.2
Empirical Application: Household Gasoline
Demand and Price Endogeneity
4.10 Alternative Differencing Coefﬁcients
4.11 The Relationship of Differencing to Smoothing

xi

57
57
57
58
58
58
59
60
61
62
63
63
64
65
66
66
67
68
69
71
71
72
73
73
76
81
83
83
84
85
85
86
87
87
88
89
90

xii

Contents

4.12 Combining Differencing and Smoothing
4.12.1 Modular Approach to Analysis of the Partial
Linear Model
4.12.2 Combining Differencing Procedures in Sequence
4.12.3 Combining Differencing and Smoothing
4.12.4 Reprise
4.13 Exercises

92
92
92
93
94
94

5 Nonparametric Functions of Several Variables
5.1 Smoothing
5.1.1
Introduction
5.1.2
Kernel Estimation of Functions of Several Variables
5.1.3
Loess
5.1.4
Nonparametric Least Squares
5.2 Additive Separability
5.2.1
Backﬁtting
5.2.2
Additively Separable Nonparametric Least Squares
5.3 Differencing
5.3.1
Two Dimensions
5.3.2
Higher Dimensions and the Curse of Dimensionality
5.4 Empirical Applications
5.4.1
Hedonic Pricing of Housing Attributes
5.4.2
Household Gasoline Demand in Canada
5.5 Exercises

99
99
99
99
101
101
102
102
103
104
104
105
107
107
107
110

6 Constrained Estimation and Hypothesis Testing
6.1 The Framework
6.2 Goodness-of-Fit Tests
6.2.1
Parametric Goodness-of-Fit Tests
6.2.2
Rapid Convergence under the Null
6.3 Residual Regression Tests
6.3.1
Overview
6.3.2
U-statistic Test – Scalar x’s, Moving
Average Smoother
6.3.3
U-statistic Test – Vector x’s, Kernel Smoother
6.4 Speciﬁcation Tests
6.4.1
Bierens (1990)
6.4.2
Härdle and Mammen (1993)
6.4.3
Hong and White (1995)
6.4.4
Li (1994) and Zheng (1996)
6.5 Signiﬁcance Tests

111
111
113
113
114
115
115
116
117
119
119
120
121
122
124

Contents

6.6

xiii

Monotonicity, Concavity, and Other Restrictions
6.6.1
Isotonic Regression
6.6.2
Why Monotonicity Does Not Enhance the Rate
of Convergence
6.6.3
Kernel-Based Algorithms for Estimating Monotone
Regression Functions
6.6.4
Nonparametric Least Squares Subject to
Monotonicity Constraints
6.6.5
Residual Regression and Goodness-of-Fit Tests
of Restrictions
6.6.6
Empirical Application: Estimation of Option Prices
Conclusions
Exercises

125
125

7 Index Models and Other Semiparametric Speciﬁcations
7.1 Index Models
7.1.1
Introduction
7.1.2
Estimation
7.1.3
Properties
7.1.4
Identiﬁcation
7.1.5
Empirical Application: Engel’s Method for
Estimation of Equivalence Scales
7.1.6
Empirical Application: Engel’s Method for Multiple
Family Types
7.2 Partial Linear Index Models
7.2.1
Introduction
7.2.2
Estimation
7.2.3
Covariance Matrix
7.2.4
Base-Independent Equivalence Scales
7.2.5
Testing Base-Independence and Other Hypotheses
7.3 Exercises

138
138
138
138
139
140

8 Bootstrap Procedures
8.1 Background
8.1.1
Introduction
8.1.2
Location Scale Models
8.1.3
Regression Models
8.1.4
Validity of the Bootstrap
8.1.5
Beneﬁts of the Bootstrap
8.1.6
Limitations of the Bootstrap
8.1.7
Summary of Bootstrap Choices
8.1.8
Further Reading

154
154
154
155
156
157
157
159
159
160

6.7
6.8

126
127
127
128
129
134
136

140
142
144
144
146
147
148
149
151

xiv

Contents

8.2
8.3

8.4

8.5

Bootstrap Conﬁdence Intervals for Kernel Smoothers
Bootstrap Goodness-of-Fit and Residual Regression Tests
8.3.1
Goodness-of-Fit Tests
8.3.2
Residual Regression Tests
Bootstrap Inference in Partial Linear and Index Models
8.4.1
Partial Linear Models
8.4.2
Index Models
Exercises

160
163
163
164
166
166
166
171

Appendixes
Appendix A – Mathematical Preliminaries
Appendix B – Proofs
Appendix C – Optimal Differencing Weights
Appendix D – Nonparametric Least Squares
Appendix E – Variable Deﬁnitions

173
175
183
187
194

References
Index

197
209

List of Figures and Tables

Figure 1.1.
Figure 1.2.

Testing equality of regression functions.
page 6
Partial linear model – log-linear cost function:
Scale economies in electricity distribution.
9
Figure 2.1.
Categorization of regression functions.
16
Figure 2.2.
Naive local averaging.
20
Figure 2.3.
Bias-variance trade-off.
21
Figure 2.4.
Naive nonparametric least squares.
24
Figure 3.1.
Engel curve estimation using moving average smoother.
31
Figure 3.2.
Alternative kernel functions.
33
Figure 3.3.
Engel curve estimation using kernel estimator.
38
Figure 3.4.
Engel curve estimation using kernel, spline, and
lowess estimators.
42
Figure 3.5.
Selection of smoothing parameters.
45
Figure 3.6.
Cross-validation of bandwidth for Engel curve estimation. 46
Figure 4.1.
Testing linearity of Engel curves.
65
Figure 4.2.
Testing equality of Engel curves.
70
Figure 4.3.
Household demand for gasoline.
74
Figure 4.4.
Household demand for gasoline: Monthly effects.
75
Figure 4.5.
Scale economies in electricity distribution.
77
Figure 4.6.
Scale economies in electricity distribution: PUC and
non-PUC analysis.
79
Figure 4.7.
Weather and electricity demand.
82
Figure 5.1.
Hedonic prices of housing attributes.
108
Figure 5.2.
Household gasoline demand in Canada.
109
Figure 6.1.
Constrained and unconstrained estimation and testing.
113
Figure 6.2A. Data and estimated call function.
131
Figure 6.2B. Estimated ﬁrst derivative.
132
Figure 6.2C. Estimated SPDs.
133
Figure 6.3.
Constrained estimation – simulated expected meansquared error.
135
xv

xvi

List of Figures and Tables

Figure 7.1.
Figure 7.2.
Figure 8.1.
Figure 8.2.

Engel’s method for estimating equivalence scales.
Parsimonious version of Engel’s method.
Percentile bootstrap conﬁdence intervals for
Engel curves.
Equivalence scale estimation for singles versus couples:
Asymptotic versus bootstrap methods.

Table 3.1. Asymptotic conﬁdence intervals for kernel
estimators – implementation.
Table 4.1. Optimal differencing weights.
Table 4.2. Values of δ for alternate differencing coefﬁcients.
Table 4.3. Mixed estimation of PUC/non-PUC effects: Scale
economies in electricity distribution.
Table 4.4. Scale economies in electricity distribution: CES
cost function.
Table 4.5. Symmetric optimal differencing weights.
Table 4.6. Relative efﬁciency of alternative differencing sequences.
Table 5.1. The backﬁtting algorithm.
Table 6.1. Bierens (1990) speciﬁcation test – implementation.
Table 6.2. Härdle and Mammen (1993) speciﬁcation
test – implementation.
Table 6.3. Hong and White (1995) speciﬁcation
test – implementation.
Table 6.4. Li (1994), Zheng (1996) residual regression test of
speciﬁcation – implementation.
Table 6.5. Residual regression test of signiﬁcance –
implementation.
Table 7.1. Distribution of family composition.
Table 7.2. Parsimonious model estimates.
Table 8.1. Wild bootstrap.
Table 8.2. Bootstrap conﬁdence intervals at f (xo ).
Table 8.3. Bootstrap goodness-of-ﬁt tests.
Table 8.4. Bootstrap residual regression tests.
Table 8.5. Percentile-t bootstrap conﬁdence intervals for β in the
partial linear model.
Table 8.6. Asymptotic versus bootstrap conﬁdence intervals:
Scale economies in electricity distribution.
Table 8.7. Conﬁdence intervals for δ in the index model:
Percentile method.

141
144
162
170

36
61
62
80
85
90
90
103
120
122
123
123
125
143
145
157
161
164
165
167
168
169

Preface

This book has been largely motivated by pedagogical interests. Nonparametric
and semiparametric regression models are widely studied by theoretical econometricians but are much underused by applied economists. In comparison with
the linear regression model y = zβ + ε, semiparametric techniques are theoretically sophisticated and often require substantial programming experience.
Two natural extensions to the linear model that allow greater ﬂexibility are the
partial linear model y = zβ + f (x) + ε, which adds a nonparametric function,
and the index model y = f (zβ) + ε, which applies a nonparametric function
to the linear index zβ. Together, these models and their variants comprise the
most commonly used semiparametric speciﬁcations in the applied econometrics
literature. A particularly appealing feature for economists is that these models
permit the inclusion of multiple explanatory variables without succumbing to
the “curse of dimensionality.”
We begin by describing the idea of differencing, which provides a simple
way to analyze the partial linear model because it allows one to remove the
nonparametric effect f (x) and to analyze the parametric portion of the model
zβ as if the nonparametric portion were not there to begin with. Thus, one can
draw not only on the reservoir of parametric human capital but one can also
make use of existing software. By the end of the ﬁrst chapter, the reader will
be able to estimate the partial linear model and apply it to a real data set (the
empirical example analyzes scale economies in electricity distribution using a
semiparametric Cobb–Douglas speciﬁcation).
Chapter 2 describes the broad contours of nonparametric and semiparametric
regression modeling, the categorization of models, the “curse of dimensionality,” and basic theoretical results.
Chapters 3 and 4 are devoted to smoothing and differencing, respectively. The
techniques are reinforced by empirical examples on Engel curves, gasoline demand, the effect of weather on electricity demand, and semiparametric translog
and CES cost function models. Methods that incorporate heteroskedasticity,
autocorrelation, and endogeneity of right-hand-side variables are included.
xvii

xviii

Preface

Chapter 5 focuses on nonparametric functions of several variables. The example on hedonic pricing of housing attributes illustrates the beneﬁts of nonparametric modeling of location effects.
Economic theory rarely prescribes a speciﬁc functional form. Typically, the
implications of theory involve constraints such as monotonicity, concavity,
homotheticity, separability, and so on. Chapter 6 begins by outlining two broad
classes of tests of these and other properties: goodness-of-ﬁt tests that compare restricted and unrestricted estimates of the residual variance, and residual
regression tests that regress residuals from a restricted regression on all the
explanatory variables to see whether there is anything left to be explained. Both
of these tests have close relatives in the parametric world. The chapter then
proceeds to constrained estimation, which is illustrated by an option pricing
example.
Chapter 7 addresses the index model with an application to equivalence scale
estimation using South African household survey data. Chapter 8 describes
bootstrap techniques for various procedures described in earlier chapters.
A cornerstone of the pedagogical philosophy underlying this book is that
the second best way to learn econometric techniques is to actually apply them.
(The best way is to teach them.1 ) To this purpose, data and sample programs are
available for the various examples and exercises at www.chass.utoronto.ca/∼
yatchew/. With the exception of constrained estimation of option prices, all
code is in S-Plus.2 The reader should be able to translate the code into other
programs such as Stata easily enough.
By working through the examples and exercises,3 the reader should be able
to

r estimate nonparametric regression, partial linear, and index models;
r test various properties using large sample results and bootstrap techniques;
r estimate nonparametric models subject to constraints such as monotonicity
and concavity.
Well-known references in the nonparametrics and semiparametrics literature
include Härdle (1990), Stoker (1991), Bickel et al. (1993), Horowitz (1998),
1

2
3

Each year I tell my students the apocryphal story of a junior faculty member complaining to a
senior colleague of his inability to get through to his students. After repeating the same lecture
to his class on three different occasions, he exclaims in exasperation “I am so disappointed.
Today I thought I had ﬁnally gotten through to them. This time even I understood the material,
and they still did not understand.”
Krause and Olson (1997) have provided a particularly pleasant introduction to S-Plus. See also
Venables and Ripley (1994).
Many of the examples and portions of the text draw upon previously published work, in particular, Yatchew (1997, 1998, 1999, 2000), Yatchew and Bos (1997), Yatchew and No (2001),
and Yatchew, Sun, and Deri (2001). The permission for use of these materials is gratefully
acknowledged.

Preface

xix

and Pagan and Ullah (1999).4 It is hoped that this book is worthy of being
squeezed onto a nearby bookshelf by providing an applied approach with numerical examples and adaptable code. It is intended for the applied economist
and econometrician working with cross-sectional or possibly panel data.5 It
is expected that the reader has had a good basic course in econometrics and
is thoroughly familiar with estimation and testing of the linear model and associated ideas such as heteroskedasticity and endogeneity. Some knowledge
of nonlinear regression modeling and inference is desirable but not essential.
Given the presence of empirical examples, the book could be used as a text in
an advanced undergraduate course and certainly at the graduate level.
I owe a great intellectual debt to too many to name them individually, and
regrettably not all of them appear in the references. Several anonymous reviewers provided extensive and valuable comments for which I am grateful. Thanks
are also due to Scott Parris at Cambridge University Press for his unﬂagging
efforts in this endeavor. My sister Oenone kindly contributed countless hours
of proofreading time. Finally, it is indeed a special privilege to thank Peter
Phillips, whose intellectual guidance shaped several aspects of this book. It was
Peter who from the start insisted on reproducible empirical exercises. Those
who are acquainted with both of us surely know to whom the errors belong.

4
5

There are also several surveys: Delgado and Robinson (1992), Härdle and Linton (1994), Powell
(1994), Linton (1995a), and Yatchew (1998). See also DiNardo and Tobias (2001).
With the exception of correlation in the residuals, time-dependent data issues have not been
covered here.

xx

1

Introduction to Differencing

1.1 A Simple Idea
Consider the nonparametric regression model
y = f (x) + ε

(1.1.1)

for which little is assumed about the function f except that it is smooth. In its
simplest incarnation, the residuals are independently and identically distributed
with mean zero and constant variance σε2 , and the x’s are generated by a process
that ensures they will eventually be dense in the domain. Closeness of the
x’s combined with smoothness of f provides a basis for estimation of the
regression function. By averaging or smoothing observations on y for which
the corresponding x’s are close to a given point, say xo , one obtains a reasonable
estimate of the regression effect f (xo ).
This premise – that x’s that are close will have corresponding values of the
regression function that are close – may also be used to remove the regression
effect. It is this removal or differencing that provides a simple exploratory tool.
To illustrate the idea we present four applications:
1.
2.
3.
4.
1

Estimation of the residual variance σε2 ,
Estimation and inference in the partial linear model y = zβ + f (x) + ε,
A speciﬁcation test on the regression function f , and
A test of equality of nonparametric regression functions.1

The ﬁrst-order differencing estimator of the residual variance in a nonparametric setting appears in Rice (1984). Although unaware of his result at the time, I presented the identical
estimator at a conference held at the IC2 Institute at the University of Texas at Austin in May
1984. Differencing subsequently appeared in a series of nonparametric and semiparametric settings, including Powell (1987), Yatchew (1988), Hall, Kay, and Titterington (1990), Yatchew
(1997, 1998, 1999, 2000), Lewbel (2000), Fan and Huang (2001), and Horowitz and Spokoiny
(2001).

1

2

Semiparametric Regression for the Applied Econometrician

1.2 Estimation of the Residual Variance
Suppose one has data (y1 , x1 ), . . . , (yn , xn ) on the pure nonparametric regression model (1.1.1), where x is a bounded scalar lying, say, in the unit interval,
ε is i.i.d. with E(ε | x) = 0, Var (ε | x) = σε2 , and all that is known about f is that
its ﬁrst derivative is bounded. Most important, the data have been rearranged
so that x1 ≤ · · · ≤ xn . Consider the following estimator of σε2 :
2
sdiff
=

n
1 
(yi − yi−1 )2 .
2n i=2

(1.2.1)

The estimator is consistent because, as the x’s become close, differencing tends
to remove the nonparametric effect yi − yi−1 = f (xi ) − f (xi−1 ) + εi − εi−1 ∼
=
εi − εi−1 , so that2
2
sdiff

n
n
n

1 
1
2 ∼ 1
2
∼
(εi − εi−1 ) =
ε −
εi εi−1 .
=
2n i=2
n i=1 i
n i=2

(1.2.2)

2
An obvious advantage of sdiff
is that no initial estimate of the regression
function f needs to be calculated. Indeed, no consistent estimate of f is im2
plicit in (1.2.1). Nevertheless, the terms in sdiff
that involve f converge to zero
sufﬁciently quickly so that the asymptotic distribution of the estimator can be
derived directly from the approximation in (1.2.2). In particular,
 D
1  2
n /2 sdiff
− σε2 → N (0, E(ε 4 )).
(1.2.3)

Moreover, derivation of this result is facilitated by the assumption that the εi
are independent so that reordering of the data does not affect the distribution of
the right-hand side in (1.2.2).
1.3 The Partial Linear Model
Consider now the partial linear model y = zβ + f (x) + ε, where for simplicity
all variables are assumed to be scalars. We assume that E(ε | z, x) = 0 and
that Var(ε | z, x) = σε2 .3 As before, the x’s have bounded support, say the unit
interval, and have been rearranged so that x1 ≤ · · · ≤ xn . Suppose that the conditional mean of z is a smooth function of x, say E(z | x) = g(x) where g  is
2

3

To see why this approximation works, suppose that the xi are equally spaced on the unit
interval and that f  ≤ L. By the mean value theorem, for some xi∗ ∈ [xi−1 , xi ] we have
f (xi ) − f (xi−1 ) = f  (xi∗ )(xi − xi−1 ) ≤ L/n. Thus, yi − yi−1 = εi − εi−1 + O(1/n).
For detailed development of the argument, see Exercise 1. If the xi have a density function
bounded away from zero on the support, then xi − xi−1 ∼
= O P (1/n) and yi − yi−1 ∼
=
εi − εi−1 + O P (1/n). See Appendix B, Lemma B.2, for a related result.
For extensions to the heteroskedastic and autocorrelated cases, see Sections 3.6 and 4.5.

Introduction to Differencing

3

bounded and Var(z | x) = σu2 . Then we may rewrite z = g(x) + u. Differencing
yields
yi − yi−1 = (z i − z i−1 )β + ( f (xi ) − f (xi−1 )) + εi − εi−1
= (g(xi ) − g(xi−1 ))β + (u i − u i−1 )β
+ ( f (xi ) − f (xi−1 )) + εi − εi−1
∼
= (u i − u i−1 )β + εi − εi−1 .

(1.3.1)

Thus, the direct effect f (x) of the nonparametric variable x and the indirect
effect g(x) that occurs through z are removed. Suppose we apply the OLS
estimator of β to the differenced data, that is,

(yi − yi−1 )(z i − z i−1 )

β̂ diff =
.
(1.3.2)
(z i − z i−1 )2
Then, substituting the approximations z i − z i−1 ∼
= u i − u i−1 and yi − yi−1 ∼
=
(u i − u i−1 )β + εi − εi−1 into (1.3.2) and rearranging, we have

n 1/2 n1 (εi − εi−1 )(u i − u i−1 )
1/2
∼
n (β̂ diff − β) =
.
(1.3.3)
1 
(u i − u i−1 )2
n
The denominator converges to 2 σu2 , and the numerator has mean zero and
variance 6 σε2 σu2 . Thus, the ratio has mean zero and variance 6 σε2 σu2 /(2σu2 )2 =
1.5 σε2 /σu2 . Furthermore, the ratio may be shown to be approximately normal
(using a ﬁnitely dependent central limit theorem). Thus, we have


1.5 σε2
D
1
n /2 (β̂ diff − β) → N 0,
.
(1.3.4)
σu2
For the most efﬁcient estimator, the corresponding variance in (1.3.4) would be
σε2 /σu2 so the proposed estimator based on ﬁrst differences has relative efﬁciency
2/ = 1/1.5. In Chapters 3 and 4 we will produce efﬁcient estimators.
3
Now, in order to use (1.3.4) to perform inference, we will need consistent
estimators of σε2 and σu2 . These may be obtained using
sε2 =

n
1 
((yi − yi−1 ) − (z i − z i−1 )β̂ diff )2
2n i=2

n
1 
P
∼
(εi − εi−1 )2 → σε2
=
2n i=2

(1.3.5)

and
su2 =

n
n
1 
1 
P
(z i − z i−1 )2 ∼
(u i − u i−1 )2 → σu2 .
=
2n i=2
2n i=2

(1.3.6)

4

Semiparametric Regression for the Applied Econometrician

The preceding procedure generalizes straightforwardly to models with multiple
parametric explanatory variables.
1.4 Speciﬁcation Test
Suppose, for example, one wants to test the null hypothesis that f is a linear
2
function. Let sres
be the usual estimate of the residual variance obtained from
2
a linear regression of y on x. If the linear model is correct, then sres
will be
approximately equal to the average of the true squared residuals:
2
sres
=

n
n
1 2
1
(yi − γ̂1 − γ̂2 xi )2 ∼
ε .
=
n i=1
n i=1 i

(1.4.1)

2
If the linear speciﬁcation is incorrect, then sres
will overestimate the residual
2
in (1.2.1) will remain a consistent estimator, thus forming
variance while sdiff
the basis of a test. Consider the test statistic
 2

2
n 1/2 sres
− sdiff
V =
.
(1.4.2)
2
sdiff

Equations (1.2.2) and (1.4.1) imply that the numerator of V is approximately
equal to



D
1 1
n /2
(1.4.3)
εi εi−1 → N 0, σε4 .
n
2
Since sdiff
, the denominator of V , is a consistent estimator of σε2 , V is asymptotically N (0,1) under H0 . (Note that this is a one-sided test, and one rejects for
large values of the statistic.)
As we will see later, this test procedure may be used to test a variety of
null hypotheses such as general parametric and semiparametric speciﬁcations,
monotonicity, concavity, additive separability, and other constraints. One simply
inserts the restricted estimator of the variance in (1.4.2). We refer to test statistics
that compare restricted and unrestricted estimates of the residual variance as
“goodness-of-ﬁt” tests.

1.5 Test of Equality of Regression Functions
Suppose we are given data (y A1 , x A1 ), . . . , (y An , x An ) and (y B1 , x B1 ), . . . ,
(y Bn , x Bn ) from two possibly different regression models A and B. Assume
x is a scalar and that each data set has been reordered so that the x’s are in
increasing order. The basic models are
y Ai = f A (x Ai ) + ε Ai
y Bi = f B (x Bi ) + ε Bi

(1.5.1)

Introduction to Differencing

5

where given the x’s, the ε’s have mean 0, variance σε2 , and are independent
within and between populations; f A and f B have ﬁrst derivatives bounded.
Using (1.2.1), deﬁne consistent “within” differencing estimators of the variance
s 2A =
s B2

n
1 
(y Ai − y Ai−1 )2
2n i

n
1 
=
(y Bi − y Bi−1 )2 .
2n i

(1.5.2)

As we will do frequently, we have dropped the subscript “diff ”. Now pool
all the data and reorder so that the pooled x’s are in increasing order:
∗
∗
(y1∗ , x1∗ ), . . . . . . , (y2n
, x2n
). (Note that the pooled data have only one subscript.)
Applying the differencing estimator once again, we have
s 2p =

2n
2
1  ∗
y j − y ∗j−1 .
4n j

(1.5.3)

The basic idea behind the test procedure is to compare the pooled estimator
with the average of the within estimators. If f A = f B , then the within and
pooled estimators are consistent and should yield similar estimates. If f A = f B ,
then the within estimators remain consistent, whereas the pooled estimator
overestimates the residual variance, as may be seen in Figure 1.1.
To formalize this idea, deﬁne the test statistic


1 
ϒ ≡ (2n) /2 s 2p − 1/2 s 2A + s B2 .
(1.5.4)
If f A = f B , then differencing removes the regression effect sufﬁciently
quickly in both the within and the pooled estimators so that


1 
ϒ ≡ (2n) /2 s 2p − 1/2 s 2A + s B2

 2n
n
n

2 
(2n)1/2   ∗
∗
2
2
∼
ε j − ε j−1 −
(ε Ai − ε Ai−1 ) −
(ε Bi − ε Bi−1 )
=
4n
j
i
i

 2n
n
n


(2n)1/2  ∗2
∗ ∗
2
2
∼
ε j − ε j ε j−1 −
ε Ai − ε Ai ε Ai−1 −
ε Bi − ε Bi ε Bi−1
=
2n
j
i
i
 n

 2n

n



1
1
∗
∗
∼
ε Ai ε Ai−1 +
ε Bi ε Bi−1 −
ε j ε j−1 .
=
(2n)1/2
(2n)1/2
i
i
j
(1.5.5)
Consider the two terms in the last line. In large samples, each is approximately N (0, σε4 ). If observations that are consecutive in the individual data

6

Semiparametric Regression for the Applied Econometrician

Within estimators of residual variance

B

A

Pooled estimator of residual variance

B

A

Figure 1.1. Testing equality of regression functions.

Introduction to Differencing

7

sets tend to be consecutive after pooling and reordering, then the covariance
between the two terms will be large. In particular, the covariance is approximately σε4 (1 − π ), where π equals the probability that consecutive observations
in the pooled reordered data set come from different populations.
It follows that under Ho : f A = f B ,


D
ϒ → N 0, 2π σε4 .
(1.5.6)
For example, if reordering the pooled data is equivalent to stacking data sets
A and B – because the two sets of x’s, x A and x B , do not intersect – then π ∼
=0
and indeed the statistic ϒ becomes degenerate. This is not surprising, since
observing nonparametric functions over different domains cannot provide a
basis for testing whether they are the same. If the pooled data involve a simple
interleaving of data sets A and B, then π ∼
= 1 and ϒ → N (0, 2σε4 ). If x A and
x B are independent of each other but have the same distribution, then for the
pooled reordered data the probability that consecutive observations come from
different populations is 1/2 and ϒ → N (0, σε4 ).4 To implement the test, one may
obtain a consistent estimate π̂ by taking the proportion of observations in the
pooled reordered data that are preceded by an observation from a different
population.
1.6 Empirical Application: Scale Economies in Electricity Distribution5
To illustrate these ideas, consider a simple variant of the Cobb–Douglas model
for the costs of distributing electricity
tc = f (cust) + β1 wage + β2 pcap
+ β3 PUC + β4 kwh + β5 life + β6 lf + β7 kmwire + ε

(1.6.1)

where tc is the log of total cost per customer, cust is the log of the number of
customers, wage is the log wage rate, pcap is the log price of capital, PUC is a
dummy variable for public utility commissions that deliver additional services
and therefore may beneﬁt from economies of scope, life is the log of the remaining life of distribution assets, lf is the log of the load factor (this measures
capacity utilization relative to peak usage), and kmwire is the log of kilometers
of distribution wire per customer. The data consist of 81 municipal distributors
in Ontario, Canada, during 1993. (For more details, see Yatchew, 2000.)
4

5

For example, distribute n men and n women randomly along a stretch of beach facing the sunset.
Then, for any individual, the probability that the person to the left is of the opposite sex is 1/2.
More generally, if x A and x B are independent of each other and have different distributions,
then π depends on the relative density of observations from each of the two populations.
Variable deﬁnitions for empirical examples are contained in Appendix E.

8

Semiparametric Regression for the Applied Econometrician

Because the data have been reordered so that the nonparametric variable cust
is in increasing order, ﬁrst differencing
√ (1.6.1) tends to remove the nonparametric effect f . We also divide by 2 so that the residuals in the differenced
Equation (1.6.2) have the same variance as those in (1.6.1). Thus, we have
√
[tci − tci−1 ]/ 2
√
√
∼ β1 [wage − wage ]/ 2 + β2 [pcap − pcap ]/ 2
=
i
i−1
i
i−1
√
√
+ β3 [PUCi − PUCi−1 ]/ 2 + β4 [kwhi − kwhi−1 ]/ 2
√
√
+ β5 [lifei − lifei−1 ]/ 2 + β6 [lfi − lfi−1 ]/ 2
√
√
+ β7 [kmwirei − kmwirei−1 ]/ 2 + [εi − εi−1 ]/ 2.
(1.6.2)
Figure 1.2 summarizes our estimates of the parametric effects β using the
differenced equation. It also contains estimates of a pure parametric speciﬁcation in which the scale effect f is modeled with a quadratic. Applying the
2
speciﬁcation test (1.4.2), where sdiff
is replaced with (1.3.5), yields a value of
1.50, indicating that the quadratic model may be adequate.
Thus far our results suggest that by differencing we can perform inference on
β as if there were no nonparametric component f in the model to begin with.
But, having estimated β, we can then proceed to apply a variety of nonparametric
techniques to analyze f as if β were known. Such a modular approach simpliﬁes
implementation because it permits the use of existing software designed for pure
nonparametric models.
More precisely, suppose we assemble the ordered pairs (yi −z i β̂ diff , xi ); then,
we have
yi − z i β̂ diff = z i (β − β̂ diff ) + f (xi ) + εi ∼
= f (xi ) + εi .

(1.6.3)

If we apply conventional smoothing methods to these ordered pairs such
as kernel estimation (see Section 3.2), then consistency, optimal rate of convergence results, and the construction of conﬁdence intervals for f remain
valid because β̂ diff converges sufﬁciently quickly to β that the approximation
in the last part of (1.6.3) leaves asymptotic arguments unaffected. (This is indeed why we could apply the speciﬁcation test after removing the estimated
parametric effect.) Thus, in Figure 1.2 we have also plotted a nonparametric
(kernel) estimate of f that can be compared with the quadratic estimate. In subsequent sections, we will elaborate this example further and provide additional
ones.
1.7 Why Differencing?
An important advantage of differencing procedures is their simplicity. Consider once again the partial linear model y = zβ + f (x) + ε. Conventional

Introduction to Differencing

Variable

9

Partial linear modela

Quadratic model

cust
cust2

Coef
−0.833
0.040

SE
0.175
0.009

Coef
–
–

wage
pcap

0.833
0.562

0.325
0.075

0.448
0.459

0.367
0.076

−0.071
−0.017
−0.603
1.244
0.445

0.039
0.089
0.119
0.434
0.086

−0.086
−0.011
−0.506
1.252
0.352

0.043
0.087
0.131
0.457
0.094

PUC
kwh
life
lf
kmwire
sε2
R2

SE
–
–

.021
.618

.018
.675

6.0

Estimated scale effect

o

o

5.6
5.4

o

o
o

o

Kernel
Quadratic

o
oo
o ooo o o
o
o
o
oo
oo
oo
o oo

o

o o
oo o o
o

o

o

oo o
o

o

5.2

log total cost per year

5.8

o
o

o

oo

o

o

o

o
o
o

o
o

oo o o
o

o

o

o

o

o

o

oo

oo o

o

o

o

o

o
o

oo

5.0

o

6

8

10

12

log customers
2 − s 2 )/
Test of quadratic versus nonparametric speciﬁcation of scale effect: V = n 1/2 (sres
diff
2 = 811/2 (.021 − .018)/.018 = 1.5, where V is N (0,1), Section 1.4.
sdiff

a

Figure 1.2. Partial linear model – Log-linear cost function: Scale economies in electricity distribution.

10

Semiparametric Regression for the Applied Econometrician

estimators, such as the one proposed by Robinson (1988) (see Section 3.6),
require one to estimate E(y | x) and E(z | x) using nonparametric regressions.
The estimated residuals from each of these regressions (hence the term “double
residual method”) are then used to estimate the parametric regression
y − E(y | x) = (z − E(z | x))β + ε.

(1.7.1)

If z is a vector, then a separate nonparametric regression is run for each component of z, where the independent variable is the nonparametric variable x. In
contrast, differencing eliminates these ﬁrst-stage regressions so that estimation
of β can be performed – regardless of its dimension – even if nonparametric
regression procedures are not available within the software being used. Similarly, tests of parametric speciﬁcations against nonparametric alternatives and
tests of equality of regression functions across two or more (sub-) samples can
be carried out without performing a nonparametric regression.
As should be evident from the empirical example of the last section, differencing may easily be combined with other procedures. In that example,
we used differencing to estimate the parametric component of a partial linear
model. We then removed the estimated parametric effect and applied conventional nonparametric procedures to analyze the nonparametric component. Such
modular analysis does require theoretical justiﬁcation, which we will provide
in Section 4.12.
As we have seen, the partial linear model permits a simple semiparametric
generalization of the Cobb–Douglas model. Translog and other linear-inparameters models may be generalized similarly. If we allow the parametric portion of the model to be nonlinear – so that we have a partial parametric model –
then we may also obtain simple semiparametric generalizations of models such
as the constant elasticity of substitution (CES) cost function. These, too, may
be estimated straightforwardly using differencing (see Section 4.7). The key
requirement is that the parametric and nonparametric portions of the model be
additively separable.
Other procedures commonly used by the econometrician may be imported
into the differencing setting with relative ease. If some of the parametric variables are potentially correlated with the residuals, instrumental variable techniques can be applied, with suitable modiﬁcation, as can the Hausman endogeneity test (see Section 4.8). If the residuals are potentially not homoskedastic,
then well-known techniques such as White’s heteroskedasticity-consistent standard errors can be adapted (see Section 4.5). The reader will no doubt ﬁnd other
procedures that can be readily transplanted.
Earlier we have pointed out that the ﬁrst-order differencing estimator of β
in the partial linear model is inefﬁcient when compared with the most efﬁcient
estimator (see Section 1.3). The same is true for the ﬁrst-order differencing estimator of the residual variance (see Section 1.2). This problem can be corrected
using higher-order differencing, as demonstrated in Chapter 4.

Introduction to Differencing

11

Most important, however, the simplicity of differencing provides a useful
pedagogical device. Applied econometricians can begin using nonparametric
techniques quickly and with conventional econometric software. Indeed, all
the procedures in the example of Section 1.6 can be executed within packages
such as E-Views, SAS, Shazam, Stata, or TSP. Furthermore, because the partial
linear model can easily accommodate multiple parametric variables, one can
immediately apply these techniques to data that are of practical interest.
Simplicity and versatility, however, have a price. One of the criticisms of
differencing is that it can result in greater bias in moderately sized samples than
other estimators.6 A second criticism is that differencing, as proposed here,
works only if the dimension of the model’s nonparametric component does not
exceed 3 (see Section 5.3). Indeed, in most of what follows we will apply differencing to models in which the nonparametric variable is a scalar. More general
techniques based on smoothing will usually be prescribed when the nonparametric variable is a vector. However, we would argue that, even if differencing techniques were limited to one (nonparametric) dimension, they have the potential
of signiﬁcant “market share.” The reason is that high-dimensional nonparametric regression models, unless they rely on additional structure (such as additive
separability), suffer from the “curse of dimensionality” which severely limits
one’s ability to estimate the regression relationship with any degree of precision.
It is not surprising, therefore, that the majority of applied papers using nonparametric regression limit the nonparametric component to one or two dimensions.
1.8 Empirical Applications
The target audience for this book consists of applied econometricians and
economists. Thus, the following empirical applications will be introduced and
carried through various chapters:

r
r
r
r
r
r
r

Engel curve estimation (South African data)
Scale economies in electricity distribution (data from Ontario, Canada)
Household gasoline consumption (Canadian data)
Housing prices (data from Ottawa, Canada)
Option prices and state price densities (simulated data)
Weather and electricity demand (data from Ontario, Canada)
Equivalence scale estimation (South African data).

Empirical results presented in tables and in ﬁgures are worked through in
exercises at the end of each chapter along with additional empirical and theoretical problems. The reader is especially urged to do the applied exercises for
6

Seifert, Gasser, and Wolf (1993) have studied this issue for differencing estimators of the
residual variance.

12

Semiparametric Regression for the Applied Econometrician

this is by the far the best way to gain a proper understanding of the techniques,
their range, and limitations.
For convenience, variable deﬁnitions are collected in Appendix E. Other data
sets may be obtained easily. For example, household survey data for various
developing countries are available at the World Bank Web site www.worldbank.
org/lsms. These are the Living Standard Measurement Study household surveys
from which our South African data were extracted.
1.9 Notational Conventions
With mild abuse of notation, symbols such as y and x will be used to denote both
the variable in question and the corresponding column vector of observations
on the variable. The context should make it clear which applies. If x is a vector,
then f (x) will denote the vector consisting of f evaluated at the components
of x. If X is a matrix and δ is a conformable parameter vector, then f (X δ) is
also a vector.
We will frequently use subscripts to denote components of vectors or matrices, for example, βi , Aij or [AB]ij . For any two matrices A, B of identical
dimension, we will on a few occasions use the notation [A  B]ij = Aij Bij .
When differencing procedures are applied, the ﬁrst few observations may be
treated differently or lost. For example,
to calculate the differencing estimator

2
of the residual variance sdiff
= in (yi − yi−1 )2 /2n, we begin the summation at
i = 2. For the mathematical arguments that follow, such effects are negligible.
.
Thus, we will use the symbol = to denote “equal except for end effects.” As
must be evident by now, we will also use the symbol ∼
= to denote approximate
P
D
equality, → for convergence in probability, and → for convergence in distribution. The abbreviation “i.i.d.” will denote “independently and identically
distributed.”
Because differencing will be one of the themes in what follows, several
estimators will merit the subscript “diff ”, as in the preceding paragraph or in
(1.3.2). For simplicity, we will regularly suppress this annotation.
To denote low-order derivatives we will the use the conventional notation
f  , f  , f  . Occasionally we will need higher-order derivatives which we will
denote by bracketed superscripts; for example, f (m) .
1.10 Exercises7
1. Suppose y = f (x) + ε, | f  | ≤ L for which we have data (yi , xi ) i = 1, . . . , n,
where the xi are equally spaced on the unit interval. We will derive the distribution
7

Data and sample programs for empirical exercises are available on the Web. See the Preface
for details.

Introduction to Differencing
2
of sdiff
=

n
i

13

(yi − yi−1 )2 /2n, which we may rewrite as

n
n
1 
1 
(εi − εi−1 )2 +
( f (xi ) − f (xi−1 ))2
2n
2n

2
=
sdiff

i=2
n

+

i=2

1
n

( f (xi ) − f (xi−1 ))(εi − εi−1 ).

i=2

(a) Show that the ﬁrst term on the right-hand side satisﬁes



1
n /2

n
1 
(εi − εi−1 )2 − σε2
2n



D

→ N (0, E(ε4 )).

i=2

(b) Show that the second term is O( n12 ).
1
(c) Show that the variance of the third term is O( n13 ) so that the third term is O P ( n 3/2
).
Thus,
1
n /2




2

2
sdiff
− σε



=

n
1 
(εi − εi−1 )2 − σε2
2n
i=2



+O

1
n 3/2

+ OP

1
.
n

2. Consider the restricted estimator of the residual variance (1.4.1) used in the differencing speciﬁcation test. Show that
2
sres
=

n
n
1
1
(yi − γ̂1 − γ̂2 xi )2 =
(εi + (γ1 − γ̂1 ) − (γ2 − γ̂2 xi ))2
n
n
i=1

=

n
1

n

i=1

i=1

εi2 + O P

1
.
n

Combine this with (1.2.2) and the results of the previous exercise to derive the
distribution of V in Section 1.4.
3. Derive the covariance between the two terms in the last line of (1.5.5). Use this to
obtain the approximate distribution of the differencing test of equality of regression
functions (1.5.6). How would the test statistic change if the two subpopulations were
of unequal size?
4. Scale Economies in Electricity Distribution
(a) Verify that the data have been reordered so that the nonparametric variable cust,
which is the log of the number of customers, is in increasing order.
(b) Fit the quadratic model in Figure 1.2, where all variables are parametric. Estimate
the residual variance, the variance of the dependent variable tc, and calculate
R 2 = 1 − sε2 /stc2 .
(c) Transform the data by ﬁrst differencing as in (1.6.2) and apply ordinary leastsquares to obtain estimates of the parametric effects in the partial linear model.
To obtain the √
standard errors, rescale the standard errors provided by the OLS
procedure by 1.5, as indicated in (1.3.4).

14

Semiparametric Regression for the Applied Econometrician
(d) Remove the estimated parametric effects using (1.6.3) and produce a scatterplot
of the ordered pairs (yi −z i β̂ diff , xi ), where the x variable is the log of the number
of customers.
(e) Apply a smoothing or nonparametric regression procedure (such as ksmooth in
S-Plus) to the ordered pairs in (d) to produce a nonparametric estimate of the
scale effect.
(f) Apply the speciﬁcation test in (1.4.2) to the ordered pairs in (d) to test the
quadratic speciﬁcation against the nonparametric alternative.

2

Background and Overview

2.1 Categorization of Models
We now turn to a description of the range of models addressed in this book.
Consider ﬁrst the pure nonparametric model y = f (x) + ε, where ε is i.i.d.
with mean 0 and constant variance σε2 . If f is only known to lie in a family of
smooth functions , then the model is nonparametric and incorporates weak
constraints on its structure. We will soon see that such models are actually
difﬁcult to estimate with precision if x is a vector of dimension exceeding
two or three. If f satisﬁes some additional properties (such as monotonicity,
concavity, homogeneity, or symmetry) and hence lies in ¯ ⊂ , we will say
that the model is constrained nonparametric. Figure 2.1 depicts a parametric
and a pure nonparametric model at opposite corners.
Given the difﬁculty in estimating pure nonparametric models with multiple explanatory variables, researchers have sought parsimonious hybrids. One
such example is the partial linear model introduced in Chapter 1. One can see
in Figure 2.1 that for any ﬁxed value of x, the function is linear in z. Partial parametric models are an obvious generalization, where y = g(z; β) +
f (x) + ε and g is a known function. For the partial parametric surface in
Figure 2.1, g is quadratic in z – a shape that is replicated for any ﬁxed value
of x.
Index models constitute another hybrid. In this case y = f (xβ) + ε. For any
ﬁxed value of the index xβ, the function f (xβ) is constant. The index model
depicted in Figure 2.1 is given by f (xβ) = cos(x1 + x2 ); thus, the function is
ﬂat along lines where x1 + x2 = constant. Partial linear index models are yet
another generalization, where y = f (xβ) + zδ + ε.
Finally, if we can partition x into two subsets xa and xb such that f is of the
form f a (xa ) + f b (xb ), where f a and f b are both nonparametric, then the model
is called additively separable. (Of course, partial linear and partial parametric models are also additively separable, but in these cases one component is
parametric and the other nonparametric.)
15

16

Semiparametric Regression for the Applied Econometrician

 is a family of smooth functions. ¯ is a smooth family with additional constraints such as monotonicity, concavity, symmetry, or other constraints.

Figure 2.1. Categorization of regression functions.

Background and Overview

17

2.2 The Curse of Dimensionality and the Need for Large Data Sets
2.2.1 Dimension Matters
In comparison with parametric estimation, nonparametric procedures can impose enormous data requirements. To gain an appreciation of the problem as
well as remedies for it, we begin with a deterministic framework. Suppose the
objective is to approximate a function f. If it is known to be linear in one variable, two observations are sufﬁcient to determine the entire function precisely;
three are sufﬁcient if f is linear in two variables. If f is of the form g(x; β),
where g is known and β is an unknown k-dimensional vector, then k judiciously
selected points are usually sufﬁcient to solve for β. No further observations on
the function are necessary.
Let us turn to the pure nonparametric case. Suppose f, deﬁned on the unit interval, is known only to have a ﬁrst derivative bounded by L (i.e., supx∈[0,1] | f  | ≤
L). If we sample f at n equidistant points and approximate f at any point by
the closest point at which we have an evaluation, then our approximation error
cannot exceed 1/2 L/n. Increasing the density of points reduces approximation
error at a rate O(1/n).
Now suppose f is a function on the unit square and that it has derivatives
bounded in all directions by L. To approximate the function, we need to sample
throughout its domain. If we distribute n points uniformly on the unit square,
each will “occupy” an area 1/n, and the typical distance between points will be
1/n 1/2 so that the approximation error is now O(1/n 1/2 ). If we repeat this argument for functions of k variables, the typical distance between points becomes
1/n 1/k and the approximation error is O(1/n 1/k ). In general, this method of approximation yields errors proportional to the distance to the nearest observation.
Thus, for n = 100, the potential approximation error is 10 times larger in 2
dimensions than in 1 and 40 times larger in 5 dimensions. One begins to see
the virtues of parametric modeling to avoid this curse of dimensionality.1
2.2.2 Restrictions That Mitigate the Curse
We will consider four types of restrictions that substantially reduce approximation error: a partial linear structure, the index model speciﬁcation, additive
separability, and smoothness assumptions.
Suppose a regression function deﬁned on the unit square has the partial linear
form zβ + f (x) (the function f is unknown except for a derivative bound). In this
case, we need two evaluations along the z-axis to completely determine β (see
the partial linear surface in Figure 2.1). Furthermore, n equidistant evaluations
1

For an exposition of the curse of dimensionality in the case of density estimation, see Silverman
(1986) and Scott (1992).

18

Semiparametric Regression for the Applied Econometrician

along the x-axis will ensure that f can be approximated with error O(1/n)
so that the approximation error for the regression function as a whole is also
O(1/n), the same as if it were a nonparametric function of one variable.
Now consider the index model. If β were known, then we would have a
nonparametric function of one variable; thus, to obtain a good approximation
of f , we need to take n distinct and say equidistant values of the index xβ.
How do we obtain β? Suppose for simplicity that the model is f (x1 + x2 β).
(The coefﬁcient of the ﬁrst variable has been normalized to 1.) Beginning at a
point (x1a , x2a ), travel in a direction along which f is constant to a nearby point
(x1b , x2b ). Because f (x1a + x2a β) = f (x1b + x2b β) and hence x1a + x2a β =
x1b + x2b β, we may solve for β. Thus, just as for the partial linear model, the
approximation error for the regression function as a whole is O(1/n), the same
as if it were a nonparametric function of one variable.
Next, consider an additively separable function on the unit square: f (xa , xb )
= f a (xa ) + f b (xb ), where the functions f a and f b satisfy a derivative bound
( f b (0) = 0 is imposed as an identiﬁcation condition). If we take 2n observations,
n along each axis, then f a and f b can be approximated with error O(1/n), so
approximation error for f is also O(1/n), once again the same as if f were a
nonparametric function of one variable.
The following proposition should now be plausible: For partially linear,
index, or additively separable models, the approximation error depends on the
maximum dimension of the nonparametric components of the model.
Smoothness can also reduce approximation error. Suppose f is twice differentiable on the unit interval with f  and f  bounded by L and we evaluate f at
n equidistant values of x. Consider approximation of f at xo ∈ [xi ,xi+1 ]. Using
a Taylor expansion, we have
f (xo ) = f (xi ) + f  (xi )(xo − xi )
+ 1/2 f  (x ∗ )(xo − xi )2

x ∗ ∈ [xi , xo ].

(2.2.1)

If we approximate f (xo ) using f (xi ) + f  (xi )(xo − xi ), the error is
O(xo − xi )2 = O(1/n 2 ). Of course we do not observe f  (xi ). However,
the bound on the second derivative implies that f  (xi ) − [ f (xi+1 ) − f (xi )]/
[xi+1 − xi ] is O(1/n) and thus
 
[ f (xi+1 ) − f (xi )]
1
(2.2.2)
f (xo ) = f (xi ) +
(xo − xi ) + O 2 .
[xi+1 − xi ]
n
This local linear approximation involves nothing more than joining the observed points with straight lines. If third-order (kth order) derivatives are bounded, then local quadratic (k − 1 order polynomial) approximations will reduce
the error further.
In this section, we have used the elementary idea that if a function is
smooth, its value at a given point can be approximated reasonably well by using

Background and Overview

19

evaluations of the function at neighboring points. This idea is fundamental to
nonparametric estimation where, of course, f is combined with noise to yield the
observed data. All results illustrated in this section have analogues in the nonparametric setting. Data requirements grow very rapidly as the dimension of the
nonparametric component increases. The rate of convergence (i.e., the rate at
which we learn about the unknown regression function) can be improved using
semiparametric structure, additive separability, and smoothness assumptions.
Finally, the curse of dimensionality underscores the paramount importance of
procedures that validate models with faster rates of convergence. Among these
are speciﬁcation tests of a parametric null against a nonparametric alternative
and signiﬁcance tests that can reduce the number of explanatory variables in
the model.
2.3 Local Averaging Versus Optimization
2.3.1 Local Averaging
In Chapter 1 we introduced the idea of differencing, a device that allowed us
to remove the nonparametric effect. Suppose the object of interest is now the
nonparametric function itself. A convenient way of estimating the function at
a given point is by averaging or smoothing neighboring observations. Suppose
we are given data (y1 , x1 ) . . . (yn , xn ) on the model y = f (x) + ε, where x is
a scalar. Local averaging estimators are extensions of conventional estimators
of location to a nonparametric regression setting. If one divides the scatterplot
into neighborhoods, then one can compute local means as approximations to the
regression function. A more appealing alternative is to have the “neighborhood”
move along the x-axis and to compute a moving average along the way. The
wider the neighborhood, the smoother the estimate, as may be seen in Figure 2.2.
(If one were in a vessel, the “sea” represented by the solid line in the bottom
panel would be the most placid.)
2.3.2 Bias-Variance Trade-Off
Suppose then we deﬁne the estimator to be
1 
fˆ(xo ) =
yi
n o N (x )
o

= f (xo ) +

1 
1 
( f (xi ) − f (xo )) +
εi
n o N (x )
n o N (x )
o

(2.3.1)

o

where summations are taken over observations in the neighborhood N (xo )
around xo , and n o is the number of elements in N (xo ). Conditional on the
x’s, the bias of the estimator consists of the second term, and the variance is
determined by the third term.

20

Semiparametric Regression for the Applied Econometrician

Data-generating mechanism yi = xi cos(4π xi ) + εi εi ∼ N (0, .09)
Observations are averaged over neighborhoods of the indicated width.

Figure 2.2. Naive local averaging.

xi ∈ [0, 1],

n = 100.

Background and Overview

21

The mean-squared error (i.e., the bias squared plus the variance) is given by

2
1 
σ2
2
ˆ
E[ f (xo ) − f (xo )] =
f (xi ) − f (xo ) + ε .
(2.3.2)
n o N (x )
no
o

Mean-squared error can be minimized by widening the neighborhood N (xo )
until the increase in bias squared is offset by the reduction in variance. (The
latter declines because n o increases as the neighborhood widens.) This tradeoff between bias and variance is illustrated in Figure 2.3, which continues the

Data-generating mechanism yi = xi cos(4π xi ) + εi

Figure 2.3. Bias-variance trade-off.

εi ∼ N (0, .09)

xi ∈ [0, 1],

n = 100.

22

Semiparametric Regression for the Applied Econometrician

example of Figure 2.2. In the ﬁrst panel, local averaging is taking place using
just 10 percent of the data at each point (of course, fewer observations are used
as one approaches the boundaries of the domain). The solid line is E[ fˆ(x)]
and the estimator exhibits little bias; it coincides almost perfectly with the true
regression function (the dotted line). The broken lines on either side correspond
to two times the standard errors of the estimator at each point: 2(Var[ fˆ(x)])1/2 .
In the second panel the neighborhood is substantially broader; we are now
averaging about 30 percent of the data at each point. The standard error curves
are tighter, but some bias has been introduced. The E[ fˆ(x)] no longer coincides
perfectly with the true regression curve. In the third panel, averaging is taking
place over 80 percent of the data. The standard error curves are even tighter,
but now there is substantial bias particularly at the peaks and valleys of the
true regression function. The expectation of the estimator E[ fˆ(x)] is fairly ﬂat,
while the true regression function undulates around it.
A more general formulation of local averaging estimators modiﬁes (2.3.1)
as follows:
n

fˆ(xo ) =
wi (xo )yi .
(2.3.3)
1

The estimate of the regression function at xo is a weighted sum of the yi ,
where the weights wi (xo ) depend on xo . (Various local averaging estimators can
be put in this form, including kernel and nearest-neighbor.) Because one would
expect that observations close to xo would have conditional means similar to
f (xo ), it is natural to assign higher weights to these observations and lower
weights to those that are farther away. Local averaging estimators have the
advantage that, as long as the weights are known or can be easily calculated, fˆ
is also easy to calculate. The disadvantage of such estimators is that it is often
difﬁcult to impose additional structure on the estimating function fˆ.
2.3.3 Naive Optimization
Optimization estimators, on the other hand, are more amenable to incorporating
additional structure. As a prelude to our later discussion, consider the following
naive estimator. Given data (y1 , x1 ) . . . (yn , xn ) on yi = f (xi ) + εi , where
xi ∈ [0, 1] and | f  | ≤ L, suppose one solves
ŷi − ŷ j
1
≤L
i, j = 1, . . . , n.
min
(yi − ŷi )2 s.t.
ŷ1 ,..., ŷn n
xi − x j
i
(2.3.4)
Here ŷi is the estimate of f at xi and fˆ is a piecewise linear function joining the
ŷi with slope not exceeding the derivative bound L. Under general conditions
this estimator will be consistent. Furthermore, adding monotonicity or concavity
constraints, at least at the points where we have data, is straightforward. As

Background and Overview

23

additional structure is imposed, the estimator becomes smoother, and its ﬁt to
the true regression function improves (see Figure 2.4).
2.4 A Bird’s-Eye View of Important Theoretical Results
The non- and semiparametric literatures contain many theoretical results. Here
we summarize – in crude form – the main categories of results that are of
particular interest to the applied researcher.
2.4.1 Computability of Estimators
Our preliminary exposition of local averaging estimators suggests that their
computation is generally straightforward. The naive optimization estimator considered in Section 2.3.3 can also be calculated easily even with additional constraints on the regression function. What is more surprising is that estimators
minimizing the sum of squared residuals over (fairly general) inﬁnite dimensional classes of smooth functions can be obtained by solving ﬁnite dimensional
(often quadratic) optimization problems (see Sections 3.1 to 3.4).
2.4.2 Consistency
In nonparametric regression, smoothness conditions (in particular, the existence
of bounded derivatives) play a central role in ensuring consistency of the estimator. They are also critical in determining the rate of convergence as well
as certain distributional results.2 With sufﬁcient smoothness, derivatives of the
regression function can be estimated consistently, sometimes by differentiating
the estimator of the function itself (see Sections 3.1 to 3.4 and 3.7).
2.4.3 Rate of Convergence
How quickly does one “discover” the true regression function? In a parametric
setting, the rate at which the variance of estimators goes to zero is typically 1/n.3
2

3

For example, in proving these results for minimization estimators, smoothness is used to
ensure that uniform (over classes of functions) laws of large numbers and uniform central limit
theorems apply (see Dudley 1984, Pollard 1984, and Andrews 1994a,b).
In the location model y = µ + ε, Var( ȳ) = σ y2 /n; hence, µ − ȳ = O P (n −1/2 ) and (µ − ȳ)2 =
O P (1/n). For the linear model y = α + βx + ε where the ordered pairs (y, x) are say i.i.d.,
we have
(α + βx − α̂ − β̂x)2 d x = (α − α̂)2

d x + (β − β̂)2

+ 2(α − α̂)(β − β̂)

x 2d x

xd x = O P (1/n)

because α̂, β̂ are unbiased and Var(α̂), Var(β̂) and Cov(α̂, β̂) converge to 0 at 1/n. The same
rate of convergence usually applies to general parametric forms of the regression function.

24

Semiparametric Regression for the Applied Econometrician

Data-generating mechanism yi = xi + εi εi ∼ N (0, .04) xi ∈ [0,1]. Simulations performed
using GAMS – General Algebraic Modeling System (Brooke, Kendrick, and Meeraus 1992).

Figure 2.4. Naive nonparametric least squares.

Background and Overview

25

It does not depend on the number of explanatory variables. For nonparametric
estimators, convergence slows dramatically as the number of explanatory variables increases (recall our earlier discussion of the curse of dimensionality), but
this is ameliorated somewhat if the function is differentiable. The optimal rate
at which a nonparametric estimator can converge to the true regression function
is given by (see Stone 1980, 1982)
[ fˆ(x) − f (x)]2 d x = O P

1
n 2m/(2m + d)

,

(2.4.1)

where m is the degree of differentiability of f and d is the dimension of x. For
a twice differentiable function of one variable, (2.4.1) implies an optimal rate
of convergence of O P (n −4/5 ) (a case that will recur repeatedly). For a function
of two variables, it is O P (n −2/3 ).
Local averaging and nonparametric least-squares estimators can be constructed that achieve the optimal rate of convergence (see Sections 3.1 through
3.3). Rate of convergence also plays an important role in test procedures.
If the model is additively separable or partially linear, then the rate of convergence of the optimal estimator depends on the nonparametric component of
the model with the highest dimension (Stone 1985, 1986). For example, for the
additively separable model y = f a (xa ) + f b (xb ) + ε, where xa , xb are scalars,
the convergence rate is the same as if the regression function were a nonparametric function of one variable. The same is true for the partial linear model
y = zβ + f (x) + ε, where x and z are scalars.
Estimators of β in the partial linear model can be constructed that are n 1/2 consistent (i.e., for which the variance shrinks at the parametric rate 1/n) and
asymptotically normal. In Section 1.3, we have already seen a simple differencing estimator with this property (see Sections 3.6 and 4.5 for further discussion).
Also, estimators of δ in the index model y = f (xδ) + ε can be constructed that
are n 1/2 -consistent asymptotically normal (see Chapter 7).
For the hybrid regression function f (z, xa , xb , xc ) = zβ + f a (xa ) + f b (xb ) +
f c (xc ), where xa , xb , xc are of dimension da , db , dc , respectively, the optimal
rate of convergence for the regression as a whole is the same as for a nonparametric regression model with number of variables equal to max{da , db , dc }.
Constraints such as monotonicity or concavity do not enhance the (large
sample) rate of convergence if enough smoothness is imposed on the model
(see Section 6.6). They can improve performance of the estimator (such as the
mean-squared error) if strong smoothness assumptions are not made or if the
data set is of moderate size (recall Figure 2.4).
2.4.4 Bias-Variance Trade-Off
By increasing the number of observations over which averaging is taking place,
one can reduce the variance of a local averaging estimator. But as progressively

26

Semiparametric Regression for the Applied Econometrician

less similar observations are introduced, the estimator generally becomes more
biased. The objective is to minimize the mean-squared error (variance plus bias
squared). For nonparametric estimators that achieve optimal rates of convergence, the square of the bias and the variance converge to zero at the same rate
(see Sections 3.1 and 3.2). (In parametric settings the former converges to zero
much more quickly than the latter.) Unfortunately, this property can complicate
the construction of conﬁdence intervals and test procedures.
2.4.5 Asymptotic Distributions of Estimators
For a wide variety of nonparametric estimators, the estimate of the regression
function at a point is approximately normally distributed. The joint distribution at a collection of points is joint normally distributed. Various functionals
such as the average sum of squared residuals are also normally distributed (see
Sections 3.1 through 3.3). In many cases, the bootstrap may be used to construct conﬁdence intervals and critical values that are more accurate than those
obtained using asymptotic methods (see Chapter 8).
2.4.6 How Much to Smooth
Smoothness parameters such as the size of the neighborhood over which averaging is being performed can be selected optimally by choosing the value
that minimizes out-of-sample prediction error. The technique, known as crossvalidation, will be discussed in Section 3.5.
2.4.7 Testing Procedures
A variety of speciﬁcation tests of parametric or semiparametric null hypotheses
against nonparametric or semiparametric alternatives are available.
Nonparametric tests of signiﬁcance are also available as are tests of additive
separability, monotonicity, homogeneity, concavity, and maximization hypotheses. A fairly uniﬁed testing theory can be constructed using either “goodnessof-ﬁt” type tests or “residual regression” tests (see Chapters 6 and 8).

3

Introduction to Smoothing

3.1 A Simple Smoother
3.1.1 The Moving Average Smoother1
A wide variety of smoothing methods have been proposed. We will begin with a
very simple moving average or “running mean” smoother. Suppose we are given
data (y1 , x1 ) . . . (yn , xn ) on the model y = f (x) + ε. We continue to assume
that x is scalar and that the data have been reordered so that x1 ≤ · · · ≤ xn . For
the time being, we will further assume that the xi are equally spaced on the
unit interval. Deﬁne the estimator of f at xi to be the average of k consecutive
observations centered at xi . (To avoid ambiguity, it is convenient to choose k
odd.) Formally, we deﬁne
1
fˆ(xi ) =
yj,
k j= i
ī

(3.1.1)

where i = i − (k − 1)/2 and ī = i + (k − 1)/2 denote the lower and upper
limits of summations. The estimator is of course equal to
1
1
fˆ(xi ) =
f (x j ) +
εj.
k j= i
k j= i
ī

ī

(3.1.2)

If k – the number of neighbors being averaged – increases with n, then
by conventional central limit theorems the second term on the right-hand side
will be approximately normal with mean 0 and variance σε2 /k. If these neighbors cluster closer and closer to xi – the point at which we are estimating
the function – then the ﬁrst term will converge to f (xi ). Furthermore, if this
1

The estimator is also sometimes called the “symmetric nearest-neighbor smoother.”

27

28

Semiparametric Regression for the Applied Econometrician

convergence is fast enough, we will have


D
1
k /2 ( fˆ(xi ) − f (xi )) → N 0, σε2 .

(3.1.3)

A 95 percent conﬁdence interval for f (xi ) is immediate
σε
fˆ(xi ) ± 1.96 1/ ,
(3.1.4)
k 2
and indeed quite familiar from the conventional estimation of a mean (σε may
be replaced by a consistent estimator). It is this simple kind of reasoning that
we will now make more precise.
3.1.2 A Basic Approximation
Let us rewrite (3.1.2) as follows:
1
fˆ(xi ) =
yj
k j= i
ī

ī
ī
1
1
=
f (x j ) +
εj
k j= i
k j= i

f  (xi ) 
∼
(x j − xi )
= f (xi ) +
k j= i
ī

ī
ī
f  (xi ) 
1
2
+
(x j − xi ) +
εj
2k j= i
k j= i

1
∼
= f (xi ) + 1/2 f  (xi )
k

ī

j= i

1
εj.
k j= i
ī

(x j − xi )2 +

(3.1.5)

In the third and fourth lines, we have applied a second-order Taylor series.2
Note that with the x j symmetric around xi , the second term in the third line is
zero. So, we may rewrite (3.1.5) as3
 
ī
1 k 2 
1
ˆf (xi ) ∼
f (xi ) +
εj.
(3.1.6)
= f (xi ) +
24 n
k j= i
2
3

In particular, f (x j ) = f (xi ) + f  (xi )(x j − xi ) + 1/2 f  (xi )(x j − xi )2 + o(x j − xi )2 . We are
obviously assuming second-order
ī derivatives exist.
1 (k 2 −1) ∼ 1 k 2
We have used the result 1/2 k1
(x j − xi )2 = 24
= 24 ( n ) . See the exercises for
j=i
n2
further details.

Introduction to Smoothing

29

The last term is an average of k independent and identical random variables so
that its variance is σε2 /k and we have


 2
1
k
fˆ(xi ) = f (xi ) + O
(3.1.7)
+ O P 1/ .
n
k 2
The bias E( fˆ(xi ) − f (xi )) is approximated by the second term of (3.1.6) and
the Var( fˆ(xi )) is approximately σε2 /k, thus, the mean-squared error (the sum
of the bias squared and the variance) at a point xi is
 4
 
k
1
2
ˆ
E[ f (xi ) − f (xi )] = O
+O
.
(3.1.8)
n
k
3.1.3 Consistency and Rate of Convergence
The approximation embodied in (3.1.6) yields dividends immediately. As long
as k/n → 0 and k → ∞, the second and third terms go to zero and we have a
consistent estimator.
The rate at which fˆ(xi ) − f (xi ) → 0 depends on which of the second or
third terms in (3.1.6) converge to zero more slowly. Optimality is achieved
when the bias squared and the variance shrink to zero at the same rate. Using
(3.1.7), one can see that this occurs if O(k 2 /n 2 ) = O P (1/k 1/2 ), which implies
that optimality can be achieved by choosing k = O(n 4/5 ). In this case,




1
1
fˆ(xi ) ∼
.
(3.1.9)
= f (xi ) + O 2/5 + O P
n
n 2/5
Equivalently, we could have solved for the optimal rate using (3.1.8). Setting
O(k 4 /n 4 ) = O(1/k) and solving, we again obtain k = O(n 4/5 ). Substituting
2
into (3.1.8) yields a rate of convergence of E[ fˆ(xi ) − f (xi )] = O(n −4/5 ) for
the mean-squared error at a point xi . This, in turn, underpins the following,


1
2
[ fˆ(x) − f (x)] d x = O P
,
(3.1.10)
n 4/5
which is a rather pleasant result in that it satisﬁes Stone’s optimal rate of convergence, (2.4.1), where the order of differentiability m = 2 and the dimension
d = 1.
3.1.4 Asymptotic Normality and Conﬁdence Intervals
Applying a central limit theorem to the last term of (3.1.6), we have


 


1 k 2 
D
1/2
ˆ
k
(3.1.11)
f (xi ) − f (xi ) −
f (xi ) → N 0, σε2 .
24 n

30

Semiparametric Regression for the Applied Econometrician

If we select k optimally, say, k = n 4/5 , then k 1/2 (k/n)2 = 1 and the construction of a conﬁdence interval for f (xi ) is complicated by the presence of
the term involving f  (xi ), which would need to be estimated. However, if we
require k to grow more slowly than n 4/5 (e.g., k = n 3/4 ), then k 1/2 (k/n)2 → 0
D
and (3.1.11) becomes k 1/2 ( fˆ(xi ) − f (xi )) → N (0, σε2 ). Intuitively, we are
adding observations sufﬁciently slowly that they are rapidly clustering around
the point of estimation. As a consequence, the bias is small relative to the variance (see (3.1.7)). In this case, a 95 percent conﬁdence interval for f (xi ) is
approximately fˆ(xi ) ± 1.96σε /k 1/2 . These are of course exactly the results we
began with in (3.1.3) and (3.1.4).
Let us pause for a moment. In these last sections, we have illustrated three
essential results for a simple moving average estimator: that it is consistent; that
by allowing the number of terms in the average to grow at an appropriate rate,
the optimal rate of convergence can be achieved; and, that it is asymptotically
normal.
3.1.5 Smoothing Matrix
It is often convenient to write moving average (and other) smoothers in matrix
notation. Let S be the smoother matrix deﬁned by

1
, . . .. k1 , 0, . . . . . . . . . . . . . . . . . . . ., 0
k

 1
 0, , . . . . . , 1 , 0, . . . . . . . . . . . . . . . . 0 

 k
k


: :


(3.1.12)
=
.
S
: :


(n−k+1)xn


 0, . . . . . . . . . . . . . . . 0, 1 , . . . . . . , 1 , 0 
k
k


0, . . . . . . . . . . . . . . . . . . 0, k1 , . . . . . , k1
Then we may rewrite (3.1.1) in vector-matrix form as
ŷ = fˆ(x) = Sy,

(3.1.13)

where x, y, ŷ, fˆ(x) are vectors.
3.1.6 Empirical Application: Engel Curve Estimation
A common problem in a variety of areas of economics is the estimation of
Engel curves. Using South African household survey data (see Appendix E),
we select the subset consisting of single individuals and plot the food share of
total expenditure as a function of the log of total expenditure in Figure 3.1. The
subset contains 1,109 observations.
We apply the moving average smoother with k = 51 to obtain the solid irregular line in the upper panel. The lack of smoothness is a feature of moving average
smoothers. Note that the estimator does not quite extend to the boundaries of

Introduction to Smoothing
Model: y = f (x) + ε, x is log of total expenditure and y is the food share of expenditure.
Data: The data consist of a sample of 1,109 single individuals (“Singles”) from South Africa.

Figure 3.1. Engel curve estimation using moving average smoother.

31

32

Semiparametric Regression for the Applied Econometrician

the data because it drops observations at either end. This shortcoming will be
remedied shortly, but boundary behavior is an important feature distinguishing
nonparametric estimators.
The lower panel uses (3.1.3) to produce 95 percent pointwise conﬁdence
intervals. At median expenditure (log (total expenditure) = 6.54), the 95 percent
conﬁdence interval for food share is 38 to 46 percent.
3.2 Kernel Smoothers
3.2.1 Estimator
Let us return now to the more general formulation of a nonparametric estimator
we proposed in Chapter 2:
fˆ(xo ) =

n


wi (xo )yi .

(3.2.1)

1

Here we are estimating the regression function at the point xo as a weighted sum
of the yi , where the weights wi (xo ) depend on xo . A conceptually convenient
way to construct local averaging weights is to use a unimodal function centered
at zero that declines in either direction at a rate controlled by a scale parameter.
Natural candidates for such functions, which are commonly known as kernels,
are probability density functions. Let K be a bounded function that integrates
to 1 and is symmetric around 0. Deﬁne the weights to be
1  xi −xo 
K λ
λn
wi (xo ) =
.
1 n  xi −xo 
K
λ
λn 1

(3.2.2)

The shape of the weights (which, by construction, sum to 1) is determined by
K, and their magnitude is controlled by λ, which is known as the bandwidth.
A large value of λ results in greater weight being put on observations that are
far from xo . Using (3.2.1) the nonparametric regression function estimator, ﬁrst
suggested by Nadaraya (1964) and Watson (1964), becomes
1 n  xi −xo 
yi
1K
λ
fˆ(xo ) = λn
.
1 n  xi −xo 
K
λ
λn 1

(3.2.3)

A variety of other kernels are available (see Figure 3.2). Generally, selection
of the kernel is less important than selection of the bandwidth over which
observations are averaged. The simplest is the uniform kernel (also known as

Figure 3.2. Alternative kernel functions.

33

34

Semiparametric Regression for the Applied Econometrician

the rectangular or box kernel), which takes a value of 1/2 on [−1,1] and 0
elsewhere. But the normal and other kernels are also widely used (see Wand
and Jones 1995 for an extensive treatment of kernel smoothing).
Much of the intuition developed using the moving average smoother applies
in the current setting. Indeed, with equally spaced x’s on the unit interval, and
the uniform kernel, the essential difference is the deﬁnition of the smoothing
parameter. The uniform kernel simply averages observations that lie in the
interval xo ± λ. With n data points in the unit interval, the proportion of observations falling in an interval of width 2λ will be 2λ, and the number of
observations will be 2λn. Thus, if one uses the substitution k = 2λn in the
arguments of Section 3.1, analogous results will be obtained for the uniform
kernel estimator, which in this case is virtually identical to the moving average
smoother.
In particular, (3.1.6) and (3.1.7) become
1
1 
fˆ(xi ) ∼
εj
= f (xi ) + (2λ)2 f  (xi ) +
24
2λn j
and
fˆ(xi ) ∼
= f (xi ) + O(λ2 ) + O P




1
.
λ1/2 n 1/2

(3.2.4)

(3.2.4a)

Analogously to the conditions on k, we impose the following two conditions
on λ: the ﬁrst is λ → 0, which ensures that averaging takes place over a shrinking neighborhood, thus eventually eliminating bias. The second is λn → ∞,
which ensures that the number of observations being averaged grows and the
variance of the estimator declines to 0.
3.2.2 Asymptotic Normality
Suppose now that the x’s are randomly distributed (say on the unit interval) with
probability density p(x). For a general kernel, the Nadaraya–Watson kernel
estimator (3.2.3) is consistent. The numerator converges to f (xo ) p(xo ) and the
denominator converges to p(xo ).
The rate of convergence is optimized if λ = O(n −1/5 ) in which case the
integrated squared error converges at the optimal rate O P (n −4/5 ), as in (3.1.10).
Conﬁdence intervals may be constructed using



p  (xo )
1 1
λ /2 n /2 fˆ(xo ) − f (xo ) − 1/2a K λ2 f  (xo ) + 2 f  (xo )
p(xo )

2
b K σε
D
→ N 0,
,
(3.2.5)
p(xo )

Introduction to Smoothing

35

where p (.) is the density of x and
aK =

u 2 K (u)du

bK =

K 2 (u)du.

(3.2.6)

Wand and Jones (1995, p. 176) provide the values of a K and b K for various
kernels.
3.2.3 Comparison to Moving Average Smoother
Equation (3.2.5) requires estimation of the ﬁrst and second derivatives of the
regression function. However, if λ shrinks to zero faster than at the optimal rate,
then the bias term disappears. Under such conditions, and assuming a uniform
kernel for which b K = 1/2, we may rewrite (3.2.5) as


σε2
D
1 1
λ /2 n /2 ( fˆ(xo ) − f (xo )) → N 0,
.
(3.2.7)
2 p(xo )
What is the probability that an observation will fall in the interval xo ± λ?
It is roughly the height of the density times twice the bandwidth or 2λp(xo ).
Now consider the variance of fˆ(xo ) implied by (3.2.7) – σε2 /2λ p(xo )n. The
denominator is approximately the number of observations one can expect to be
averaging when calculating the estimate of f at xo . Compare this to the variance
of the moving average estimator in Section 3.1, which is σε2 /k.
3.2.4 Conﬁdence Intervals
Again let us assume that the bias term is made to disappear asymptotically by
permitting the bandwidth to shrink at a rate that is faster than the optimal rate.
Applying (3.2.5), deﬁne the standard error of the estimated regression function
at a point to be

b K σ̂ε2
s fˆ (xo ) =
,
(3.2.8)
λ p̂(xo )n
where
1 
K
λn 1
n

p̂(xo ) =



xi − xo
λ


(3.2.9)

is the denominator of (3.2.3). (See footnote to Table 3.1 for values of b K .) Then
a 95 percent pointwise conﬁdence interval can be constructed using
fˆ(xo ) ± 1.96 s fˆ .

(3.2.10)

36

Semiparametric Regression for the Applied Econometrician

Table 3.1. Asymptotic conﬁdence intervals for kernel
estimators – implementation.
1. Select λ so that n 1/5 λ → 0, for example, λ = O(n −1/4 ). This ensures that
the bias term does not appear in (3.2.5).
2. Select a kernel K and obtain b K = K 2 (u)du. For the uniform kernel on
[−1,1] b K = 1/2 .a
3. Estimate f using the 
Nadaraya–Watson estimator (3.2.3).
4. Calculate σ̂ε2 = 1/n (yi − fˆ(xi ))2 .
5. Estimate p(xo ) using (3.2.9). If the uniform kernel is used, p̂(xo ) equals
the proportion of xi in the interval xo ± λ divided by the width of the
interval 2λ.
6. Calculate the 
conﬁdence interval at f (xo ) using
fˆ(xo ) ± 1.96 b K σ̂ε2 / p̂(xo )λn
7. Repeat at other points if desired.
a
For other kernels, the values of b K are as follows: triangular, 2/3; quartic or
biweight, 5/7; Epanechnikov, 3/5; triweight, 350/429; normal, 1/(2π 1/2 ).

Table 3.1 provides implementation details. For conﬁdence intervals when
the residuals are heteroskedastic, see the bootstrap procedures in Chapter 8,
Table 8.2.
3.2.5 Uniform Conﬁdence Bands4
A potentially more interesting graphic for nonparametric estimation is a conﬁdence band or ribbon around the estimated function. Its interpretation is that, in
repeated samples, 95 percent of the estimated conﬁdence bands will contain the
entire true regression function f . The plausibility of an alternative speciﬁcation
(such as a parametric estimate, a monotone or concave estimate) can then be
assessed by superimposing this speciﬁcation on the graph to see if it falls within
the band. Without loss of generality, assume that the domain of the nonparametric regression function is the unit interval. Returning to the assumption that
λ → 0 at a rate faster than optimal (but slowly enough to ensure consistency),
a uniform 95 percent conﬁdence band or ribbon can be constructed around the
function f using



(K  (u))2
ˆf (x) ± c + d + 1 ln
s fˆ ,
(3.2.11)
d
2d
4π 2 K 2 (u)
√
where d = 2 ln(1/λ), c satisﬁes exp[−2exp(−c)] = .95, and s fˆ is the
4

See Härdle and Linton (1994, p. 2317). See also Eubank and Speckman (1993) for an alternative
approach to constructing uniform conﬁdence bands for the case where the x’s are equally spaced.

Introduction to Smoothing

37

estimated standard error of the estimated regression function deﬁned in
(3.2.8).
3.2.6 Empirical Application: Engel Curve Estimation
We now apply kernel estimation to the South African data set on single individuals considered earlier. The upper panel of Figure 3.3 illustrates a kernel
estimate (using a triangular kernel). It is considerably smoother than the simple
moving average estimator in Figure 3.1. The lower panel of Figure 3.3 displays
95 percent pointwise conﬁdence intervals as well as a 95 percent uniform conﬁdence band around the estimate. Note that the uniform band – because it is
designed to capture the entire function with 95 percent probability – is wider
than the pointwise intervals.
3.3 Nonparametric Least-Squares and Spline Smoothers
3.3.1 Estimation
In Section 2.3.3, we introduced a primitive nonparametric least-squares estimator that imposed smoothness by bounding the slope of the estimating function.
We will need a more tractable way to impose constraints on various order
derivatives. Let C m be the set of functions that have continuous derivatives up
to order m. (For purposes of exposition we restrict these functions to the unit
interval.) A measure of smoothness that is particularly convenient is given by
the Sobolev norm

1/2
 (m) 2
2
 2
 2
 f Sob =
f + ( f ) + ( f ) + ··· + f
dx
,
(3.3.1)
where (m) denotes the mth-order derivative. A small value of the norm implies
that neither the function nor any of its derivatives up to order m can be too large
over a signiﬁcant portion of the domain. Indeed, bounding this norm implies that
all lower-order derivatives are bounded in supnorm. Recall from Section 2.3.3
and Figure 2.4 that even bounding the ﬁrst derivative produces a consistent
nonparametric least-squares estimator.
Suppose we take our estimating set ᑤ to be the set of functions in C m for
which the square of the Sobolev norm is bounded by say L, that is, ᑤ = { f ∈ C m ,
 f 2Sob ≤ L}. The task of ﬁnding the function in ᑤ that best ﬁts the data appears
to be daunting. After all, ᑤ is an inﬁnite dimensional family. What is remarkable
is that the solution fˆ that satisﬁes
s 2 = min
f

1
[yi − f (xi )]2 s.t.  f 2Sob ≤ L
n i

(3.3.2)

38

Semiparametric Regression for the Applied Econometrician

Model: y = f (x) + ε, x is log total expenditure and y is the food share of expenditure.
Data: The data consist of a sample of 1,109 single individuals (“Singles”) from South Africa.

Figure 3.3. Engel curve estimation using kernel estimator.

Introduction to Smoothing

39

can be obtained by minimizing a quadratic objective
function subject to a quadratic constraint. The solution is of the form fˆ = n1 ĉi r xi , where r x1 , . . . , r xn
are functions computable from x1 , . . . , xn and ĉ = (ĉ1 , . . . , ĉn ) is obtained by
solving
min
c

1
[y − Rc] [y − Rc]
n

s.t. c Rc ≤ L .

(3.3.3)

Here y is the n × 1 vector of observations on the dependent variable, and R
is an n × n matrix computable from x1 , . . . , xn . Note that even though one is
estimating n parameters to ﬁt n observations, the parameters are constrained;
thus, there is no immediate reason to expect perfect ﬁt.
The r xi are called representor functions and R, the matrix of inner products of
the r xi , the representor matrix (see Wahba 1990, Yatchew and Bos 1997). Details
of these computations are contained in Appendix D. An efﬁcient algorithm for
solving (3.3.3) may be found in Golub and Van Loan (1989, p. 564).
Furthermore, if x is a vector, the Sobolev norm (3.3.1) generalizes to include various order partial derivatives. The optimization problem has the same
quadratic structure as in the one-dimensional case above, and the functions
r x1 , . . . , r xn as well as the matrix R are directly computable from the data
x1 , . . . , xn . Further results may be found in Chapters 5 and 6 and Appendix D.
3.3.2 Properties5
The main statistical properties of the procedure are these: fˆ is a consistent
estimator of f ; indeed, low-order derivatives of fˆ consistently estimate the
corresponding derivatives of f . The rate at which fˆ converges to f satisﬁes the
optimal rates given by Stone, (2.4.1). The optimal convergence result is useful
in producing consistent tests of a broad range of hypotheses.
The average minimum sum of squared residuals s 2 is a consistent estimator of
the residual variance σε2 . Furthermore, in large samples, s 2 is indistinguishable
from the true average sum of squared residuals in the sense that


1 2 P
1/2
2
n
εi → 0.
s −
(3.3.4)
n

Next, since n 1/2 (1/n εi2 − σε2 ) → N (0, Var(ε 2 )) ( just apply an ordinary
central limit theorem), (3.3.4) implies that
 D
1 
n /2 s 2 − σε2 → N (0, Var(ε 2 )).
(3.3.5)
5

These results are proved using empirical processes theory, as discussed in Dudley (1984) and
Pollard (1984).

40

Semiparametric Regression for the Applied Econometrician

As explained in Section 3.6.2, this result lies at the heart of demonstrating
that nonparametric least squares can be used to produce n 1/2 -consistent normal
estimators in the partial linear model.
3.3.3 Spline Smoothers
The nonparametric least-squares estimator is closely related to spline estimation. Assume for the moment η > 0 is a given constant,6 and consider the
“penalized” least-squares problem
1
min
(3.3.6)
[yi − f (xi )]2 + η f 2Sob .
f
n i
The criterion function trades off ﬁdelity to the data against smoothness of the
function f . There is a penalty for selecting functions that ﬁt the data extremely
well but as a consequence are very rough (recall that the Sobolev norm measures
the smoothness of a function and its derivatives). A larger η results in a smoother
function being selected.
If one solves (3.3.2), our nonparametric least-squares problem, takes the
Lagrangian multiplier, say η̂ associated with the smoothness constraint, and
then uses it in solving (3.3.6), the resulting fˆ will be identical.
In their simplest incarnation, spline estimators use ( f  )2 as the measure of
smoothness (see Eubank 1988, Wahba 1990). Equation (3.3.6) becomes
1
min
(3.3.7)
[yi − f (xi )]2 + η ( f  )2 .
f
n i
As η increases, the estimate becomes progressively smoother. In the limit, f 
is forced to zero, producing a linear ﬁt. At the other extreme, as η goes to zero,
the estimator produces a function that interpolates the data points perfectly.
3.4 Local Polynomial Smoothers
3.4.1 Local Linear Regression
A natural extension of local averaging is the idea of local regression. Suppose
one runs a linear regression using only observations that lie in a neighborhood
of xo which we will denote by N (xo ). If included observations were given equal
weight, one would solve

min
(3.4.1)
[yi − a(xo ) − b(xo )xi ]2
a,b

6

xi ∈N (xo )

Actually, it is selected using cross-validation, which is a procedure we will discuss shortly.

Introduction to Smoothing

41

where the dependence of the regression coefﬁcients on xo is emphasized by the
notation. The estimate of f at xo would be given by
fˆ(xo ) = â(xo ) + b̂(xo )xo .

(3.4.2)

Repeating this procedure at a series of points in the domain, one obtains a
nonparametric estimator of the regression function f .
Alternatively, one could perform a weighted regression assigning higher
weights to closer observations and lower ones to those that are farther away.
(In the preceding procedure, one assigns a weight of 1 to observations in N (xo )
and 0 to others.) A natural way to implement this is to let the weights be determined by a kernel function and controlled by the bandwidth parameter λ. The
optimization problem may then conveniently be written as



xi − xo
2
min
.
(3.4.3)
[yi − a(xo ) − b(xo )xi ] K
a, b
λ
i
Solutions are once again plugged into (3.4.2). This procedure is sometimes
referred to as “kernel regression” because it applies kernel weights to a local
regression. By replacing the linear function in (3.4.3) with a polynomial, the
procedure generalizes to local polynomial regression.
Key references in this literature include Cleveland (1979), Cleveland and
Devlin (1988), and Fan and Gijbels (1996). The latter is a monograph devoted
to the subject and contains an extensive bibliography.
3.4.2 Properties
Under general conditions, local polynomial regression procedures are consistent, achieve optimal rates of convergence with suitable selection of the bandwidth, and yield point estimates that are asymptotically normal. For construction
of conﬁdence intervals, see Fan and Gijbels (1996, pp. 116–118). Furthermore,
the behavior of local polynomial regression procedures at the boundary is often
superior to kernel and spline estimation. An algorithm used to implement local
polynomial regression proceeds as follows:
For a point xo ﬁnd the k nearest-neighbors. These will constitute the neighborhood N (xo ). Deﬁne the span to be k/n. It is the fraction of total observations used in the local regression.
Let (xo ) be the largest distance between xo and any other point in the
neighborhood N (xo ). Assign weights to each point in the neighborhood
using K (|xi − xo |/(xo )), where K is the triweight kernel in Figure 3.2.
Calculate the weighted least-squares estimator using the observations in the
neighborhood and produce the ﬁtted value fˆ (xo ). Repeat at the other values
of x.

42

Semiparametric Regression for the Applied Econometrician

Model: y = f (x) + ε, x is log total expenditure and y is the food share of expenditure.
Data: The data consist of a sample of 1,109 single individuals (“Singles”) from South Africa.

Figure 3.4. Engel curve estimation using kernel, spline, and lowess estimators.

Variants on these estimators include loess (local regression;7 Cleveland and
Devlin 1988) and lowess (locally weighted scatterplot smoothing; Cleveland
1979). After initial estimates using local regression, lowess seeks to increase
robustness by assigning lower weights to those observations with large residuals
and repeating the local regression procedure.
3.4.3 Empirical Application: Engel Curve Estimation
Figure 3.4 illustrates the application of kernel, spline, and lowess estimators to
the data on single South Africans. Relative to the precision of estimation (as
illustrated by the conﬁdence intervals and bands in Figure 3.3), the estimators
track each other closely. One should keep in mind that this is a sizable data set
with over 1,000 observations.

7

Evidently the name was chosen because a loess is a surface of loamy, silt, or clay deposits
common in river valleys and usually formed by wind (see Chambers and Hastie 1993, p. 314).

Introduction to Smoothing

43

3.5 Selection of Smoothing Parameter8
3.5.1 Kernel Estimation
We now turn to selection of smoothing parameters for kernel estimators. If the
bandwidth λ is too large, then oversmoothing will exacerbate bias and eliminate
important features of the regression function. Selection of a value of λ that is
too small will cause the estimator to track the current data too closely, thus
impairing the prediction accuracy of the estimated regression function when
applied to new data (see Figures 2.2 and 2.3). To obtain a good estimate of f
one would like to select λ to minimize the mean integrated squared error (MISE)
MISE(λ) = E

[ fˆ(x; λ) − f (x)]2 d x,

(3.5.1)

where we write fˆ(x; λ) to denote explicitly that the kernel estimator depends
on the choice of λ. Of course we do not observe f , so the MISE cannot be minimized directly. Nor will selecting λ by minimizing the estimate of the residual
variance
n
1
σ̂ 2ε (λ) =
[yi − fˆ(xi ; λ)]2
(3.5.2)
n i=1
lead to a useful result, for the minimum of (3.5.2) occurs when λ is reduced to
the point where the data are ﬁt perfectly. However, this idea can be modiﬁed
to produce useful results. Consider a slight variation on (3.5.2) known as the
cross-validation function
n
1
CV(λ) =
[yi − fˆ−i (xi ; λ)]2 .
(3.5.3)
n i=1
The only difference between (3.5.2) and (3.5.3) is that the kernel estimator
is subscripted with a curious “−i” which is used to denote that fˆ−i is obtained
by omitting the ith observation. The estimate of f at each point xi is obtained
by estimating the regression function using all other observations and then
predicting the value of f at the omitted observation. (Thus, for a given value
of λ, CV(λ) requires calculation of n separate kernel estimates.)9
8
9

Cross-validation was ﬁrst proposed for the kernel estimator by Clark (1975) and for spline
estimation by Wahba and Wold (1975).
The notion that out-of-sample prediction is a useful criterion for estimation and testing is, of
course, quite generally applied in statistics. In the simplest case, one can imagine dividing a
sample in two, using one part to estimate the model and the other to assess its accuracy or
validity. This naive approach, however, does not make optimal use of the data, a problem that
is resolved through the cross-validation device.

44

Semiparametric Regression for the Applied Econometrician

This subtle change results in some extremely propitious properties, (see, e.g.,
Härdle and Marron 1985, and Härdle, Hall, and Marron 1988). In particular,
suppose an optimal λ, say λOPT , could be chosen to minimize (3.5.1). Let λ̂ be
the value that minimizes (3.5.3). Then MISE(λ̂)/MISE(λOPT ) converges in large
samples to 1. That is, in large samples, selecting λ through cross-validation is
as good as knowing the λ that minimizes the integrated mean-squared error.
3.5.2 Nonparametric Least Squares
The heuristics of smoothness bound selection for nonparametric least squares
are similar. If one selects L in (3.3.2) to be much larger than the squared true
norm of the function f , then the estimator will be less efﬁcient though it will
be consistent. If one selects a bound that is smaller, then the estimator will
generally be inconsistent. The cross-validation function is deﬁned as
CV(L) =

n
1
[yi − fˆ−i (xi )]2 ,
n i=1

(3.5.4)

where fˆ−i is obtained by solving
1
[y j − f (x j )]2 s.t.  f 2Sob ≤ L .
n j =i
n

min
f

(3.5.5)

The interpretation of the smoothing parameter is somewhat different. In kernel estimation it corresponds to the width of the interval over which averaging
takes place; in nonparametric least squares it is the diameter of the set of functions over which estimation takes place.
Figure 3.5 illustrates the behavior of the cross-validation function for both
kernel and nonparametric least-squares estimators. The data-generating mechanism is given by the model yi = xi + εi , εi ∼ N (0,.01), i = 1, . . . , 25 where the
xi are equally spaced on the interval [0,1]. The minimum of the cross-validation
function for the kernel estimator is approximately at a bandwidth of .25.
For the nonparametric least-squares cross-validation function, note ﬁrst that
the square of the second-order Sobolev norm (3.3.1) of the true regression
function is given by 10 x 2 + 1 = 11/3 for m ≥ 2. Thus, L = 11/3 would be the
smallest value that would ensure consistency of the nonparametric least-squares
problem (3.3.2). In the simulations (Figure 3.5), the minimum of the crossvalidation function is between 1.4 and 1.5.10
10

For optimality results on cross-validation in a spline setting, see Li (1986, 1987), Wahba (1990,
p. 47).

Introduction to Smoothing

45

Data-generating mechanism: yi = xi + εi , εi ∼ N (0,.01), i = 1, . . . 25, where xi are equally
spaced on the interval [0,1]. Kernel cross-validation performed using triangular kernel. Nonparametric least-squares cross-validation performed using Fortran code written by the author.

Figure 3.5. Selection of smoothing parameters.

46

Semiparametric Regression for the Applied Econometrician

Model: y = f (x) + ε, x is log total expenditure and y is the food share of expenditure.
Data: 1,109 single individuals (“Singles”) from South Africa.
Kernel: triangular kernel

Figure 3.6. Cross-validation of bandwidth for Engel curve estimation.

3.5.3 Implementation
Various researchers have investigated alternate procedures for selecting the
smoothing parameter. Unfortunately, unlike the case of kernel estimation of density functions, no convenient rules of thumb are available for kernel regression.11
However, by simply trying different values for the smoothing parameter and
visually examining the resulting estimate of the regression function, it is often
possible to obtain a useful indication of whether one is over- or undersmoothing.
Furthermore, cross-validation can be automated relatively easily. The kernel
cross-validation function in Figure 3.5 was obtained using the “regcvl” function
in XploRe. S-Plus uses cross-validation to produce its spline estimates, and other
automated procedures are also available.12
Figure 3.6 contains the cross-validation function for the data on food expenditures by South African singles (see Figures 3.1, 3.3, and 3.4). We have used a
triangular kernel (because of the speed of computation). The minimum appears
to be at about .35.
11

12

For “rules of thumb” in a kernel density estimation setting, see Scott (1992). For alternatives
to cross-validation in a nonparametric regression setting, see, for example, Simonoff (1996,
p. 197) and references therein.
See Venables and Ripley (1994, p. 250).

Introduction to Smoothing

47

3.6 Partial Linear Model
3.6.1 Kernel Estimation
Given i.i.d. data (y1 , x1 , z 1 ), . . . , (yn , xn , z n ), consider the semiparametric regression model discussed in Chapter 1,
y = zβ + f (x) + ε,

(3.6.1)

where E(y | z, x) = zβ + f (x), σε2 = Var[y | z, x]. The function f is not
known to lie in a particular parametric family. An early and important analysis
of this model was that of Engle et al. (1986), who used it to study the impact
of weather on electricity demand, which is an example that we too consider in
Section 4.6.3.
Robinson’s (1988) inﬂuential paper – one that was paralleled by Speckman
(1988) in the statistics literature – demonstrates that β can be estimated at
parametric rates, that is, β̂ − β = O P (n −1/2 ), despite the presence of the nonparametric function f. Speciﬁcally, Robinson rewrites (3.6.1) conditioning on
the nonparametric variable x as follows:
y − E(y | x) = y − E(z | x)β − f (x) = (z − E(z | x))β + ε. (3.6.2)
If E(y | x) and E(z | x) are known, then ordinary least squares on (3.6.2)
yields an estimate of β,which is asymptotically normal with variance σε2 /nσu2 ,
where σu2 is the variance of z conditional on x. (For the moment we will assume
that this conditional variance is constant.)
Of course, the regression functions E(y | x) and E(z | x) are generally not even
known to have particular parametric forms. Robinson then produces nonparametric (kernel) estimators of h(x) = E(y | x) and g(x) = E(z | x) that converge
sufﬁciently quickly that their substitution in the OLS estimator does not affect
its asymptotic distribution. The estimator is distributed as13
1
n /2 (β̂


σε2
− β) → N 0, 2 .
σu
D



(3.6.3)

It is often called the “double residual” estimator because it involves the residuals
from initial nonparametric regressions of y on x and z on x, as may be seen
from (3.6.2).

13

For general results of this nature, see Newey (1994a). See also Linton (1995b) who has analyzed higher-order properties of β̂. Bickel and Kwon (2001) have discussed inference for
semiparametric models in a general setting.

48

Semiparametric Regression for the Applied Econometrician

3.6.2 Nonparametric Least Squares
Returning to (3.6.1), consider the conditional distribution of y, z | x
z = E(z | x) + u = g(x) + u
y = E(y | x) + v = h(x) + v = (g(x)β + f (x)) + (uβ + ε) (3.6.4)
then,

   2
σu
u
Cov
≡
v
...

σuv
σv2




=

σu2

σu2 β

...

σu2 β 2 + σε2


.

(3.6.5)

Under sufﬁcient smoothness assumptions, the nonparametric least-squares
estimator
can be
equation by equation. The sample variances
 (3.3.2)
 applied
su2 =
û i2 /n, sv2 =
v̂ i2 /n are n 1/2 -consistent, asymptotically normal estimators of the corresponding population
variances σu2 , σv2 (using (3.3.5)). It can

also be demonstrated that suv =
û i v̂ i /n is a n 1/2 -consistent, asymptotically
normal estimator of σuv . In summary, the sample analogue to (3.6.5), that is, the
matrix of estimated variances and covariances, is n 1/2 -consistent asymptotically
normal.
Now β = σuv /σu2 , so that it is fairly straightforward to show that its sample
analogue, β̂ = suv /su2 , is also n 1/2 -consistent, asymptotically normal. Furthermore, its variance is given by σε2 /nσu2 , which is the same variance attained by
the Robinson estimator. Inference may be conducted using (3.6.3). 14
3.6.3 The General Case
Suppose one is given data (y1 , x1 , z 1 ), . . . . , (yn , xn , z n ), where z i is a pdimensional vector and xi and yi are scalars.15 Let z i = g (xi ) + u i , where
g is now a vector function
with ﬁrst derivatives bounded, E (u i | xi ) = 0, and

E (Var (z i | xi )) = z|x . Write the model as
y = Z β + f (x) + ε ,

n×1

n× p p×1

n×1

(3.6.6)

n×1

where Z is the n × p matrix with ith row z i . In this case, the double residual
method requires that a separate nonparametric regression be performed for the
dependent variable and for each parametric variable. Let ĥ(x) be the estimates
n×1

14

15

We note that one can perform semiparametric least squares on the model (3.6.1) by minimizing
the sum of squared residuals with respect to β and f subject to a smoothness constraint on f,
but the resulting estimator of β would not in general converge at n 1/2 (see Rice 1986 and Chen
1988).
The case where x is also a vector requires nonparametric regression of several variables, which
is covered in Chapter 5. Otherwise, the arguments of the current section apply directly. For
proofs of the assertions below, see Robinson (1988) and Speckman (1988).

Introduction to Smoothing

49

resulting from a nonparametric regression of y on x. Let ĝ(x) be the estimates
n× p

resulting from a nonparametric regression of each column of Z on x. (A kernel,
spline, nonparametric least-squares, local polynomial, or other nonparametric
smoother may be used). Write
y − ĥ(x) ∼
= (Z − ĝ(x))β + ε.

(3.6.7)

Then the double residual estimator of β is given by
β̂ = ((Z − ĝ(x)) (Z − ĝ(x)))−1 (Z − ĝ(x)) (y − ĥ(x))
with large sample distribution

−1 
D
1/2
2
n (β̂ − β) → N 0, σε
.
z|x

(3.6.8)

(3.6.9)

The residual variance may be estimated consistently using
s2 =

1
(y − ĥ(x) − (Z − ĝ(x))β̂) (y − ĥ(x) − (Z − ĝ(x))β̂),
n
(3.6.10)

and a consistent estimate of the covariance matrix of β̂ is given by

ˆ
= s 2 ((Z − ĝ(x)) (Z − ĝ(x)))−1 .
β̂

(3.6.11)

Equations (3.6.9) to (3.6.11) may be used to construct conﬁdence intervals
for β. Linear restrictions of the form Rβ = r may be tested using the conventional statistic, which – if the null hypothesis is true – has the following
distribution:
  −1
D
ˆ
2
(R β̂ − r ) R
R
(R β̂ − r ) → χrank(R)
.
(3.6.12)
β̂

Equivalently one may use
 2

n sres
− s2 D 2
→ χrank(R) ,
s2

(3.6.13)

2
where s 2 is the unrestricted estimator obtained in (3.6.10), and sres
is obtained
by estimating the model (3.6.6) subject to the linear constraints16 and then
applying (3.6.10). Finally, one can perform a kernel regression of y − Z β̂ on x
to obtain fˆ.

16

Recall that for a linear model, the restricted OLS estimator may be obtained by redeﬁning
variables. The “double-residual” model being estimated in (3.6.2) is linear.

50

Semiparametric Regression for the Applied Econometrician

3.6.4 Heteroskedasticity
Consider the basic linear regression model expressed in matrix notation y =
Zβ + ε, where coefﬁcients are estimated using OLS: β̂ ols = (Z  Z )−1 Z  y. If
the observations have unknown covariance matrix  then
1
Var(β̂ ols ) =
n



ZZ
n

−1

Z  Z
n



ZZ
n

−1

.

(3.6.14)

White (1980) demonstrated that to estimate this covariance matrix, one need
only obtain a consistent estimator of plim (Z  Z /n) and not of  itself. In the
ˆ /n where the diagonal elements of
case of heteroskedasticity, he proposed Z  Z
ˆ
 are the squares of the estimated OLS residuals ε̂i2 . (Off-diagonal elements are
P
ˆ /n →
zero.) He then showed that Z  Z /n − Z  Z
0. Substitution into (3.6.14)
yields a heteroskedasticity-consistent covariance matrix estimator fo β̂ OLS .
Note that in the case of heteroskedasticity, the interior matrix of (3.6.14) may
be written as
Z  Z
1 2 
=
σi z i z i ,
n
n

(3.6.15)

where σi2 are the diagonal entries in  and z i is the ith row of the Z matrix.
The estimate may be computed using
ˆ
Z  Z
1 2 
=
ε̂i z i z i .
n
n

(3.6.16)

Let us return to the partial linear model y = Zβ + f (x) + ε and suppose
that the residuals have covariance matrix . Then the covariance matrix of the
OLS estimator (3.6.8) is approximately

(Z − ĝ(x)) (Z − ĝ(x)) −1
n



(Z − ĝ(x)) (Z − ĝ(x)) (Z − ĝ(x)) (Z − ĝ(x)) −1
×
,
n
n

Var(β̂ OLS ) ∼
=

1
n



(3.6.17)
and in the case of heteroskedasticity, the interior matrix may be estimated using
ˆ
(Z − ĝ(x)) (Z
− ĝ(x))
1 2
=
ε̂i (z i − ĝ(xi )) (z i − ĝ(xi )),
n
n
(3.6.18)

Introduction to Smoothing

51

ˆ is a diagonal matrix with entries ε̂i2 , the estimated residuals from
where 
(3.6.7), and ĝ(xi ) is the ith row of ĝ(x).17
n× p

3.6.5 Heteroskedasticity and Autocorrelation
Return once again to the pure linear regression model y = Zβ + ε. Then for
arbitrary residual covariance matrix , the variance of the OLS estimator is
given by (3.6.14). Furthermore, the interior matrix of (3.6.14) may be written as
Z  Z
1
σij z i z j ,
(3.6.19)
=
n
n i, j
where σij denotes elements of  and z i , z j are the ith and jth rows of the
Z matrix. If the covariances are zero for observations that are more than say
L periods apart, then (3.6.19) becomes
Z  Z
1 
σij z i z j ,
(3.6.20)
=
n
n i, j
|i− j|≤L

which may be estimated using
ˆ
Z  Z
1 
=
ε̂i ε̂j z i z j .
n
n i, j

(3.6.21)

|i− j|≤L

ˆ is the matrix ε̂ ε̂ with all terms whose expectation is known to be zero
Here 
set to zero. (Thus, all entries more than L subdiagonals from the main diagonal
are zero.) The results may be found in White (1985).
Consistency is retained even if distant correlations are never actually zero but
die off sufﬁciently quickly. In this case, L is permitted to increase with sample
size.
As a practical matter, however, (3.6.21) need not be positive deﬁnite. To resolve this, Newey and West (1987) proposed a modiﬁcation as follows. Rewrite
(3.6.21) as
1 
ε̂i ε̂j z i z j
n i, j
|i− j|≤L

=
17

L
n
1 2 
1 

ε̂i z i z i +
ε̂i ε̂i− (z i z i− + z i−
z i ),
n i
n =1 i=+1

(3.6.22)

ˆ as the matrix ε̂ ε̂  with all terms whose
Here and in the next section it is convenient to think of 
expectation is known to be zero set to zero (which in the case of pure heteroskedasticity means
that all off-diagonal terms are set to zero).

52

Semiparametric Regression for the Applied Econometrician

but now insert weights to obtain
 
L 
n
1 2 

1

1−
ε̂i z i z i +
ε̂i ε̂i− (z i z i− + z i−
z i ).
n i
n =1
L + 1 i=+1
(3.6.23)
Thus, the matrix (3.6.20) may be estimated using either the White (1985)
estimator in (3.6.21) or the Newey–West estimator in (3.6.23).
Return once again to the partial linear model. For arbitrary , the OLS
estimator (3.6.8) has a covariance matrix given by (3.6.17). Analogously to
(3.6.21), the interior matrix may be estimated using (White 1985)
ˆ
(Z − ĝ(x)) (Z
− ĝ(x))
1 
ε̂i ε̂ j (z i − ĝ(xi )) (z j − ĝ(x j )),
=
n
n i, j
|i− j|≤L

(3.6.24)
ˆ equals ε̂ ε̂ with all entries more than L subdiagonals from the main
where 
diagonal set to zero. Alternatively, following (3.6.23), the interior matrix may
be estimated using the Newey–West approach as follows:
1 2
ε̂ (z i − ĝ(xi )) (z i − ĝ(xi ))
n i i
 
L 
n

1
1−
+
ε̂i ε̂i− ((z i − ĝ(xi )) (z i− − ĝ(xi− ))
n =1
L + 1 i=+1
+ (z i− − ĝ(xi− ) (z i − ĝ(xi ))).
3.7
3.7.1

(3.6.25)

Derivative Estimation
Point Estimates

A variety of derivative estimators based on kernel procedures have been proposed. Conceptually, the simplest approach is to take derivatives of the kernel
estimate. This may be done analytically or numerically. For example, suppose
fˆ is a kernel estimate of f, and let h → 0 as sample size increases. Ullah (1988)
and Rilstone and Ullah (1989) proposed
1 ˆ

fˆ (x) =
( f (x + h) − fˆ(x − h))
2h

(3.7.1)

and

fˆ (x) =

1
( fˆ(x + 2h) − 2 fˆ(x) + fˆ(x − 2h)).
(2h)2

(3.7.2)

Introduction to Smoothing

53

However, the rate at which the optimal bandwidth goes to zero depends on the
derivative being estimated. Generally, higher-order derivatives require greater
smoothing of the function itself and hence a bandwidth that shrinks to zero
more slowly.18
Nonparametric least-squares estimators of the kind discussed in Section 3.3
(and later in 5.1) can also be differentiated to produce estimates of derivatives.
Generally it is required that the measure of smoothness incorporate at least two
more derivatives than the derivative that is of interest. For example, if one is
interested in the ﬁrst derivative, than the Sobolev norm (3.3.1) should be of
order three or more. If one is interested in the second derivative (as we will be
later when estimating the state price density in an option pricing model), then
the norm should be of order four or more.19
3.7.2

Average Derivative Estimation

Average derivatives or functionals of average derivatives are also frequently of
interest. Consider
E( f  (x)) =

f  (x) p(x)d x,

(3.7.3)

where p(x) is the density function of x. Let p̂(x) be an estimate of p(x). A
simple “direct” estimator may be obtained using

fˆ (x) p̂(x)d x.

(3.7.4)

Or, one can use
n
1  ˆ
f (xi ).
n i=1

(3.7.5)

An alternative “indirect” estimator was proposed by Härdle and Stoker (1989).
Suppose one is interested in the regression function on the interval [a,b] and
that the density function is zero at the endpoints, that is, p(a) = p(b) = 0. Let
s(x) be the negative of the score function, that is, s(x) = − p  (x)/ p(x). Then
an alternative expression for the average derivative is given by
E( f  (x)) = E(s(x)y).
18

19

(3.7.6)

See, for example, Härdle (1990, Proposition 3.1.2, p. 33), which imples that estimation of the ﬁrst
derivative has an associated optimal bandwidth of order O(n −1/7 ); for the second derivative, it
is O(n −1/9 ). Recall that when estimating the function itself, the optimal bandwidth is O(n −1/5 ).
Thus, the usual spline estimators that penalize the second derivative (see, e.g., (3.3.7)) will
not in general produce consistent estimators of ﬁrst and second derivatives. For spline estimators that penalize higher-order derivatives, see Jim Ramsay’s S-Plus routines available at
www.psych.mcgill.ca/faculty/ramsay/ramsay.html.

54

Semiparametric Regression for the Applied Econometrician

The result is obtained using integration by parts, in particular
E(s(x)y) = E(s(x) f (x)) = −
= − p(x) f (x)| ab +

p  (x) f (x)d x
p(x) f  (x)d x

(3.7.7)

= E( f  (x))
because the density of x is zero at the endpoints. The sample analogue of (3.7.6)
is given by
−

n
1  p̂  (xi )
yi .
n i=1 p̂(xi )

(3.7.8)

In regions of sparse data, fˆ , the estimate of the derivative used in (3.7.5)
may be poor. Similarly, the estimated value of the density p̂(xi ) appearing in
the denominator of (3.7.8) may be close to zero, leading to inaccurate estimates
of the average derivative. To mitigate this problem, the estimators are usually
modiﬁed to remove such observations. In particular, (3.7.5) becomes
n
1  ˆ
f (xi )Ii ,
n i=1

(3.7.5a)

and (3.7.8) becomes
−

n
1  p̂  (xi )
yi Ii ,
n i=1 p̂(xi )

(3.7.8a)

where Ii is an indicator function that is zero if the density p̂(xi ) is close to zero.
3.8 Exercises20
1. Derive the approximation used in Footnote 3 to justify (3.1.5). In particular, prove
that
ī
k2 − 1
1
(x j − xi )2 =
,
k
12n 2
j= i

where x1 , . . . , xn are equally spaced on the unit interval.
20

Data and sample programs for empirical exercises are available on the Web. See the Preface for
details.

Introduction to Smoothing

55

2. Properties of Kernel Estimators: Suppose x’s are uniformly distributed on the unit
interval and we are using the uniform kernel to estimate the model y = f (x) + ε.
By adapting the results in (3.1.5)–(3.1.11),
(a) derive conditions under which the kernel estimator is consistent at a point, say
xo ;
(b) derive the optimal rate of convergence for the bandwidth parameter λ;
(c) derive a 95 percent conﬁdence interval for f (xo ).
3. Moving Average Estimation: The purpose of this exercise is to perform a nonparametric regression using the simple moving average estimator and to construct pointwise
conﬁdence intervals.
(a) Open the South African survey data and select the subset of the data where the
number of adults is 1 and the number of children 0. You will need data on the
food share of total expenditure (FoodShr) and the log of total expenditure (ltexp).
The number of observations will be 1,109.
(b) Sort the data so that ltexp is in increasing order and produce a scatterplot of the
data.
(c) Using k = 51, apply the moving average estimator (3.1.1). You will obtain an
estimate of the food share function f for observations 26 through 1,084. Superimpose the results on your scatterplot.
(d) Calculate 95 percent pointwise conﬁdence intervals using (3.1.4) and superimpose these on a plot of the estimated regression function. Your results should be
similar to those in Figure 3.1.
4. Kernel Estimation, Conﬁdence Intervals, and Bands: The purpose of this exercise is
to perform a kernel regression and to construct pointwise conﬁdence intervals and a
uniform conﬁdence band for the South African food share data on single individuals
(see Exercise 3).
(a) We will use the triangular kernel (Figure 3.2). Show that K 2 (u) = 2/3 and that
K  (u)2 = 2.
(b) Estimate the share function f using the triangular kernel and a bandwidth of .5.
(c) Estimate the residual variance σε2 using the average sum of squared residuals
from this regression.
(d) Calculate s fˆ at all the points at which you have data using (3.2.8).
(e) Plot fˆ and the 95 percent pointwise conﬁdence intervals using (3.2.10).
(f) Calculate the 95 percent uniform conﬁdence band and superimpose it on your
previous plot. Your results should be similar to those in Figure 3.3.
5. Estimation Using Spline and Lowess Estimators: The purpose of this exercise is to
produce spline and lowess estimates using the South African food share data on single
individuals (see Exercise 3).
(a) Estimate the share function f using a spline estimator (e.g., smooth.spline in
S-Plus).
(b) Estimate f using a lowess estimator (e.g., lowess in S-Plus).

56

Semiparametric Regression for the Applied Econometrician
(c) Plot the kernel estimator of f from Exercise 4(b) above. Superimpose the spline
and lowess estimates. Your results should be similar to Figure 3.4.

6. Estimation Using the Super-smoother: A smoother related to our simple moving
average in Section 3.1 is the super-smoother. However, rather than taking a moving
average, one performs least squares on k consecutive observations. Furthermore,
at each point, cross-validation is used to select k. By allowing variability in the
span (deﬁned to be k/n), regions where the function has greater ﬂuctuation will
be estimated using observations that tend to be closer together, thus reducing bias.
We will use the South African food share data on single individuals (see Exercise 3).
(a) Estimate the share function f using the super-smoother (e.g., supsmu in S-Plus)
and plot the results.
(b) Superimpose the moving average estimate from Exercise 3(c) and the kernel
estimate from Exercise 4(b) above.
7. Cross-Validation Simulation: Let yi = xi + εi , εi ∼ N (0,.01), i = 1, . . . , 25, where
the xi are equally spaced on the interval [0,1]. Generate a data set using this model.
Using a triangular kernel, calculate the cross-validation function (3.5.3) for values of
the bandwidth λ in the range [.1,.5] in increments of say .025. Plot the cross-validation
function, which should be similar to the upper panel of Figure 3.5.
8. Cross-Validation and Engel Curve Estimation: Open the South African food share
data on single individuals (see Exercise 3). Using a triangular kernel, calculate the
cross-validation function (3.5.3) for values of the bandwidth λ in the range [.2,.6] in
increments of say .025. Plot the cross-validation function, which should be similar
to that of Figure 3.6.
9. Engel Curves with Heteroskedastic Variance
(a) Using South African food share data on single individuals, apply a nonparametric
smoother (such as kernel or loess) to estimate the Engel curve y = f (x) + ε,
where x is “log expenditure,” and y is “food share.”
(b) Calculate the residuals from this procedure ε̂1 , . . . , ε̂n , and use a nonparametric
smoother to estimate the model ε̂ 2 = g(x) + v. You have estimated a model for
the residual variance as a function of x. Plot your results.

4

Higher-Order Differencing
Procedures

4.1 Differencing Matrices
4.1.1 Deﬁnitions
In the previous chapter we introduced the idea of smoothing, which was used to
estimate a nonparametric regression function. Now we will return to the idea of
differencing, which in Chapter 1 was used to remove the nonparametric effect
from a regression model.
Let m be the order of differencing and d0 , d1 , . . . , dm differencing weights
that satisfy the conditions
m

j=0

dj = 0

m


d 2j = 1.

(4.1.1)

j=0

The purpose of these restrictions will be made clear shortly. Deﬁne the differencing matrix


d0 , d1 , d2 , . . dm , 0, . . . . . . . . . . . . . . . . . . . , 0
 0, d0 , d1 , d2 , . . dm , 0, . . . . . . . . . . . . . . . . . , 0 




:
:


:
:


 0, . . . . . . . . . . . . . . . . . . 0, d , d , d , . . d , 0 
(4.1.2)
0 1 2
m
.
D =


n×n
0,
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0,
d
,
d
,
d
,
.
.
d

0 1 2
m 


 0, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . , 0 


:
:
0, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . , 0
The last m rows have been ﬁlled with zeros so that D is square. It will be
convenient to use lag matrices L i . For i > 0, deﬁne L i to be a square matrix with
0’s everywhere except on the ith diagonal below the main diagonal, where it has
1’s. If i < 0, L i has 1’s on the ith diagonal above the main diagonal. The matrix
.
L 0 is deﬁned to be the usual identity matrix, L i = L −i , and L i L j = L i+ j . It is
57

58

Semiparametric Regression for the Applied Econometrician

.
evident from (4.1.2) that (except for end effects, which we denote using =) the
differencing matrix of order m is a weighted sum of lag matrices, that is,
.
D = d0 L 0 + d1 L 1 + · · · + dm L m .
(4.1.3)
4.1.2 Basic Properties of Differencing and Related Matrices
We will need the matrix D  D, which has a symmetric band structure with 1’s
.
on the main diagonal. Hence, tr(D  D) = n. As
one moves away from the main
diagonal, consecutive diagonals take values m−k
j=0 d j d j+k k = 1, . . . , m. The
remainder of the matrix is zero. Equivalently, using (4.1.3) and the properties
of lag matrices, one can obtain


.
D  D = L 0 + (L 1 + L 1 )
d j d j+1 + (L 2 + L 2 )
d j d j+2 + · · · ·
m−1

m−2

j=0

j=0

+ (L m−1 + L m−1 )

1


d j d j+m−1 + (L m + L m ) d0 dm .

(4.1.4)

j=0

Because band structure is preserved by matrix multiplication, the matrix
D  D D  D will also have this property as well as being symmetric. The value on
the main diagonal may be determined by multiplying the expansion in (4.1.4) by
itself. Note that only products of the form L k L k and L k L k yield (except for end
effects) the identity matrix L 0 . Thus, the common diagonal value of D  D D  D
will be the sum of the coefﬁcients of L 0 , L 1 L 1 , L 1 L 1 , L 2 L 2 , L 2 L 2 , . . . , L m L m ,
L m L m , that is,
 m−k
2
m


.


[D D D D]ii = 1 + 2
d j d j+k .
(4.1.5)
k=1

j=0

A particularly useful quantity will be
 m−k
2
m


δ=
d j d j+k .
k=1

(4.1.6)

j=0

.
.
We now have [D  D D  D]ii = 1 + 2δ and tr(D  D D  D) = n(1 + 2δ).1
4.2 Variance Estimation
4.2.1 The mth-Order Differencing Estimator
Let us return to the problem of estimating the residual variance in a pure nonparametric regression model yi = f (xi ) + εi , where ε | x is distributed with
1

The trace of D  D D  D may be obtained alternatively as follows. Because for any symmetric
matrix A, tr (AA) is the sum of squares of elements of A, we may use (4.1.4) to conclude that
.
tr(D  D D  D) = n(1 + 2δ).

Higher-Order Differencing Procedures

59

mean 0, variance σε2 , and E(ε 4 | x) = ηε , and f has ﬁrst derivative bounded.
Given observations on the model (y1 , x1 ) . . . . (yn , xn ), where the x’s have been
reordered so that they are in increasing order, deﬁne y  = (y1 , . . . , yn ) and
f (x) = ( f (x1 ), . . . , f (xn )). In vector notation we have
y = f (x) + ε.

(4.2.1)

Applying the differencing matrix, we have
Dy = D f (x) + Dε.

(4.2.2)

A typical element of the vector Dy is of the form
d0 yi + · · · + dm yi + m = d0 f (xi ) + · · · + dm f (xi + m )
+ d0 εi + · · · + dm εi+m ,

(4.2.3)

and thus the role of the constraints (4.1.1) is now evident. The ﬁrst condition
ensures that, as the x’s become close, the nonparametric effect is removed. The
second condition ensures that the variance of the weighted sum of the residuals
remains equal to σε2 .
The mth-order differencing estimator of the residual variance is now deﬁned
to be
2
sdiff

n−m
1
1
=
(d0 yi + d1 yi+1 + · · · + dm yi+m )2 = y  D  Dy. (4.2.4)
n i=1
n

4.2.2 Properties
Because differencing removes the nonparametric effect, in large samples we
have
2 ∼ 1  
sdiff
= ε D Dε.
n

(4.2.5)

Using the mean and variance of a quadratic form (see Appendix B,
Lemma B.1), we have
 2 
1
.
∼
E sdiff
= σε2 tr(D  D) = σε2
n
and


 2 

1 
2
∼
Var sdiff
ηε − 3σε4 + σε4 tr(D  D D  D)
=
n
n

1
=
Var(ε 2 ) + 4σε4 δ ,
n

where δ is deﬁned in (4.1.6) and ηε = E(ε 4 | x).

(4.2.6)

(4.2.7)

60

Semiparametric Regression for the Applied Econometrician

4.2.3 Optimal Differencing Coefﬁcients
From (4.2.7) it is evident that, to minimize the large sample variance of the
differencing estimator, one needs to make δ as small as possible. Using time
series techniques, Hall et al. (1990) have shown that if the d j are selected to
minimize δ, then
m−k

j=0

d j d j+k = −

1
, k = 1, 2, . . . , m
2m

and

δ=

1
.
4m

(4.2.8)

In this case, matrix D  D has (except for end effects) 1’s on the main diagonal,
1
− 2m
on the m adjacent diagonals, and 0’s elsewhere. That is,
1
.
D D = L 0 −
(L 1 + L 1 + · · · + L m + L m ),
2m
.
so that tr(D  D) = n.2 Using (4.25) yields,

1 
2 ∼ εε
sdiff
−
ε (L 1 + L 1 + · · · + L m + L m )ε.
=
n
2mn
Applying (4.2.7) and δ = 1/4m from (4.2.8), we have


 2  1
σε4
2
∼
Var sdiff =
Var (ε ) +
.
n
m

(4.2.9)

(4.2.10a)

(4.2.10b)

On the other hand, if the regression function were parametric (e.g., if it were
known to be linear and we used a conventional OLS estimator), then
 
n
n
1
1
1 2
2
sOLS
, (4.2.11a)
=
(yi − γ̂1 − γ̂2 xi )2 ∼
εi + O P
=
n i=1
n i=1
n
in which case
 2  1
∼
Var sOLS
(4.2.11b)
= Var(ε 2 ).
n
Comparing the variances of the two residual estimators (4.2.10b) and
(4.2.11b), we see that, as the order of differencing m increases, the variance of
the differencing estimator approaches that of parametric estimators.
Optimal differencing weights do not have analytic expressions but may be
calculated easily using standard optimization techniques. Hall et al. (1990)
present weights to order m = 10. With minor modiﬁcations, these are reproduced in Table 4.1. Appendix C discusses calculation of optimal weights and
contains weights for certain higher values of m.
2

The matrix D  D D  D has a symmetric band structure with 1 + 1/2m on the main diagonal so
.
that tr(D  D D  D) = n(1 + 1/2m).

Higher-Order Differencing Procedures

61

Table 4.1. Optimal differencing weights.a
m

(d0 , d1 , . . . , dm )

1
2
3
4
5
6
7
8
9
10

(0.7071,−.0.7071)
(0.8090,−0.5000,−0.3090)
(0.8582,−0.3832,−0.2809,−0.1942)
(0.8873,−0.3099,−0.2464,−0.1901,−0.1409)
(0.9064,−0.2600,−0.2167,−0.1774,−0.1420,−0.1103)
(0.9200,−0.2238,−0.1925,−0.1635,−0.1369,−0.1126,−0.0906)
(0.9302,−0.1965,−0.1728,−0.1506,−0.1299,−0.1107,−0.0930,−0.0768)
(0.9380,−0.1751,−0.1565,−0.1389,−0.1224,−0.1069,−0.0925,−0.0791,−0.0666)
(0.9443,−0.1578,−0.1429,−0.1287,−0.1152,−0.1025,−0.0905,−0.0792,−0.0687,−0.0588)
(0.9494,−0.1437,−0.1314,−0.1197,−0.1085,−0.0978,−0.0877,−0.0782,−0.0691,−0.0606,−.0527)

a

In contrast to those in Hall et al. (1990), all the optimal weight sequences provided here decline
in absolute value toward zero.

4.2.4 Moving Average Differencing Coefﬁcients
Recall that our objective is to remove the nonparametric effect at a point, say
xi . Suppose we average consecutive observations centered at xi (omitting the
one at xi ) and subtract the average from yi . In this case, the differencing matrix
will have the form
1
D = 
n×n
1 + m1
 1

− m , . . . , − m1 , 1, − m1 . . . − m1 , 0, . . . . . . . . . . . . . , 0


 0, − 1 , . . . , − 1 , 1, − 1 . . . − 1 , 0, . . . . . . . . . . . . , 0 


m
m
m
m


:
:




:
:


:
:




1
1
1
1
× 0, . . . . . . . . . . . . 0, − m , . . . , − m , 1, − m . . . − m , 0 .




 0, . . . . . . . . . . . . . . 0, − m1 , . . . , − m1 , 1, − m1 . . . − m1 


 0, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . , 0 






:
:
0, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . , 0
(4.1.2a)
Again we have ﬁlled the last m rows with zeros so that D is square.
The differencing weights sum to zero and the constant multiplying the
matrix ensures that their sum of squares is 1. The value of δ – which through
(4.2.7) plays an important role in large sample efﬁciency – does not have a
simple closed-form solution such as the one available for optimal differencing

62

Semiparametric Regression for the Applied Econometrician

Table 4.2. Values of δ for alternate differencing coefﬁcients.a
m

Optimal

Moving average

2
4
6
8
10
20
50
100
200
500

.12500
.06250
.04167
.03125
.02500
.01250
.00500
.00250
.00125
.00050

.47222
.22500
.14683
.10880
.08636
.04246
.01680
.00837
.00418
.00167

a

See (4.1.6) for deﬁnition of δ.

coefﬁcients. (In that case it is 1/4m.) Nevertheless, its value can be computed
directly using (4.1.6). Table 4.2 lists values of δ for moving average and optimal
differencing weights. The moving average values are larger. On the other hand,
symmetry of the moving average weights is likely to reduce bias relative to the
optimal weights which decline monotonically in one direction. We compare
alternative differencing weights in Section 4.10.
4.2.5 Asymptotic Normality
Proposition 4.2.1: Let d0 , d1 , . . . , dm be arbitrary differencing
weights satisfying (4.1.1); then,



1  2
n /2 sdiff
− σε2 ∼ N 0, ηε − σε4 + 4σε4 δ


= N 0, ηε − 3σε4 + 2σε4 (1 + 2δ) ,
(4.2.12a)
which, if one uses optimal differencing weights, becomes



σ4
1  2
n /2 sdiff
− σε2 ∼ N 0, ηε − σε4 + ε .
m

(4.2.12b)

To make use of these results, we will need a consistent estimator of ηε = E(ε 4 )
for which we will use fourth-order powers of the differenced data. To motivate
the estimator, it is convenient to establish the following:
 m

 m−1

m



E(d0 εi + · · · + dm εi+m )4 = ηε
di4 + 6σε4
di2
d 2j .
i=0

i=0

j=i+1

(4.2.13)

Higher-Order Differencing Procedures

63

This result may be obtained by ﬁrst noting that, when the left-hand side is
expanded, only two types of terms will have nonzero expectations: those that
involve the fourth power of a residual (e.g., Eεi4 = ηε ) and those that involve
2
4
products of squares of residuals (e.g., Eεi2 εi+
j = σε , j = 0). Equation (4.2.13)
is then obtained by summing the coefﬁcients of such terms. We now have the
following result.
Proposition 4.2.2: Let
 2 2 m−1 2 m
1 n−m
(d0 yi + · · · + dm yi+m )4 − 6 sdiff
d
d2
i=1
i=0 i
j=i+1 j
n
η̂ε =
;
m
di4
i=0

(4.2.14a)
P

then, η̂ε → ηε .
Equations (4.2.13) and (4.2.14a) are valid for arbitrary differencing weights.
If the order of differencing is large (say m ≥ 25 ), then the denominator approaches 1 and the right-hand side of the numerator approaches 0; thus
n−m
1
η̂ε ∼
(d0 yi + · · · + dm yi + m )4 .
=
n i=1

(4.2.14b)

These results may be used to test equality of residual variances for two possibly different regression models: y A = f A (x A ) + ε A and y B = f B (x B ) + ε B .
Let s 2A , η̂ A , s B2 , η̂ B be optimal differencing estimators of the residual variances
and fourth-order moments obtained using (4.2.4) and (4.2.14a). Then, Propositions 4.2.1 and 4.2.2 imply that, under the null hypothesis,


s 2A − s B2
η̂ A + (1/m − 1)s 4A η̂ B + (1/m − 1)s B4
+
nA
nB

1/2 ∼ N (0, 1).

(4.2.15)

4.3 Speciﬁcation Test3
4.3.1 A Simple Statistic
We remain with the pure nonparametric model y = f (x) + ε, where f has ﬁrst
derivative bounded, E(ε | x) = 0, and Var(ε | x) = σε2 . Let h(x, γ ) be a known
3

A variety of procedures are available for testing a parametric null against a nonparametric
alternative. See Chapter 6.

64

Semiparametric Regression for the Applied Econometrician

function of x and an unknown parameter γ (h can of course be linear). We wish
to test the null hypothesis that the regression function has the parametric form
h(x, γ ) against the nonparametric alternative f (x). Let γ̂LS be obtained using,
for example, parametric nonlinear least squares. Deﬁne the restricted estimator
of the residual variance
1
2
sres
(yi − h(xi , γ̂ L S ))2 .
=
(4.3.1)
n
Proposition 4.3.1: Suppose H0 : f (x) = h(x, γ ) is true, where h is
2
a known function. Deﬁne sres
as in (4.3.1). For arbitrary differencing weights
satisfying (4.1.1) we have

 1/2  2
2
sres − sdiff
n
D
→ N (0, 1).
(4.3.2a)
2
4δ
sdiff
If one uses optimal differencing weights, the statistic becomes
 2

2
D
1/2 sres − sdiff
(mn)
→ N (0, 1).
2
sdiff

(4.3.2b)

2
2
In the denominator, sdiff
may be replaced by sres
because, under the null, both
estimators of the residual variance are consistent.
A test of the signiﬁcance of x is a special case of the preceding procedure. In
this case, f is a constant function; thus, the restricted estimator of the regression
function is just the sample mean of the yi .

4.3.2 Heteroskedasticity
Suppose now the residuals are independent but heteroskedastic with unknown
2
covariance matrix  and one uses optimal differencing weights to obtain sdiff
.
The speciﬁcation test in Proposition 4.3.1 may be extended as follows. If the
null hypothesis is true and optimal differencing coefﬁcients are used, then
 2

2
D
1/2 sres − sdiff
(mn)
→ N (0, 1),
(4.3.2c)
ξ̂ 1/2
where
1
ξ̂ =
m


 
1 2 2
1
2 2
ε̂i ε̂i−1 + · · · · +
ε̂i ε̂i−m .
n
n

(4.3.2d)

As in Proposition 4.3.1, the result is readily modiﬁed for arbitrary differencing
coefﬁcients.

Higher-Order Differencing Procedures

65

4.3.3 Empirical Application: Log-Linearity of Engel Curves

0.6

0.8

We return to our South African data on single individuals and ask whether a
model that is linear in the log of expenditures provides an adequate representation. There is a considerable body of evidence in the empirical literature to
support the proposition that food Engel curves are approximately linear. On the
other hand, as we will see later, nonlinearities in Engel curves are important
in identifying equivalence scales. Figure 4.1 displays a kernel estimate and the
linear OLS estimate. The kernel estimate lies close to the linear model except
at the right tail of the income distribution. (At the left tail there also appears to
be a departure from linearity, but it is modest.) Furthermore, there are relatively
few observations in the right tail of the distribution, and, as a consequence,
the nonparametric model is not being estimated precisely (see the conﬁdence
intervals in Figure 3.3). Suppose we compare the ﬁt of the linear and kernel
models. The former has an R 2 of .477. If we estimate the residual variance

0.4
0.0

0.2

food share

Kernel
Linear

5.5

6.0

6.5

7.0

7.5

8.0

8.5

log of total expenditure
2
Optimal differencing estimate of residual variance using m = 25: sdiff
= .01941. Implied R 2 =
2 =
.490. Linear model: FoodShr = 1.75 − .203 ∗ ltexp. Standard errors in parentheses. sres
(.043)
(.0064)
.01993, R 2 = .477.

Figure 4.1. Testing linearity of Engel curves.

66

Semiparametric Regression for the Applied Econometrician

using m = 25 and optimal differencing coefﬁcients (see (4.2.4)), then the R 2 ,
2
which we deﬁne to be 1 − sdiff
/s y2 , increases to .490. This is not a dramatic
increase in explanatory power. A formal test of the linear speciﬁcation may
be implemented using (4.3.2b), yielding a value of 4.45, which suggests rejection of the null. In contrast, a test of signiﬁcance of the expenditure variable
using (4.3.2b) yields an overwhelming rejection of the null with a statistic of
160.2.
4.4 Test of Equality of Regression Functions4
4.4.1 A Simpliﬁed Test Procedure
Suppose one has data (y A1 , x A1 ), . . . , (y An A , x An A ) and (y B1 , x B1 ), . . . , (y Bn B ,
x Bn B ) from two possibly different regression models A and B. We emphasize
that the data have already been ordered so that within each subpopulation the x’s
are in increasing order. Let n = n A + n B be the total number of observations.
The basic models are
y Ai = f A (x Ai ) + ε Ai
y Bi = f B (x Bi ) + ε Bi

(4.4.1)

where, given the x’s, the ε’s have mean 0, variance σε2 , and are independent
within and between populations; f A and f B have ﬁrst derivatives bounded.
Using (4.2.4), deﬁne consistent differencing estimators of the variance, say s 2A
and s B2 . Let sw2 be the “within” estimator of σε2 obtained by taking the weighted
sum of the two individual estimates:
sw2 =

nA 2 nB 2
s +
s .
n A
n B

(4.4.2)

Concatenate the data on the dependent variable to obtain the n-dimensional
column vectors x = (x A1 , . . . , x An A , x B1 , . . . , x Bn B ) and y = (y A1 , . . . , y An A ,
y B1 , . . . , y Bn B ) . Since under the null hypothesis f A = f B , we may estimate
the common function using, say, a kernel smoother. Furthermore, if an optimal
bandwidth is used, then the average sum of squared residuals from this restricted
4

A number of procedures are available for testing equality of nonparametric regression functions.
These include Hall and Hart (1990), Härdle and Marron (1990), King, Hart, and Wehrly (1991),
Delgado (1993), Kulasekera (1995), Pinkse and Robinson (1995), Young and Bowman (1995),
Baltagi, Hidalgo, and Li (1996), Kulasekera and Wang (1997), Koul and Schick (1997), Fan
and Lin (1998), Munk and Dette (1998), Lavergne (2001) and Hall and Yatchew (2002). See
also Hart (1997, p. 236).

Higher-Order Differencing Procedures

regression will satisfy


n
1 2 P
1/2
2
n
sres −
ε → 0.
n i=2 i

67

(4.4.3)

This is true because the estimator satisﬁes the optimal rate of convergence
(3.1.10). Thus, for ﬁxed order of differencing m, we may apply the speciﬁcation
2
test in Proposition 4.3.1, setting sdiff
= sw2 .
4.4.2 The Differencing Estimator Applied to the Pooled Data
An alternative test procedure applies the differencing estimator to the pooled
data. Deﬁne Pp , the “pooled” permutation matrix, to be the matrix that reorders
the pooled data so that the x’s are in increasing order. Thus, if x ∗ = Pp x, then
the consecutive elements of the reordered vector x ∗ are in increasing order.
Apply the differencing estimator of the variance (4.2.4) to the reordered data
to obtain
s 2p =

1   
y Pp D D Pp y,
n

(4.4.4)

the pooled estimator of the residual variance.
Proposition 4.4.1: For arbitrary differencing coefﬁcients satisfying
D
2
(4.1.1), we have n 1/2 (sdiff
−σε2 ) → N (0, ηε − 3σε4 +2σε4 (1+2δ)). If, in addition,
1/2 2
2
f A = f B , then n (s p − σε ) has the same approximate distribution. If optimal
differencing coefﬁcients are used, then the approximate distribution becomes
N (0, ηε + (1/m − 1)σε4 ).
The asymptotic variances of sw2 , s 2p may be obtained using (4.2.7) and
(4.2.10b) (see also Appendix B, Lemma B.1, or Hall et al. 1990). Asymptotic
normality follows from ﬁnitely dependent central limit theory.
We will now consider a statistic based on the difference of the pooled and
within estimates of the variance. Deﬁne
 . 1
1 
ϒ ≡ n /2 s 2p − sw2 = 1/ y  [Pp D  D Pp − D  D]y.
n 2

(4.4.5)

The second occurrence of D  D corresponds to the within estimator and is –
more precisely – a block diagonal matrix with two D  D matrices, one for each
of the two subpopulations. (Except for end effects, a block diagonal D  D matrix
is identical to the second such matrix in (4.4.5).)

68

Semiparametric Regression for the Applied Econometrician

Proposition 4.4.2: Let Q ϒ = Pp D  D Pp −D  D and π̂ϒ = tr(Q ϒ Q ϒ )/
P
n. Suppose π̂ϒ → πϒ > 0. Then, under the null hypothesis that f A = f B ,
 D 

1 
(4.4.6)
ϒ = n /2 s 2p − sw2 → N 0, 2πϒ σε4 .
D

Thus, ϒ/sw2 (2π̂ϒ )1/2 → N (0, 1), and one would reject for large positive values
of the test statistic. The quantity πϒ no longer admits the simple interpretation
associated with π in Chapter 1; however, it remains between 0 and 1 (see
Yatchew 1999). The condition πϒ > 0 ensures that, in the pooled reordered
data, the proportion of observations that are near observations from a different
probability law does not go to 0. (Heuristically, this requires that the data
intermingle upon pooling and that neither n A /n nor n B /n converge to zero.)
Ideally, of course, one would like the pooled x’s to be well intermingled so
that s 2p contains many terms incorporating f s (x ∗j ) − f t (x ∗j−1 ), where s = t and ∗
denotes data reordered after pooling.
4.4.3 Properties
The test procedure described in the last subsection is consistent. For example,
suppose we use ﬁrst-order differencing (m = 1), x A and x B are independently
and uniformly distributed on the unit interval, and an equal number of observations are taken from each subpopulation (i.e., n A = n B = 1/2 n). If f A = f B , then
the within estimator in (4.4.2) remains consistent, whereas the pooled estimator
in (4.4.4) converges as follows
s 2p → σε2 + 1/4

( f A (x) − f B (x))2 d x

(4.4.7)

so that the mean of ϒ deﬁned in (4.4.6) diverges (see Yatchew 1999). In general,
power depends not only on the difference between the two regression functions
but also on the degree to which data are generated from both populations at
points where the difference is large. For ﬁxed differencing of order m, the
procedure will detect local alternatives that converge to the null at a rate close
to n −1/4 . The rate may be improved by permitting the order of differencing to
grow with sample size. Nonconstant variances across equations can readily be
incorporated (see Yatchew 1999).
The test procedure may be applied to the partial linear model. Consider
y A = z A β A + f A (x A ) + ε A and y B = z B β B + f B (x B ) + ε B . Suppose one obtains
n 1/2 - consistent estimators of β A and β B (e.g., by using the differencing estimator
in Section 4.5). To test f A = f B one can apply (4.4.2), (4.4.4), and (4.4.5) to
y A − z A β̂ A ∼
= f A (x A ) + ε A and y B − z B β̂ B ∼
= f B (x B ) + ε B without altering
the asymptotic properties of the procedure. Alternatively, one can apply the
simpliﬁed procedure outlined in Section 4.4.1 that uses Proposition 4.3.1.

Higher-Order Differencing Procedures

69

4.4.4 Empirical Application: Testing Equality of Engel Curves
Equivalence scales are used to compare the welfare of families of different
composition. For example, if a single person spends $50,000 and a couple
needs $82,500 to achieve the same standard of living, we say the equivalence
scale is 1.65. Engel’s method for constructing equivalence scales is premised
on the idea that two households are equally well off if they spend the same
proportion of their total expenditures on food. The existence of equivalence
scales and how to calculate them have been a matter of much debate, and we
will devote considerable effort later to their estimation.
The upper panel of Figure 4.2 illustrates the food Engel curves for singles
and couples with no children. There are 1,109 observations in the ﬁrst group
and 890 in the second. Geometrically, Engel’s method asks whether a leftward
horizontal shift of the “couples” Engel curve would cause it to be superimposed
on the “singles” Engel curve. The magnitude of the shift measures the log of the
equivalence scale. (Keep in mind that the horizontal variable is the log of total
expenditures.) Later – in the context of index model estimation – we will show
how to estimate that shift, but assume for the moment that it is .5, in which case
the equivalence scale is 1.65 = exp(.5).
The lower panel of Figure 4.2 illustrates the two Engel curves following a
horizontal shift of the couples Engel curve. They seem to track each other fairly
closely, and we can now ask whether the two Engel curves coincide. (If they
do not, there is no single equivalence scale that works at all levels of income.)
Let y A and y B be the food shares for singles and couples. Let x A be the log of
total expenditures for singles. Deﬁne x B to be the log (total expenditures/1.65),
which equals log (total expenditures) − .5.
To apply the simpliﬁed procedure based on the speciﬁcation test in Proposition 4.3.1, we ﬁrst estimate the common regression function using a kernel esti2
mator. The average sum of squared residuals sres
is .018356. Using m = 25, we
2
2
calculate sw to be .018033 and substitute this quantity for sdiff
in (4.3.2b) to obtain a statistic of 4.01. Next we apply the test of equality of regression functions
in Proposition 4.4.2. For m = 25, the standardized test statistic ϒ/sw2 (2π̂ϒ )1/2
takes a value of 1.76. Thus, there is some evidence against the hypothesis that
the two Engel curves coincide. However, the rejection – given the size of the
data set – is hardly overwhelming.
In performing these tests it is useful to compare implied values of R 2 = 1 −
2 2
sε /s y , where s y2 = .04106 is the variance of the food share variable across
all 1,999 observations. We can take sw2 to be an unconstrained estimator of
2
the residual variance. The implied “unrestricted” ﬁt is 56.1 percent. Both sres
,
2
the estimated residual variance from the kernel regression, and s p , the pooled
estimator of the residual variance, impose the restriction of equality of regression functions. The corresponding implied ﬁts are 55.5 and 55.8 percent,

0.8
0.6
0.4
0.0

0.2

food share

Singles
Couples

5

6

7

8

0.6

0.8

log of total expenditure

0.4
0.0

0.2

food share

Singles
Couples

5

6

7
log of total expenditure

Figure 4.2. Testing equality of Engel curves.

70

8

Higher-Order Differencing Procedures

71

respectively. Thus, the imposition of the constraint results in a modest deterioration in ﬁt.
4.5 Partial Linear Model
4.5.1 Estimator
Let us return to estimation of the partial linear model yi = z i β + f (xi ) + εi ,
where xi is a scalar, z i is a p-dimensional row vector, εi | xi , z i is distributed
with mean 0, variance σε2 , and f has a bounded ﬁrst derivative. We will also
assume that the vector of parametric variables z has a smooth regression relationship with the nonparametric variable x. Thus, we may write z i = g(xi ) + u i ,
where g is a vector
function with ﬁrst derivatives bounded, E(u i | xi ) = 0, and
E(Var (z i | xi )) = z | x . Assume that each observation is independently drawn
and that the data (y1 , x1 , z 1 ), . . . , (yn , xn , z n ) have been reordered so that the x’s
are in increasing order. Deﬁne y  = (y1 , . . . , yn ), f (x) = ( f (x1 ), . . . , f (xn )),
and Z as the n × p matrix with ith row z i . In matrix notation, we have
y = Zβ + f (x) + ε.

(4.5.1)

Applying the differencing matrix, we have
Dy = D Zβ + D f (x) + Dε ∼
= D Zβ + Dε.

(4.5.2)

The following proposition contains our main result.
Proposition 4.5.1: For arbitrary differencing coefﬁcients satisfying
(4.1.1), deﬁne β̂ diff = [(D Z ) D Z ]−1 (D Z ) Dy. Then,
−1
D
1
n /2 (β̂ diff − β) → N 0, (1 + 2δ)σε2
(4.5.3)
z|x

1
P
2
sdiff
= (Dy − D Z β̂ diff ) (Dy − D Z β̂ diff ) → σε2
n

1
P 
ˆ
= (D Z ) D Z →
.
z|x
z|x
n

(4.5.4)
(4.5.5)

For optimal differencing coefﬁcients, replace 1 + 2δ in (4.5.3) with 1 +

1
.
2m

The covariance matrix of the differencing estimator of β may be estimated
using

ˆ
β̂

= (1 + 2δ)

2 −1
sdiff
ˆ
.
z|x
n

(4.5.6)

Proposition 4.5.2: Linear restrictions of the form Rβ = r may be
tested using the conventional statistic which – if the null hypothesis is true – has

72

Semiparametric Regression for the Applied Econometrician

the following distribution:

ˆ
R
(R β̂ − r ) R
β̂

−1

D

2
(R β̂ − r ) → χrank(R)
.

Equivalently, one may use
 2

2
n sdiff
D
res − sdiff
2
→ χrank(R)
,
2
sdiff (1 + 2δ)

(4.5.7a)

(4.5.7b)

2
2
where sdiff
is the unrestricted differencing estimator in (4.5.4) and sdiff
res is
obtained by estimating the differenced model (4.5.2) subject to the linear
constraints5 and then applying (4.5.4).

The statistic in (4.5.7b) is but a thinly disguised version of its analogue in
the usual analysis of linear models, which compares restricted and unrestricted
sum of squares.
A heuristic proof of Proposition 4.5.1 in the case in which all variables are
scalars is provided in Chapter 1. More detailed proofs of Propositions 4.5.1 and
4.5.2 may be found in Appendix B. As is evident from (4.5.3), the estimator is
n 1/2 -consistent and asymptotically normal. It is close to being asymptotically
efﬁcient for moderate values of m. For example, at m = 10, it has a relative
efﬁciency of 95 percent. By increasing the order of differencing as sample size
increases, the estimator becomes asymptotically efﬁcient.
4.5.2 Heteroskedasticity
Suppose now that Var (εε  ) = ; then, the residual vector in (4.5.2) has covariance matrix DD  , and




1 Z  D  D Z −1 Z  D  DD  D Z Z  D  D Z −1
Var (β̂ diff ) ∼
.
=
n
n
n
n
(4.5.8)
If the order of differencing is large or, better still, if m increases with sample
size, then we may estimate the interior matrix using
ˆ Z
Z  D  D
,
n

(4.5.9)

ˆ is a diagonal matrix of squared estimated residuals from (4.5.2).
where 
5

Recall that for a linear model the restricted OLS estimator may be obtained by redeﬁning
variables. The model we are estimating in (4.5.2) is approximately linear since, as a result of
differencing, D f (x) is close to zero.

Higher-Order Differencing Procedures

73

Suppose, on the other hand, that the order of differencing m is ﬁxed at some
low level. Now consider the structure of DD  . Because  is diagonal, the
nonzero elements of DD  consist of the main diagonal and the m adjacent diagonals (m is the order of differencing). This is because differencing introduces
a moving average process of order m into the residuals.6 White’s (1985) generalizations may then be applied to our differenced model Dy ∼
= D Zβ + Dε,
where, as usual, differencing has (approximately) removed the nonparametric
  to be the matrix
effect.  To mimic the structure of DD  we deﬁne DD
Dε Dε with all terms more than m diagonals away from the main diagonal set
to zero. In this case we may estimate the interior matrix of (4.5.8) using
 DZ
Z  D  DD
.
(4.5.10)
n
With some additional effort, autocorrelation-consistent standard errors may
also be constructed for differencing estimators, though the double residual
method outlined in Section 3.6 generally results in simpler implementation.

4.6 Empirical Applications
4.6.1 Household Gasoline Demand in Canada
In a recent paper, Yatchew and No (2001) estimated a partial linear model
of household demand for gasoline in Canada – a model very similar to those
estimated by Hausman and Newey (1995) and Schmalensee and Stoker (1999).
The basic speciﬁcation is given by
dist = f (price) + β1 income + β2 drivers + β3 hhsize + β4 youngsingle
+ β5 age + β6 retire + β7 urban + monthly dummies + ε,
(4.6.1)
where dist is the log of distance traveled per month by the household, price
is the log of price of a liter of gasoline, drivers is the log of the number of
licensed drivers in the household, hhsize is the log of the size of the household,
youngsingle is a dummy for singles up to age of 35, age is the log of age, retire
is a dummy for those households where the head is over the age of 65, and
urban is a dummy for urban dwellers. Figure 4.3 summarizes the results. The
“parametric estimates” refer to a model in which price enters log-linearly. The
“double residual estimates” use Robinson (1988) (see Section 3.6). That procedure requires one to estimate regression functions of the dependent variable
and each of the parametric independent variables on the nonparametric variable.
6

Alternatively, note that DD  = (d0 L 0 + · · · + dm L m )(d0 L 0 + · · · + dm L m ). The lag
matrices L i , L i shift the main diagonal of  to the ith off-diagonals.

Variable

Coef
price
income
drivers
hhsize
youngsingle
age
retire
urban
Monthly Effects
(see Figure 4.4)

−0.9170
0.2890
0.5610
0.1000
0.1930
−0.0780
−0.2070
−0.3310

SE

Coef

SE

0.0960
0.0200
0.0330
0.0260
0.0610
0.0440
0.0320
0.0200

−
0.3000
0.5650
0.0940
0.1980
−0.0750
−0.1980
−0.3250

−
0.0200
0.0330
0.0260
0.0610
0.0440
0.0320
0.0200

.5003
.2635

HCSE

Differencing
estimates
Coef

SE

−
−
−
0.0201
0.2816 0.0209
0.0318
0.5686 0.0338
0.0256
0.0892 0.0274
0.0651
0.2099 0.0622
0.0419 −0.1171 0.0555
0.0342 −0.2113 0.0387
0.0195 −0.3331 0.0203

.5053
.2563

.4997
.2644

7.4

7.6

Kernel
Linear
Loess

7.0

7.2

log distance per month

7.8

8.0

sε2
R2

Robinson doubleresidual estimates

Parametric estimates

−1.0

−0.8

−0.6

−0.4

−0.2

log price

Variance of dependent variable is .6794. Order of differencing m = 10. Number of observations is
6230. Robinson estimates of parametric effects produced using kernel procedure ksmooth in S-Plus.
Solid line is kernel estimate applied to data after removal of estimated parametric effect. Dotted
line is parametric estimate of price effect. Speciﬁcation test of log-linear model for price effect
yields value of .3089. Nonparametric signiﬁcance test for price effect yields test statistic of 3.964.

Figure 4.3. Household demand for gasoline.

74

Higher-Order Differencing Procedures

75

0.4

The residuals are then used to estimate the parametric effects. We implement
Robinson’s method using ksmooth, a kernel regression estimation procedure in
S-Plus.
The “differencing estimates” use tenth-order differencing and Proposition 4.5.1 to estimate the parametric effects. The three sets of estimates are
very similar except that the standard errors of the differencing estimates are
marginally larger.
The estimated parametric effects, which have been estimated by differencing,
are then removed, and kernel regression is applied to obtain a nonparametric
estimate of the price effect (the solid line in Figure 4.3). Applying the speciﬁcation test in Proposition 4.3.1 yields a value of .31, suggesting that the log-linear
speciﬁcation is adequate. A test of the signiﬁcance of the price effect using the
same proposition yields a value of 4.0, which indicates that the price variable
is signiﬁcant. Figure 4.4 displays seasonal effects.
Figure 4.3 also contains heteroskedasticity-consistent standard errors(HCSE)
for the double residual estimates. The HCSE were computed using (3.6.17)
and (3.6.18). There is very little difference in standard errors relative to the
homoskedastic case.

0.2
0.1
−0.1

0.0

percentage change

0.3

Differencing
Double-residual
Linear

2

4

6

8
month

Figure 4.4. Household demand for gasoline: Monthly effects.

10

12

76

Semiparametric Regression for the Applied Econometrician

4.6.2 Scale Economies in Electricity Distribution7
We now consider the example of Section 1.6 in considerably more detail. Suppose we have a slightly more general speciﬁcation that is a semiparametric
variant of the translog model (variable deﬁnitions may be found in Appendix E):
tc = f (cust) + β1 wage + β2 pcap + 1/2 β11 wage2 + 1/2 β22 pcap2
+ β12 wage · pcap + β31 cust · wage + β32 cust · pcap + β4 PUC
+ β5 kwh + β6 life + β7 lf + β8 kmwire + ε.
(4.6.2)
Note that, in addition to appearing nonparametrically, the scale variable cust
interacts parametrically with wages and the price of capital. One can readily
verify that, if these interaction terms are zero (i.e., β31 = β32 = 0), then the
cost function is homothetic. If in addition β11 = β22 = β12 = 0, then the model
reduces to the log-linear speciﬁcation of Section 1.6.
Differencing estimates of the parametric component of (4.6.2) are presented
in Figure 4.5. (We use third-order optimal differencing coefﬁcients, in which
case m = 3.) Applying Proposition 4.5.2, we do not ﬁnd signiﬁcant statistical
evidence against either the homothetic or the log-linear models. For example,
the statistic testing the full version (4.6.2) against the log-linear speciﬁcation,
which sets ﬁve parameters to zero and is distributed χ52 under the null, takes
a value of 3.23. Estimates of nonprice covariate effects exhibit little variation
as one moves from the full translog model to the homothetic and log-linear
models.
The last column of Figure 4.5 contains HCSEs reported two ways: the ﬁrst
uses (4.5.9), which does not incorporate off-diagonal terms; the second uses
(4.5.10), which does. We believe the latter to be more accurate here given the
low order of differencing the small data set permits.
We may now remove the estimated parametric effect from the dependent
variable and analyze the nonparametric effect. In particular, for purposes of the
tests that follow, the approximation yi − z i β̂ = z i (β − β̂) + f (xi ) + εi ∼
=
f (xi ) + εi does not alter the large sample properties of the procedures. We use
the estimates of the log-linear model to remove the parametric effect.
Figure 4.5 displays the ordered pairs (yi − z i β̂ diff , xi ) as well as a kernel
estimate of f. Parametric null hypotheses may be tested against nonparametric
alternatives using the speciﬁcation test in Section 4.3. If we insert a constant
function for f , then the procedure constitutes a test of signiﬁcance of the scale
variable x against a nonparametric alternative. The resulting statistic is 9.8,
indicating a strong scale effect. Next we test a quadratic model for output.
The resulting test statistic is 2.4, suggesting that the quadratic model may be
inadequate.

7

For a detailed treatment of these data, see Yatchew (2000).

Full model: semiparametric translog

Variable

Homothetic model: semiparametric homothetic

Coef

SE

5.917
2.512
0.311
0.073

13.297
2.107
2.342
0.083

6.298
1.393
0.720
0.032

12.453
1.600
2.130
0.066

wage · pcap
cust · wage
cust · pcap

0.886
0.054
0.039

0.738
0.086
0.049

0.534

0.599

PUC
kwh
life
lf
kmwire

0.083
0.031
0.630
1.200
0.396

0.039
0.086
0.117
0.450
0.087

0.086
0.033
0.634
1.249
0.399

0.039
0.086
0.115
0.436
0.087

wage
pcap
1/ wage2
2
1/ pcap2
2

sε2
R2

Coef

.01830
.668

Log-linear model: semiparametric Cobb–Douglas

SE

Coef

HCSE
HCSE
Eqn. 4.5.9 Eqn. 4.5.10
0.623 0.320
0.343
0.361
0.545 0.068
0.078
0.112

0.075
0.008
0.628
1.327
0.413

.0185
.665

SE

0.038
0.086
0.113
0.434
0.084

0.034
0.074
0.095
0.326
0.090

0.033
0.089
0.097
0.304
0.115

.01915
.653

5.8

6.0

Estimated scale effect

5.6
5.4
5.0

5.2

log total cost per year

Kernel
Quadratic

6

8

10

12

log customers

Test of full (translog) model versus log-linear (Cobb–Douglas) model: χ52 under Ho : 3.23. Test of
2 − s 2 )/s 2
=
quadratic versus nonparametric speciﬁcation of scale effect: V = (mn)1/2 (sres
diff diff
1/
2
(3 ∗ 81) (.0211 − .0183)/.0183 = 2.4 where V is N (0,1). Kernel estimate produced using
ksmooth function in S-Plus. The last two columns of the table contain heteroskedasticity-consistent
standard errors (HCSEs).

Figure 4.5. Scale economies in electricity distribution.

77

78

Semiparametric Regression for the Applied Econometrician

To provide further illustrations of differencing procedures we divide our
data into two subpopulations: those that deliver additional services besides
electricity, that is, public utility commissions (PUC), and those that are pure
electricity distribution utilities (non-PUC). The numbers of observations in the
two subpopulations are n PUC = 37 and n nonPUC = 44. We denote differencing
2
,
estimates of parametric effects and of residual variances as β̂ PUC , β̂ nonPUC , sPUC
2
and snonPUC . For each subpopulation, we estimate the log-linear model using
the differencing estimator and report the results in Figure 4.6.
To test whether PUC and non-PUC entities experience the same parametric
effects, we use
−1

ˆ β̂ + 
ˆ β̂
(β̂ PUC − β̂ nonPUC ) 
PUC
nonPUC
D

2
× (β̂ PUC − β̂ nonPUC ) → χdim(β)
.

(4.6.3)

The computed value of the χ62 test statistic is 6.4, and thus the null is not
rejected. Next, we constrain the parametric effects to be equal across the two
types of utilities while permitting distinct nonparametric effects. This is accomplished by taking a weighted combination of the two estimates
−1 


ˆ −1 · β̂ PUC + 
ˆ −1 + 
ˆ −1
ˆ −1 · β̂ nonPUC
β̂ weighted = 

β̂
β̂
β̂
β̂
PUC

nonPUC

PUC

nonPUC

(4.6.4)
with estimated covariance matrix

ˆ β̂
ˆ −1 + 
ˆ −1

= 
β̂ PUC

weighted

−1

β̂ nonPUC

.

(4.6.5)

The results are reported in Table 4.3.8 The data can be purged of the estimated
parametric effects, and separate nonparametric curves can be ﬁtted to each
8

A numerically similar estimator with the same large sample properties may be constructed by
differencing the data within each subpopulation and then stacking as follows



DyPUC
DynonPUC



=





D Z PUC
β+
D Z nonPUC



D f PUC (xPUC )
D f nonPUC (xnonPUC )

 
+



DεPUC
.
DεnonPUC

Let β̂ be the OLS estimator applied to the preceding equation. Then, the common residual
variance may be estimated using
s2 =

1
n



 

DyPUC
−
DynonPUC



D Z PUC
β̂
D Z nonPUC





 

DyPUC
−
DynonPUC )



D Z PUC
β̂ ,
D Z nonPUC

and the covariance matrix of β̂ may be estimated using


ˆ

β̂

=

1+

1
2m

−1
s2 
(D Z PUC ) (D Z PUC ) + (D Z nonPUC ) (D Z nonPUC )
,
n

where m is the order of (optimal) differencing.

Higher-Order Differencing Procedures

79

Partial linear modela
Variable

PUC

non-PUC

Coef
wage
pcap
kwh
life
lf
kmwire

SE

Coef

SE

0.65
0.424

0.348
0.090

1.514
0.632

0.684
0.113

0.108
−0.495
1.944
0.297

0.121
0.131
0.546
0.109

0.079
−0.650
0.453
0.464

0.123
0.199
0.702
0.123

sε2

0.013

0.023

6.2

Estimated scale effect

o

o
x

5.8

o

5.4

5.6

x

x

x x
ox xo x

PUC
non-PUC
x

x
x

x
x
o
oo
x
x
x
xo o x
x
o x
oo x
o
o
x
o
o ooo
o o o
xx
o
o
o
o
o

x
x
x
x x
x o
o
o
o
o

x x

5.0

5.2

log total cost per year

6.0

x

6

8

10

xx
o

x
x
x

x
o

o

x

x

xx

x
o

x
o
12

log customers
a

Order of differencing m = 3.

Figure 4.6. Scale economies in electricity distribution: PUC and non-PUC analysis.

80

Semiparametric Regression for the Applied Econometrician

Table 4.3. Mixed estimation of PUC/non-PUC effects: Scale
economies in electricity distribution.a
Variable

Coef

SE

wage
pcap
kwh
life
lf
kmwire

0.875
0.526
0.066
−0.547
1.328
0.398

0.304
0.067
0.086
0.107
0.422
0.078

a

Estimates of parametric effects are obtained separately for PUC and
non-PUC subpopulations. Hence, no PUC effect is estimated. The
estimates above are obtained using (4.6.4) and (4.6.5).

subset of the data, as in the bottom panel of Figure 4.6. The PUC curve lies
below the non-PUC curve consistent with our earlier ﬁnding that PUC entities
have lower costs (see PUC coefﬁcients in Figure 4.5).
We may now adapt our test of equality of regression functions in Section 4.4.2
to test whether the curves in Figure 4.6 are parallel, that is, whether one can be
superimposed on the other by a vertical translation. This may be accomplished
simply by removing the mean of the purged dependent variable from each of
the two subpopulations.
Deﬁne the within estimate to be the weighted average of the subpopulation
variance estimates, keeping in mind that the estimated parametric effect has
been removed using, say, β̂ weighted :
sw2 =

n PUC 2
n nonPUC 2
s
snonPUC .
+
n PUC
n

(4.6.6)

purge

Let yPUC be the vector of data on the dependent variable for PUCs with
the estimated parametric effect removed and then centered around 0 and deﬁne
purge
ynonPUC similarly.9 Now stack these two vectors and the corresponding data on
purge
the nonparametric variable x to obtain the ordered pairs (yi
, xi ) i = 1, . . . , n.
Let Pp be the permutation matrix that reorders these data so that the nonparametric variable x is in increasing order. Note that, because separate equations
9

Because the hypothesis that the parametric effects are the same across the two populations
2 and β 2
has not been rejected, one may use subpopulation estimates βPUC
nonPUC or the weighted
2 , s2
2.
,
and
s
estimate β̂ weighted when computing sPUC
nonPUC
p

Higher-Order Differencing Procedures

81

were estimated for the two subpopulations, z does not contain the PUC dummy.
Deﬁne
s 2p =

1 purge  
Pp D D Pp y purge .
y
n

(4.6.7)

If the null hypothesis is true, then differencing will still remove the nonparametric effect in the pooled data and s 2p will converge to σε2 . Otherwise, it
will generally converge to some larger value. Applying Proposition 4.4.2 with
m = 1, we obtain a value of 1.77 for ϒ/sw2 (2π̂ϒ )1/2 , which, noting that this
is a one-sided test, suggests that there is some evidence against the hypothesis that the scale effects are parallel. Finally, we note that, given the size of
the two subsamples, one must view the asymptotic inferences with some caution. An alternative approach that generally provides better inference in moderately sized samples would be based on the bootstrap, which is discussed in
Chapter 8.

4.6.3 Weather and Electricity Demand
In a classic paper, Engle et al. (1986) used the partial linear model to study
the impact of weather and other variables on electricity demand. We estimate
a similar model in which weather enters nonparametrically and other variables
enter parametrically. Our data consist of 288 quarterly observations in Ontario
for the period 1971 to 1994. The speciﬁcation is
elect = f (tempt ) + β1 relpricet + β2 gdpt + ε,

(4.6.8)

where elec is the log of electricity sales, temp is heating and cooling degree days
measured relative to 68 ◦ F, relprice is the log of the ratio of the price of electricity
to the price of natural gas, and gdp is the log of gross provincial product. We
begin by testing whether electricity sales and gdp are cointegrated under the
assumption that relprice and temp are stationary (setting aside issues of global
warming). The Johansen test indicates a strong cointegrating relationship. We
therefore reestimate the model in the form
elect − gdpt = f (tempt ) + β1 relpricet + ε.

(4.6.9)

Figure 4.7 contains estimates of a pure parametric speciﬁcation for which
the temperature effect is modeled using a quadratic as well as estimates of
the partial linear model (4.6.9). The price of electricity relative to natural
gas is negative and quite strongly signiﬁcant. In the partial linear model, the

82

Semiparametric Regression for the Applied Econometrician
Variable

Coef
constant
temp
temp2
relprice

Partial linear modela

Quadratic model

−1.707
−1.29 × 10−4
4.07 × 10−7
−0.0695

s2
R2

Newey–West
SE
0.0286
3.80 × 10−5
5.08 × 10−8
0.0255

Coef
−
−
−
−0.073

.00312
.788

Newey–West
SE
−
−
−
.0252
.00282
.809

Estimated temperature effect

Quadratic
Loess

a The partial linear model was estimated using the double residual procedure with loess as the
smoother. The scatterplot consists of points with the parametric (relative price) effect removed.

Figure 4.7. Weather and electricity demand.

Higher-Order Differencing Procedures

83

ratio of the coefﬁcient estimate to the Newey–West standard error is −2.9
(see Section 3.6).
4.7 Partial Parametric Model
4.7.1 Estimator
A natural generalization of the partial linear model replaces the linear portion
with a nonlinear parametric speciﬁcation yi = f (xi ) + r (z i , β) + εi , where the
regression function r is known and β is a p-dimensional vector. Suppose that
the data (y1 , x1 , z 1 ), . . . , (yn , xn , z n ) have been reordered so that the x’s are in
increasing order. Let x  = (x1 , . . . , xn ), y  = (y1 , . . . , yn ), and Z be the n × p
matrix with ith row z i .
Deﬁne f (x) = ( f (x1 ), . . . , f (xn )) to be the column vector of nonparametric
effects and r (Z , β) = (r (z 1 , β), . . . , r (z n , β)) to be the column vector of
parametric effects. Let ∂r (z, β)/∂β be the p × 1 column vector of partial
derivatives of r with respect to β and ∂r (Z , β)/∂β the p × n matrix of partials
of r (z 1 , β), . . . , r (z n , β) with respect to β. In matrix notation we may write the
model as
y = f (x) + r (Z , β) + ε.

(4.7.1)

Applying the differencing matrix, we have
Dy = D f (x) + Dr (Z , β) + Dε.

(4.7.2)

Proposition 4.7.1: For arbitrary differencing weights satisfying
(4.1.1), let β̂ diffnls satisfy
1
min (Dy − Dr (Z , β)) (Dy − Dr (Z , β));
β n

(4.7.3)


σε2 −1
,
∼ N β, (1 + 2δ)
∂r
n
∂β |x

(4.7.4)

then,
β̂ diffnls
where

A





∂r
|x
.
= E Var
∂β



∂r
∂β

|x

(4.7.5)

Furthermore,
2
sdiffnls
=

1
P
(Dy − Dr (Z , β̂ nls )) (Dy − Dr (Z , β̂ nls )) → σε2 ,
n
(4.7.6)

84

Semiparametric Regression for the Applied Econometrician

and

ˆ
∂r
∂β

|x

=

1 ∂r (Z , β̂)  ∂r (Z , β̂) P 
→
.
DD
∂r
n
∂β
∂β 
∂β |x

(4.7.7)

For optimal differencing weights, replace 1 + 2δ with 1 + 1/2m in (4.7.4).
As will be illustrated in Section 4.7.2, nonlinear least-squares procedures
(e.g., in S-Plus) may be applied to (4.7.2) to obtain estimates of β. However,
the covariance matrix produced by such programs needs to be multiplied by
1 + 2δ as indicated by (4.7.4) (see also Footnote to Table 4.4.)
4.7.2 Empirical Application: CES Cost Function
We continue with our example on electricity distribution costs. Consider a
conventional constant elasticity of substitution (CES) cost function (see, e.g.,
Varian 1992, p. 56)
tc = β0 +

1
log(β1 WAGEρ + (1 − β1 )PCAPρ ),
ρ

(4.7.8)

where tc is the log of total cost per customer and WAGE and PCAP denote
factor prices in levels. (Elsewhere we use wage and pcap to denote logs of
factor prices. See Appendix E for variable deﬁnitions.) We are interested in
assessing whether cost per customer is affected by the scale of operation, that
is, the number of customers. We therefore introduce a nonparametric scale effect
(as well as several covariates)
1
log(β1 WAGEρ + (1 − β1 )PCAPρ )
ρ
+ β2 PUC + β3 kwh + β4 life + β5 lf + β6 kmwire + ε.
(4.7.9)
√
First differencing and dividing by 2 so that the variance of the residual
remains the same yields
√
[tci − tci−1 ]/ 2

1
ρ
ρ
∼
log β1 WAGEi + (1 − β1 )PCAPi
=
ρ
√

ρ
ρ 
− log β1 WAGEi−1 + (1 − β1 )PCAPi−1 / 2
√
√
+ β2 [PUCi − PUCi−1 ]/ 2 + β3 [kwhi − kwhi−1 ]/ 2
√
√
+ β4 [lifei − lifei−1 ]/ 2 + β5 [lfi − lfi−1 ]/ 2
√
√
+ β6 [kmwirei − kmwirei−1 ]/ 2 + [εi − εi−1 ]/ 2. (4.7.10)
tc = f (cust) +

Higher-Order Differencing Procedures

85

Table 4.4. Scale economies in electricity distribution: CES cost function.
Parametric model

Partial parametric modela

Variable

Coef

SE

Coef

SE

cust
cust2
WAGE
PCAP
ρ
PUC
kwh
life
lf
kmwire

−0.739
0.036
0.544
1 − .544
0.197
−0.082
0.001
−0.594
1.144
0.4293

0.177
0.010
0.221
—
0.560
0.038
0.087
0.122
0.433
0.086

—
—
0.701
1 − .701
0.467
−0.081
−0.008
−0.492
1.241
0.371

—
—
0.200
—
0.585
0.047
0.092
0.149
0.479
0.096

sε2
R2

.0214
.611

.0177
.678

Order of differencing m = 1. Model estimated using √
nonlinear least squares in
S-Plus. Standard errors produced by S-Plus multiplied by 1.5 as per (4.7.4). Test of
quadratic versus nonparametric speciﬁcation of scale effect using the differencing
2
2
2
test statistic in (4.3.2b) yields V = (mn)1/2 (sres
− sdiff
)/sdiff
= 811/2 (.0214−.0177)/
.0177 = 1.88.
a

Our parametric null consists of a quadratic speciﬁcation for the scale effect,
that is, f (cust) = γ0 + γ1 cust + γ2 cust2 in (4.7.9). Results for this parametric speciﬁcation and for (4.7.10) are presented in Table 4.4. The model was
estimated using nonlinear least squares in S-Plus. Applying the differencing
speciﬁcation test yields an asymptotically N (0, 1) statistic of 1.88.
The effects of covariates (PUC, kwh, life, lf, and kmwire) remain fairly similar
across the various parametric and semiparametric speciﬁcations contained in
Figures 1.2 and 4.5. Variants of the Leontief model may be implemented by
imposing the restriction ρ = 1, which is a parameter that is estimated quite
imprecisely in this speciﬁcation.

4.8 Endogenous Parametric Variables in the Partial Linear Model
4.8.1 Instrumental Variables
We return to the framework of Section 4.5, the partial linear model. Suppose
one or more of the p parametric variables in Z are correlated with the residual

86

Semiparametric Regression for the Applied Econometrician

and suppose W is an n × q matrix of observations on instruments for Z . We
will assume that there are at least as many instruments as parametric variables,
that is, q ≥ p, and that each instrument has a smooth regression function on x,
the nonparametric variable. Let 
D Z be the predicted values of DZ:

D Z = DW ((DW ) DW )−1 (DW ) D Z .

(4.8.1)

As is the case for the conventional linear model, instrumental variable estimation may be motivated by multiplying (4.5.2) by ( 
D Z ) :
(
D Z ) Dy = ( 
D Z ) D f (x) + ( 
D Z ) D Zβ + ( 
D Z ) Dε.

(4.8.2)

Because differencing removes the nonparametric effect in large samples, this

suggests the two-stage-least-squares estimator
(( 
D Z ) D Z )−1 ( 
D Z )
Dy. De
ﬁne the conditional moment matrices: w | x = E x Var(w | x) and zw | x =
E x Cov(z, w | x), where Cov(z, w | x) is the p × q matrix of covariances between the z and w variables conditional on x.
Proposition 4.8.1: For arbitrary differencing weights satisfying
(4.1.1),
β̂ diff2sls = [( 
D Z ) D Z ]−1 ( 
D Z ) Dy



−1  −1
σε2 
D
→ N β, (1 + 2δ)
w|x
zw|x
zw|x
n
1
P
(Dy − D Z β̂ diff2sls ) (Dy − D Z β̂ diff2sls ) → σε2
n

1
P 
ˆ
= (DW ) DW →
w|x
w|x
n

1
P 
ˆ
= (D Z ) DW →
.
zw|x
zw|x
n

2
sdiff2sls
=

(4.8.3)
(4.8.4)
(4.8.5)
(4.8.6)

For optimal differencing weights, replace 1 + 2δ with 1 + 1/2m in (4.8.3).
4.8.2 Hausman Test
We can now produce a Hausman-type test (Hausman 1978) of endogeneity. The
covariance matrices of each of the two estimators may be replaced by consistent
estimates.

Higher-Order Differencing Procedures

87


Proposition 
4.8.2: Let β̂ be the large sample covariance matrix of
diff
β̂ diff (Eq. (4.5.3)) and β̂
the corresponding covariance matrix for β̂ diff2sls
diff2sls
(Eq. (4.8.3)); then, under the null hypothesis that z is uncorrelated with ε,

−1
D
(β̂ diff − β̂ diff2sls ) β̂ diff2sls − β̂ diff (β̂ diff − β̂ diff2sls ) → χ p2 ,
(4.8.7)
where p is the dimension of β.
4.9 Endogenous Nonparametric Variable
4.9.1 Estimation
Suppose that in the pure nonparametric regression model, the explanatory variable x is correlated with the residual. That is,
y = f (x) + ε

E(ε | x) = 0.

(4.9.1)

In general, this model is difﬁcult to estimate because conventional instrumental variable techniques are not directly transferable to a nonlinear or nonparametric setting. However, suppose an instrument w exists for x that is uncorrelated
with the residual
x = wπ + u

E(u | w) = 0

E(ε | w) = 0.

(4.9.2)

Suppose further that E(ε | x, u) = ρu, in which case we may write
ε = ρu + v. This is a fairly strong assumption, but in this case we have
y = f (x) + uρ + v

E(v | x, u) = 0.

(4.9.3)

Equation (4.9.3) is a partial linear model. To estimate it we need to perform
the linear regression in (4.9.2), and save the residuals and insert them into
(4.9.3), from which we may estimate ρ. If ρ̂ is signiﬁcantly different from zero,
then x is endogenous.
The model generalizes readily to the case of the partial linear model y =
f (x) + zβ + ε. In this case, if x is correlated with the residual we need to
perform the ﬁrst-stage regression (4.9.2). We then rewrite the model as y =
f (x) + ρu + zβ + v, reorder so that x is in increasing order, and regress the
differenced values of y on the differenced values of û and z.
The approach described here originates with Hausman (1978). See also Holly
and Sargan (1982) and Blundell and Duncan (1998). Generalizations may be

88

Semiparametric Regression for the Applied Econometrician

found in Newey, Powell, and Vella (1999). The model may of course also be
estimated using the double residual method in Section 3.6.

4.9.2 Empirical Application: Household Gasoline Demand
and Price Endogeneity
Earlier we have estimated household demand for gasoline using Canadian microdata (see Section 4.6.1). The price variable, which enters nonparametrically, has a signiﬁcant and negative effect on consumption, as illustrated in
Figure 4.3.
However, the interpretation of the price effect is in question. If one examines
the variation in prices within a given urban area, the coefﬁcient of variation
may be found to be as much as 5 percent or higher for regular gasoline.10 Thus,
individuals who drive more are likely to encounter a broader range of prices;
hence, their search and transaction costs for cheap gasoline are lower. Furthermore, these same individuals derive greater beneﬁt from cheap gas and therefore
would be willing to incur higher search and transactions costs. Thus, one might
expect price to be negatively correlated with the residual in an equation in which
the dependent variable is distance traveled or the level of gasoline consumption.
In this case the price coefﬁcient would overestimate the true responsiveness of
consumption to price.11 To separate these two effects one should ideally have
much more precise data on location. One could then instrument the observed
price variable with the average price over a relatively small geographic area
(such as the average intracity price). This level of detail is not available in these
(public) data; however, as a check on our estimates we can instrument our price
variable with the ﬁve provincial or regional dummies. These will serve the role
of w in (4.9.2). Following (4.9.3), (4.6.1) may be rewritten as
y = f (price) + uρ + zβ + v,

(4.9.4)

where z is the collection of parametric variables, u is the residual in the instrumental variable equation price = regional dummies · π + u, and E(v | price,
u, z) = 0. After estimating u from an OLS regression, (4.9.4) was
estimated using differencing. The coefﬁcient of u was .31 with a standard

10

11

The coefﬁcient of variation of the price of regular gasoline is about 9 percent in our complete
data set. After adjusting for geographic and time-of-year effects, the coefﬁcient falls to about
7 percent.
In the extreme case, demand could be perfectly inelastic at the same time that the estimated
price effect is signiﬁcantly negative.

Higher-Order Differencing Procedures

89

error of .25, which, given the available instruments, does not suggest endogeneity.
4.10 Alternative Differencing Coefﬁcients
Hall et al. (1990, p. 515, Table 2), henceforth HKT, compared the relative
efﬁciency of alternative differencing estimators of the residual variance. They
found that, for small m, optimal weights performed substantially better than
a “spike” sequence in which the differencing weight near the middle of the
sequence was close to unity whereas others were equal and close to zero. This
is essentially equivalent to using a running mean smoother. For large m, they
found both types of weights to have similar properties.
They also compared optimal weights to the usual weights used for numerical
differentiation. (These are equivalent to mth-order divided differences for equally spaced data.) They found that these weights become progressively less
efﬁcient relative to optimal weights as m increases.
Seifert et al. (1993) studied the mean-squared error of various differencingtype estimators of the residual variance. They found that the bias resulting from
the use of HKT optimal weights can be substantial in some cases, particularly
if sample size is small and the signal-to-noise ratio is high. The mean-squared
error of differencing estimators of the partial linear model has apparently not
been studied.
Because HKT differencing weights put maximum weight at the extreme of a
sequence, one would expect that in some cases bias would be exacerbated. On
the other hand, weights that are symmetric about a midpoint and decline as one
moves away might have better bias properties. In particular, for even m (so that
the number of weights is odd), we solve the optimization problem given by
 m−k
2
m


min δ =
djdj +k
d0 ,...,dm

s.t.

m


k=1

dj = 0

j=0

d0 = dm

j=0
m


d 2j = 1

(4.10.1)

j=0

d1 = dm−1

d2 = dm−2 . . . . . . dm/2−1 = dm/2+1

dm/2+1 ≤ dm/2+2 · · · · ≤ dm .
The constraints impose (4.1.1), symmetry, and monotonicity toward zero as
one moves away from the centermost weight. Optimal values are presented in
Table 4.5.
The optimization problems were solved using GAMS (see Brooke et al.
1992). Table 4.6 compares the efﬁciency of optimal weights to symmetric

90

Semiparametric Regression for the Applied Econometrician

Table 4.5. Symmetric optimal differencing weights.
m

(d0 , d1 , . . . , dm )

2
4
6
8
10

(−0.4082, 0.8165, −0.4082)
(−0.1872, −0.2588, 0.8921, −0.2588, −0.1872)
(−0.1191, −0.1561, −0.1867, 0.9237, −0.1867, −0.1561, −0.1191)
(−0.0868, −0.1091, −0.1292, −0.1454, 0.9410, −0.1454, −0.1292, −0.1091, −0.0868)
(−0.0681, −0.0830, −0.0969, −0.1091, −0.1189, 0.9519, −0.1189, −0.1091, −0.0969, −0.0830, −0.0681)

Table 4.6. Relative efﬁciency of alternative differencing sequences.
(1 + 2δ)
m

Optimal

Symmetric optimal

Moving average

2
4
6
8
10
20
100
200
500

1.250
1.125
1.083
1.063
1.050
1.025
1.005
1.003
1.001

1.940
1.430
1.276
1.204
1.161
1.079
1.015
1.008
1.003

1.944
1.450
1.294
1.218
1.173
1.085
1.017
1.008
1.003

optimal weights. It is not surprising that symmetric optimal weights are substantially less efﬁcient (since we are free to choose only about half as many
coefﬁcients). For discussion of HKT optimal weights, see Section 4.2.3 and
Appendix C.
4.11 The Relationship of Differencing to Smoothing
Chapter 3 focused on smoothing techniques. The essential objective was to produce good estimates of a nonparametric regression function. Take, for example,
the basic model
y = f (x) + ε.

(4.11.1)

If one smooths the data by applying a smoother S that takes local averages,
then one can expect a reasonable approximation to the function f
Sy = S f (x) + Sε ∼
(4.11.2)
= Sf ∼
= f,
where Sε ∼
= 0 because smoothing random noise produces the zero function.

Higher-Order Differencing Procedures

91

The present chapter has discussed differencing procedures. The objective has
been to remove a nonparametric effect
Dy = D f (x) + Dε ∼
= Dε.

(4.11.3)

The essence of the relationship between smoothing and differencing is this.
A smoothing procedure can always be used to remove a nonparametric effect.
For example, using (4.11.2) we may write
(I − S)y = (I − S) f (x) + (I − S)ε ∼
= (I − S)ε,

(4.11.4)

and we may think of D = I − S as a differencing procedure.
Differencing, on the other hand, will not in general contain an implicit useful
estimate of the nonparametric effect. This is because there is no requirement for
the order of differencing to increase with sample size. It is, however, a convenient
device for producing test procedures, as we have seen in Section 4.3. (We will
make use of this device in Chapter 6 to produce a general class of goodness-of-ﬁt
tests.)
As we have seen, differencing also yields a simple estimator of the partial
linear model
Dy = D f (x) + D Zβ + Dε
∼
= D Zβ + Dε,

(4.11.5)

where D is applied to data that have been reordered so that the x’s (but not
necessarily the z’s) are close. The asymptotic properties of the differencing
estimator are similar to those produced by the double residual method
(I − S)y = (I − S) f (x) + (I − S)Zβ + (I − S)ε
∼
= (I − S)Zβ + (I − S)ε
∼
= (I − S)Zβ + ε,

(4.11.6)

where S smooths data by a nonparametric regression on the x variable. The
residuals in the differenced model (4.11.5) are Dε and are approximately equal
to ε only if the order of differencing is large. There is an additional distinction
between smoothing and differencing obscured by the preceding notation but
one we have emphasized earlier. When applying differencing, we use a single
differencing matrix D and apply it to all the data. However, when smoothing is
applied in the double residual procedure, it is common to run separate nonparametric regressions for y and each column of Z on the nonparametric variable x.
This implies that the smoothing matrix S will in general be different in each of
these regressions.

92

Semiparametric Regression for the Applied Econometrician

4.12 Combining Differencing and Smoothing
4.12.1 Modular Approach to Analysis of the Partial Linear Model
Our applications of the partial linear model y = zβ + f (x) + ε leave some untidy loose ends. Typically, our analysis is divided into two components: ﬁrst we
obtain a differencing estimate of β and undertake inference procedures on β as
if f were not present in the model. Then we analyze f by performing nonparametric estimation and inference on the newly constructed data (yi − z i β̂ diff , xi )
as if β were known. Is such a modular approach valid? Separate analysis of the
parametric portion is justiﬁed by virtue of results like Proposition 4.5.1. However, a little more justiﬁcation is necessary with respect to the appropriateness
of our analysis of the nonparametric part.12
In the following we will provide justiﬁcation for various modular procedures
we have already implemented – whether they involve combining differencing
procedures in sequence or combining differencing and smoothing procedures.
4.12.2 Combining Differencing Procedures in Sequence
Recall the estimator of the residual variance in the partial linear model y =
zβ + f (x) + ε as deﬁned in (4.5.4):
1
(Dy − D Z β̂ diff ) (Dy − D Z β̂ diff )
n
1
= (y − Z β̂ diff ) D  D(y − Z β̂ diff ).
n

2
sdiff
=

(4.12.1)

It is easy to show that β̂ converges to β sufﬁciently quickly so that the
approximation yi − z i β̂ diff ∼
= f (xi ) + εi remains valid. In particular, we have

1
1
n /2
(y − Z β̂ diff ) D  D(y − Z β̂ diff )
n

1
P
 
− ( f (x) + ε) D D( f (x) + ε) → 0.
(4.12.2)
n
This in turn implies that inference on the residual variance (Propositions 4.2.1
and 4.2.2 and Equation (4.2.15)), speciﬁcation testing (Proposition 4.3.1), and
tests of equality of regression functions (Propositions 4.4.1 and 4.4.2) may be
applied to the data with the estimated parametric effect removed. In each case,
differencing is used ﬁrst to estimate the parametric effect and then to perform
a speciﬁc inference procedure.
12

For example, we have applied tests of speciﬁcation, tests of equality of nonparametric regression
functions, and conventional kernel and spline estimation procedures after removing estimated
parametric effects.

Higher-Order Differencing Procedures

93

4.12.3 Combining Differencing and Smoothing
Suppose we perform a kernel regression of yi − z i β̂ diff on xi . For simplicity,
assume the x’s to be uniformly distributed on the unit interval and that the
uniform kernel is used. Deﬁne λ as the bandwidth and N (xo ) = {xi | xi ∈ xo ±λ}
to be the neighborhood of xo over which smoothing is being performed. Using
a Taylor approximation (see Sections 3.1 and 3.2), we have
1 
fˆ(xo ) ∼
yi − z i β̂ diff
=
2λn N (x )
o

1 
1 
1 
=
f (xi ) +
εi + (β − β̂ diff )
zi
2λn N (x )
2λn N (x )
2λn N (x )
o

o

∼
= f (xo ) + 1/2

1 
f  (xo )
(xi − xo )2
2λn N (x )

o

o

1 
1 
+
εi + (β − β̂ diff )
zi .
2λn N (x )
2λn N (x )
o

(4.12.3)

o

The neighborhood N (xo ) will have close to 2λn terms so that in each summation
we are calculating a simple average.
Consider
the term involving the second derivative, which corresponds to the

bias: N (xo ) (xi − xo )2 /2λn is like the variance of a uniform variable on an
interval of width 2λ centered at xo , in which case it is O P (λ2 ).
The next term corresponds to the variance term: it has mean 0 and variance
σε2 /2λn so that it is O P ((λn)1/2 ). The last term arises out of the removal of the
estimated parametric effect, where β has been estimated n 1/2 -consistently so it
is of order O P (n −1/2 )O P (1). Summarizing, we have
1
1
fˆ(xo ) − f (xo ) = O P (λ2 ) + O P ((λn)− /2 ) + O P (n − /2 )O P (1).
(4.12.4)

So long as λ → 0 and λn → ∞, consistency of the kernel estimator is unaffected because all three terms converge to zero. Furthermore, λ = O(n −1/5 )
still minimizes the rate at which the (sum of the) three terms converge to zero.
1
fˆ(xo ) − f (xo ) = O P (n −2/5 ) + O P (n −2/5 ) + O P (n − /2 )O P (1),
(4.12.5)

so that the optimal rate of convergence is unaffected. The order of the ﬁrst two
terms is O P (n −2/5 ), whereas the third term converges to zero more quickly and
independently of λ. Conﬁdence intervals may also be constructed in the usual

94

Semiparametric Regression for the Applied Econometrician

way, for applying (4.12.4) we have
1
(λn) /2 ( fˆ(xo ) − f (xo ))
1
1
= O P ((λn) /2 λ2 ) + O P (1) + O P (λ /2 )
= O P (1) + O P (1) + O P (n −1/10 )
if λ = O(n −1/5 ),

(4.12.6)

and the third term goes to zero, albeit slowly. If the optimal bandwidth
λ = O(n −1/5 ) is selected, then conﬁdence intervals must correct for a bias term.
Similar arguments apply to other nonparametric estimators. For example, if
one uses a nonparametric least-squares or spline estimator in a regression of
yi − z i β̂ diff on xi , then the estimator fˆ remains consistent and its rate of convergence is unchanged.
4.12.4 Reprise
The practical point of this section is that for the partial linear model y =
zβ + f (x)+ε (or more generally the partial parametric model), we can separate
the analysis of the parametric portion from the analysis of the nonparametric
portion. Given a differencing estimate of β (or for that matter, any n 1/2 -consistent
estimate), we may construct the new dependent variable yi∗ = yi − z i β̂ diff , set
aside the original yi , and analyze the data (yi∗ , xi ) as if they came from the pure
nonparametric model yi∗ = f (xi ) + εi . None of the large sample properties we
have discussed will be affected. This holds true regardless of the dimension of
the parametric variable z.
This idea – that so long as the rate of convergence of an estimator is fast
enough we can treat it as known – will be used extensively to simplify testing
and inference procedures in later chapters. Indeed, we have already used it to
derive a simple speciﬁcation test (Sections 1.4 and 4.3). In the speciﬁcation test
setting, the parametric model estimates converged fast enough so that we could
replace the estimated sum of squared residuals with the actual sum of squared
residuals when deriving an approximate distribution for the test statistic (see,
e.g., (1.4.1)).
4.13 Exercises13
1. (a) Suppose the components of ϑ = (ϑ1 , . . . , ϑξ ) are i.i.d. with Eϑi = 0, Var (ϑi ) =
σϑ2 , Eϑi4 = ηϑ , and covariance matrix σϑ2 Iξ . If A is a symmetric matrix, show
that E(ϑ  Aϑ) = σϑ2 trA and Var (ϑ  Aϑ) = (ηϑ − 3σϑ4 )trA  A + σϑ4 trA A.
(If A, B are matrices of identical dimension, deﬁne [A  B]ij = Aij Bij .)

13

Data and sample programs for empirical exercises are available on the Web. See the Preface
for details.

Higher-Order Differencing Procedures

95

(b) Consider the heteroskedastic case where Var (ϑi ) = σi2 , Eϑi4 = ηi , ϑ has the
diagonal covariance matrix , and η is the diagonal matrix with entries ηi .
Then E(ϑ  Aϑ) = trA and Var (ϑ  Aϑ) = tr(η  A  A − 32  A  A) +
2tr(AA).
2. (a) Suppose x has support the unit interval with density bounded away from 0.
Given n observations on x, reorder them so that they are in increasing
order:
x1 ≤ · · · · ≤ xn . Then for any  positive and arbitrarily close to 0, 1/n (xi −
xi−1 )2 = O P (n −2(1−) ).14
(b) For an arbitrary
collection of points in the unit interval, prove that the maximum
value that 1/n (xi −xi−1 )2 can take is 1/n. (This occurs when all observations
are at one of the two endpoints of the interval.)
3. (a) Using the results of Exercise 2, prove that
n

1/2

2
sdiff
−

1  
P
ε D Dε → 0.
n

2
(b) Using Exercise 1, derive the mean and variance of sdiff
as in (4.2.6) and (4.2.7).
(c) Now assemble the results and use a ﬁnitely dependent central limit theorem to
prove Proposition 4.2.1.

4. Derive (4.2.13) and use it to prove Proposition 4.2.2.
5. Prove Proposition 4.3.1.
6. Prove Proposition 4.4.1.
7. Prove Proposition 4.4.2.
8. (a) Suppose an equal number of observations are drawn from two populations
A and B with differing regression functions f A = f B . Furthermore, suppose
the data are such that upon pooling and reordering, the x’s become perfectly
interleaved. That is, an observation from subpopulation A is always followed
by an observation from subpopulation B. Show that the pooled estimator in
(4.4.4) with ﬁrst-order differencing converges as follows:
s 2p → σε2 + 1/2

( f A (x) − f B (x))2 d x.

(b) Suppose, more generally, that x A and x B are independently and uniformly distributed on the unit interval and an equal number of observations are taken from
each subpopulation (i.e., n A = n B = 1/2 n). Show that the pooled estimator in
(4.4.4) with ﬁrst-order differencing converges as in (4.4.7).
9. South African Food Share Engel Curves – Testing Parametric Speciﬁcations.
Results should be similar to Figure 4.1.
(a) Using South African food share data on single individuals, ﬁt a linear regression
model of the form FoodShr = α + βltexp + ε, where FoodShr is the food share
14

Because  may be chosen arbitrarily close
1/n
 to zero, we2 will write
O P (n −2 ). Note also that for ﬁxed j, 1/n
(xi − xi− j ) ∼
= O P (n −2 ).



(xi − xi−1 )2 ∼
=

96

Semiparametric Regression for the Applied Econometrician
and ltexp is the log of total expenditure. Obtain the estimate of the residual
variance and the R 2 .
(b) Fit a kernel smooth fˆ to the data and plot the linear and kernel ﬁts on one graph.
Estimate the residual variance using the kernel ﬁt by applying
2
sker
=

1
(yi − fˆ(xi ))2 .
n

(c) Estimate the residual variance using the optimal differencing estimator (4.2.4)
for m = 5, 10, 25.
(d) Test the linear ﬁt by applying Proposition 4.3.1.
10. South African Food Share Data – Testing Similarity of Shape of Engel Curves.
Results should be similar to Figure 4.2.
(a) From the South African food share data, select two subsets: the ﬁrst consisting
of single individuals and the second consisting of couples with no children.
(b) Fit kernel smooths to the model FoodShr = f (ltexp) + ε for each of the two
subsets and plot these on a single graph.
(c) For the couples data, subtract .5 from ltexp, the log of total expenditure (which
is equivalent to dividing total expenditure by 1.65). Replot the kernel smooths
from Part (b).
(d) Using the “singles” data and the translated “couples” data, apply Proposition 4.4.2 to test equality of regression functions.
11. Household Gasoline Consumption: The objective of this problem is to apply differencing and double residual methods. The results should be similar to those in
Figures 4.3 and 4.4.
(a) Using the data on household gasoline consumption, estimate (4.6.1) under the
assumption that f is linear. Calculate the R 2 = 1 − sε2 /s y2 .
(b) Reorder the data so that the price variable is in increasing order. Use optimal
differencing coefﬁcients (say m = 10) to remove the nonparametric effect and
estimate the parametric effects in (4.6.1). Estimate the residual variance using
(4.5.4). Estimate the standard errors using (4.5.5) and (4.5.6). Calculate R 2 =
2
1 − sdiff
/s y2 .
(c) Apply the double residual method outlined in Section 3.6 by ﬁrst doing a kernel
regression of the dependent variable and each of the parametric variables in
(4.6.1) on the log of price. (You will perform 18 plus 1 regressions.) Save the
residuals from each and apply ordinary least squares. The OLS procedure will
produce the estimated standard errors for the parametric effects. Calculate the
average sum of squared residuals from this OLS regression and use this as the
estimate of the residual variance. Calculate R 2 = 1 − sε2 /s y2 .
(d) Using the estimates from the differencing procedure in Part (b), remove the
estimated parametric effect from the dependent variable and perform a kernel
regression of this purged variable on the log of price. Plot this and the estimated
parametric effect of price from Part (a).

Higher-Order Differencing Procedures

97

(e) Apply Proposition 4.3.1 to test linearity of the price effect. Run a linear regression of model (4.6.1), omitting the price variable. Using the estimated residual
variance, apply Proposition 4.3.1 to test the signiﬁcance of the price effect.
(f) Estimate the heteroskedasticity-consistent standard errors using (3.6.17) and
(3.6.18).
12. Scale Economies in Electricity Distribution: The results should be similar to those
in Figures 4.5 and 4.6 and Table 4.3.
(a) Using the electricity distribution data and (4.6.2), estimate semiparametric variants of translog, homothetic, and Cobb–Douglas models. In each case the scale
variable (number of customers) is nonparametric and other variables are parametric.
(b) Test the semiparametric Cobb–Douglas variant against the full semiparametric
translog using Proposition 4.5.2.
(c) Divide your data into two subsets consisting of public utility commissions
(PUC) and non-PUC. Using the Cobb–Douglas variant of (4.6.2), apply the
differencing estimator of Proposition 4.5.1 to estimate the separate parametric
effects for each subpopulation. Apply (4.6.3) to test the null that the parametric
effects are the same.
(d) Calculate the weighted combination of these two estimates β̂ weighted using
(4.6.4) and (4.6.5). For each subpopulation, use β̂ weighted to remove the estimated parametric effects by subtracting β̂ weighted (z − z̄) from the dependent
variable. The vector mean of the independent variables z̄ should be calculated
separately within each subpopulation.
(e) Test whether the nonparametric regression effects for the two subpopulations
are parallel.
(f) Estimate heteroskedasticity-consistent standard errors using the two methods
outlined in Section 4.5 (in particular see (4.5.9) and (4.5.10)).
13. Weather and Electricity Demand: Results should be similar to Figure 4.7.
(a) Estimate the relationship between electricity consumption, the price of electricity relative to natural gas, and a quadratic temperature effect. Obtain Newey–
West standard errors for the coefﬁcients of this parametric speciﬁcation.
(b) Estimate the partial linear model (4.6.9) using the double residual method and
the loess estimator. Obtain heteroskedasticity- and autocorrelation-consistent
standard errors using the procedures outlined in Section 3.6.5 (assume
L = 5).
14. CES Cost Function and Scale Economies in Electricity Distribution: Results should
be similar to those in Table 4.4.
(a) Using the electricity distribution data and a nonlinear least-squares procedure,
estimate (4.7.9) assuming the function f that measures the scale effect is
quadratic.

98

Semiparametric Regression for the Applied Econometrician
(b) After ensuring that the data are reordered so that the scale effect is increasing,
ﬁrst difference the speciﬁcation as indicated in (4.7.10)
√ and apply nonlinear
least squares. Rescale the estimated standard errors by 1.5.
(c) Using the estimated residual variances from the two preceding procedures,
apply the speciﬁcation test in Section 4.3 to assess the quality of the quadratic ﬁt.

15. Endogeneity of Observed Price of Gasoline
(a) Regress the log of price on the Regional Dummies and save the residuals û.
(b) Estimate (4.9.4) where z consists of all parametric variables appearing in (4.6.1).
Test the endogenity of price by determining whether the coefﬁcient of û is
signiﬁcantly different from zero.

5

Nonparametric Functions
of Several Variables

5.1 Smoothing
5.1.1 Introduction
In economics it is rarely the case that one is interested in a function of a
single variable. Moreover, even if one is comfortable incorporating most of
the explanatory variables parametrically (e.g., within a partial linear model),
more than one variable may enter nonparametrically. The effects of geographic
location – a two-dimensional variable – provides a good example. (Indeed,
in Section 5.4.1 we estimate the effects of location on housing prices nonparametrically while permitting other housing characteristics to be modeled
parametrically.)
In this chapter we therefore turn to models in which there are several nonparametric variables. A variety of techniques are available. We will focus primarily
on kernel and nonparametric least-squares estimators. However, the elementary “moving average smoother” which we considered in Section 3.1 has a
close multidimensional relative in nearest-neighbor estimation. Spline techniques have natural generalizations (see particularly Wahba 1990 and Green
and Silverman 1994). Local linear and local polynomial smoothers also have
multivariate counterparts (see Fan and Gijbels 1996).
5.1.2 Kernel Estimation of Functions of Several Variables
Suppose f is a function of two variables and one has data (y1 , x1 ), . . . , (yn , xn )
on the model yi = f (xi1 , xi2 ) + εi , where xi = (xi1 , xi2 ). We will assume f is
a function on the unit square [0,1]2 . We want to estimate f (xo ) by averaging
nearby observations; in particular, we will average observations falling in a
square of dimension 2λ × 2λ, which is centered at xo . If the xi are drawn from,
say, a uniform distribution on the unit square, then there will be (approximately)
4λ2 n observations in the neighborhood N (xo) = {xi |xi1 ∈ xo1 ± λ, xi2 ∈ xo2 ± λ}.
For example, any square with sides 2λ = .5 has area .25 and will capture about
99

100

Semiparametric Regression for the Applied Econometrician

25 percent of the observations. Consider then
1 
fˆ(xo ) = 2
yi
4λ n N (x )
o

1 
1 
= 2
f (xi ) + 2
εi
4λ n N (x )
4λ n N (x )
o

∼
=
∼
=

o

1 
f (xo ) + O(λ2 ) + 2
εi
4λ n N (x )
o


1
.
f (xo ) + O(λ2 ) + O P
λn 1/2

(5.1.1)

We have mimicked the reasoning in Sections 3.1 and 3.2 but this time for the
bivariate uniform kernel estimator. (As before, we have assumed that f is twice
differentiable but spared the reader the details of the Taylor series expansion.)
The last line of (5.1.1) may now be compared with its counterpart in the univariate case, (3.2.4a). Note the subtle difference. The bias term is still proportional
to λ2 , but the variance term is now O P (1/λn 1/2 ) rather than O P (1/λ1/2 n 1/2 ) since
we are averaging approximately 4λ2 n values of εi .
Hence, for consistency, we now need λ → 0 and λn 1/2 → ∞. As before,
convergence of fˆ (xo ) to f (xo ) is fastest when the bias and variance terms go
to zero at the same rate, that is, when λ = O(n −1/6 ). The second and third
terms of the last line of (5.1.1) are then O(n −1/3 ) and O P (n −1/3 ), respectively.
Furthermore, [ fˆ (x) − f (x)]2 d x = O P (n −2/3 ), which is optimal (see (2.4.1)).
More generally, if the xi are d-dimensional with probability density p(x)
deﬁned, say, on the unit cube in Rd and we are using a kernel K then the
estimator becomes
 xij −xoj 
1 n !d
1 yi
j=1 K
λ
dn
λ
fˆ(xo ) =
(5.1.2)
 xij −xoj  .
1 n !d
K
λ
λd n 1 j=1
Again, if K is the uniform kernel that takes the value 1/2 on [−1,1], then
the product of the kernels (hence the term product kernel) is 1/2d only if xij ∈
[xoj − λ, xoj + λ] for j = 1, . . . , d, that is, only if xi falls in the d-dimensional
cube centered at xo with sides of length 2λ. The estimator is consistent if
λ → 0 and λd/2 n 1/2 → ∞. Indeed, the numerator converges to f (xo ) p(xo ) and
the denominator converges to p(xo ), where p(x) is the density function of x.
Conﬁdence interval construction is simpliﬁed if the bias term converges to zero
sufﬁciently quickly so that it does not affect the asymptotic distribution. For
d = 2, one requires λ = o(n −1/6 ); for d = 3, the condition is λ = o(n −1/7 ).
In the preceding paragraph we have introduced a simple kernel estimator for
functions of several variables that averages observations over a cube centered

Nonparametric Functions of Several Variables

101

at xo . A multitude of variations and alternatives exist. For example, one could
select different bandwidths for each dimension so that averaging would take
place over rectangular cubes rather than perfect cubes. Or, one might select
different kernels for each dimension. Still more generally, one could average
over nonrectangular regions such as spheres or ellipsoids.1
5.1.3 Loess
The loess estimator we described in Section 3.4 extends readily to the multivariate setting. To estimate the function at a point, say xo , k nearest-neighbors
are selected and a weighted local regression is performed. In S-Plus, one can
choose between local linear and local quadratic regressions. In addition one
can choose to rescale the explanatory variables by their standard deviations.
For more details, see Chambers and Hastie (1993, pp. 309–376). In the applications to follow, we use loess to estimate the ﬁrst-stage nonparametric regression
in a double residual estimator of the partial linear model.
5.1.4 Nonparametric Least Squares
In Section 3.3 we introduced a nonparametric least-squares estimator for functions of one variable. The estimator is a linear combination of functions called
representors, and it uses a measure of smoothness (3.3.1) that integrates the
square of a function and two or more of its derivatives. Suppose we are given data
(y1 , x1 ), . . . , (yn , xn ) on the model yi = f (xi1 , xi2 ) + εi , where xi = (xi1 , xi2 ).
We will assume f is a function on the unit square [0,1]2 . Deﬁne the Sobolev
norm  f Sob as in Appendix D. Suppose fˆ satisﬁes
s 2 = min
f

1
[yi − f (xi )]2
n i

s.t.  f 2Sob ≤ L .

(5.1.3)


Then the solution is of the form fˆ = n1 ĉi r xi , where r x1 , . . . , r xn are functions
computable from x1 , . . . , xn , and ĉ = (ĉ1 , . . . , ĉn ) is obtained by solving
1
min [y − Rc] [y − Rc]
c n

s.t. c Rc ≤ L .

(5.1.4)

Here y is the n × 1 vector of observations on the dependent variable, and
R is the matrix of inner products of the representors r xi . The difference between the one-dimensional problem (3.3.3) and its two-dimensional counterpart is the calculation of the representor matrix. Fortunately, in our setup,
1

See Scott (1992, pp. 149–155) and Wand and Jones (1995, pp. 103–105).

102

Semiparametric Regression for the Applied Econometrician

two-dimensional representors are products of one-dimensional representors. In
particular, let r xi (x1 , x2 ) be the representor function at the point xi = (xi1 , xi2 ).
Then r xi (x1 , x2 ) = r xi1 (x1 )r xi2 (x2 ), where r xi1 (x1 ) is the representor in the
Sobolev space of functions of x1 , and r xi2 (x2 ) is deﬁned analogously. Theoretical and computational details are contained in Appendix D. See also Wahba
(1990) and Yatchew and Bos (1997).
5.2 Additive Separability
5.2.1 Backﬁtting
The nonparametrics literature has devoted considerable attention to improving
the rate of convergence of nonparametric estimators using additive models,
which, in the simplest case, are of the form f (xa , xb ) = f a (xa ) + f b (xb ).2 A
powerful and general algorithm used to estimate additively separable models
is motivated by the observation that
E[y − f a (xa ) | xb ] = f b (xb )

and

E[y − f b (xb ) | xa ] = f a (xa ).
(5.2.1)

If fˆa is a good estimate of f a , then f b may be estimated by nonparametric
regression of y − fˆa (xa ) on xb . A parallel argument holds for estimation of f a .
Beginning with these observations, the algorithm in Table 5.1 has been widely
studied.
The initial estimates f a0 , f b0 in Table 5.1 may be set to zero or to the estimates
from a parametric procedure (such as a linear regression).
The procedure may be generalized in the obvious fashion to additively separable models with more than two additive terms, where each term may be a
function of several variables. Assuming that optimal nonparametric estimators
are applied to each component, the rate of convergence of the estimated regression function equals the rate of convergence of the component with the largest
number of explanatory variables.
For example, if y = f a (xa ) + f b (xb ) + f c (xc ) + ε, where xa , xb , xc are scalars, and optimal estimators are applied in the estimation of f a , f b , f c , then the
rate of convergence of fˆa + fˆb + fˆc is the same as if the regression model were
a function of only one variable. That is, (( fˆa + fˆb + fˆc ) − ( f a + f b + f c ))2 =
O P (n −2m/(2m+1) ), where m is the number of bounded derivatives.3
2

3

See, for example, Stone (1985, 1986); Buja, Hastie, and Tibshirani (1989) Hastie and
Tibshirani (1987, 1990); Linton (1997); Linton, Mammen, and Nielsen (1999); Linton (2000)
and references therein.
Applying (2.4.1) and assuming two bounded derivatives, we have ( fˆa + fˆb + fˆc )2 =
O P (n −4/5 ) as compared with O P (n −4/7 ) for the model f (xa , xb , xc ) or O P (n −1 ) for the
parametric model.

Nonparametric Functions of Several Variables

103

Table 5.1. The backﬁtting algorithm.a
Initialization:
Iteration:
Convergence:

a

Select initial estimates f a0 , f b0 .
Obtain fˆia by nonparametric regression of y − fˆi−1
b (x b ) on x a .
Obtain fˆib by nonparametric regression of y − fˆi−1
a (x a ) on x b .
Continue iteration until there is little change in individual function
estimates.

See Hastie and Tibshirani (1990), Chapter 4 and references therein.

More generally, if f is additively separable with parametric and nonparametric components, say y = zβ + f a (xa ) + f b (xb ) + ε, where z, xa , xb are of
dimension dz , da , db , respectively, then the rate of convergence of the optimal
estimator is given by4
((z β̂ + fˆa (xa ) + fˆb (xb )) − (zβ + f a (xa ) + f b (xb )))2


−2m
= O P n 2m+max{da ,db } .

(5.2.2)

The result is very similar to (2.4.1) except that d has been replaced by
max{da ,db}.
In performing the component nonparametric regressions, a variety of techniques may be used, including kernel, spline, and loess estimation. Indeed, the
algorithm is particularly versatile in that different techniques may be selected
for different components. For example, f a may be estimated using kernel regression and f b using nonparametric least squares (or even nonparametric least
squares subject to constraints). The backﬁtting algorithm is available in S-Plus
using the function gam (generalized additive model).
An alternative procedure for estimation of additive (and multiplicative) models based on marginal integration has been proposed by Newey (1994b) and
Linton and Nielsen (1995). One of the major attractions to their approach is
that, in contrast to backﬁtting, their estimator has simple statistical properties.
5.2.2 Additively Separable Nonparametric Least Squares
We turn now to nonparametric least-squares estimation of the additively separable model. The optimization problem in (5.1.3) becomes
1
min
[yi − f a (xai ) − f b (xbi )]2 s.t.  f a + f b 2Sob ≤ L ,
fa , fb n
i
(5.2.3)
4

Again assuming two bounded derivatives and supposing that the dimensions dz , da , db are
3, 2, 1, respectively, then the optimal rate of convergence is O P (n −2/3 ) as compared with
O P (n −4/10 ) for the model f (z, xa , xb ) or O P (n −1 ) for the parametric model.

104

Semiparametric Regression for the Applied Econometrician

which can be transformed into the ﬁnite dimensional optimization problem
1
min [y − Ra ca − Rb cb ] [y − Ra ca − Rb cb ]
ca ,cb n
s.t. ca Ra ca + cb Rb cb ≤ L ,

(5.2.4)

where y is the n × 1 vector of observations on the dependent variable, ca ,
cb are n × 1 vectors of unknowns, and Ra , Rb are (representor) matrices
that may be computed directly from xa1 , . . . , xan and xb1 , . . . , xbn , respectively. The estimated regression function is of the form fˆa (xa ) + fˆb (xb ) =

n
1 ĉai r xai (x a ) + ĉbi r xbi (x b ). The optimization problem involves a quadratic
objective function and a quadratic constraint. (There is also an identifying restriction that may be imposed as a linear function of the ci , but this does not
complicate the problem appreciably.) See Appendix D and Yatchew and Bos
(1997).
A similar procedure is available if the model is multiplicatively separable, that
is, f (xa , xb ) = f a (xa ) · f b (xb ). Note that this restriction is useful in imposing
homotheticity.
5.3 Differencing
5.3.1 Two Dimensions
Consider again the pure nonparametric model
y = f (x) + ε,

(5.3.1)

where x is a vector of dimension 2. Suppose, as in Section 1.2, that we are
interested in estimating the residual variance. We would like to ensure that the
data (y1 , x1 ) . . . . (yn , xn ) have been reordered so that the x’s are “close”.
For illustrative purposes, suppose the x’s constitute a uniform grid on the unit
square. Each point may be thought of as “occupying” an area of 1/n, and the
distance between adjacent observations is therefore 1/n 1/2 .5 Suppose further
that the data have been reordered so that xi − xi−1  = n −1/2 , where · denotes
the usual
If we use the ﬁrst-order differencing estimator
nEuclidean distance.
2
sdiff
= i=2
(yi − yi−1 )2 /2n, then


n
n
 2

1 
1 
2
2
2
sdiff − σε =
(εi − εi−1 ) − σε +
( f (xi ) − f (xi−1 ))2
2n i=2
2n i=2
+

5

n
2 
(εi − εi−1 )( f (xi ) − f (xi−1 )).
2n i=2

(5.3.2)

If x is a scalar and the data are equally spaced on the unit interval, the distance between
adjacent observations is 1/n, which is much closer.

Nonparametric Functions of Several Variables

105

The ﬁrst term on the right-hand-side is O P (n −1/2 ). Assume that f satisﬁes a
Lipschitz constraint | f (xa ) − f (xb )| ≤ Lxa − xb . Then for the second term
we have
 
n
n
1 
1  2
1
2
2
( f (xi ) − f (xi−1 )) ≤
L xi − xi−1  = O
.
2n i=2
2n i=2
n
(5.3.3)
Consider now


n
n
2 
σ2 
Var
εi ( f (xi ) − f (xi−1 )) = ε2
( f (xi ) − f (xi−1 ))2
2n i=2
n i=2
 
n
σε2  2
1
2
(5.3.4)
≤ 2
L xi − xi−1  = O 2 ,
n i=2
n
from which we can conclude that the third term of (5.3.2) is O P (1/n).
The point of this exercise is that if x is two-dimensional, differencing removes
the nonparametric effect sufﬁciently quickly, so that


 
 
n

 2

1
1
1
2
2
2
sdiff − σε =
(εi − εi−1 ) − σε + O
+ OP
.
2n i=2
n
n
(5.3.5)
Compare this with the one-dimensional case in which the second and third
terms are O(1/n 2 ) and O P (1/n 3/2 ), respectively.
5.3.2 Higher Dimensions and the Curse of Dimensionality
With n points distributed on the uniform grid in the unit cube in Rq , each
point continues to “occupy”a volume 1/n, but the distance between adjacent
observations is 1/n 1/q and ( f (xi ) − f (xi−1 ))2 = O(n 1−2/q ). Thus, (5.3.3)
becomes O(1/n 2/q ), and the variance of the third term becomes O(1/n 1+2/q ).
Hence, with the x’s lying in a q-dimensional cube, (5.3.5) becomes






n
 2

1
1
1 
2
2
2
sdiff − σε =
(εi − εi−1 ) − σε + O 2/q + O P
2n i=2
n
n 1/2 + 1/q






1
1
1
= OP
+
O
+
O
.
(5.3.6)
P
n 1/2
n 2/q
n 1/2 + 1/q
How does the dimensionality of x affect the differencing estimator? First,
2
sdiff
remains consistent regardless of the dimension of x. However, the bias term
(the second term of (5.3.6)) converges to 0 more slowly as q increases. Second,

106

Semiparametric Regression for the Applied Econometrician

2
sdiff
is n 1/2 -consistent and
1  2
n /2 sdiff

−

σε2



−


1
n /2

n
1 
(εi − εi−1 )2 − σε2
2n i=2


P

→0

(5.3.7)

if q does not exceed 3. This is important because, whenever we have derived
the large sample distribution of an estimator or test statistic that uses differencing, we have used the property that the nonparametric effect is being removed
sufﬁciently quickly that it can be ignored. Essentially, this has required that a
condition like (5.3.7) hold.
With random x’s, similar results hold so long as reasonable ordering rules
are used. If x is a scalar, the obvious ordering rule is x1 ≤ · · · ≤ xn . If x is of
dimension 2 or 3, we propose the following ordering rule based on the nearestneighbor algorithm because it is simple to compute. (Other ordering rules for
which the conclusion of Proposition 5.3.1 holds can easily be devised.)
Proposition 5.3.1: Suppose x has support the unit cube in Rq with
density bounded away from 0. Select  positive and arbitrarily close to 0. Cover
the unit cube with subcubes of volume 1/n 1− , each with sides 1/n (1−)/q .
Within each subcube, construct a path using the nearest-neighbor algorithm.
Following this, knit the paths together by joining endpoints in contiguous
 subcubes to obtain a reordering of all the data. Then, for any  > 0, 1/n xi −
xi−1 2 = O P (n −2(1−)/q ).
Because  may be chosen arbitrarily close to 0, we write 1/n
xi−1 2 ∼
= O P (n −2/q ).



xi −

We may now assert that propositions in Chapter 4, where we considered only a scalar nonparametric variable, continue to hold so long as
q, the number of nonparametric variables, does not exceed 3 and the
preceding ordering rule is employed. In particular, the speciﬁcation test
of Section 4.3 and the analysis of the partial linear model of Section 4.5
continue to apply in this more general setting.6
However, it is important to keep in mind that, as dimension increases from 1
to 2 to 3, and the x’s become progressively more dispersed, bias becomes a much
more important issue. From this point of view, using a smoother that efﬁciently
6

Additional testing procedures using differencing may be found in Yatchew (1988). Tests of
signiﬁcance, symmetry, and homogeneity are proposed. Although that paper uses sample
splitting to obtain the distribution of the test statistic, the device is often unnecessary, and
the full data set can be used to calculate the restricted and unrestricted estimators of the
residual variance. A test of homotheticity may also be devised. Chapter 6 contains an extensive
discussion of hypothesis testing.

Nonparametric Functions of Several Variables

107

removes nonparametric effects becomes more appealing. Nevertheless, there
are applications for which differencing may be applied effectively as will be
illustrated in Section 5.4.2.
5.4 Empirical Applications
5.4.1 Hedonic Pricing of Housing Attributes
Housing prices are very much affected by location, which is an effect that
has no natural parametric speciﬁcation. The price surface may be unimodal,
multimodal, or have ridges (for example, prices along subway lines are often
higher). Therefore, we include a two-dimensional nonparametric location effect
f (x1 , x2 ), where x1 , x2 are location coordinates.
The partial linear model that follows was estimated by Ho (1995) using semiparametric least squares. The data consist of 92 detached homes in the Ottawa
area that sold during 1987. The dependent variable y is saleprice; the z variables
include lot size (lotarea), square footage of housing (usespc), number of bedrooms (nrbed), average neighborhood income (avginc), distance to highway
(dhwy), presence of garage (grge), ﬁreplace ( frplc), or luxury appointments
(lux).
Figure 5.1 contains estimates of a pure parametric model in which the location
effect is modeled using a linear speciﬁcation. It also contains estimates of
the partial linear model. Having estimated the parametric effects using the
double residual method, where loess is applied in the ﬁrst stage, the constructed
data (yi − z i β̂ diff , x1i , x2i ) are then smoothed to estimate the nonparametric
effect. For an alternative semiparametric hedonic pricing model in the real
estate market, see Anglin and Gencay (1996).
5.4.2 Household Gasoline Demand in Canada
We now respecify the model in (4.6.1), allowing price and age to appear nonparametrically:
dist = f (price, age) + β1 income + β2 drivers + β3 hhsize
+ β4 youngsingle + β5 urban + monthly dummies + ε.
(5.4.1)
The upper panel of Figure 5.2 illustrates the scatter of data on price and age
and the path we take through the data points to apply differencing. Estimates of
the parametric effects are provided using differencing and the double residual
method. These do not differ substantially from those in which only price is
modeled nonparametrically (see Figure 4.3). A test of the joint signiﬁcance
of the nonparametric variables using Proposition 4.3.1 yields a value of 5.96,
which is strongly signiﬁcant. A test of a fully parametric speciﬁcation where

108

Semiparametric Regression for the Applied Econometrician

Estimated models
y = α + zβ + γ1 x1 + γ2 x2 + ε

y = zβ + f (x1 , x2 ) + ε

OLS

Double residual using loess

Coeff

SE

74.0
11.7
11.8
60.7
.478
−15.3
3.2
6.6
21.1

18.0
6.2
5.1
10.5
.22
6.7
2.3
4.9
11.0

γ1
γ2

7.5
−3.2

2.2
2.5

R2
2
sres

.62
424.3

α
frplc
grge
lux
avginc
dhwy
lotarea
nrbed
usespc

Data with parametric effect removed

frplc
grge
lux
avginc
dhwy
lotarea
nrbed
usespc

R2
2
sdiff

Coeff

SE

12.6
12.9
57.6
.60
1.5
3.1
6.4
24.7

5.8
4.9
10.6
.23
21.4
2.2
4.8
10.6

.66
375.5

Estimated location effects

2 = 507.4. For the partial linear model
Under the null that location has no effect, ( f is constant), sres
2 /s 2 . The loess function in S-Plus is used in applying the double-residual
we calculate R 2 = 1 − sdiff
y
method and to produce the “Estimated location effects” after removal of parametric effects. In the
latter case, the dependent variable is yi − z i β̂ diff .

Figure 5.1. Hedonic prices of housing attributes.

109

0.021
0.035
0.029
0.063
0.020

.270
.496

0.287
0.532
0.122
0.198
−0.331

SE

SE
0.021
0.033
0.027
0.061
0.020

.263
.501

0.291
0.571
0.093
0.191
−0.332

Coef

Double residual

Figure 5.2. Household gasoline demand in Canada.

Dependent variable is the log of distance traveled.
Var(dependent variable) = .679. Order of differencing
m = 10.
Upper panel right illustrates ordering of data: data are
reordered ﬁrst by age; then for even ages, data are
reordered so that price is decreasing, and for odd ages,
data are reordered so that price is increasing.
Lower panel right illustrates nonparametric estimate
of age and price effects after removal of estimated
parametric effects.
Monthly dummies were included in estimation but are
not reported here.

R2
s2

income
drivers
hhsize
youngssingle
urban

Coef

Differencing

110

Semiparametric Regression for the Applied Econometrician

price and age enter log-linearly yields a statistic of 2.16. For more extensive
analysis of these data, including the application of other testing procedures, see
Yatchew and No (2001).
5.5 Exercises7
1. (a) Using a second-order Taylor series expansion for functions of two variables,
verify (5.1.1).
(b) Assume that p(x) is uniform on the unit square and that one is using the uniform
kernel. Show that the denominator of (5.1.2) converges to 1 and that the numerator
reduces to the ﬁrst line of (5.1.1) if the dimension d = 2.
2. Hedonic Housing Prices: The objective is to estimate a partial linear speciﬁcation
in which the two-dimensional location effect is modeled using nonparametric and
additively separable speciﬁcations.
(a) Open the data on housing prices. Estimate a fully parametric model using a
linear speciﬁcation for the location effect. Repeat using a quadratic speciﬁcation.
Calculate the corresponding values of R 2 .
(b) Using the double residual method, estimate the partial linear model y = zβ +
f (x1 , x2 ) + ε contained in Figure 5.1. (Use loess to estimate the effects of
the location variables x1 , x2 on the dependent variable y and the parametric
independent variables z.) Calculate R 2 .
(c) Estimate the model allowing an additively separable location effect y = zβ +
f 1 (x1 ) + f 2 (x2 ) + ε. (The gam function in S-Plus allows one to estimate the
nonparametric effects using alternative nonparametric estimators such as kernel,
spline, or loess.) Calculate R 2 .
3. Household Gasoline Consumption: The objective is to estimate a partial linear speciﬁcation with a two-dimensional nonparametric effect.
(a) Estimate the gasoline consumption model (5.4.1) using differencing and the ordering procedure pictured in the upper panel of Figure 5.2. Calculate the implied
R2.
(b) Test the signiﬁcance of the nonparametric effects using the speciﬁcation test in
Proposition 4.3.1.
(c) Reestimate using the double residual method. (You may use loess to estimate
the ﬁrst-stage nonparametric regressions.) Compare the estimates of parametric
effects, their standard errors, and the corresponding R 2 with the results obtained
in Part (a).
(d) Reestimate (5.4.1) using an additively separable speciﬁcation for the nonparametric variables f (price, age) = f 1 ( price) + f 2 (age) and compare the resulting
R 2 ﬁt with those obtained in Parts (a) and (b).
(e) Estimate a fully parametric speciﬁcation in which price and age enter log-linearly.
Using the results of Part (a) and Proposition 4.3.1, perform a speciﬁcation test
to assess the adequacy of the parametric model.
7

Data and sample programs for empirical exercises are available on the Web. See the Preface
for details.

6

Constrained Estimation
and Hypothesis Testing

6.1 The Framework
Economic theory rarely dictates a speciﬁc functional form. Instead, it typically
speciﬁes a collection of potentially related variables and general functional
properties of the relationship. For example, economic theory may imply that
the impact of a given variable is positive or negative (monotonicity), that doubling of prices and incomes should not alter consumption patterns (homogeneity
of degree zero), that a proportionate increase in all inputs will increase output
by the same proportion (constant returns to scale or, equivalently, homogeneity
of degree one), that the effect of one variable does not depend on the level of
another (additive separability), that the relationship possesses certain curvature properties such as concavity or convexity, or that observed consumption
patterns result from optimization of utility subject to a budget constraint (the
maximization hypothesis).
Empirical investigation is then required to assess whether one or another
variable is signiﬁcant or whether a particular property holds. In parametric
regression modeling, a functional form is selected and properties are tested by
imposing restrictions on the parameters. However, rejection of a hypothesis
may be a consequence of the speciﬁc functional form that has been selected
(but not implied by economic theory). Thus, although the translog production
function is richer and more ﬂexible than the Cobb–Douglas, it may not capture
all the interesting features of the production process and may indeed lead to
incorrect rejection of restrictions. Nonparametric procedures, on the other hand,
provide both richer families of functions and more robust tests for assessing the
implications of economic theory. Within this framework it is also possible to
test whether a speciﬁc parametric form is adequate.
In the following sections, we therefore focus on the imposition of additional
constraints on nonparametric regression estimation and testing of these constraints. However, before proceeding, we provide some standardized notation.

111

112

Semiparametric Regression for the Applied Econometrician

(The ideas are illustrated graphically in Figure 6.1.) Begin with the true model
y = f (x) + ε.

(6.1.1)

We will maintain that f lies in the set , which is a set of smooth functions.
The corresponding (unrestricted) estimator is denoted as fˆunr with corresponding estimated residual variance
1
2
sunr
(yi − fˆunr (xi ))2 .
=
(6.1.2)
n
We also want to estimate f subject to constraints of the form f ∈ ¯ ⊂ ,
where the set ¯ combines smoothness with additional functional properties. We
denote the restricted estimator as fˆres with corresponding estimated residual
variance
1
2
sres
(yi − fˆres (xi ))2 .
=
(6.1.3)
n
In some cases, it is a simple matter to ensure that the restricted estimator fˆres
satisﬁes these additional properties everywhere in its domain. In other cases,
the estimator may only satisfy the restrictions asymptotically.
Our general null and alternative hypotheses will be of the form
H0 : f ∈ ¯
H1 : f ∈ .

(6.1.4)

Deﬁne f¯ to be the “closest” function to f in the restricted set ¯ in the sense
that
f¯ satisﬁes min

f ∗ ∈ ¯

( f ∗ − f )2 d x.

(6.1.5)

¯ the integral is equal
When the null hypothesis is true, f¯ = f (since f ∈ ),
¯
to zero and the restricted estimator converges to f = f . If the null hypothesis
is not true, we will assume fˆres converges to f¯ = f .
One ﬁnal important notational convention follows. Because certain tests will
depend on the difference between the true regression function and the closest
¯ we will reserve special notation for it. In particular,
function in ,
f  = f − f¯ .

(6.1.6)

If the null hypothesis is true, f  = 0.
Much of the testing literature for parametric regression models can be embedded in a general and uniﬁed theoretical framework. The situation is somewhat
less gratifying for nonparametric models, although several approaches show
promise. We therefore begin by outlining two generic testing procedures that directly test hypotheses on f – one is analogous to a conventional goodness-of-ﬁt

Constrained Estimation and Hypothesis Testing

113

 is the “unrestricted” set of functions, ¯ is the “restricted” set of functions. Let f¯ be the closest
function in ¯ to the true regression function f. If Ho is true, then f lies in  and f¯ = f . If Ho is
false, then the difference f  = f − f¯ = 0.

Figure 6.1. Constrained and unconstrained estimation and testing.

test (such as the familiar F or χ 2 tests); the other involves performing a residual
regression. (For a third general approach, see Hall and Yatchew 2002.)
6.2 Goodness-of-Fit Tests
6.2.1 Parametric Goodness-of-Fit Tests
A natural basis for testing constraints imposed on the regression function is to
compare the restricted estimate of the residual variance with the unrestricted
estimate
 2

2
n 1/2 sres
− sunr
.
(6.2.1)
2
sunr
The reader will recognize that, in the linear regression model with normal
residuals, multiplying this statistic by n 1/2 /r , where r is the number of restrictions, yields a statistic very similar to the traditional F-statistic.

114

Semiparametric Regression for the Applied Econometrician

The difﬁculty in applying the usual F-statistic in a nonparametric setting
lies in the derivation of its (approximate) distribution. Consider the problem of
testing a parametric null hypothesis against a nonparametric alternative (i.e., a
speciﬁcation test). How would one calculate the numerator degrees of freedom,
that is, the degrees of freedom associated with restricting a nonparametric form
to be parametric?1
6.2.2 Rapid Convergence under the Null
In some cases, however, it is relatively straightforward to obtain an approximate
distribution for (6.2.1). Suppose that the restricted estimator fˆres converges
quickly enough so that


 n

n
n

1 2
1 2 P
1/2
1/2 1
2
2
n
sres −
ε =n
ε̂ −
ε → 0.
(6.2.2)
n i=1 i
n i=1 i
n i=1 i
Furthermore, if we use the differencing estimator to calculate the unrestricted
variance, then the distribution of the test statistic is greatly simpliﬁed. For
example, if we use optimal differencing coefﬁcients, then


ε ε
1 
P
1
2
n /2 sdiff
−
(6.2.3)
+
ε (L 1 + L 2 + · · · + L m ) ε → 0.
n
mn
Differencing removes the nonparametric regression effect sufﬁciently quickly
2
2
so that (6.2.3) holds.2 If we replace sunr
with sdiff
and combine (6.2.2) and
(6.2.3), the numerator of (6.2.1) becomes

1  2
2
∼
n /2 sres
− sdiff
=

1 
ε (L 1 + L 2 + · · · + L m ) ε,
mn 1/2

(6.2.4)



and it is easy to show that the right-hand side is approximately N 0, σε4 /m ,
in which case
 2

2
D
1/2 sres − sdiff
V = (mn)
→ N (0, 1).
(6.2.5)
2
sdiff
For arbitrary differencing coefﬁcients satisfying (4.1.1), a similar argument
yields

 1/2  2
2
sres − sdiff
n
D
V =
→ N (0, 1),
(6.2.6)
2
4δ
sdiff
1

2

Although, a number of authors have proposed that this be done by calculating the trace of
certain “smoother” matrices. See Cleveland (1979), Cleveland and Devlin (1988), and Hastie
and Tibshirani (1990, pp. 52–55 and Appendix B).
See (4.2.10a) and Lemmas B.2 and B.3 in Appendix B.

Constrained Estimation and Hypothesis Testing

115

where δ is deﬁned in (4.1.6). This is of course precisely the speciﬁcation test
we developed in Section 4.3 except that now the restrictions are not necessarily
parametric. They need only be such that the convergence is fast enough to
produce (6.2.2).
We have already used this idea to produce a simple test of equality of regression functions in Section 4.4. We can also use it to test other restrictions such
as monotonicity, concavity, or additive separability. As in Section 4.3, the test
can readily be modiﬁed to incorporate heteroskedasticity. Finally, we note that
the power of the test can be increased by permitting the order of differencing
m to increase with sample size.
6.3 Residual Regression Tests
6.3.1 Overview
An alternative approach used by Li (1994), Fan and Li (1996), and Zheng (1996)
begins by rewriting the regression model y = f (x) + ε as
y = f̄ (x) + [ f (x) − f¯(x) + ε] = f¯(x) + f (x) + ε.

(6.3.1)

We assume that the restricted regression estimator fˆres estimates f¯ con¯ then
sistently and note that if the null hypothesis is true, that is, if f ∈ ,
f  = 0. Thus, if we perform an “auxiliary” regression of the estimated residuals yi − fˆres (xi ) on xi to estimate f  and perform a signiﬁcance test, we will
¯ 3
have a test of the null hypothesis f ∈ .
Equivalently, observe that
E ε,x [(y − f¯ (x))E ε [y − f¯ (x) | x]] = E ε,x [(y − f¯ (x)) f  (x)]
(6.3.2)
= E x f 2 (x) ≥ 0,
where the expression equals zero only if the null hypothesis is true. One way
to obtain a sample analogue of the second expression is to calculate
1
(6.3.3)
(yi − fˆres (xi ))( fˆunr (xi ) − fˆres (xi )).
n
Note that fˆunr − fˆres is an estimator of f  = f − f¯ .
A closely related procedure uses the sample analogue
1
(6.3.4)
(yi − fˆres (xi )) fˆ (xi ),
n
where fˆ is obtained by performing an (unrestricted) nonparametric regression
of yi − fˆres (xi ) on the xi .
3

The idea, of course, is not new and has been exploited extensively for speciﬁcation testing in
parametric regression models (see, e.g., MacKinnon 1992).

116

Semiparametric Regression for the Applied Econometrician

6.3.2 U-statistic Test – Scalar x’s, Moving Average Smoother
We begin with the case in which x is a scalar, say, in the unit interval. We assume
the data have been reordered so that the xi are in increasing order. Consider the
moving average smoother (Section 3.1) of f  given by
ī
1 
fˆ (xi ) =
(y j − fˆres (x j )),
k j=i, j =i

(6.3.5)

where this time k is even; i = i − k/2 and ī = i + k/2 denote the lower and
upper limits of summations. We are averaging k values of (y j − fˆres (x j )) in
the neighborhood of xi . (Momentarily, we will explain why the summation in
(6.3.5) excludes the ith observation.) Then (6.3.4) becomes
U=

ī
1  
(yi − fˆres (xi ))(y j − fˆres (x j )).
kn i j= i, j =i

(6.3.6)

Using the substitution yi − fˆres (xi ) = εi + ( f (xi ) − fˆres (xi )), we expand U
to obtain
U ∼
= U1 + U2 + U3
=

ī
1  
εi ε j
kn i j=i, j =i

+

ī
1  
( f (xi ) − fˆres (xi ))( f (x j ) − fˆres (x j )) (6.3.7)
kn i j=i, j =i

ī
2  
+
εi ( f (x j ) − fˆres (x j )).
kn i j=i, j =i

The following results can be demonstrated:


2σε4
U1 ∼ N 0,
kn
 

1
U2 = O P
( f (xi ) − fˆres (xi ))2
n

1/2 
1
1
2
ˆ
U3 = 1/ O P
.
( f (xi ) − f res (xi ))
n
n 2

(6.3.8)

Constrained Estimation and Hypothesis Testing

117

The random variable U1 is a U -statistic, and its distribution has been studied
extensively.4 Omission of the ith term avoids terms like εi2 and thus ensures
that the mean of U1 is zero. A modicum of insight into the asymptotic distribution of U1 may be gleaned by the following reasoning. U1 is a quadratic
form for which the interposing matrix is related to the smoother matrix S for
the moving average estimator deﬁned in (3.1.12). It is band diagonal with
0’s on the main diagonal, 1/kn on the k/2 diagonals adjacent to the main
diagonal, and
 0’s everywhere else. Thus, summing each diagonal, we have
U1 ∼
= 1/kn i 2εi εi−1 + · · · + 2εi εi−k/2 . Note that U1 is an average of about
kn/2
distinct objects that are uncorrelated but not independent. Next note that
Var ( i 2εi εi−1 + · · · + 2εi εi−k/2 ) ∼
= 4σ 4 kn/2, and thus Var (U1 ) ∼
= 2σ 4 /kn.
Finally, apply a central limit theorem for dependent processes such as those in
McLeish (1974). Keep in mind that the number of terms under the summation
sign is growing as k grows.
Suppose now the null hypothesis is true and fˆres → f . If this convergence is sufﬁciently rapid, the distribution of U is determined by the distribution of U1 , which will form the basis for tests of a variety of null hypotheses.
Put another way, because k 1/2n 1/2U1 has constant variance, the objective is to
select a restricted estimator fˆres that converges to f sufﬁciently quickly (under
the null hypothesis) and a rate of growth for k so that k 1/2n 1/2U2 and k 1/2n 1/2U3
both converge to zero. (Note that if the null hypothesis is false, then k 1/2n 1/2U
diverges.)
Let us pause for a moment to consider a special case. If the restricted estimate
is a parametric model, then U2 and U3 are O P (n −1 ).5 Since (for consistency
of the estimator) we require k/n → 0, then k 1/2n 1/2U2 and k 1/2n 1/2U3 both go to
zero. Hence k 1/2n 1/2U ∼ N (0, 2σε4 ). This result is a simple variant of the Fan
and Li (1996) speciﬁcation test discussed below.

6.3.3 U-statistic Test – Vector x’s, Kernel Smoother
We now assume the xi are random vectors with probability law p (x) and
dim (x) = d. We will use a product kernel estimator (see Section 5.1).

4

5

The U signiﬁes that such statistics were designed to be unbiased estimators of distribution
characteristics. The seminal paper is due to Hoeffding (1948). See Serﬂing (1980, Chapter 5)
for a general introduction as well as Lee (1990). The results on U-statistics most relevant here
are contained in Hall (1984) and De Jong (1987).
For example, if we are ﬁtting a mean, then, using (6.3.8), U2 = O P (( ȳ − µ)2 ) = O P (n −1 )
and U3 = n −1/2 O P (( ȳ − µ)2 )1/2 = n −1/2 O P (n −1/2 ) = O P (n −1 ).

118

Semiparametric Regression for the Applied Econometrician

Consider


d
"
1 
x jk − xik
ˆ
(y j − f res (x j ))
K
.
λd n j =i
λ
k=1

(6.3.9)

In contrast to (6.3.5), this is not a consistent estimator of f (xi ) (the denominator of the kernel estimator is missing; see (5.1.2)). However, it is a
consistent estimator of f  (xi ) p(xi ). The conditional moment (replacing 6.3.2)),
which motivates the test statistic, is given by


E ε,x [(y − f¯ (x))E ε [y − f¯ (x) | x] p(x)] = E x f 2 (x) p(x) ≥ 0,
(6.3.10)
where equality holds only if the null hypothesis is true. The U statistic becomes
1 
U = d 2
(yi − fˆres (xi ))(y j − fˆres (x j ))
λ n i j =i
×



d
"
x jk − xik
K
.
λ
k=1

(6.3.11)

Its behavior again depends on the rate at which fˆres converges to f (as in
(6.3.8)). We now state the more general result. Suppose
1
( f (xi ) − fˆres (xi ))2 = O P (n −r ),
n

(6.3.12)

nλd/2 n −r = n 1−r λd/2 → 0,

(6.3.13)



nλd/2 U ∼ N 0, 2σε4 p 2 (x) K 2 (u) ,

(6.3.14)

and

then,

and σU2 = Var (U ) = 2σε4 p 2 (x) K 2 (u)/λd n 2 may be estimated using
2 
σ̂U2 = 4 2d
(yi − fˆres (xi ))2 (y j − fˆres (x j ))2
n λ
i
j =i
×

d
"
k=1



K2


xjk − xik
.
λ

(6.3.15)

We can apply (6.3.11) to (6.3.15) to produce tests of a broad variety of
hypotheses, including speciﬁcation, signiﬁcance, additive separability, monotonicity, concavity, homotheticity, and demand theory. In each case we need to
produce a restricted estimator with a sufﬁciently rapid rate of convergence.

Constrained Estimation and Hypothesis Testing

119

6.4 Speciﬁcation Tests6
Assume g(x, θ ) is a known function of its arguments, where θ is a ﬁnite dimensional parameter vector. In this section we are concerned with tests of
H0 : f ∈ ¯ = { f ∈  | f = g(·, θ) for some θ }.

(6.4.1)

Let θ̂ be an estimator of the parameter θ, such as nonlinear least squares,
which converges at a rate n −1/2 to θ̄ ; θ̄ = θ if H0 is true.
6.4.1 Bierens (1990)
If the parametric speciﬁcation under the null hypothesis (6.4.1) is true, then
E ε [y −g(x, θ̄ ) | x] = E ε [y −g(x, θ) | x] = E ε [ε | x] = 0 for all x. Suppose that
the null hypothesis is false and that the probability that E ε,x [y −g(x, θ̄ ) | x] = 0
is less than 1. Then, for (almost) any real number τ , the following holds:
E ε,x [eτ x (y − g(x, θ̄ ))] = E x [eτ x ( f (x) − g(x, θ̄ ))] = 0.

(6.4.2)

n 1/2

Bierens proposes a test based on
times the sample analogue of the left
expression

1 1
B(τ ) = n /2
(6.4.3)
eτ xi (yi − g(xi , θ̂ )).
n
He then demonstrates that, under the null hypothesis, B(τ ) is asymptotically
normal with mean zero and variance given by

2 

2
2
τx
 −1 dg(x, θ )
σ B(τ ) = E ε,x (y − g(x, θ)) · e − b(τ ) A
,
dθ
(6.4.4)
where



dg(x, θ)
b(τ ) = E x eτ x
dθ


A = Ex

dg(x, θ)
dθ




dg(x, θ ) 
.
dθ
(6.4.5)

Estimates of b(τ ) and A, say b̂(τ ), Â, are obtained by using sample analogues
and replacing θ with θ̂ from a parametric estimation procedure. The variance
6

There is a huge literature on speciﬁcation testing. In this section we focus speciﬁcally on
tests in which the alternative involves a nonparametric component to the regression function.
Procedures not discussed here but worthy of note include Azzalini, Bowman, and Härdle
(1989); Eubank and Spiegelman (1990); Lee (1991); Eubank and Hart (1992); Wooldridge
(1992); Azzalini and Bowman (1993); Gozalo (1993); Whang and Andrews (1993); Horowitz
and Härdle (1994); Bierens and Ploeberger (1997); Dette (1999); Ellison and Ellison (2000);
Aı̈t-Sahalia, Bickel, and Stoker (2001); Horowitz and Spokoiny (2001); and Stengos and Sun
(2001). See also Yatchew (1988, 1992).

120

Semiparametric Regression for the Applied Econometrician

Table 6.1. Bierens (1990) speciﬁcation test – implementation.
Test Statistic: Test H0 : E(y | x) = θ1 + θ2 x; under H0 :
n 1/2 1  τ xi
B(τ )
=
e (yi − θ̂ 1 − θ̂ 2 xi ) ∼ N (0, 1)
σ̂ B(τ )
σ̂ B(τ ) n
θ̂ 1 , θ̂ 2 are OLS estimators,
2
σ̂ B(τ
)

1
=
(yi − θ̂ 1 − θ̂ 2 xi )2 ·
n




1  τ xi
e
 n


b̂(τ ) = 


1  τ xi
xi e
n







e

τ xi

− b̂(τ ) Â




Â = 




1
1
xi
n

−1

1
xi

2


1
xi

n



1 2
xi
n

2
σ B(τ
) is estimated using
2
σ̂ B(τ
)

2

1
2
τ xi
 −1 dg(x i , θ )
(yi − g(xi , θ̂ )) · e − b̂(τ ) Â
=
.
n
dθ
(6.4.6)

What we have not dealt with so far is how to select τ . Bierens proposes that τ
2
be selected to maximize B 2 (τ )/σ̂ B(τ
) . The resulting test procedure is consistent,
does not require nonparametric estimation of the regression function, and is
applicable if x is a vector (see Table 6.1 for implementation).
6.4.2 Härdle and Mammen (1993)
Härdle and Mammen base their speciﬁcation test on the integrated squared
difference I = ( fˆres (x) − fˆunr (x))2 d x. Here fˆunr is a kernel estimator of f,
and fˆres is (for technical reasons a smoothed version of ) the parametric estimate
g(x, θ̂ ). Their statistic is given by
I = nλ /2
1

(Kg(x; θ̂ ) − fˆunr (x))2 π(x) d x,

(6.4.7)

where π(x) is a weight function selected by the user that permits discrepancies
between the nonparametric and parametric estimators to be weighted differently
in different parts of the domain, and

K ((x − xi )/λ) g(xi , θ̂ )
Kg(x, θ̂ ) = i 
,
(6.4.8)
i K ((x − x i )/λ)

Constrained Estimation and Hypothesis Testing

121

where K is the kernel function and λ = O(n −1/5 ).7
Let K (2) (·) and K (4) (·) be the 2-times and 4-times convolution products of
8
K. Let p(x) be the density of x, which is assumed to have bounded support
(e.g., the unit interval), with p(x) bounded away from zero on the support. Then
in large samples and under the null hypothesis that the parametric speciﬁcation
is correct,

1
I ∼ N λ− /2 K (2) (0)σε2 π(x)/ p(x) d x,

(4)
4
2
2
2K (0)σε
π (x)/ p (x) d x .
(6.4.9)
All elements can be either computed or estimated (see Table 6.2 for implementation). If one sets π(x) = p(x), then greater weight is assigned in regions
where there are likely to be more observations and, hence, presumably, the
discrepancy is being estimated more accurately. In this case, the integrals in
(6.4.9) become d x, and if one uses the uniform kernel the result simpliﬁes to


1
I ∼ N 1/2 λ− /2 σε2 d x, 2/3 σε4 d x .
(6.4.10)
In simulations, Härdle and Mammen found that this normal approximation is
substantially inferior to bootstrapping the critical values of the test statistic. They
demonstrated that the “wild” bootstrap (see Chapter 8) yields a test procedure
that has correct asymptotic size under the null and is consistent under the
alternative. (They also demonstrated that conventional bootstrap procedures
fail.) Finally, the test can be applied to circumstances under which x is a vector
and ε is heteroskedastic.
6.4.3 Hong and White (1995)
Hong and White proposed tests based on series expansions, in particular the
ﬂexible Fourier form (Gallant 1981). To test a quadratic null, the unrestricted

7

8

Härdle and Mammen (1993) impose the following conditions: K is symmetric, twice continuously differentiable, integrates to one, and has compact support. The last condition would,
strictly speaking, rule out the normal kernel.
Recall that K (·) may be viewed as a density. Let u 1 , u 2 , u 3 , u 4 be i.i.d. with density K (·). The
convolution products K (2) (·) and K (4) (·) are the densities of u 1 + u 2 and u 1 + u 2 + u 3 + u 4 ,
respectively. We will need to evaluate these densities at 0. If K (·) is the uniform density on
√
(4)
(2)
1
1
[−1, 1], then K (2) (0)
√ = /2 and K (0) = /3. If K (·) is N (0, 1), then K (0) = 1/(2 π )
(4)
and K (0) = 1/(2 2π ).

122

Semiparametric Regression for the Applied Econometrician

Table 6.2. Härdle and Mammen (1993) speciﬁcation test – implementation.
Test Statistic: Test H0 : E(y | x) = θ0 + θ1 x; using Ho using a uniform kernel:a
I = nλ1/2 (K (θ̂ 0 + θ̂ 1 x) − fˆunr (x))2 p̂(x) d x
∼N



1/2 λ−1/2 σε2

d x,



2/3 σε4

dx .

1. Regress y on x to obtain θ̂ 0 + θ̂ 1 x.
2. Perform kernel regression using uniform kernel on (θ̂ 0 + θ̂ 1 xi , xi ) to obtain the
smoothed parametric estimate K(θ̂ 0 + θ̂ 1 x).
3. Perform kernel regression using uniform kernel on (yi , xi ) to obtain fˆunr (x)
2
and sunr
.
4. Obtain p̂(x) using a kernel density estimation procedure.



2
5. Calculate I − 1/2 λ−1/2 sunr

dx

#

4
2/3 sunr

dx

1/2

and compare to N (0, 1).

a

Any symmetric twice-differentiable kernel with compact support may be used, but then
the constants in the asymptotic approximation will change.

regression model is given by
∗

f (x) = θ0 + θ1 x + θ2 x +
2

N


Y1 j cos( j x) + Y2 j sin( j x),

(6.4.11)

j=1

where the number of unknown coefﬁcients 3 + 2N ∗ increases with sample size.
The rate at which N ∗ may be permitted to grow depends on the null hypothesis
2
2
being tested. Let sunr
be obtained by estimating model (6.4.11) and sres
by
estimating the parametric regression. Then in large samples
 2

2
n sres
− sunr
∼ N (3 + 2N ∗ , 2(3 + 2N ∗ )).
(6.4.12)
2
sunr
Table 6.3 provides implementation details.
6.4.4 Li (1994) and Zheng (1996)
These authors proposed speciﬁcation tests based on residual regression, which
is discussed in a general setting in Section 6.3. If x is a scalar, the test statistic
U of (6.3.11) becomes
1
U =
(yi − g(xi , θ̂ ))
n i





1
xi − x j 
×
.
(6.4.13)
(y j − g(x j , θ̂ ))K
λn j =1
λ

Constrained Estimation and Hypothesis Testing

123

Table 6.3. Hong and White (1995) speciﬁcation test – implementation.
Test Statistic: Test H0 : E(y | x) = θ0 + θ1 x + θ2 x 2 ; under the null:a



2
2
n sres
− sunr



∼ N (3 + 2N ∗ , 2(3 + 2N ∗ )).

2
sunr

1. Rescale the data on x so that xi ∈ [0, 2π].
2
2. Estimate y = θ0 + θ1 x + θ2 x 2 + ε by OLS to obtain sres
.
3. Determine number of terms in unrestricted model: N ∗ = O(n .10 log(n)).
4. Generate explanatory variables cos( j xi ), sin( j xi ), j = 1, . . 
. N ∗ .∗
N
2
5. Perform the (unrestricted) regression y = θo + θ1 x + θ2 x +
γ cos( j x)
j=1 1 j
2
+ γ2 j sin( j x) + ε to obtain sunr .
6. Calculate test statistic and perform a one-tailed test using a critical value
from the N (0, 1).
a

If one is testing the linear model, set θ2 = 0, N ∗ = O(n .19 log(n)).

Table 6.4. Li (1994) and Zheng (1996) residual regression test
of speciﬁcation – implementation.
Test Statistic: Test H0 : E(y | x) = θ0 + θ1 x; under Ho using a uniform kernel:



1
1 
U=
(yi − θ̂o − θ̂ 1 xi )
(y j − θ̂o − θ̂ 1 x j )Kij
n
λn
i


∼N

0,

2σε4

p 2 (x)
λn 2

K2



j =i


,

where Kij is deﬁned below.
1. Perform (restricted) regression y on x to obtain θ̂ o + θ̂ 1 xi .
2. Calculate the kernel matrix Kij , where
Kij = 1/2 if |x j − xi | ≤ λ j = i (note that diagonal elements Kii = 0)
Kij = 0 otherwise.
3. Calculate U .
4. Deﬁne σU2 = Var(U ) = 2σε4 p 2 (x) K 2 /λn 2 and estimate it using
σ̂U2 =

2 
(yi − θ̂ o − θ̂ 1 xi )2 (y j − θ̂ o − θ̂ 1 x j )2 K2ij .
n 4 λ2
i

j =i

5. Perform a one-sided test comparing U/σ̂U with the critical value from the N (0, 1).

124

Semiparametric Regression for the Applied Econometrician

The term in square brackets is a consistent estimator of f  (x) p(x) = ( f (x) −
g(x, θ̄ )) p(x). Under the null hypothesis this is the zero function, and we have
from (6.3.14)


1/2
4
2
2
nλ U ∼ N 0, 2σε p (x) K (u) .
(6.4.14)
Implementation details are contained in Table 6.4. We note that the test is valid
under heteroskedasticity.
6.5 Signiﬁcance Tests
Let us begin by disposing of signiﬁcance tests where the null hypothesis is of
the form f (x) = µ, and µ is a constant. This null model constitutes the simplest
possible parametric speciﬁcation, and so all speciﬁcation testing methodologies
proposed in the previous section immediately yield tests of signiﬁcance of this
kind.9 If x is a vector, then this null hypothesis corresponds to testing the joint
signiﬁcance of all the explanatory variables.
What is more challenging – and more useful – is the derivation of tests of
signiﬁcance for a subset of the explanatory variables. Consider the following
hypotheses:
H0 : f ∈ ¯ = { f ∈  | f (x1 , x2 ) is smooth and constant wrt x2 }
H1 : f ∈  = { f | f (x1 , x2 ) is smooth}.
(6.5.1)
As before, f¯ is the “closest” function to f , the true regression function, in the
restricted set ¯ (Eq. (6.1.5)). If the null hypothesis is true, then f¯ (x) = f (x)
for all x.
The residual regression tests of Section 6.3 may be used for testing hypotheses
of this type. To test hypotheses like the null in (6.5.1), we may apply the
results in (6.3.11) through (6.3.15). The restricted estimator under the null
fˆres is any one-dimensional nonparametric estimator that converges, say, at
the optimal rate. Under the alternative, a two-dimensional kernel estimator is
applied. Implementation details are contained in Table 6.5. Additional tests
of signiﬁcance may be found in Racine (1997), Lavergne and Vuong (2000),
Aı̈t-Sahalia et al. (2001), and Delgado and Manteiga (2001).
9

For example, the Bierens (1990) test statistic reduces to
B(τ )
=
σ̂ B(τ )


1
n

n 1/2 n1




i

eτ xi (yi − ȳ)

(y − ȳ)2 eτ xi −
i i

where ȳ is the sample mean.

1
n


j

eτ x j

2

1/
2

∼ N (0, 1),

Constrained Estimation and Hypothesis Testing

125

Table 6.5. Residual regression test of signiﬁcance – implementation.a
Test Statistic: Test H0 : f (x1 , x2 ) is smooth and constant with respect to x2 , against
H1 : f (x1 , x2 ) is smooth; using the uniform kernel, under the null we have



1 
1
(yi − fˆres (x1i )) 2
(y j − fˆres (xij ))Kij
U=
n
λn
i

j =i



∼N

0,

2σ 4

p 2 (x1 , x2 )
λ2 n 2

K2





,

where Kij is deﬁned below.
1. Perform the restricted regression of y on x1 to obtain fˆres (x1 ).
The estimator may be a kernel regression, nonparametric least squares, or another
estimator that converges at the optimal rate.
2. Calculate the product kernel matrix Kij :
Kij = 1/4 if |x1i − x1 j | ≤ λ and |x2i − x2 j | ≤ λ i = j
Kij = 0 otherwise.
3. Calculate U .
4. Determine λ. For example, if the fˆres was obtained using an optimal bandwidth,
then its rate of convergence is O P (n −4/5 ), that is, r = 4/5 . Now using (6.3.13),
select λ so that λn 1/5 → 0, thus λ = O(n −1/4 ) sufﬁces.
5. Estimate σU2 = Var(U ) using
2 
σ̂U2 = 4 4
(yi − fˆres (x1i ))2 (y j − fˆres (x1 j ))2 K2ij .
n λ
i

j =i

6. Perform a one-sided test comparing U/σ̂U with the critical value from the N (0, 1).
a

The test described here is similar to those found in Li (1994), Fan and Li (1996), and
Zheng (1996).

6.6 Monotonicity, Concavity, and Other Restrictions10
6.6.1 Isotonic Regression
Suppose we are interested in imposing monotonicity on our estimate of the
regression function and in testing this property, that is,
H0 : f ∈ ¯ = { f ∈  | f smooth and monotone}
H1 : f ∈  = { f | f smooth}.
(6.6.1)
10

There is a substantial literature on estimation and testing subject to constraints such as monotonicity and concavity (convexity). Work on monotonicity and/or concavity includes Wright
and Wegman (1980); Schlee (1982); Friedman and Tibshirani (1984); Villalobas and Wahba
(1987); Mukarjee (1988); Ramsay (1988); Kelly and Rice (1990); Mammen (1991); Goldman
and Ruud (1992); Yatchew (1992); Mukarjee and Stern (1994); Yatchew and Bos (1997);
Bowman, Jones, and Gijbels (1998); Diack and Thomas-Agnan (1998); Ramsay (1998);
Mammen and Thomas-Agnan (1999); Diack (2000); Gijbels et al. (2000); Hall and Heckman
(2000); Hall and Huang (2001); Groeneboom, Jongbloed, and Wellner (2001); Juditsky and
Nemirovski (2002); and Hall and Yatchew (2002).

126

Semiparametric Regression for the Applied Econometrician

The isotonic regression literature, in the simplest case, considers leastsquares regression subject only to monotonicity constraints; that is, given data
(y1 , x1 ), . . . , (yn , xn ) on the model yi = f (xi ) + εi , the optimization problem
is given by
min

ŷ1 ,..., ŷn

1
(yi − ŷi )2
n i

s.t. ŷ j ≤ ŷi

for x j ≤ xi ,

(6.6.2)

if f is increasing. The literature goes back several decades (see e.g., Barlow et al.
(1972) and Robertson, Wright, and Dykstra (1988)). The estimation problem in
(6.6.2) differs from our setup in (6.6.1) in that we impose additional smoothness
constraints so that the regression function is estimable under the alternative.
Isotonic regression may be implemented using the function monreg in XploRe
(see Härdle, Klinke and Turlach 1995) or using GAMS (Brooke et al. 1992).
6.6.2 Why Monotonicity Does Not Enhance the Rate of Convergence
If one is willing to impose sufﬁcient smoothness on the estimated regression
function and if the true regression function is strictly monotone, then monotonicity constraints will not improve the rate of convergence.
To see why this is the case, consider the following example in a simpliﬁed
parametric setting. Suppose one is estimating the model y = µ + ε subject to
the constraint µ ≤ 2. The usual (unconstrained) estimator of µ is the sample
mean ȳ. An estimator µ̂ that incorporates the inequality constraint would set
µ̂ = ȳ if ȳ ≤ 2, and µ̂ = 2 if ȳ > 2. If the true mean is, say, 1.5, then as
sample size increases, the probability that the unconstrained estimator equals the
constrained estimator goes to 1. Thus, the constraint becomes nonbinding.11
In nonparametric regression, an analogous result holds. If the true regression
function is strictly monotone (e.g., if the ﬁrst derivative is bounded away from
zero), then with sufﬁcient smoothness assumptions, the monotonicity restrictions become nonbinding as sample size increases. (This happens if the ﬁrst
derivative is estimated consistently, in which case, as sample size increases,
the derivative estimate will also be bounded away from zero with probability
going to 1.) The constrained estimator then has the same convergence rate as
the unconstrained estimator.12 This negative ﬁnding, however, does not imply
that monotonicity will be uninformative in small samples (nor does it preclude testing for the presence of this property). Indeed, one could argue that,
given the paucity of a priori information present in nonparametric estimation,
any additional constraints should be exploited as far as possible particularly
11
12

See Wolak (1989) and references therein for tests of inequality constraints in parametric
models.
Utreras (1984), Mammen (1991), and Yatchew and Bos (1997) ﬁnd this result for different
estimators.

Constrained Estimation and Hypothesis Testing

127

in moderately sized samples. (Recall Figure 2.4 in which the imposition of
monotonicity results in better ﬁt.)
6.6.3 Kernel-Based Algorithms for Estimating Monotone
Regression Functions
Mammen (1991) analyzed two estimators that combine smoothing with monotonicity constraints in estimation. The ﬁrst estimator consists of two steps:
smoothing of the data by applying a kernel estimator followed by determination of the closest set of monotonic points to the smoothed points. That is,
given data (y1 , x1 ), . . . . ,(yn , xn ), let ( ỹ1 , x1 ), . . . . , ( ỹn , xn ) be the set of points
obtained by applying a kernel estimator; then, solve
min

ŷ1 ,..., ŷn

1
( ỹi − ŷi )2
n i

s.t. ŷi ≤ ŷ j

if xi ≤ x j .

(6.6.3)

The second estimator examined by Mammen reverses the two steps.
Mammen demonstrated that, if the true regression function is strictly monotone
and if one chooses the optimal bandwidth for twice differentiable functions (i.e.,
λ = n −1/5 ), then both estimators converge at the same rate as a conventional
kernel estimator. Hall and Huang (2001) proposed an alternative procedure for
producing a monotone estimate from an initial “smooth” estimate.
6.6.4 Nonparametric Least Squares Subject to Monotonicity Constraints
An alternative approach involves augmenting the nonparametric least-squares
optimization problem (3.3.3) with monotonicity constraints. Assume the data
have been ordered so that x1 ≤ · · · ≤ xn . For expositional purposes, suppose
R is invertible and set Rc in (3.3.3) equal to ŷ. Consider
1
min [y − ŷ] [y − ŷ]
ŷ
n
s.t. ŷ  R −1 ŷ ≤ L
ŷi−1 ≤ ŷi i = 2,. . . . , n.

(6.6.4a)

If f is strictly increasing, the monotonicity constraints are nonbinding in
large samples so that the estimator achieves the optimal rate of convergence
n −2m/(2m+1) , where m is the degree of differentiability. Equation (6.6.4a) illustrates the relative ease with which the nonparametric least-squares estimator
can incorporate additional constraints.
Alternatively, let R (1) be the matrix of ﬁrst derivatives of the representors
r x1 , . . . , r xn evaluated at the data points x1 , . . . , xn . Then one may write
1
min [y − Rc] [y − Rc] s.t. c Rc ≤ L
c n

R (1) c ≥ 0.

(6.6.4b)

128

Semiparametric Regression for the Applied Econometrician

Versions (6.6.4a) and (6.6.4b) are slightly different. Using the mean value
theorem, the former ensures that the estimated derivative is positive at some
point between each pair of consecutive points. The latter requires the estimated
derivative to be positive at the points x1 , . . . , xn . Neither procedure ensures that
the estimated function is monotone everywhere in small samples, but as data
accumulate, the smoothness requirement prevents nonmonotonicity.
6.6.5 Residual Regression and Goodness-of-Fit Tests of Restrictions
Let fˆres be any of the estimators discussed above that impose smoothness and
monotonicity. If f is twice differentiable, they converge at a rate n −4/5 . Using
(6.3.13), if we set λ = o(n −2/5 ), then the U -statistic of (6.3.11) with d = 1
has the normal distribution speciﬁed in (6.3.14) under the null hypothesis of
monotonicity.
Suppose we estimate the isotonic regression in (6.6.2), which imposes only
monotonicity. Van de Geer (1990) demonstrated that, in this case, ( fˆ − f )2 ∼
=
O P (n −2/3 ). Thus, again using (6.3.13), we need λ = o(n −2/3 ).
Furthermore, all these estimators converge sufﬁciently quickly so that (6.2.2)
2
holds and we may apply a goodness-of-ﬁt test. For example, let smon
be the
estimated residual variance from a smooth monotone or isotonic regression and
2
sdiff
a differencing estimate. Then,
 2

2
D
1/2 smon − sdiff
(mn)
→ N (0, 1).
(6.6.5)
2
sdiff
Tests of convexity (concavity) as well as of other restrictions may be implemented in a similar fashion. For example, convexity constraints may be imposed
using
1
min
(yi − ŷi )2
ŷ1 ,..., ŷn n
s.t. ŷ  R −1 ŷ ≤ L
(6.6.6a)
xi+2 − xi+1
xi+1 − xi
ŷi +
ŷi+2 ∀i.
ŷi+1 ≤
xi+2 − xi
xi+2 − xi
Alternatively, let R (2) be the matrix of second derivatives of the representors
r x1 , . . . , r xn evaluated at the data points x1 , . . . , xn . Then one may write
1
min [y − Rc] [y − Rc] s.t. c Rc ≤ L
ŷ n

R (2) c ≥ 0.

(6.6.6b)

If sufﬁcient derivatives are bounded and if the function is strictly convex, then
the convexity constraints will not enhance the large sample rate of convergence.
The residual regression test may be used to produce tests of separability. The procedure gam (generalized linear model) in S-Plus estimates such

Constrained Estimation and Hypothesis Testing

129

speciﬁcations. Additional tests of additively separable models may be found in
Barry (1993); Eubank et al. (1995); Sperlich, Tjostheim, and Yang (1999); Dette
and Von Lieres und Wilkau (2001); Gozalo and Linton (2001); and Derbort,
Dette, and Munk (2002).
Implications of demand theory can also be imposed and tested. See Epstein
and Yatchew (1985), Varian (1985, 1990), Matzkin (1994) and references
therein, Hausman and Newey (1995), Lewbel (1995), and Yatchew and Bos
(1997).
In general, validity of the goodness-of-ﬁt and residual regression tests require ﬁrst demonstrating that the restricted estimator of the regression function
converges sufﬁciently quickly. If the model is partially linear, then the estimated parametric effect may be ﬁrst removed (using a n 1/2 -consistent estimator)
without altering the asymptotic validity of either the residual regression or the
goodness-of-ﬁt tests that are subsequently applied to the nonparametric portion
of the model.
6.6.6 Empirical Application: Estimation of Option Prices13
Option price data have characteristics that are both nonparametric and parametric. The economic theory of option pricing predicts that the price of a call
option should be a monotone decreasing convex function of the strike price. It
also predicts that the state price density, which is proportional to the second
derivative of the call function, should be a valid density function over future
values of the underlying asset price and hence should be nonnegative and integrate to 1. Except in a few polar cases, the theory does not prescribe speciﬁc
functional forms. All this points to a nonparametric approach to estimation.
On the other hand, multiple transactions are typically observed at a ﬁnite
vector of strike prices. Thus, one could argue that the model for the option
price as a function of the strike price is intrinsically parametric. Indeed, given
sufﬁcient data, one can obtain a good estimate of the call function by simply
taking the mean transactions price at each strike price. Unfortunately, even
with large data sets, accurate estimation of the call function at a ﬁnite number
of points does not ensure good estimates of its ﬁrst and second derivatives.
In this example we apply the nonparametric least-squares estimator and show
how it can incorporate various “shape” constraints such as monotonicity and
convexity of the call function.
Suppose we are given data (x1 , y1 ), . . . , (xn , yn ), where xi is the strike price
and yi is the option price. Let X = (X 1 , . . . , X k ) be the vector of k distinct
strike prices. We will assume that the vector X is in increasing order. As usual,
13

This application is drawn from Yatchew and Härdle (2001), who apply the techniques to
options data on the DAX index. See also Aı̈t-Sahalia and Duarte (2000).

130

Semiparametric Regression for the Applied Econometrician

x, y, and X will denote both the variable in question and the vector of observations on that variable. Our model is given by
yi = f (xi ) + εi

i = 1, . . . , n.

(6.6.7)

We assume the following. The regression function f is four times differentiable, which will ensure consistent and smooth estimates of the function,
its ﬁrst and second derivatives. (Other orders of differentiation can readily be
accommodated using the framework that follows.) The vector of distinct strike
prices X lies in the interval [a,b]. The residuals εi are independent but possibly
heteroskedastic, and  is the diagonal matrix of variances σ12 , . . . , σn2 .
We have generated 20 independent transactions prices at each of 25 strike
prices. The top panel of Figure 6.2A depicts all 500 observations and the
“true” call function. Note that the variance decreases as the option price declines. The second panel depicts the estimated call function obtained by taking the mean transactions price at each of the 25 strike prices. The estimate
lies close to the true function. However, under closer examination it may be
seen that the estimate is not convex. Although the differences would seem to
be de minimis, we will soon see that this results in rather poor estimates of
derivatives.
Consider the following naive approximations to the ﬁrst and second derivatives that use ﬁrst and second divided differences of the point mean estimates
Ŷ1 , . . . , Ŷk . In particular, deﬁne
Ŷ j − Ŷ j−1
X j − X j−1

j = 2, . . . , k

(6.6.8)

and
Ŷ j+1 − Ŷ j
Ŷ j − Ŷ j−1
−
X j+1 − X j
X j − X j−1
X j − X j−1

j = 3, . . . , k.

(6.6.9)

By the mean value theorem, these should provide reasonable approximations
to the ﬁrst and second derivatives. The upper panels of Figures 6.2B and 6.2C
depict divided difference estimates of the ﬁrst and second derivatives using
point means and (6.6.8) and (6.6.9). The estimates are poor, particularly at low
strike prices where the variance of the residual is relatively larger.
Consider now the following nonparametric least-squares problem, which
incorporates a smoothness constraint


1  yi − f (xi ) 2
min
s.t.  f 2Sob ≤ L .
f n
σ
i
i

(6.6.10)

800

o
o
o
o
o
o
o
o
o
o
o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o o
o

o
o
o
o
o
o
o
o
o
o
o
o
o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o

o
o
o
o
o
o
o
o
o
o
o

o
o
o
o
o
o
o
o
o
o

o
o
o
o
o
o
o
o
o
o o o
o o
o
o
o

True function

o
o
o
o
o
o
o
o
o

o
o
o
o
o
o o
o
o
o
o
o o
o o
o
o o
o
o
o o
o
o
o o
o o
o o
o o
o o
o
o o
o
o
o
o
o
o o
o

0

200

400

600

o
o
o
o
o
o
o
o
o
o
o
o
o

4600

4800

5000

5200

600

4400

o
o
o
o
o
o
o o
o o
o
o o
o
o
o
o o
o o
o
o
o o
o
o
o o
o
o o o

5400

5600

0

200

400

Point means
True function

4600

4800

5000

5200

5400

5600

5200

5400

5600

200

400

600

4400

0

Smooth constrained estimate
True function

4400

4600

4800

5000

Figure 6.2A. Data and estimated call function.

131

Semiparametric Regression for the Applied Econometrician

−1.0

−0.5

0.0

132

−1.5

First-order divided differences using point means
True function

4800

5000

5200

5400

5600

5400

5600

−0.2

0.0

4600

−1.0

−0.8

−0.6

−0.4

Smooth constrained estimate
True function

4400
a

4600

4800

5000

5200

Note the change in vertical scale between the two graphs.

Figure 6.2B. Estimated ﬁrst derivative.a

133

−0.01

0.0

0.01

Constrained Estimation and Hypothesis Testing

−0.02

Second-order divided differences using point means
True function

0.0015

4600

4800

5000

5200

5400

0.0005

0.0010

Smooth constrained estimate
True function

4600
a

4800

5000

5200

Note the change in vertical scale between the two graphs.

Figure 6.2C. Estimated SPDs.a

5400

5600

134

Semiparametric Regression for the Applied Econometrician

We will rewrite (6.6.10) to reﬂect that option pricing data are usually characterized by repeated observations at a ﬁxed vector of strike prices. Let B be
the n × k matrix such that
Bij = 1

if xi = X j

=0

otherwise

(6.6.11)

so that (6.6.10) becomes
min
f

1  y − B f (X )   −1  y − B f (X ) 
n×1 n×k k×1
n n×1 n×k k×1 n×n
s.t.  f 2Sob ≤ L .

(6.6.12)

The problem may be solved using
1
min [y − B Rc]  −1 [y − B Rc] s.t. c Rc ≤ L ,
(6.6.13)
c n
where calculation of the k × k representor matrix R is detailed in Appendix D.
One can impose monotonicity and convexity by supplementing (6.6.13) with
constraints like those in (6.6.4a) and (6.6.6a), or (6.6.4b) and (6.6.6b). Consider
then
−1
1
min [y − B Rc]
[y − B Rc]
c n

s.t. c Rc ≤ L
(6.6.14)
R (1) c ≤ 0
R (2) c ≥ 0.
The bottom panels of Figures 6.2A, 6.2B, and 6.2C depict the resulting smooth
constrained estimates fˆ(X ) = R ĉ, fˆ (X ) = R (1) ĉ, and fˆ (X ) = R (2) ĉ that
evidently provide much improved approximations to the true derivatives.
6.7 Conclusions
Because economic theory rarely provides parametric functional forms, exploratory data analysis and testing that rationalizes a speciﬁc parametric regression function is particularly beneﬁcial. In this connection, we have described a
variety of speciﬁcation tests.
Even though parametric speciﬁcation is not its forte, economic theory does
play a role in producing other valuable restrictions on the regression function. By
specifying which variables are potentially relevant to an equation and excluding
myriad others from consideration, rate of convergence is improved. (Exclusion
restrictions may come disguised; e.g., as homogeneity of degree zero.) The
imposition of exclusion restrictions on either local averaging or minimization
estimators is straightforward – one simply reduces the dimensionality of the

Constrained Estimation and Hypothesis Testing

135

EMSE

0.03

0.04

0.05

 

Simulated EMSE: E n1 (µ̂(xt ) − µ(xt ))2

n = 25

0.02

Smooth 2-dim
Separable

Smooth 1-dim
0.01

Monotone

Linear

0.0

n = 100

Data-generating mechanism: y = x1 +ε, x1 ∈ [1, 2], ε ∼ N (0, σ 2 = .25),  · 2Sob =
d x1 = 3.33.
Estimated models: y = µ( · ) + ε,  · 2Sob ≤ 10.0.
Smooth 2-dim
Separable
Smooth 1-Dim
Monotone
Linear

2
1



x12 +1



µ(·) = f (x1 , x2 )
µ(·) = f 1 (x1 ) + f 2 (x2 )
µ(·) = f (x1 )
µ(·) = f (x1 ), f (x1t ) ≤ f (x1τ ), x1t ≤ x1τ
µ(·) = β0 + β1 x1

Each model is estimated using nonparametric least squares. Sobolev smoothness norms are of
fourth order. In each case, 1,000 replications were performed.

Figure 6.3. Constrained estimation – simulated expected mean-squared error.

regression function. Other restrictions that may be driven by considerations of
economic theory and that enhance convergence rates are additive separability
and semiparametric speciﬁcations. Monotonicity and concavity restrictions do
not enhance the (large sample) rate of convergence if sufﬁcient smoothness is
imposed but are beneﬁcial in small samples. Alternatively, their presence can
be used to reduce the dependency on smoothness assumptions.
Figure 6.3 illustrates the consequences of imposing progressively more stringent restrictions on a model that, unbeknown to the investigator, is linear in one
variable. The beneﬁts of “learning” that the model is a function of one variable
rather than two are evident. The expected mean-squared error (EMSE), given
ﬁxed sample size, declines by more than 40 percent as one moves from the
smooth two-dimensional to a smooth one-dimensional model. This observation
underscores the importance of powerful signiﬁcance tests for nonparametric
models. As expected, separability and linearity can also substantially improve
the accuracy of the estimator.

136

Semiparametric Regression for the Applied Econometrician

In this chapter we have focused on constrained estimation and hypothesis
testing. Because of the curse of dimensionality and the consequences for convergence rates, it is extremely desirable to improve the accuracy of estimates
by validating parametric speciﬁcations. Accordingly, we have provided implementation details for a variety of speciﬁcation tests. Reducing the number of
explanatory variables or imposing a separable structure also enhances convergence rates.
The discussion of estimation subject to monotonicity and concavity constraints underlines one of the advantages of the nonparametric least-squares estimator: such constraints can be imposed relatively easily. Other implications of
economic theory can also be incorporated into the nonparametric least-squares
estimation procedure with little difﬁculty.
As in parametric approaches, a general methodology for testing hypotheses can be based upon an examination of the residuals from the constrained
regression. If the null hypothesis is true, these residuals should be unrelated
to the explanatory variables. Thus, the procedure involves a nonparametric regression of the constrained residuals on all explanatory variables. The resulting
test, which can be applied in a wide variety of circumstances, is based on a
U -statistic. An alternative class of procedures that compare restricted and unrestricted estimates of the residual variance is also available.
6.8 Exercises14
1. South African Food Share Engel Curves — Testing Parametric Speciﬁcations.
Using data on single individuals, ﬁt linear and quadratic models for FoodShr as
a function of the log of total expenditure ltexp. Test these speciﬁcations using the
following procedures:
(a)
(b)
(c)
(d)

Bierens (1990), Table 6.1
Härdle and Mammen (1993), Table 6.2
Hong and White (1995), Table 6.3
Li (1994) and Zheng (1996) residual regression test, Table 6.4

Compare your conclusions to those obtained using the differencing test procedure,
Chapter 4, Exercise 9.
2. Repeat Exercise 1 using data on couples with no children and couples with one child.
3. Option Pricing: The purpose of this exercise is to estimate a call function and its ﬁrst
and second derivatives using individual point means and a spline estimator.
(a) Using simulated option pricing data and produce a scatterplot of option prices
against strike prices. (There are 20 observations at each of 25 strike prices.)
14

Data and sample programs for empirical exercises are available on the Web. See the Preface
for details.

Constrained Estimation and Hypothesis Testing

137

(b) Superimpose the true values (X 1 , f (X 1 )), . . . , (X 25 , f (X 25 )) on the plot in
Part (a).
(c) Estimate the call function using individual means at each strike price. Plot the
estimates and the true function.
(d) Use the divided difference formulas in (6.6.8) and (6.6.9) to approximate the ﬁrst
and second derivatives of the call function. Plot these against the true ﬁrst and
second derivatives.
(e) Estimate the call function and its ﬁrst two derivatives using a spline estimator
(such as smooth.spline in S-Plus). Plot these estimates against the true ﬁrst and
second derivatives. (Note, however, that spline estimators such as smooth.spline,
which penalize only the second derivative, as in (3.3.7), do not ensure consistency
of the estimates of ﬁrst and second derivatives.)
4. Option Pricing: The purpose of this exercise is to estimate a call function and its ﬁrst
two derivatives using a constrained nonparametric least-squares procedure. Results
should be similar to the bottom panels in Figures 6.2A, 6.2B, and 6.2C.15
(a) Open the ﬁle containing the simulated data and the representor matrices R, R (1) ,
and R (2) . Using GAMS (or similar optimization program), solve optimization
problem (6.6.14), setting L = .1 and  equal to the identity matrix.
(b) Calculate the estimates of the call function and its ﬁrst two derivatives at the vector
of observed strike prices: fˆ(X ) = R ĉ, fˆ (X ) = R (1) ĉ, and fˆ (X ) = R (2) ĉ. Plot
these estimates against the corresponding true functions.
(c) At each strike price calculate the variance of observed option prices. Use these
ˆ Solve (6.6.14) using .
ˆ
to construct an estimator of , say .
15

Note that this part will require using a constrained optimization program such as GAMS
(Brooke et al. 1992), which stands for General Algebraic Modeling System and is a general
package for solving a broad range of linear, nonlinear, integer, and other optimization problems
subject to constraints. It should not be confused with the gam function in S-Plus, which stands
for generalized additive models.

7

Index Models and Other
Semiparametric Speciﬁcations

7.1 Index Models
7.1.1 Introduction
A natural generalization of the conventional linear regression model y = xδ +ε
is given by the speciﬁcation
y = f (xδ) + ε,

(7.1.1)

where x is a vector of explanatory variables and f is an unknown but smooth
function. The regression is a nonparametric function of the linear index xδ from
which the term index model arises. The objective is to estimate δ and f .
Such speciﬁcations are appealing because they can accommodate multiple
explanatory variables (within the linear index) while retaining nonparametric
ﬂexibility (through the function f ) without succumbing to the curse of dimensionality. The reason is that the nonparametric portion of the model is a function
of only one variable, the linear index itself.1
7.1.2 Estimation
Suppose one is given independent observations (y1 , x1 ), . . . , (yn , xn ), where
the xi are, say, p-dimensional row vectors. As usual, y and ε denote both the
variable in question and the corresponding column vector of data; X is the
n × p matrix of data on the explanatory variables, and f (X δ) an n-dimensional
column vector. We now rewrite (7.1.1) in matrix notation
y = f (X δ) + ε.
1

138

(7.1.2)

If the dependent variable y is binary, then (7.1.1) constitutes the semiparametric analogue
of probit and logit models. For a foothold into this literature, see Cosslett (1987), Klein and
Spady (1993), Cavanagh and Sherman (1998), and Horowitz (1998, Chapter 3) and references
therein.

Index Models and Other Semiparametric Speciﬁcations

139

For a ﬁxed δ, one can estimate f using a conventional smoother to obtain
fˆδ . One can then calculate the estimated residual variance using the average
residual sum of squares. A basic estimation strategy proposed by Ichimura
(1993) and Klein and Spady (1993) consists of searching over different values
of δ until the one that minimizes the estimated residual variance is found2 :
1
s 2 = min [y − fˆδ (X δ)] [y − fˆδ (X δ)].
δ n

(7.1.3)

The estimate δ̂ is the value that satisﬁes the minimum in (7.1.3), and fˆδ is
the corresponding estimate of the unknown regression function f .
Härdle, Hall, and Ichimura (1993) developed a methodology for optimal
selection of the smoothing parameter in the estimation of f . Essentially, the
grid search in (7.1.3) is embedded in a broader optimization problem in which
the smoothing parameter is chosen simultaneously.
7.1.3 Properties
Let
V = E[ f  (xδo )2 (x − E(x | xδo )) (x − E(x | xδo )) | xδo ],

(7.1.4)

where δo is the true value of δ, and x is a p-dimensional row vector. Then, under
general conditions,
D

n /2 (δ̂ − δo ) → N (0, σε2 V −1 ).
1

(7.1.5)

Note that the ﬁnite dimensional parameter δo is estimated n 1/2 -consistently.
Let S be a smoother that regresses onto the vector X δ̂. Then, (I − S)X
regresses the columns of X onto the vector X δ̂ and takes the residuals.
For an arbitrary vector a, let diag(a) be the diagonal matrix with the elements
of a on the main diagonal. Next, estimate the derivative of f and evaluate at
the vector X δ̂. Call this estimated vector fˆ (·). Then, a consistent estimate of
V may be obtained using
1
((I − S)X ) diag( fˆ (·)2 )((I − S)X ).
n

(7.1.6)

Furthermore, σε2 may be estimated using s 2 in (7.1.3).
2

For an alternative estimation strategy based on average derivatives, see Härdle and Stoker
(1989); Powell, Stock, and Stoker (1989); Härdle and Tsybakov (1993); Horowitz and Härdle
(1996); and Hristache, Juditsky, and Spokoiny (2001). See also Stoker (1986, 1991). For a test
of a linear null against the linear index model alternative, see Horowitz and Härdle (1994).

140

Semiparametric Regression for the Applied Econometrician

7.1.4 Identiﬁcation
The following conditions are sufﬁcient for identiﬁcation of δ and f and are
likely to be satisﬁed in many practical applications. First, there is at least one
continuous explanatory variable in the vector x. The coefﬁcient of the ﬁrst continuous variable is set to 1. Such a normalization is required because rescaling
of the vector δ by a constant and a similar rescaling of the function f by the
inverse of the constant will produce the same regression function.3 Second,
the function f is differentiable and not constant on the support of xδ. Third,
the matrix X is of full rank; this is a common assumption that avoids multicollinearity. Finally, varying the discrete components of x does not divide the
support of xδ into disjoint subsets. (For additional details, see Horowitz 1998,
pp. 14–20.)
7.1.5 Empirical Application: Engel’s Method for Estimation
of Equivalence Scales
Earlier, in the context of testing equality of nonparametric Engel curves, we
introduced the idea of equivalence scales (Section 4.4.4).4 Engel’s initial observation was that richer households spend a smaller fraction of their income
on food. His method for calculating equivalence scales is premised on the assumption that two households of different demographic composition are equally
well off if they spend the same share of income on food. Figure 7.1 displays
nonparametric estimates of Engel curves for households consisting of single
individuals and those consisting of couples with no children. Engel’s method
amounts to calculating the horizontal difference between the two curves.
Our objective is to estimate this horizontal shift and to test whether the
two curves are parallel. If they are, then the equivalence scale is said to be
“base-independent” because it does not depend on the income levels at which
comparisons are made.
To see that this problem can be put in an index model framework, let log x
be the log of household expenditure and let z be a dummy variable that is zero
for singles and one for couples. Then, we may write
y = f (log x − zδ) + ε.

(7.1.7)

We have normalized the coefﬁcient of the continuous variable log x to 1.
Because z is a dummy variable, zδ simply shifts the Engel curve for couples
horizontally by δ. The actual equivalence scale is  = exp(δ).
3
4

Alternatively, one could set δ = 1, where the double bars denote the usual Euclidean norm.
Equivalence scale estimation has a long tradition. For an overview, see Deaton (1997), Lewbel
(1997), and Van Praag and Warnaar (1997).

141

0.6

0.8

Index Models and Other Semiparametric Speciﬁcations

0.0

0.2

food share
0.4

Singles
Couples

5

6

7
log of total expenditure

8

ssq

0.0184

0.0185

0.0186

0.0187

0.0188

2
2
Using the optimal differencing estimator with m = 25, we obtain sSingles
= .0194 and sCouples
=
.0174. The number of observations is as follows: n Singles = 1,109, n Couples = 890.

0.3

0.4

0.5

0.6

delta
ˆ = exp(δ̂) = 1.65 with s.e. s ˆ = ·0.039
ˆ
= .064
δ̂ = .5 with s.e. sδ̂ = .039. 


Figure 7.1. Engel’s method for estimating equivalence scales.

0.7

142

Semiparametric Regression for the Applied Econometrician

Figure 7.1 reports the results of applying the estimation procedure we have
outlined. The search grid for δ was the interval [.3, .7] that (taking the exponents
of the endpoints) maps to equivalence scales for couples versus singles in the
range [1.35, 2.01]. In theory one would expect costs for couples not to exceed
two times that of singles. The lower panel of Figure 7.1 displays the values of
the function being minimized in (7.1.3) for different values of δ. The function
is ﬂat over the range .5 to .6. The numerical minimum is about δ̂ = .5 with an
estimated standard error of .039. To obtain an estimate of the actual equivalence
ˆ = exp(δ̂) = 1.65. Applying the “delta method” (an unfortunate
scale we take 
ˆ
coincidence of nomenclature5 ), we ﬁnd its standard error to be sˆ = ·0.039
=
.064.
A simple test of base-independence may now be conducted. It is essentially
a test of whether the couples curve overlays the singles curve after a horizontal
shift of .5. Even though δ is not known but estimated, because it achieves n 1/2 convergence, we can use the tests outlined in Section 4.4, treating δ as known.
Using order of differencing m = 25, we estimate the residual variances for
singles and couples to be .0194 and .0174, respectively. Following (4.4.2), we
calculate the weighted average of these using
n Singles 2
n Couples 2
sw2 =
(7.1.8)
sSingles +
sCouples .
n
n
This is the “within” or unrestricted estimate of the residual variance. The
value is .0183. This is to be compared with the minimized value of the objective
function (7.1.3), which is .0184. Applying Proposition 4.3.1 (see discussion in
Section 4.4), we obtain a test statistic of 1.27, which is consistent with baseindependence.
7.1.6 Empirical Application: Engel’s Method for Multiple Family Types
The procedure outlined in the previous section may be used to estimate equivalence scales for any pair of family types – couples with one child versus
singles, couples with one child versus childless couples, and so on. Table 7.1
summarizes the distribution of family types in the South African data. There is
considerable variation in family size and composition, and the possible number
of pairwise estimates is large.
On the other hand, it is reasonable to assume that families of similar composition are informative about each other’s equivalence scales. Thus, from an
efﬁciency point of view it is useful to embed these various comparisons within
a single model.
5

See, for example, Greene (2000, p. 118).

Index Models and Other Semiparametric Speciﬁcations

143

Table 7.1. Distribution of family composition.
(Number of families)
Children
Adults

0

1

2

1
2
3
4
5
6

1109
890
373
222
105
50

138
526
314
227
117
44

2749

1366

3

4

5

126
524
322
230
144
71

85
309
233
160
116
78

61
144
138
104
66
45

14
65
67
66
43
32

1,533
2,458
1,447
1,009
591
320

1417

981

558

287

7,358

Yatchew et al. (2003) discuss alternative formulations. We will focus on a
parsimonious speciﬁcation. Suppose the equivalence scale  is the following
function of the number of adults (A) and the number of children (K )
 = exp(δ) = (A + β2 K )β1 .

(7.1.9)

Here β1 reﬂects scale economies in the household and β2 measures the effect
on the equivalence scale of children relative to adults. Both parameters are
restricted to be between 0 and 1.6 Then we may write the model as
y = f (log x − β1 log(A + β2 K )) + ε.

(7.1.10)

This is a mildly more general speciﬁcation than the linear index model (7.1.1)
because the index is nonlinear. Nevertheless, the estimation strategy that we
described earlier remains valid, and we may search over a grid of values of
(β1 , β2 ), as in (7.1.3). The difference lies in the calculation of the covariance
matrix of the estimates. We will replace (7.1.4)–(7.1.6) with their more general
counterparts in Section 7.2.
For current purposes it is important to note that the search over (a subset of)
a two-dimensional space is substantially more time consuming. As one adds
more variables and parameters to the index, the problem becomes progressively
more difﬁcult. Figure 7.2 plots the results of the grid search and provides the
estimates of the parameters (β1 , β2 ). Evidently, the function is quite ﬂat in the
direction of β2 . The procedure was performed in S-Plus. Much faster searches
6

See, for example, Citro and Michael (1995, p. 176), who recommend values around .7 for β1
and β2 .

144

Semiparametric Regression for the Applied Econometrician

ssq

0.0184 0.0186 0.0

188 0.019

Objective function -- Equation (7.1.3)

0.8
0.7

5

0.8
0.7
be
ta2

0.7
0.6

5

0.5

0.6
1

beta

The estimates are β̂ 1 = .6; β̂ 2 = .8; sβ̂ 1 = .0215; sβ̂ 2 = .0880.

Figure 7.2. Parsimonious version of Engel’s method.

can be conducted using Fortran (we use the matrix version Fortran 90) and other
programming languages.
We may now use the “delta method” to estimate the standard errors of the
log equivalence scales δ and the actual equivalence scales . For the various
family types appearing in Table 7.1, these values are tabulated in Table 7.2. For
example, for couples with no children, the estimated equivalence scale is 1.52
relative to a single individual with a standard error of .023, which is much more
precise than that obtained using pairwise estimation. (See Figure 7.1, where
our pairwise estimate was 1.65 with standard error .064.) Yatchew et al. (2003)
found that a parsimonious speciﬁcation similar to (7.1.10), which incorporates
multiple family types, can produce dramatic reductions in standard errors
relative to pairwise estimation.
7.2 Partial Linear Index Models
7.2.1 Introduction
We now consider a somewhat more general speciﬁcation that is a hybrid of
the index model and the partial linear model. Suppose we are given data

Index Models and Other Semiparametric Speciﬁcations

145

Table 7.2. Parsimonious model estimates.
β̂ 1
.6
(.0215)

β̂ 2
.8
(.0880)

corr(β̂ 1 , β̂ 2 )
−0.592

Adults

Children

Equivalence
scale
ˆ = exp(δ̂)


se
ˆ


Log
equivalence
scale δ̂

se
δ̂

1
1
1
1
1
1
2
2
2
2
2
2
3
3
3
3
3
3
4
4
4
4
4
4
5
5
5
5
5
5
6
6
6
6
6
6

0
1
2
3
4
5
0
1
2
3
4
5
0
1
2
3
4
5
0
1
2
3
4
5
0
1
2
3
4
5
0
1
2
3
4
5

1.00
1.42
1.77
2.08
2.37
2.63
1.52
1.85
2.16
2.43
2.69
2.93
1.93
2.23
2.50
2.75
2.99
3.21
2.30
2.56
2.81
3.05
3.27
3.48
2.63
2.87
3.10
3.32
3.53
3.74
2.93
3.16
3.38
3.59
3.79
3.98

0.000
0.034
0.058
0.078
0.096
0.112
0.023
0.035
0.055
0.075
0.093
0.110
0.046
0.052
0.067
0.083
0.100
0.116
0.068
0.073
0.084
0.098
0.113
0.127
0.091
0.095
0.104
0.116
0.129
0.142
0.113
0.117
0.125
0.135
0.147
0.159

0.00
0.35
0.57
0.73
0.86
0.97
0.42
0.62
0.77
0.89
0.99
1.08
0.66
0.80
0.92
1.01
1.09
1.17
0.83
0.94
1.03
1.11
1.18
1.25
0.97
1.05
1.13
1.20
1.26
1.32
1.08
1.15
1.22
1.28
1.33
1.38

0.000
0.024
0.033
0.038
0.041
0.043
0.015
0.019
0.026
0.031
0.035
0.038
0.024
0.023
0.027
0.030
0.033
0.036
0.030
0.029
0.030
0.032
0.034
0.037
0.035
0.033
0.034
0.035
0.036
0.038
0.038
0.037
0.037
0.038
0.039
0.040

n = 7,358 s 2 = .01836

R 2 = .509

146

Semiparametric Regression for the Applied Econometrician

(y1 , w1 , z 1 ), . . . , (yn , wn , z n ) on the model yi = f (r (wi , β)) + z i η + εi , where
wi and z i are ﬁnite dimensional vectors of exogenous variables, f is a nonparametric function, r is a known function, β and η are ﬁnite dimensional parameter
vectors, βo and ηo are the true parameter values, ro = r (w, βo ), and εi | wi , z i
are i.i.d with mean 0 and variance σ 2 . Set up the model in matrix notation,
where the ith rows of W and Z are wi and z i , respectively:
y = f (r (W, β)) + Z η + ε.

(7.2.1)

The regression function is composed of an index function (with possibly
nonlinear index) and a linear component. (We could have made this nonlinear
parametric, but the extension will be natural from our discussion below.)
7.2.2 Estimation
To estimate this model we need to modify our previous procedures slightly.
First, note that if β is known, then (7.2.1) is a partial linear model that may be
estimated in a variety of ways. We proceed as follows. For a ﬁxed β, calculate
the vector r (W, β). Let S be a nonparametric smoother that regresses onto the
vector r (W, β) and apply the double residual method (Section 3.6):
(I − S)y ∼
= (I − S) f (r (W, βo )) + (I − S)Z ηo + (I − S)ε. (7.2.2a)
If the selected β is close to the true value βo , then (I − S) f (r (W, βo )) ∼
= 0.
Obtain an estimate of η:
η̂β = [((I − S)Z ) ((I − S)Z )]−1 ((I − S)Z ) (I − S)y.

(7.2.3a)

By a grid search over values of β, ﬁnd
1
s 2 = min ((I −S)y−(I −S)Z η̂β ) ((I −S)y−(I −S)Z η̂β ).
β n

(7.2.4a)

Let β̂ be the value that satisﬁes (7.2.4). The estimator of η is η̂ = η̂β̂ .
Note that the double residual method requires one to compute a separate
nonparametric regression for each column of the matrix Z in (7.2.2), which can
be time-consuming. The procedure can be accelerated by using a differencing
procedure. For ﬁxed β, let Pβ be the permutation matrix that reorders the vector
r (W, β), so that it is in increasing order, and let D be a differencing matrix.
Then
D Pβ y ∼
= D Pβ f (r (W, βo )) + D Pβ Z ηo + D Pβ ε.

(7.2.2b)

Our estimate of η is given by
η̂β = [(D Pβ Z ) (D Pβ Z )]−1 (D Pβ Z ) D Pβ y,

(7.2.3b)

Index Models and Other Semiparametric Speciﬁcations

147

and the optimization problem becomes
1
s 2 = min (D Pβ y − D Pβ Z η̂β ) (D Pβ y − D Pβ Z η̂β ).
β n

(7.2.4b)

7.2.3 Covariance Matrix
Recall that ro = r (w, βo ). To obtain large-sample standard errors, deﬁne the
following conditional covariance matrices:
z = E [(z − E(z | ro )) (z − E(z | ro )) | ro ]





∂r
∂r
z f  = E f  (ro )
−E
| ro
(7.2.5)
(z − E(z | ro )) | ro
∂β
∂β

 





∂r
∂r
∂r
∂r
 f  = E f  (ro )2
| ro .
−E
| ro
−E
| ro
∂β
∂β
∂β
∂β
Let


V =

then,
1
n /2

z

z f 



z f   f 



η̂ − ηo
β̂ − βo

;

D

→ N (0, σ 2 V −1 ).

(7.2.6a)

(7.2.7a)

Let fˆ be a consistent estimator of the ﬁrst derivative of f and deﬁne
diag( fˆ (·)) to be the diagonal matrix with diagonal elements the components
of the vector fˆ (r (W, β̂)). Deﬁne R to be the matrix whose ith row is the
vector partial derivative ∂r (wi , β̂)/∂β. Let S be a smoother that regresses onto
the vector r (W, β̂). Then the submatrices of the matrix V in (7.2.6a) may be
estimated consistently as follows:
1
P
((I − S)Z ) ((I − S)Z ) → z
n
1
P
((I − S)R) diag( fˆ (·))((I − S)Z ) → z f 
n
1
P
((I − S)R) diag( fˆ (·)2 )((I − S)R) →  f  .
n

(7.2.8a)

Proof of the preceding result, which may be found in Yatchew et al. (2003),
is a straightforward variation on existing proofs in the literature, particularly
Ichimura (1993), Klein and Spady (1993), and Carroll et al. (1997).

148

Semiparametric Regression for the Applied Econometrician

For the parsimonious model (7.2.12) below, β = (β1 , β2 ), w = (x, A, K ),
r (w, β) = log x − β1 log(A + β2 K ), and the matrix R has ith row


∂r (wi , β) ∂r (wi , β)
,
∂β1
∂β2
β̂ 1 ,β̂ 2


β̂ 1 K i
.
(7.2.9)
= − log(Ai + β̂ 2 K i ), −
Ai + β̂ 2 K i
Equation (7.2.9) also applies to the parsimonious model (7.1.10). However,
in that speciﬁcation η = 0, so that (7.2.6a) and (7.2.7b) become
V = f

(7.2.6b)

and
D

n /2 (β̂ − βo ) → N (0, σ 2 V −1 ).
1

(7.2.7b)

Furthermore, V =  f  is estimated using the last equation in (7.2.8a)
1
P
((I − S)R) diag( fˆ (·)2 )((I − S)R) →  f  .
n

(7.2.8b)

7.2.4 Base-Independent Equivalence Scales
To make these ideas more concrete, it is helpful to outline the problem of
equivalence scale estimation further. Engel’s method, although widely used, is –
strictly speaking – not quite correct. The reasons are outlined extensively in
Deaton (1997), but the essence of the argument is this: families with children
are likely to spend a larger share of income on food than families without
children even if they are at the same level of utility. This occurs by virtue of the
needs and consumption patterns of children versus adults. The problem can be
corrected by a simple modiﬁcation to Engel’s approach: rather than searching
for a horizontal shift that superimposes one Engel curve on another, a combined
horizontal and vertical shift that achieves the superimposition is sought instead.7
Return to the problem of ﬁnding the equivalence scale for childless couples versus single individuals, which we considered earlier. Equation (7.1.7) is
modiﬁed to
y = f (log x − zδ) + zη + ε,
7

(7.2.10)

See Blundell, Duncan, and Pendakur (1998) and Pendakur (1999) for econometric models of
this type. For the underlying theoretical arguments, see Lewbel (1989), and Blackorby and
Donaldson (1989, 1993, 1994) and references therein.

Index Models and Other Semiparametric Speciﬁcations

149

where, as before, log x is the log of household expenditure, and z is a dummy
variable that is 1 for couples and 0 for singles.8
Equation (7.2.10) is a partial linear index model, and it can be generalized
easily to accommodate multiple family types. Suppose then that there are q + 1
family types and select the ﬁrst type as the reference to which the other q types
will be compared. Let z be a q-dimensional row vector of dummy variables
for the q (nonreference) types and store data on these in a matrix Z . Then the
model may be written in matrix notation as


y = f log x − Z δ
+ Z η + ε .
(7.2.11)
n×1

n×1

n×q q×1

n×q q×1

n×1

If this model is correct, then base-independent equivalence scales exist and
are given by the q-dimensional vector  = exp(δ).
In our empirical example there are 36 family types (see Table 7.1), and so
estimation of (7.2.11) requires search in a 36-dimensional space. As before,
a parsimonious version may be speciﬁed by making the equivalence scale a
function of the number of adults and children in the family (Eq. (7.1.9)). Our
model becomes



y = f log x −β1 log A + β2 K
+ Z η + ε , (7.2.12)
n×1

n×1

n×1

n×1

n×q q×1

n×1

where A and K are vectors indicating the number of adults and children in each
family. This yields a much simpler estimation problem, for it requires search
over a two-dimensional space to estimate (β1 , β2 ); the dimension of η affects
estimation speed only marginally because it is not estimated by a search.
One of the advantages of semiparametric models of equivalence scales of the
form (7.2.12) is that the parametric function inside f may readily be modiﬁed
to incorporate other demographic variables such as age.
7.2.5 Testing Base-Independence and Other Hypotheses
We will want to test base-independence as well as several other hypotheses such
as the validity of the parsimonious speciﬁcation (7.2.12). In each case we will
use a goodness-of-ﬁt type statistic (Section 6.2). For convenience, under the
alternative hypothesis we will use an optimal differencing estimator to obtain
the unrestricted sample variance. Under the null we will use an estimator that
8

The parameter η has a speciﬁc interpretation; it is the elasticity of the equivalence scale with
respect to the price of food. For details, see Pendakur (1999) and references therein.

150

Semiparametric Regression for the Applied Econometrician

satisﬁes (6.2.2) and thus our statistics will be of the form
 2

2
D
1/2 sres − sunr
(mn)
→ N (0, 1).
2
sunr

(7.2.13)

Consider a test of the base-independent parsimonious speciﬁcation (7.2.12)
against the alternative that Engel curves for the various family types are not
similar in shape. That is, under the alternative we have q + 1 distinct
models,
y j = f j (log x j ) + ε j

j = 0, 1, . . . q,

(7.2.14)

where y j , log x j , and ε j are column vectors of length n j for the jth family
type. In this case we may use the differencing estimator (4.2.4) to estimate
2
2
sdiff,
j , the residual variance for each family type j. We then construct sunr as
their weighted combination, where the weights reﬂect the relative sizes of the
subpopulations. That is,
2
sunr

=

q

nj
j=0

n

2
sdiff,
j

q
1  
=
y D Dy j .
n j=0 j

(7.2.15)

2
To complete the test, the restricted estimator sres
is obtained directly from
(7.2.4a) (or (7.2.4b)), and the test in (7.2.13) may be applied.
Next, consider testing the parsimonious speciﬁcation (7.2.12) against the
more general alternative (7.2.10). Once estimation of the latter is complete (and
this may take a while), one may perform the following test procedure. Obtain
2
sres
from (7.2.4a). Using δ̂, η̂ from the unrestricted procedure, construct the set
of ordered pairs: (yi − z i η̂, log xi − z i δ̂) i = 1, . . . , n, where the log xi − z i δ̂ are
2
in increasing order. Deﬁne the unrestricted variance sunr
to be the differencing
estimator (4.2.4) applied to these ordered pairs. Finally, calculate the test statistic
(7.2.13).
In selecting the order of differencing m for the unrestricted estimators of the
residual variance, the objective is to under smooth estimation of the alternative
relative to the null. This ensures that test statistic (7.2.13) admits the simple
standard normal approximation under the null. For further details, see Yatchew
et al. (2003). Tests of other hypotheses, such as whether adult households and
households with children can be embedded in a single model, may also be
readily constructed. Finally, tests on η may be constructed using the estimated
covariance matrix of η̂.9

9

Indeed it may be interesting to ﬁnd a more parsimonious speciﬁcation for the additively
separable portion of the model zη. After all, one would expect that “similar” family types
should have similar values for η as well.

Index Models and Other Semiparametric Speciﬁcations

151

7.3 Exercises10
1. Engel Equivalence Scales for Couples Versus Singles: The purpose of this exercise
is to estimate equivalence scales for pairs of household types using the index model
speciﬁcation (7.1.7). Results should be similar to those in Figure 7.1.
(a) Using the South African data for single individuals, estimate the Engel curve for
food share as a function of log expenditure. (Use a smoother such as kernel or
loess.) Graph your estimate. Repeat using the data on couples with no children.
(b) Set up a grid of values for δ. Let z be a dummy variable that is 0 for singles and
1 for couples. By searching over the grid, ﬁnd the value δ̂ that satisﬁes
1
s 2 = min [y − fˆδ (log x − zδ)] [y − fˆδ (log x − zδ)],
δ n
where y, z, and log x are n-dimensional vectors.
(c) Estimate the ﬁrst derivative vector fˆ (·) by applying the perturbation method in
(3.7.1) to the ordered pairs (yi , log xi − z i δ̂). Plot your estimate.
(d) Use a nonparametric smoother, say S, to regress the dummy variable z on the
vector log x − z δ̂. Take the residuals to obtain the vector (I − S)z.
(e) Estimate V in (7.1.4) using ((I − S)z)
diag fˆ(·)2 ((I − S)z)/n.
(f) Calculate the standard error of δ̂, sδ̂ = s 2 V̂ −1 /n.
ˆ = exp(δ̂). Using the delta method, obtain an
(g) Calculate the equivalence scale 
ˆ · sδ̂ .
estimate of the standard error of the equivalence scale sˆ = 
(h) Estimate the residual variance for each of these data sets using a low-order
differencing estimator. Use (7.1.8) to calculate the weighted average of the
residual variances sw2 . Are your results consistent with the hypothesis of baseindependence?
2. Repeat Exercise 1 for couples with one child versus singles and couples with two
children versus singles.
3. Engel Equivalence Scales for Multiple Family Types: The purpose of this exercise
is to estimate equivalence scales across multiple family types using the index model
speciﬁcation (7.1.10). Results should be similar to those in Figure 7.2 and Table 7.2.
(a) Assemble the data on all family types. Set up a grid of values for β = (β1 , β2 ).
By searching over the grid, ﬁnd the value (β̂ 1 , β̂ 2 ) that satisﬁes
1
s 2 = min [y − fˆβ (log x − β1 log(A + β2 K ))]
β n
× [y − fˆβ (log x − β1 log(A + β2 K ))].
(b) Estimate the ﬁrst derivative vector fˆ (·) by applying the perturbation method
in (3.7.1) to the ordered pairs (yi , log xi − β̂ 1 log(Ai + β̂ 2 K i )), i = 1, . . . , n.
Plot your estimate.
10

Data and sample programs for empirical exercises are available on the Web. See the Preface
for details.

152

Semiparametric Regression for the Applied Econometrician

(c) Calculate the matrix R whose ith row is given by



β̂ 1 K i
− log(Ai + β̂ 2 K i ), −
Ai + β̂ 2 K i



.

Use a nonparametric smoother, say S, to regress each column of this matrix on
the vector log x − β̂ 1 log(A + β̂ 2 K ) and take the residuals to obtain the matrix
(I − S)R.
(d) Estimate the covariance matrix of (β̂ 1 , β̂ 2 ) using s 2 V̂ −1 /n, where V̂ is obtained
using (7.2.8b).
ˆ = exp(δ̂) for the various combinations
(e) Calculate δ̂ = β̂ 1 log(A + β̂ 2 K ) and 
of adults and children in Table 7.1 and apply the delta method to obtain their
standard errors.
4. General Equivalence Scales for Couples versus Singles: The purpose of this exercise
is to estimate equivalence scales for pairs of household types using the partial linear
index model speciﬁcation (7.2.10). The results should be similar to Yatchew et al.
(2003). Note that because you are estimating both δ and η, the precision of your
equivalence scales will decline substantially relative to Exercise 1.
(a) Let z be a dummy variable that is 0 for singles and 1 for couples. For ﬁxed δ,
let Pδ be the permutation matrix that reorders the vector log x − zδ so that it
is in increasing order. Let D be a differencing matrix. For example, you may
use D = I − S, where S is the nearest-neighbor smoothing matrix deﬁned in
(3.1.12). Set up a grid of values for δ. Use the method outlined in Section 7.2.2
to obtain estimates δ̂ and η̂.
(b) Estimate the ﬁrst derivative vector fˆ (·) by applying the perturbation method
in (3.7.1) to the ordered pairs (yi − z i η̂, log xi − z i δ̂). Plot your estimate.
(c) Use a nonparametric smoother, say S, to regress the dummy variable z on the
vector log x − z δ̂. Take the residuals to obtain the vector (I − S)z.
(d) Estimate V in (7.2.6) using (7.2.8), which in this case becomes
1
((I − S)z) ((I − S)z)
n
1
((I − S)z) diag( fˆ (·))((I − S)z)
n
1
((I − S)z) diag( fˆ (·)2 )((I − S)z).
n
(e) Calculate the standard errors of δ̂ and η̂.
ˆ = exp(δ̂). Using the delta method, obtain an
(f) Calculate the equivalence scale 
ˆ · sδ̂ .
estimate of the standard error of the equivalence scale sˆ = 
5. General Equivalence Scales for Multiple Family Types: The purpose of this exercise
is to estimate equivalence scales for multiple household types using the partial linear
index model speciﬁcation (7.2.12). The results should be similar to those of Yatchew
et al. (2003). Note that because you are estimating both β and η, the precision of
your equivalence scales will decline substantially relative to Exercise 3.

Index Models and Other Semiparametric Speciﬁcations

153

(a) Assemble the data on all family types. Set up a grid of values for β = (β1 , β2 ). Let
Pβ be the permutation matrix that reorders the vector log x − β1 log(A + β2 K ) so
that it is in increasing order. Let D be a differencing matrix and deﬁne S = I − D.
Use the method outlined in Section 7.2.2 to obtain estimates β̂ 1 , β̂ 2 , and η̂.
(b) Estimate the ﬁrst derivative vector fˆ (·) by applying the perturbation method
in (3.7.1) to the ordered pairs (yi − z i η̂, log xi − β̂ 1 log(Ai + β̂ 2 K i )),
i = 1, . . . , n. Plot your estimate.
(c) Calculate the R and (I − S)R matrices as in 3(c) above. Calculate (I − S)Z .
(d) Estimate V in (7.2.6) using (7.2.8). Calculate the standard errors of β̂ and η̂
using s 2 V̂ −1 /n.
ˆ = exp(δ̂) for the various combinations
(e) Calculate δ̂ = β̂ 1 log(A + β̂ 2 K ) and 
of adults and children in Table 7.1 and apply the delta method to obtain their
standard errors.

8

Bootstrap Procedures

8.1 Background
8.1.1 Introduction
Bootstrap procedures, widely attributed to Efron (1979),1 are simulation-based
techniques that provide estimates of variability, conﬁdence intervals, and critical
values for tests. The fundamental idea is to create replications by treating the
existing data set (say of size n) as a population from which samples (of size n)
are obtained. In the bootstrap world, sampling from the original data becomes
the data-generating mechanism (DGM). Variation in estimates occurs because,
upon selection, each data point is replaced in the population.
In many circumstances, bootstrap procedures are simpler to implement than
their asymptotic counterparts. In addition, they are often more accurate. By
drawing correctly sized samples from the original data2 the simulated distribution inherits higher-order moment properties of the true DGM. The conventional
asymptotic normal approximation ignores such information.
It is not surprising that major advances in bootstrap techniques and nonparametric procedures have occurred more or less contemporaneously. Both
have been driven by the precipitous drop in computing costs. The emergence of
automated data collection – which has produced very large data sets – has also
contributed indirectly to the development of nonparametric techniques. Furthermore, although the bootstrap requires resampling many times, calculations
need not be done serially but can be performed contemporaneously, making the
bootstrap particularly suitable for parallel processing.

1

2

Actually, Monte Carlo inference techniques had been recommended by several authors prior to
Efron’s work, among them Barnard (1963) and Hartigan (1969, 1971). For additional precursors
see Hall (1992, p. 35) and Davison and Hinkley (1997, p. 59).
One hopes the data are representative of the underlying population. Prospects for this of course
improve as sample size n increases.

154

Bootstrap Procedures

155

We begin with a rudimentary introduction to the bootstrap. (References for
further reading in this voluminous literature are provided at the end of this
section.) This is followed by a delineation of several bootstrap techniques in
nonparametric and semiparametric settings. Throughout this chapter, the superscript B will signify a bootstrap sample, estimate, conﬁdence interval, or test
statistic.
At ﬁrst sight, it might appear that there is a natural, unique way to perform
resampling: one should just mimic the methodology used to obtain the original
sample. On closer examination, one discovers that there are often several sensible ways to resample the data. In the regression setting, which has been our
mainstay throughout this book, we rely principally on taking random samples
from estimated residuals, although other approaches are possible.
8.1.2 Location Scale Models
As an example, consider the usual location scale model where y1 , . . . , yn are
i.i.d. with mean µ y and variance σ y2 . The distributional family is unknown. The
variance of ȳ is estimated using s ȳ2 = s y2 /n, where s y2 = (yi − ȳ)2 /(n − 1). The
D
central limit theorem, which states that n 1/2 ( ȳ − µ y )/s y → N (0, 1), provides
the basis for asymptotic conﬁdence intervals and test procedures on µ y .
Bootstrap inference on µ y proceeds as follows. Take many random samples
of size n from the original sample, each time calculating the sample mean ȳ B .
The bootstrap estimate of the variance of ȳ is obtained by calculating the sample
variance of the ȳ B . Indeed, a bootstrap approximation to the sampling distribution of ȳ can be obtained by plotting the histogram of the ȳ B . By calculating
the .025 and .975 quantiles of this distribution, one can obtain a 95 percent
conﬁdence interval for µ y . The procedure we have just described is an example
of the “percentile” method because it works with the percentiles or quantiles
of the bootstrap distribution for the parameter estimator of interest, in this case
the sample mean.
There is substantial evidence – both theoretical and empirical – that it is usually better to simulate the distribution of a statistic that is a pivot (or at least an
asymptotic pivot).3 In this case, bootstrap inference on the mean would proceed
as follows.
3

Pivots are statistics whose distributions do not depend on any unknown parameters. There
are precious few “true pivots”. Among them are the following. If the data are i.i.d. normal,
then n 1/2 ( ȳ − µ)/s y is precisely tn − 1 and (n − 1)s y2 /σ 2 is precisely χn2 − 1 . The Kolmogorov–
Smirnov statistic is also a true pivot. On the other hand, there are numerous asymptotic pivots.
For example, if the data are i.i.d. with unknown distribution but have a ﬁnite mean and variance,
then n 1/2 ( ȳ − µ)/s y converges to a standard normal in large samples and so is an asymptotic
pivot.

156

Semiparametric Regression for the Applied Econometrician

Take many random samples of size n from the original sample, each time
calculating the statistic t B = n 1/2 ( ȳ B − ȳ)/s yB , where ȳ B and s yB are the mean
and standard deviation of the bootstrapped data. To construct a 95 percent
conﬁdence interval for µ y , obtain the .025 and .975 quantiles of the distribution of t B , say c.025 , c.975 , and isolate µ y within the probability statement:
Prob [c.025 ≤ n 1/2 ( ȳ − µ y )/s y ≤ c.975 ] ∼
= .95. A two-sided test of the hypothesis
H0 : µ = µo at a 5 percent signiﬁcance level can be performed by determining
whether the resulting conﬁdence interval [ ȳ − c.975 s y /n 1/2 , ȳ − c.025 s y /n 1/2 ]
contains µo . This procedure is an example of the “percentile-t” method because it uses an asymptotic pivot that takes the form of a t-statistic.
An alternative approach to testing H0 : µ = µo , which will be instructive
shortly, is to impose the null hypothesis on the bootstrap DGM. In this case, using the original data and sample mean, one calculates the residuals ε̂i = yi − ȳ.
One then takes repeated samples of size n from ε̂1 , . . . , ε̂n , constructs the bootstrap data set yiB = µo + ε̂iB , i = 1, . . . , n, and calculates t B = n 1/2 ( ȳ B − µo )/s yB
each time. (Note that the bootstrap DGM satisﬁes the null hypothesis.) To obtain critical values c.025 , c.975 for a two-sided test at a 5 percent signiﬁcance
level, obtain the .025 and .975 quantiles of the (simulated) distribution of t B .
Finally, accept the null hypothesis if the interval contains t = n 1/2 ( ȳ − µo )/s y ;
otherwise, reject it.
8.1.3 Regression Models
Suppose we now have data (y1 , x1 ), . . . , (yn , xn ) on the model y = f (x) + ε,
where f may or may not lie in a parametric family. The εi are i.i.d. with
mean 0, variance σε2 , and are independent of x. A “joint” resampling methodology involves drawing i.i.d. observations with replacement from the original
collection of ordered pairs.
Residual resampling, on the other hand, proceeds as follows. First, f is
estimated using, say, fˆ. The estimated residuals ε̂i = yi − fˆ(xi ) are assembled
and centered so that their mean is zero (just as the true εi have a mean of zero).
One then samples independently from these to construct a bootstrap data set:
(y1B , x1 ), . . . , (ynB , xn ), where yiB = fˆ(xi ) + ε̂iB . Statistics of interest are then
computed from these simulated data.
An alternative residual resampling methodology known as the “wild” or “external” bootstrap is useful particularly in heteroskedastic settings. In this case,
for each estimated residual ε̂i = yi − fˆ(xi ) one creates a two-point distribution
for a random variable, say, ωi with probabilities as shown in Table 8.1.
The random variable ωi has the properties E(ωi ) = 0, E(ωi2 ) = ε̂i2 , E(ωi3 ) =
3
ε̂i . One then draws from this distribution to obtain ε̂iB . The bootstrap data set
(y1B , x1 ), . . . , (ynB , xn ) is then constructed, where yiB = fˆ(xi ) + ε̂iB , and statistics of interest are calculated. See Wu (1986) and Härdle (1990, pp. 106–108,247).

Bootstrap Procedures

157

Table 8.1. Wild bootstrap.
ωi
√
ε̂i (1 − √5)/2
ε̂i (1 + 5)/2

Prob(ωi )
√
(5 + √5)/10
(5 − 5)/10

8.1.4 Validity of the Bootstrap
Suppose that the statistic being used to produce a conﬁdence interval or test
statistic has a nondegenerate limiting distribution. To establish that bootstrapbased conﬁdence intervals have correct coverage probabilities in large samples
or that bootstrap test procedures have correct asymptotic size, three conditions
are typically sufﬁcient (see Beran and Ducharme, 1991, Proposition 1.3, p. 19,
and Proposition 4.3, p. 49). The ﬁrst condition requires that the DGM used for
bootstrap simulations of the statistic converges to the true DGM. This is the
case if the original estimator is consistent. The second is a continuity condition
requiring that small changes in the true DGM will result in small changes to
the limiting distribution of the statistic. The third condition – “triangular array
convergence” – which is usually the most difﬁcult to verify, requires that, along
any path of DGMs converging to the true DGM, the exact sampling distribution
of the statistic converges to the limiting distribution under the true DGM. Put
another way, if the bootstrap DGM is close to the true DGM and the sample
is large, then the distribution of the bootstrap statistic should be close to the
limiting distribution of the true statistic.
8.1.5 Beneﬁts of the Bootstrap
In the following sections we describe bootstrap techniques in nonparametric
and semiparametric regression settings. We advocate their use because they
are often more accurate than the asymptotic procedures we have proposed
earlier. Moreover, in some cases, there is ambiguity in the implementation of
the asymptotic technique with potentially different outcomes.4 In other cases,
no convenient asymptotic approximation is available.
The increased accuracy that can result from bootstrapping has been formally
analyzed using Edgeworth expansions. We outline the argument in its simplest
form. Let y1 , . . . , yn be i.i.d. with mean µ and variance σ 2 ; ȳ and s 2 are the
4

For example, consider estimation of standard errors for the parameters of the index model that
depend on the derivative f  of the nonparametric regression function (see (7.1.4)). The degree
of smoothing that one uses to estimate f  can inﬂuence the standard errors signiﬁcantly. As we
will see shortly, the bootstrap can be helpful in such situations.

158

Semiparametric Regression for the Applied Econometrician

sample mean and variance, respectively. Then t = n 1/2 ( ȳ − µ)/s converges to
a standard normal and is therefore an asymptotic pivot. Under quite general
regularity conditions, the distribution of t can be expanded as a power series
in n 1/2
 
1
1
,
(8.1.1)
P[t ≤ x] = "(x) + 1/ q(x)φ(x) + O
2
n
n
where φ and " are the standard normal density and distribution functions,
respectively, and q is a polynomial whose coefﬁcients depend on moments
(or cumulants) of ȳ. Equation (8.1.1) is an Edgeworth expansion. As n gets large,
the right-hand side converges to the standard normal, as one would expect. Furthermore, the error implicit in the normal approximation is P[t ≤ x] − "(x) =
O(1/n 1/2 ).
Now, let t B = n 1/2 ( ȳ B − ȳ)/s B be the bootstrap analogue of t. Then it, too,
has an expansion similar to (8.1.1),
 
1
1
P[t B ≤ x] = "(x) + 1/ q̂(x)φ(x) + O P
,
(8.1.2)
n 2
n
where q̂ is obtained from q by replacing moments (which appear in the coefﬁcients of q) with corresponding bootstrap estimates. Because moments
(and smooth functions of moments) can be estimated n 1/2 -consistently, we
have q̂(x) − q(x) = OP(1/n 1/2 ). Thus, subtracting (8.1.2) from (8.1.1), one
obtains
 
1
B
P[t ≤ x] − P[t ≤ x] = OP
.
(8.1.3)
n
That is, the error of approximation of the bootstrap distribution is OP(1/n)
rather than O(1/n 1/2 ), which results from using the asymptotic normal.
A similar argument can be advanced in nonparametric regression, which
typically entails taking local rather than global averages. Consider the simple
moving average smoother of Section 3.1 and suppose that k, the number of
neighbors being averaged, increases sufﬁciently slowly so that the bias term in
(3.1.11) disappears quickly. In this case t = k 1/2 ( fˆ(xo ) − f (xo ))/s converges to
a standard normal, where s 2 estimates the residual variance. Then the statistic
admits an Edgeworth expansion (compare with (8.1.1)),


1
1
P[t ≤ x] = "(x) + 1/ q(x)φ(x) + o 1/ .
(8.1.4)
k 2
k 2
Let t B = k 1/2 ( fˆ B (xo ) − fˆ(xo ))/s B be the bootstrap analogue of t. Then it too
has an expansion similar to (8.1.4):


1
1
B
(8.1.5)
P[t ≤ x] = "(x) + 1/ q̂(x)φ(x) + oP 1/ ,
k 2
k 2

Bootstrap Procedures

159

where q̂(x) − q(x) = o P (1) so that



1
P[t ≤ x] − P[t B ≤ x] = oP 1/ .
k 2

(8.1.6)

In summary, (8.1.3) and (8.1.6) indicate that the bootstrap can result in
approximations to sampling distributions that are superior to the asymptotic
normal. For detailed Edgeworth analysis of an extensive range of bootstrap
procedures, see Hall (1992). For an overview, see Horowitz (2001).
8.1.6 Limitations of the Bootstrap
Although the bootstrap works in a broad class of models, there are cases in
which it fails or at least requires modiﬁcation to work properly. A particularly
simple example of relevance to econometricians was provided by Andrews
(2000). Suppose y1 , . . . , yn are drawn from an N (µ, 1) distribution and one
knows that µ ≥ 0. The maximum likelihood estimator is then µ̂ = max{ ȳ, 0}.
If the true mean is zero, the bootstrap will fail to approximate the distribution
of n 1/2 (µ̂ − µ) correctly even in large samples. The basic idea extends to much
more general models whenever a parameter is inequality constrained and its
true value lies on the boundary of the parameter space. Inequality constraints
are common; take for example the estimation of equivalence scales that involves
a priori constraints on parameters (see discussion following (7.1.9)). In these
circumstances, it is possible to modify the bootstrap to regain consistency by
taking bootstrap samples of size m  n.
A second example is the speciﬁcation test proposed by Härdle and Mammen
(1993), which we discussed in Section 6.4.2. In that case, the wild bootstrap
succeeds where the conventional bootstrap fails. For various other examples,
see Andrews (2000) and Beran (1997) and references therein. In a time series
context, bootstrap failures can be spectacular; see Phillips (2001).
8.1.7 Summary of Bootstrap Choices
As we mentioned earlier, a variety of methods are available for implementing
the bootstrap. In the regression setting, one chooses ﬁrst between joint sampling and residual sampling. We will use the latter exclusively. Having done
so, one needs to decide whether to assume the residuals are homoskedastic –
in which case one can sample them randomly – or whether the residuals are
heteroskedastic – in which case a device like the wild bootstrap is required. One
must also decide whether to use the “percentile” method or the “percentile-t”
method. Although the former will produce conﬁdence intervals and critical values that are asymptotically valid, the latter, which uses a pivot, is typically more
accurate. For a detailed discussion of the alternative bootstrap methodologies
in a nonparametric setting, their advantages, and disadvantages, see Hall (1992,

160

Semiparametric Regression for the Applied Econometrician

Sections 4.4 and 4.5) and Horowitz (2001, Sections 4.2 and 4.3). Finally, one
must select the number of bootstrap replications. There is a growing literature
on this subject (see, e.g., Andrews and Buchinsky 2000). A practical approach
involves increasing the number of bootstrap iterations until there is little change
in the resulting critical value or conﬁdence interval.
8.1.8 Further Reading
Efron and Tibshirani (1993) provide a readable introduction to the bootstrap.
Shao and Tu (1995) provide a more technical survey of various developments.
Härdle (1990) discusses applications of the bootstrap in a nonparametric setting.
See also Hall (1992, pp. 224–234). Hall (1992) provides extensive Edgeworth
analysis explaining why the bootstrap can outperform the traditional asymptotic approach. An abbreviated version of the arguments in Hall (1992) may
be found in Hall (1994). LePage and Billard (1992) and Mammen (1992) contain explorations into the limitations of the bootstrap. Beran and Ducharme
(1991) provide an approachable treatment of the large sample validity of the
bootstrap. Horowitz (1997) offers theory and numerical analysis for a variety
of bootstrap methods, whereas Horowitz (2001) provides an extensive review
of the bootstrap in econometrics. Davison and Hinkley (1997) contains a practical review of bootstrap methods and their applications with some attention to
nonparametric regression.
8.2 Bootstrap Conﬁdence Intervals for Kernel Smoothers
Consider the problem of constructing pointwise conﬁdence intervals for a nonparametric regression function. We have described asymptotic procedures for
doing so in Chapter 3. If one uses an optimal bandwidth, these are complicated
by the presence of biases (as in (3.2.5)). Indeed, this was the reason that undersmoothing was used to simplify conﬁdence interval construction (see (3.2.7) to
(3.2.10)).
Table 8.2 outlines the implementation of several bootstrap procedures. The
upper panel delineates construction of percentile conﬁdence intervals. These
require only reestimation of the regression function f (xo ) from each bootstrap
sample and are therefore the simplest to implement.
The second panel indicates how to construct percentile-t conﬁdence intervals.
Because the bootstrap is attempting to mimic the distribution of the asymptotic
pivot t = ( fˆ(xo ) − f (xo ))/s fˆ , one needs to calculate fˆ B (xo ) and s Bfˆ (xo ) from
each bootstrap sample. (For the latter, see Section 3.2 and particularly (3.2.8).)
The third panel explains how to modify the previous procedures to allow for
heteroskedasticity.
In Figure 8.1 we illustrate the application of these techniques to our
South African data on food expenditures by single individuals. Note that the

Bootstrap Procedures

161

Table 8.2. Bootstrap conﬁdence intervals at f (xo ).a
Percentile bootstrap conﬁdence interval at f (xo ):
1. Using cross-validation, ﬁnd the optimal bandwidth λ = O(n −1/5 ). Estimate f and
call this estimate fˆλ .
2. Reestimate f using a wider bandwidth, say λ̄ = 1.1λ (which will result in some
oversmoothing) and call this estimate fˆλ̄ .
3. Reestimate f using a narrower bandwidth, say .9λ (which will result in some
undersmoothing) and calculate the residuals ε̂i .
4. (a) Center the residuals obtained in Step 3 and sample with replacement to obtain bootstrap residuals ε̂iB . Construct a bootstrap data set yiB = fˆλ̄ (xi ) + ε̂iB , i = 1, . . . , n.
(b) Estimate f (xo) using the bootstrap data and the original optimal λ to obtain
fˆλB (xo ).
(c) Repeat the resampling many times saving the results from (b).
5. To calculate a 95 percent conﬁdence interval for f (xo ), obtain the .025 and .975
quantiles of the distribution of fˆλB (xo ).
Percentile-t bootstrap conﬁdence interval at f (xo ):
Replace Steps 4 and 5 above with
4. (a) Resample with replacement from the centered residuals to obtain bootstrap
residuals ε̂iB and construct a bootstrap data set yiB = fˆλ̄ (xi ) + ε̂iB , i = 1, . . . , n.
(b) Calculate fˆλB (xo ) and s Bfˆ (xo ) using (3.2.3) and (3.2.8). Then calculate
t B = ( fˆ B (xo ) − fˆ(xo ))/s Bˆ .
f

(c) Repeat the resampling many times saving t B each time.
5. To calculate a 95 percent conﬁdence interval for f (xo ), obtain c.025 and c.975 , the
.025 and .975 quantiles of the empirical distribution of t B from Step 4(c). A
95 percent conﬁdence interval is given by
[ fˆ(xo ) − c.975 · s fˆ (xo ), fˆ(xo ) − c.025 · s fˆ (xo )],
where fˆ(xo ) and s fˆ (xo ) are calculated using the original data.
Heteroskedasticity:
Replace 4(a) in either of the preceding procedures with
4. (a) Sample using the wild bootstrap (Table 8.1) from the uncentered residuals to
obtain bootstrap residuals ε̂iB and construct a bootstrap data set
yiB = fˆλ̄ (xi ) + ε̂iB , i = 1, . . . , n.
a
For deﬁnitions of various estimators, see Section 3.2. Note that xo may be a vector, in
which case one is producing a collection of pointwise conﬁdence intervals.
Source: Härdle (1990, pp. 106–107).

162

Semiparametric Regression for the Applied Econometrician

Data: Food share of expenditure by single individuals from South Africa; compare with Figure 3.3.

Figure 8.1. Percentile bootstrap conﬁdence intervals for Engel curves.

Bootstrap Procedures

163

heteroskedastic conﬁdence intervals in the lower panel are narrower at high
levels of income than their homoskedastic counterparts. This is because there
is less variance in food share expenditures at high levels of income, as may be
seen from the scatterplot in Figure 3.3.
8.3 Bootstrap Goodness-of-Fit and Residual Regression Tests
8.3.1 Goodness-of-Fit Tests
We have proposed a simple goodness-of-ﬁt statistic for testing various restrictions on the regression function. It involves comparison of the restricted estimate
of the residual variance to the differencing estimate. To simplify exposition, we
will assume that optimal differencing coefﬁcients are used so that the statistic
has the form (see Sections 4.3 and 6.2, and (6.2.5))
 2

2
s − sdiff
D
1/2 res
V = (mn)
→ N (0, 1),
(8.3.1)
2
sdiff
where m is the order of differencing. The essential idea underlying bootstrap
critical values is the creation of a DGM that satisﬁes the null hypothesis. This
is done by imposing the restrictions of the null hypothesis on the estimate of
the regression function. The resulting restricted estimate of f and the (centered) estimated residuals constitute the bootstrap DGM. Repeated samples are
then taken, and the test statistic is recomputed each time. Behavior of the test
statistic under the null hypothesis is assessed (and critical values are obtained)
by observing the behavior of the bootstrapped test statistic. Table 8.3 contains
implementation details. The bootstrap approach requires only the ability to
compute the various components of the test statistic and, as such, it is applicable to a variety of hypotheses. Computing time depends on sample size, the
number of bootstrap samples that are taken, and the time required to compute
the various components of the test statistic.
Goodness-of-ﬁt statistics like (8.3.1) can be obtained in a variety of ways. In
2
calculating the differencing estimator sdiff
one need not use optimal differencing coefﬁcients. For arbitrary coefﬁcients the statistic takes the form (6.2.6).
2
More generally, one can replace sdiff
with other unrestricted estimators of the
residual variance such as those obtained by applying a smoother. The key requirement is that one undersmooth when estimating the residual variance under
the alternative.
A variety of estimators may also be available under the null, depending on
the speciﬁc restrictions being imposed. For the null, the key condition is that
the difference between the estimated sum of squared residuals and the true sum
of squared residuals converge to zero sufﬁciently quickly (see (6.2.2)).

164

Semiparametric Regression for the Applied Econometrician

Table 8.3 Bootstrap goodness-of-ﬁt tests.
¯ H1 : f ∈  where  is a smooth set of functions and ¯ is a
Hypotheses: Ho : f ∈ ,
smooth set of functions with additional constraints.
2
2
2
2
Test statistic: V = (mn)1/2 (sres
− sdiff
)/sdiff
where sres
is the estimated residual variance
2
from a restricted regression and sdiff is an optimal differencing estimator of order m.
Bootstrap test
1. Perform the restricted regression where the constraints of the null hypothesis are
imposed. Save the estimates of the regression function fˆres (x1 ), . . . . , fˆres (xn ), the
2
residuals ε̂res,1 , . . . . , ε̂res,n , and the estimated residual variance sres
.
2
2. Calculate sdiff
.
3. Calculate the value of the test statistic V .
4. (a) Sample with replacement from the centered restricted residuals to obtain
ε̂1B , . . . . , ε̂nB .
(b) Construct a bootstrap data set (y1B , x1 ), . . . . , (ynB , xn ), where yiB = fˆres (xi ) + ε̂iB .
(c) Using the bootstrap data set, estimate the model under the null and calculate
2B
2B
sres
, sdiff
, and V B .
(d) Repeat Steps (a)–(c) multiple times, each time saving the value of the test
statistic V B . Deﬁne the bootstrap critical value for a 5 percent signiﬁcance level test
to be the 95th percentile of the V B .
5. Compare V , the actual value of the statistic, with the bootstrap critical value.

The hypotheses that can be tested using these goodness-of-ﬁt-type statistics
include a parametric null, a semiparametric null (such as the partial linear
model or index model), equality of regression functions, additive separability,
monotonicity, concavity, and base-independence of equivalence scales.

8.3.2 Residual Regression Tests
As with goodness-of-ﬁt tests, a wide variety of hypotheses can be tested using
residual regression procedures. Recall that the test statistic may be decomposed
into three components (U = U1 + U2 + U3 ) as in (6.3.7). The key is to ensure
that the estimator of the regression function under the null hypothesis converges
sufﬁciently quickly so that the large sample distribution of U is determined by
the ﬁrst term U1 (see particularly (6.3.12) and (6.3.13)).
Table 8.4 summarizes a procedure for obtaining bootstrap critical values for
the class of residual regression tests we have discussed in Section 6.3 when
the residuals are homoskedastic. If the residuals are heteroskedastic, then one
replaces random sampling from the centered restricted residuals with sampling
using the wild bootstrap.

Bootstrap Procedures

165

Table 8.4 Bootstrap residual regression tests.
¯ H1 : f ∈ , where  is a smooth set of functions and ¯ is a
Hypotheses: H0 : f ∈ ,
smooth set of functions with additional constraints.
Test statistic: We implement using the uniform kernel. Let Kij be the ijth entry of the
kernel matrix deﬁned by
Kij = 1/2
Kij = 0

|x j − xi | ≤ λ

if

j = i (note that diagonal elements Kii = 0)

otherwise.

Let







2σε4 p 2 (x) K 2
1 
1
(yi − fˆres (xi ))
(y j − fˆres (x j ))Kij ∼ N 0,
U=
n
λn
λn 2


.

j =i

i

Deﬁne σU2 = Var(U ) = 2σε4 p 2 (x) K 2/λn 2 , which may be estimated using
σ̂U2 =

2 
n 4 λ2

i

(yi − fˆres (xi ))2 (y j − fˆres (x j ))2 K2ij .

j =i

Bootstrap test
1. Perform the restricted regression where the constraints of the null hypothesis are
imposed. Save the estimates of the regression function fˆres (x1 ), . . . . , fˆres (xn ) and
the residuals ε̂res,1 , . . . . , ε̂res,n . Center the residuals so that their mean is zero.
2. Calculate U , σ̂U , and U/σ̂U .
3. (a) Sample with replacement from the centered restricted
 residuals
 to obtain

ε̂1B , . . . . , ε̂nB and construct a bootstrap data set y1B , x1 , . . . . , ynB , xn , where
yiB = fˆres (xi ) + ε̂iB .
(b) Using the bootstrap data set, estimate the model under the null and calculate σ̂UB ,
U B , and U B /σ̂UB .
(c) Repeat Steps (a) and (b) multiple times, each time saving the value of the
standardized test statistic U B /σ̂UB . Deﬁne the bootstrap critical value for a 5 percent
signiﬁcance level test to be the 95th percentile of the U B /σ̂UB .
4. Compare U/σ̂U , the actual value of the statistic, with the bootstrap critical value.
Heteroskedasticity:
Replace 3.(a) with
3. (a) Sample using the wild bootstrap (Table 8.1) from ε̂res,1 , . . . . , ε̂res,n to obtain
ε̂1B , . . . . , ε̂nB .

Li and Wang (1998) found that the bootstrap approximation to the distribution of residual regression tests is superior to the asymptotic approximation.
Yatchew and Sun (2001) found similar results for goodness-of-ﬁt tests. A principal reason is that the bootstrap corrects for the nonzero mean in the ﬁnite
sample distributions of these test statistics.

166

Semiparametric Regression for the Applied Econometrician

8.4 Bootstrap Inference in Partial Linear and Index Models
8.4.1 Partial Linear Models
Let us return to the partial linear model y = zβ + f (x) + ε, which we have
discussed in Sections 1.3, 3.6, and 4.5. Table 8.5 summarizes how to construct
bootstrap conﬁdence intervals for the components of β using the double residual
estimation procedure. Under homoskedasticity, we use random sampling from
centered residuals. Under heteroskedasticity, we use the wild bootstrap. Validity
of bootstrap procedures in the partial linear model has been conﬁrmed (see, e.g.,
Mammen and Van de Geer 1997 and Yatchew and Bos 1997). Linton (1995b)
studied higher-order approximations to the distributions of estimators of the
partial linear model. See also Härdle, Liang, and Gao (2000).
We illustrate two of the preceding bootstrap procedures by applying them
to our data on electricity distribution costs (see Sections 1.6 and 4.6.2). With
wages and capital prices entering in a Cobb–Douglas format, the speciﬁcation
is given by
tc = f (cust) + β1 wage + β2 pcap + β3 PUC + β4 kwh
+ β5 life + β6 lf + β7 kmwire + ε.

(8.4.1)

We reestimate this model using the double residual method and apply the
bootstrap procedure outlined in Table 8.5 to obtain percentile-t conﬁdence
intervals for the components of β. The process is repeated using the wild bootstrap. The results are summarized in Table 8.6 in which asymptotic conﬁdence
intervals are also reported.
8.4.2 Index Models
Let us return to the index model
y = f (xδ) + ε,

(8.4.2)

which we have studied in Chapter 7. To estimate the standard error of δ̂ one
needs to estimate the derivative f  (see (7.1.4)–(7.1.6)). The result can be quite
sensitive to the smoothing parameter used to estimate f  . Unfortunately, crossvalidation does not provide good guidance for smoothing parameter selection
if one is interested in the derivative of a function.
The bootstrap provides an alternative mechanism for calibrating standard
errors or, more importantly, for directly obtaining conﬁdence intervals for δ.
However, the estimator δ̂ requires a grid search and thus the bootstrap can be
time-consuming. This limitation continues to diminish as computing speeds
increase and search algorithms become more intelligent. In the examples and

Bootstrap Procedures

167

Table 8.5 Percentile-t bootstrap conﬁdence intervals for β in the partial
linear model.a
Let y = f (x) + zβ + ε, where z is a p-dimensional vector. To construct bootstrap
conﬁdence intervals for the components of β using the double residual estimation
procedure, proceed as follows:
1. (a) Estimate h(x) = E(y | x) and g(x) = E(z | x) to obtain the vector ĥ(x) and the
n × p matrix ĝ(x).
(b) Estimate β using the double residual estimator
β̂ = ((Z − ĝ(x)) (Z − ĝ(x)))−1 (Z − ĝ(x)) (y − ĥ(x)).
(c) Estimate the residual variance
s 2 = (y − ĥ(x) − (Z − ĝ(x))β̂) (y − ĥ(x) − (Z − ĝ(x))β̂)/n.
ˆ β̂ = s 2 ((Z − ĝ(x)) (Z − ĝ(x)))−1 .
(d) Estimate the covariance matrix of β̂ using 
(e) Perform a kernel regression of y − Z β̂ on x to obtain fˆ.
The DGM for the regression function is the vector Z β̂ + fˆ(x). The estimated residuals
ε̂ = y − Z β̂ − fˆ(x) will provide the DGM for the residuals.
2. (a) Sample with replacement from the centered residuals to obtain ε̂1B , . . . . , ε̂nB .
(b) Construct a bootstrap data set (y1B , x1 , z 1 ), . . . . , (ynB , xn , z n ), where
yiB = fˆ(xi ) + z i β̂ + ε̂iB .
B
(c) Perform a kernel regression of y B on x to obtain ĥ (x).
(d) Calculate the bootstrap estimate
B
β̂ B = ((Z − ĝ(x)) (Z − ĝ(x)))−1 (Z − ĝ(x)) (y B − ĥ (x)).
(e) Calculate the bootstrap residual variance
B
B
B
s 2 = (y B − ĥ (x) − (Z − ĝ(x))β̂ B ) (y B − ĥ (x) − (Z − ĝ(x))β̂ B )/n.
B
(f) Calculate the estimated covariance matrix of β̂ using
B
2B

−1
ˆ
β̂ = s ((Z − ĝ(x)) (Z − ĝ(x))) .
(g) For each component of β, calculate the bootstrap t-statistic

 # B 1/2
ˆ
t jB = β̂ Bj − β̂ j

, j = 1, . . . , p.
β̂

jj

(h) Repeat Steps (a)–(g) multiple times saving the results from Step (g).
3. To calculate a 95 percent conﬁdence interval for, say, β j , proceed as follows. Let
c.025 and c.975 be the .025 and .975 quantiles of the empirical distribution of t jB from
Step 2 (g). A 95 percent conﬁdence interval is given by





ˆ β̂
β̂ j − c.975 · 

1/2
jj

,



ˆ β̂
β̂ j − c.025 · 

1/2 
jj

.

Heteroskedasticity:
Replace 2(a) with
2. (a) Sample using the wild bootstrap (Table 8.1) from the uncentered residuals
ε̂1 , . . . . , ε̂n to obtain ε̂1B , . . . . , ε̂nB .
a

See, in particular, Section 3.6.3 for deﬁnitions of terms and notation.

168

Semiparametric Regression for the Applied Econometrician

Table 8.6 Asymptotic versus bootstrap conﬁdence intervals: Scale economies
in electricity distribution.
Partial linear modela

Variable

Coef

wage

0.692

0.279

0.146
0.031
0.079

1.238
1.326
1.351

pcap

0.504

0.066

0.374
0.358
0.354

0.634
0.659
0.651

PUC

−0.067

0.035

−0.136
−0.144
−0.134

0.003
0.019
−0.004

0.015

0.080

−0.142
−0.156
−0.155

0.172
0.194
0.202

−0.500

0.111

−0.716
−0.747
−0.734

−0.281
−0.240
−0.283

lf

1.279

0.398

0.499
0.387
0.510

2.057
2.120
2.064

kmwire

0.356

0.081

0.197
0.200
0.196

0.516
0.538
0.524

kwh

life

s2
R2

Asy SE

95 percent Conﬁdence intervals
Asymptotic
Percentile-t Bootstrap
Wild Bootstrap

.017
.692

a

Model estimated using kernel double residual method. Number of bootstrap replications = 1,000.

exercises to follow, we use S-Plus. Much faster results can be obtained if one
uses Fortran or other programming languages.
A further advantage of the bootstrap is that it incorporates nonnormality,
which can emerge if the sample is of moderate size and the ex ante bounds
on δ are tight. For example, suppose one is estimating an equivalence scale
by searching over values of δ in the interval [a, b]. The bootstrap can often

Bootstrap Procedures

169

Table 8.7 Conﬁdence intervals for δ in the index model: Percentile method.
Let y = f (xδ) + ε. To construct bootstrap conﬁdence intervals for the components of
δ, proceed as follows:
1. Using a grid search, ﬁnd δ̂ the value that minimizes (7.1.3). Perform a
nonparametric regression on the ordered pairs (yi , xi δ̂) to obtain
fˆ(xi δ̂), i = 1, . . . , n. Calculate the residuals ε̂i = yi − fˆ(xi δ̂).
2. (a) Sample with replacement from the centered residuals to obtain ε̂iB , i = 1, . . . , n.
(b) Construct the bootstrap data set (y1B , x1 ), . . . . , (ynB , xn ) where
yiB = fˆ(xi δ̂) + ε̂iB .
(c) Using the bootstrap data, obtain δ̂ B by minimizing (7.1.3).
3. Repeat Step 2 multiple times to obtain the bootstrap distribution of δ̂ B . For a
95 percent conﬁdence interval for a component of δ, extract the corresponding .025
and .975 quantiles.
Heteroskedasticity:
Replace 2 (a) with
2. (a) Sample using the wild bootstrap (Table 8.1) from ε̂i to obtain ε̂iB for i = 1, . . . , n.

outperform the normal approximation to the sampling distribution of δ̂, particularly if estimates of δ are frequently at or near the boundary (although as we
have indicated above, the bootstrap fails if the true value of δ is actually on the
boundary). Table 8.7 outlines a procedure for constructing bootstrap conﬁdence
intervals for δ. We describe the percentile method because of its simplicity and
because no estimate of f  is required.
The discussion thus far has been of index models, but all the points that have
been made apply also to the partial linear index models discussed in Section 7.2.
The bootstrap procedure described in Table 8.7 can readily be extended to this
case.
Figure 8.2 illustrates the application of the procedure to estimation of the
equivalence scale for couples versus singles using the South African data set. If
Engel’s method is used, then the index model becomes y = f (log x − zδ) + ε,
where x is household expenditure, z is a dummy variable distinguishing couples
from singles, and δ is the log equivalence scale.5 The resulting estimate is
ˆ = exp(δ̂) = 1.65 with asymptotic standard error .064 (see Figure 7.1). The

95 percent asymptotic conﬁdence interval is given by [1.52, 1.78]. The bootstrap
conﬁdence interval is very similar at [1.55,1.80].

5

See Gozalo (1997) for an alternative analysis of equivalence scale estimation using the bootstrap.

170

Semiparametric Regression for the Applied Econometrician

Asymptotic conﬁdence interval:

Engel equivalence scale
[1.52,1.78]

General equivalence scale
[1.17, 2.13]

Bootstrap conﬁdence interval:

[1.55,1.80]

[1.43, 1.89]

5

6

Bootstrap sampling distribution of ∆ˆ

4
3
0

1

2

Bootstrap Density

Engel Equiv Scale
General Equiv Scale

1.4

1.6

1.8

2.0

Equivalence Scale

Figure 8.2. Equivalence scale estimation for singles versus couples: Asymptotic versus
bootstrap methods.

Next we use the theoretically more valid approach to constructing equivalence
scales embodied in the model
y = f (log x − zδ) + zη + ε.

(8.4.3)

In this case we need to estimate both δ and a vertical shift parameter η
ˆ remains at 1.65, but
(see Section 7.2 for further discussion). The estimate 
its standard error increases substantially to .24 (Chapter 7, Exercise 4). The
asymptotic conﬁdence interval is [1.17,2.13]. Note that the asymptotic approach
does not preclude conﬁdence intervals extending beyond the domain of search.
By comparison, the bootstrap interval is much tighter at [1.43,1.89].
Figure 8.2 summarizes these results and plots the bootstrap distributions of
the estimated equivalence scale using Engel’s method and the more general
approach using the partial linear index model.

Bootstrap Procedures

171

8.5 Exercises6
1. South African Food Share Data: For single individuals in this data set, perform the
following:
(a) Construct pointwise conﬁdence intervals for food share as a function of log of
total expenditure. Use the bootstrap procedure outlined in Table 8.2. (You may
use either the percentile or the percentile-t method.)
(b) Repeat using the wild bootstrap.
(c) Test the log-linearity of the preceding Engel curve by constructing critical values
using:
(i) the goodness-of-ﬁt test procedure outlined in Table 8.3 with order of differencing m = 10,
(ii) the residual regression test procedure outlined in Table 8.4.
(d) Repeat the residual regression test using the wild bootstrap to allow for the
possibility of heteroskedasticity.
(e) Repeat Parts (c) and (d) but this time testing whether food share is quadratic in
the log of total expenditure.
2. South African Food Share Data: Repeat Exercise 1 for childless couples and for
couples with one child.
3. Scale Economies in Electricity Distribution:
(a) Using the percentile-t bootstrap procedure outlined in Table 8.5, construct 95 percent two-sided conﬁdence intervals for the seven parameters in the parametric
portion of the “Semiparametric Cobb–Douglas” speciﬁcation in Table 8.6 (see
also Figure 4.5).
(b) Repeat Part (a) using the wild bootstrap.
(c) Outline a bootstrap procedure for performing tests of the general linear hypothesis Rβ = r in the partial linear model. (For the asymptotic test, see Section 3.6.)
Test the “Semiparametric Translog” speciﬁcation in Figure 4.5 against the “Semiparametric Cobb–Douglas.”
(d) Remove the estimated parametric effects and test a quadratic speciﬁcation for the
scale effect against the “Semiparametric Cobb–Douglas” by using the bootstrap
procedures outlined in Tables 8.3 and 8.4.
(e) Following the procedure outlined in Table 8.2, design a methodology for constructing conﬁdence intervals for the nonparametric portion of a partial linear
model. Why is your procedure valid? Use the procedure to construct a 95 percent conﬁdence interval for the scale effect in the Semiparametric Cobb–Douglas
speciﬁcation.
4. Equivalence Scales for Singles Versus Couples: This exercise applies bootstrap procedures to produce conﬁdence intervals for equivalence scales using the food share
data for South African singles and couples.
6

Data and sample programs for empirical exercises are available on the Web. See the Preface for
details.

172

Semiparametric Regression for the Applied Econometrician

(a) Simulate the sampling distribution of the estimated Engel equivalence scale using
the method in Table 8.7. Plot your results and calculate a 95 percent conﬁdence
interval. Compare your results to the asymptotic conﬁdence interval (see Chapter
7, Exercise 1).
(b) Adapt the bootstrap procedure in Table 8.7 so that it applies to the partial linear
index model in (8.4.3) (refer to Section 7.2). Simulate the sampling distribution of the estimated general equivalence scale. Plot your results and calculate a
95 percent conﬁdence interval. Compare your results to the asymptotic conﬁdence interval (see Chapter 7, Exercise 4.).
5. Equivalence Scales for Childless Couples Versus Couples with Children:
(a) Repeat Exercise 4 for childless couples versus couples with one child.
(b) Repeat Exercise 4 for childless couples versus couples with two children.

Appendix A – Mathematical
Preliminaries

Suppose an , n = 1, . . . , ∞ is a sequence of numbers. Then the sequence an is
of smaller order than the sequence n −r , written an = o(n −r ) if n r an converges
to zero. For example, if an = n −1/4 , then an = o(n −1/5 ) because n 1/5 · n −1/4 →
0. A sequence is o(1) if it converges to 0.
The sequence an is the same order as the sequence n −r , written an = O(n −r )
if n r an is a bounded sequence. For example, the sequence an = 7n −1/4 + 3n −1/5
= O(n −1/5 ) because n 1/5 an converges to 3 and hence is a bounded sequence.
A sequence is O(1) if it is bounded.
Now suppose an , n = 1, . . . , ∞ is a sequence of random variables. Then,
−r
an = o P (n
) if n r an converges in probability to zero. For example, let an =
n
ε̄n = 1/n i=1 εi , where εi are i.i.d. with mean zero and variance σε2 . Then
E(ε̄n ) = 0, Var (ε̄n ) = σε2 /n. Because the mean is 0 and the variance converges
to 0, the sequence ε̄n converges in probability to 0 and is o P (1). Furthermore,
for any r < 1/2, ε̄n = o P (n −r ) since Var (n r ε̄n ) = σε2 /n 1−2r converges to 0.
A sequence of random variables bn , n = 1, . . . , ∞ is bounded in probability
if, for any δ > 0, no matter how small, there exists a constant Bδ and a point in
the sequence n δ such that for all n > n δ , Prob [|bn | > Bδ ] < δ.
Write an = O P (1) if an is bounded in probability and an = O P (n −r ) if n r an
is bounded in probability. For example, suppose n r an converges to a random
variable with ﬁnite mean and variance, then an = O P (n −r ). Thus, using the
central limit theorem we ﬁnd that n 1/2 ε̄n converges to an N (0, σε2 ), in which
case ε̄n = O P (n −1/2 ) and n 1/2 ε̄n = O P (1).
Suppose yi = µ y + εi , where µ y is a constant, and deﬁne the sample mean
ȳn based on n observations. Then
 −1 
ȳn = O P (µ y + ε̄n ) = µ y + O P (ε̄n ) = O(1) + O P n /2 and
n /2 ( ȳn − µ y ) = n /2 ε̄n = O P (1).
1

1

Let λn be a sequence of real numbers converging to zero. In the main text,
λ has usually been used to represent the shrinking bandwidth in kernel regression
173

174

Semiparametric Regression for the Applied Econometrician

(although we have suppressed the n subscript). Typically, we consider
λn nsequences
of the form λn = n −r , where 0 < r < 1. Let an = 1/(λn n) i=1
εi be the
average of the ﬁrst λn n = n 1−r values of εi . For example, if n = 100, r = 1/5,
then we are averaging the ﬁrst 39.8 ∼
= 40 observations. Then,
E[an ] = 0
Hence,

and

Var [an ] = σε2 /λn n = σε2 /n 1−r .


 −1

−1 
an = O P (λn n) /2 = O P n /2(1−r ) .

Suppose now that we draw n observations from the uniform distribution on
[0,1]. Assume again that 0 < r < 1, in which case 0 < λn = n −r < 1. Then
the proportion of observations falling in an interval of width 2λn will be approximately 2λn and the number of observations in the same interval will be
about 2nλn .
If we draw n observations from the uniform distribution on the unit square
[0,1]2 then the proportion of observations falling in a square of dimension
2λn · 2λn will be approximately 4λ2n , and the number of observations in the
same square will be about 4nλ2n .

Appendix B – Proofs

Notation: If A, B are matrices of identical dimension, deﬁne [A 
B]ij = Aij Bij .
Lemma B.1: (a) Suppose the components of ϑ = (ϑ1 , . . . , ϑξ ) are
i.i.d. with Eϑi = 0, Var(ϑi ) = σϑ2 , Eϑi4 = ηϑ , and covariance matrix σϑ2 Iξ .
If A is a symmetric matrix, then E(ϑ  Aϑ) = σϑ2 trA and Var(ϑ  Aϑ) = (ηϑ −
3σϑ4 )trA  A + σϑ4 2trA A.
(b) Consider the heteroskedastic case in which Var(ϑi ) = σi2 , Eϑi4 = ηi , ϑ
has the diagonal covariance matrix , and η is the diagonal matrix with entries
ηi . Then E(ϑ  Aϑ) = trA and Var(ϑ  Aϑ) = tr(η  A  A − 32  A  A) +
2tr(AA). For results of this type see, for example, Schott (1997, p. 391,
Theorem 9.18), or they may be proved directly.
Lemma B.2: Suppose x has support the unit interval with density
bounded away from 0. Given n observations on x, reorder them so that they
are in increasing
order: x1 ≤ · · · ≤ xn . Then for any  positive and arbitrarily
close to 0, 1/n (xi − xi−1 )2 = O P (n −2(1−) ).
Proof: Partition the unit interval into n 1− subintervals and note that
the probability of an empty subinterval goes to zero as n increases. The maximum distance between observations in adjacent subintervals is 2/n 1− , and the
maximum distance between observations within a subinterval is 1/n 1− , from
which the result follows immediately. 
Comment on
Lemma B.2: Because  may be chosen arbitrarily close
to zero,
we
write
1/n
(xi − xi−1 )2 ∼
= O P (n −2 ). Note also that for ﬁxed j,

2 ∼
−2
1/n (xi − xi− j ) = O P (n ). For an 
arbitrary collection of points in the unit
interval, the maximum value that 1/n (xi − xi−1 )2 can take is 1/n, which
occurs when all observations are at one of the two endpoints of the interval.
175

176

Semiparametric Regression for the Applied Econometrician

Lemma B.3: Suppose (xi , εi ), i = 1, . . . , n are i.i.d. The xi have
density bounded away from zero on the unit interval, and εi | xi ∼ (0, σε2 ).
Assume data have been reordered so that x1 ≤ · · · ≤ xn . Deﬁne f (x) =
( f (x1 ), . . . , f (xn )) , where the function f has a bounded ﬁrst derivative. Let
D be a differencing matrix of say order m. Then f (x) D  D f (x) = O P (n −1+ )
and Var( f (x) D  Dε) = O P (n −1+ ), where  is positive and arbitrarily close
to 0.
Proof: The result follows immediately from Yatchew (1997, Appendix, Equations (A.2) and (A.3)). 
Lemma B.4: Suppose (yi , xi , z i , wi ), i = 1, . . . , n are i.i.d., where
y and x are scalars and z and w are p- and q-dimensional row vectors, respectively. Suppose the data have been reordered so that x1 ≤ · · · ≤ xn . Let
Z be the n × p matrix of observations on z and W the n × q matrix of
observations on w. Suppose E(z | x) and E(w | x) are smooth vector functions of x having ﬁrst derivatives bounded. Let z|x = E x Var(z | x), w|x =
E x Var(w | x), and zw|x = E x Cov(z, w | x), where Cov(z, w | x) is the p × q
matrix of covariances between the z and w variables conditional on x. Let
d0 , d1 , . . . , dm be differencing weights satisfying constraints (4.1.1), deﬁne δ
using (4.1.6), and let D be the corresponding differencing matrix as in (4.1.2).
Then
Z  D D Z P
→ z|x
n
W  D  DW P
→ w|x
n
Z  D  DW P
→ zw|x
n

Z  D D D D Z P
→ (1 + 2δ)z|x
n
W  D  D D  DW P
→ (1 + 2δ)w|x
n
Z  D  D D  DW P
→ (1 + 2δ)zw|x .
n

Proof: Because z has a smooth regression function on x, write
z i = g(xi ) + u i , where g is a vector function with ﬁrst derivatives bounded,
E(u i | xi ) = 0 , and E(Var(z i | xi )) = z|x . Let g(x) be the n × p matrix with
ith row g(xi ). Let U be the n × p matrix with ith row u i . Then
Z  D D Z
U  D  DU
g(x) D  Dg(x) g(x) D  DU
U  D  Dg(x)
=
+
+
+
n
n
n
n
n


 
U D DU
1
∼
.
+ OP
=
n
n 3/2

Proofs

177

The second line uses Lemma B.3. Using (4.1.4), write
U  D  DU . U  L 0 U 
U  (L 1 + L 1 )U
d j d j+1
=
+
n
n
n
j=0
m−1

+ · · · + d0 dm

U  (L m + L m )U
n

and note that all terms but the ﬁrst on the right-hand side converge to zero maP
P
trices. Thus, U  DD  U /n → z|x and Z  DD  Z /n → z|x . Because the diagonal entries of D  DD  D are 1 + 2δ, we may use similar arguments to show that
P
P
U  D  DD  DU /n → (1 + 2δ)z|x and that Z  D  DD  DZ /n → (1 + 2δ)z|x .
Convergence of other quantities in the statement of the lemma may be proved
by analogous reasoning. 
Comments on Lemma B.4: More generally, suppose (yi , xi , h(z i )),
i = 1, . . . , n are i.i.d., where h is a p-dimensional vector function such that
E(h(z) | x) has the ﬁrst derivative bounded. Deﬁne h(z)|x to be the p × p
conditional covariance matrix of h(z) given x. Let h(Z ) be the n × p matrix
whose ith row is h(z i ). Then
h(Z ) D  Dh(Z ) P
→ h(z)|x
n

h(Z )D DD Dh(Z ) P
→ (1 + 2δ)h(z)|x .
n

Proof of Proposition 4.2.1: For the mean and variance use (4.2.6)
2
and (4.2.7). From (4.1.4) note that sdiff
has a band structure and thus a ﬁnitely
dependent central limit theorem may be applied (see e.g., Serﬂing 1980). 
Proof of Proposition 4.2.2: Use (4.2.13) to conclude that


 m

n−m
m−1
m



1
P
di4 + 6σε4 
di2
d 2j ,
(d0 yi + · · · + dm yi+m )4 → ηε
n i=1
i=0
i=0
j=i+1
from which the result follows immediately.



Proof of Proposition 4.3.1: If optimal differencing coefﬁcients are
used, then in large samples,

1  2
2
n /2 sres
− sdiff
 

1
1
1/2
2
2
∼
εi −
(d0 εi + d1 εi+1 + · · · + dm εi+m )
=n
n
n

178

Semiparametric Regression for the Applied Econometrician





m−1
m−2



2
.
1
= − n /2 
d j d j+1 
d j d j+2 
εi εi+1 + 
n
j=0
j=0

2
2
×
εi εi+2 + · · · + d0 dm
εi εi+m 
n
n


n 1/2 1 
1
1
=+
εi εi+1 +
εi εi+2 + · · · +
εi εi+m ,
m
n
n
n
which is asymptotically N (0, σε4 /m). To
obtain the third line, use the condition

m 2
d
=
1.
To
obtain
the
fourth,
use
j d j d j+k = −1/2m, k = 1, . . . , m.
0 j
For arbitrary differencing coefﬁcients, use (4.1.6). 
Proof of Propositions 4.4.1 and 4.4.2: See Yatchew (1999).



Proof of Proposition 4.5.1: Deﬁne g(x) and U as in the proof of
Lemma B.4. Using Lemma B.3, note that differencing removes both f (x), the
direct effect of x, and g(x), the indirect effect of x, sufﬁciently quickly that we
have the following approximation:
  

Z D D Z −1 Z  D  Dε
1/2
∼
n (β̂ − β) =
n
n 1/2
  
−1    
U D DU
U D Dε
∼
.
(B4.5.1)
=
n
n 1/2
Using Lemma B.4 and δ = 1/4m (see (4.2.8) and Appendix C), note that with
optimal differencing coefﬁcients
   

  


U D Dε
1
U D D D  DU P 2
2
Var
= σε E
→ σε 1 +
z|x
n
2m
n 1/2
P

−1
and (U  D D  U/n)−1 → z|x
. Thus,


 P
1
1 
−1
Var n /2 β̂ − β → σε2 1 +
.
z|x
2m

Use (4.2.9) to write
U  D  Dε . U  L 0 ε
1
=
−
1/2
1/2
2m
n
n




U (L m + L m )ε
U (L 1 + L 1 )ε
,
+ · ··· · +
n 1/2
n 1/2

and conclude asymptotic normality.
For arbitrary differencing coefﬁcients, use (4.1.6).



Proofs

179

Proof of Proposition 4.5.2: Using Proposition 4.5.1, we have under
the null hypothesis
−1
A
1
n /2 (R β̂ diff − r ) ∼ N 0, (1 + 2δ)σε2 R
R ,
z|x

from which (4.5.7) follows immediately.
To derive (4.5.7a) we will rely on the analogous analysis for conventional linear models. Thus, rewrite (4.5.2) to be approximately y ∗ = Z ∗ β + ε ∗ . Deﬁne
β̂ unr to be the OLS estimator applied to this model (this is just the usual differencing estimator in (4.5.3)). Deﬁne β̂ res to be the restricted OLS estimator.
Then




β̂ res − β̂ unr = −(Z ∗ Z ∗ )−1 R  [R(Z ∗ Z ∗ )−1 R  ]−1 (R β̂ unr − r ).
Next write the difference of the residual sums of squares from these two regressions as






∗ ∗
∗
∗
ε̂res
ε̂res − ε̂unr
ε̂unr
= (β̂ res − β̂ unr ) (Z ∗ Z ∗ )(β̂ res − β̂ unr ).

See for example, Greene (2000, pp. 281–3). Combining these two results yields






∗ ∗
∗
∗
ε̂res
ε̂res − ε̂unr
ε̂unr
= (R β̂ unr − r ) [R(Z ∗ Z ∗ )−1 R  ]−1 (R β̂ unr − r ).
2
2
Divide both sides by sunr
(1 + 2δ) = sdiff
(1 + 2δ) to obtain

 2

2
n sres
− sdiff
2
sdiff
(1 + 2δ)

= (R β̂ − r )

  −1
ˆ
R
R
(R β̂ − r )
β

2
and recall that sres
is the restricted differencing estimator.



Proof of Proposition 4.7.1: Consistency and asymptotic normality
may be shown using standard proofs for nonlinear least squares. To derive
the large sample covariance matrix, proceed as follows. Take the ﬁrst-order
conditions for β
1 ∂r (Z , β̂) 
D D(y − r (Z , β̂)) = 0;
n
∂β
then, expand in a ﬁrst-order Taylor series. Note that terms involving the second
derivatives of r (·) converge to zero. Thus one obtains
1 ∂r (Z , β) 
1 ∂r (Z , β)  ∂r (Z , β)
D Dε −
DD
(β̂ diffnls − β) ∼
= 0.
n
∂β
n
∂β
∂β 

180

Semiparametric Regression for the Applied Econometrician

Refer to Comments on Lemma B.4 and set h(z) = ∂r (z, β)/∂β to conclude
1 ∂r (Z , β)  ∂r (Z , β) P 
→
DD
∂r
n
∂β
∂β 
∂β |x

1 ∂r (Z , β) 
∂r (Z , β) P
→
(1
+
2δ)
.
D DD  D
∂r
n
∂β
∂β 
∂β |x
The convergence is retained if we replace β with β̂ diffnls . Thus, we may write
 −1 ∂r (Z ,β) D  Dε
1 
n /2 β̂ diffnls − β ∼
·
.
=
∂r
∂β
n 1/2
∂β |x
Next, note that




∂r (Z ,β) D  Dε
1 ∂r (Z , β) 
2
 ∂r (Z , β)
Var
= σε E
D DD D
∂β
n
∂β
∂β 
n 1/2

P
→(1 + 2δ)
,
∂r
∂β

and thus

|x

−1
 1 
 P
Var n /2 β̂ nls − β → (1 + 2δ)
.
∂r
∂β

|x

Asymptotic normality follows straightforwardly.



Proof of Theorem 4.8.1: Deﬁne g(x) and U as in the proof of
Lemma B.4 above. Because w, the vector of instruments, has a smooth regression function on x, write wi = h(xi ) + vi , where h is a vector function with
ﬁrst derivatives bounded, E(vi | xi ) = 0, and E(Var(wi | xi )) = w|x . Let W
be the n × q matrix with ith row wi . Let h(x) be the n × q matrix with ith row
h(xi ). Let V be the n × q matrix with ith row vi . Using Lemma B.3, note that
differencing removes f (x), g(x), and h(x) sufﬁciently quickly that we have
the following approximation:
n /2 (β̂ diff2sls − β)
  



Z D DW W  D  DW −1 W  D  D Z −1
∼
=
n
n
n


−1
W  D  Dε
Z  D  DW W  D  DW
×
1/
n
n
n 2
  
  
−1  

V D DU −1
U D DV V D DV
∼
=
n
n
n
  
−1  
 
U D DV V D DV
V D Dε
×
.
1/
n
n
n 2
1

(B4.8.1)

Proofs

181

Using Lemma B.4
   

  
V D Dε
V D D D  DV P 2
2
Var
=
σ
E
→ σε (1 + 2δ)w|x
ε
n
n 1/2
P

V  D  DV /n → w|x
P

U  D  DV /n → zw|x .
Thus, after simpliﬁcation,
 1 
 P

−1
−1 
Var n /2 β̂ diff2sls − β → σε2 (1 + 2δ) zw|x w|x
zw|x
.
Finally, expand

V  D  Dε
n 1/2

and conclude asymptotic normality.



Proof of Proposition 4.8.2: Note that

1 
Var n /2 β̂ diff − β̂ diff2sls
= nVar (β̂ diff ) + nVar (β̂ diff2sls ) − 2nCov (β̂ diff , β̂ diff2sls ).
Using Propositions 4.5.1 and 4.8.1, we know that the ﬁrst term converges to
−1
−1 
σε2 (1 + 2δ)z|x
and the second to σε2 (1 + 2δ)[zw|x w|x
zw|x ]−1 . We need
to establish the limit of the third term. Using (B4.5.1) and (B4.8.1) and Lemma
B.4, we have


1
1
nCov(β̂ diff , β̂ diff2sls ) ∼
= E n /2 (β̂ diff − β)n /2 (β̂ diff2sls − β)
∼
=E



Z  D D Z
n

−1

Z  D  Dε ε  D  DW
n 1/2
n 1/2



W  D  DW
n

−1


−1 


W  D  D Z Z  D  DW W  D  DW −1 W  D  D Z

×
n
n
n
n




Z  D  D Z −1 Z  D  D  D  DW W  D  DW −1
2
= σε E
n
n
n
×





W D DZ
n







Z D DW
n







W D DW
n

−1

−1 
W D DZ

n






−1
P
−1
−1 
−1 
zw|x w|x
→ σε2 (1 + 2δ)z|x
zw|x zw|x w|x
zw|x
−1
= σε2 (1 + 2δ)z|x
.

182

Semiparametric Regression for the Applied Econometrician

Hence,
−1

 1
 P

−1 
−1
,
Var n /2 (β̂ diff − β̂ diff2sls ) → σε2 (1 + 2δ) zw|x w|x
zw|x
− z|x
and the result follows immediately.



Proof of Proposition 5.3.1: There are n 1− subcubes, and note that
the probability of an empty subcube goes to zero as n increases. The maximum
segment within each q-dimensional subcube is proportional to 1/n (1−)/q as is
the maximum segment between points in contiguous subcubes from which the
result follows immediately. 

Appendix C – Optimal Differencing
Weights

Proposition C1: Deﬁne δ as in Equation (4.1.6) and consider the
optimization problem

2
m
m−k
m
m





min δ =
d j d j+k  s.t.
dj = 0
d 2j = 1;
d0 ,d1 ,...,dm

k=1

j=0

j=0

j=0

(C1.1)
then,
m−k


d j d j+k = −

j=0

1
2m

k = 1, . . , m,

(C1.2)

in which case δ = 1/4m. Furthermore, d0 , d1 , . . . , dm may be chosen so that
the roots of
d0 z m + d1 z m−1 + · · · + dm−1 z 1 + dm = 0

(C1.3)

lie on or outside the unit circle.
Proof of Proposition C1: For purposes of interpretation it will be
convenient to think of differencing of residuals as a moving average process.
Deﬁne
εi∗ = d0 εi + · · · + dm εi+m
and
ρk =

m−k


d j d j+k

k = 0, . . . , m.

j=0

183

184

Semiparametric Regression for the Applied Econometrician


∗
Note that ρk = corr(εi∗ , εi+k
). For k = 0, we have ρ0 = mj=0 d 2j = 1. Next
we have

2


m
m
m
m−k
m






0=
dj =
d 2j + 2
d j d j+k  = 1 + 2
ρk ,
j=0

which implies

m


j=0

k=1

j=0

k=1

ρk = − 1/2. Thus (C1.1) may be written as

k=1

min

ρ1 ,...,ρm

m

k=1

ρk2

s.t.

m


ρk = − 1/2 ,

k=1

which is minimized when the ρk are equal to each other, in which case ρk =
−1/2m, k = 1, . . . , m. Thus we have proved (C1.2).
As Hall et al. (1990) point out, the objective is to select moving average
weights that reproduce the covariance structure
cov (d0 εi + · · · + dm εi+m , d0 εi+k + · · · + dm εi+m+k )
1 2
=−
σ
k = 1, . . . , m
2m ε
=0
k >m

(C1.4)

var (d0 εi + · · · + dm εi+m ) = σε2 .
This can be achieved by solving
1 2m
[z + z 2m−1 + · · · + z m+1 − 2mz m + z m−1 + · · · + z + 1] = 0.
2m
It is easy to show that “1” is a root of R(z) with multiplicity 2. Furthermore,
the polynomial is “self-reciprocal” (see, e.g., Anderson 1971, p. 224, and
Barbeau 1995, pp. 22–23, 152) so that if r√= (a + bı) is a root, then so is
1/r = (a − bı)/(a 2 + b2 ), where ı denotes −1.
Thus, the set of all roots is given by  = {1, r2 , . . . , rm , 1, 1/r2 , . . . , 1/rm },
where |r j | = |a j + b j ı| = (a 2j + b2j )1/2 > 1, j = 2, . . . , m. A self-reciprocal
polynomial may be rewritten in the form R(z) = z m M(z)M(1/z), where M
is a polynomial with real coefﬁcients. There are, however, multiple ways to
construct M. In particular, obtain any partition of the roots  = S∪S c satisfying
the following conditions: if s ∈ S, then 1/s ∈ S c ; if in addition s is complex
and s ∈ S, then s̄ ∈ S. Compose M using the roots in S and normalize the
coefﬁcients of M so that their sum of squares equals 1. Then, by construction,
the coefﬁcients reproduce the covariance structure in (C1.4) and are therefore
optimal differencing weights. Valid partitioning requires only that reciprocal
pairs be separated (so that z m M(z)M(1/z) = R(z)) and that conjugate pairs be
R(z) = −

Optimal Differencing Weights

185

kept together (to ensure that M has real coefﬁcients). Of course, there is only
one partition that separates the two unit roots and those that are respectively
inside and outside the unit circle. 
Comment: For example, if m = 4, then the roots of R(z) are given by
 = {1, r2 = −.2137 − 1.7979ı, r3 = −.2137 + 1.7976ı,
r4 = −1.9219, 1, 1/r2 , 1/r3 , 1/r4 }.
Note that r2 , r3 , r4 lie outside the unit circle. If one takes S = {1, r2 , r3 , r4 },
then the differencing weights are (0.8873, −0.3099, −0.2464, −0.1901,
−0.1409). Table 4.1 in Chapter 4 tabulates differencing weights up to order 10,
where S consists of the root “1” and all roots outside the unit circle. Note that
the “spike” occurs at d0 whether m is even or odd. The remaining weights
d1 , . . . , dm are negative and monotonically increasing to 0. (Order or sign reversal preserves optimality of a sequence.) The pattern persists for all values
of m.
In contrast, if one takes S = {1, 1/r2 , 1/r3 , r4 } then the differencing weights
become (0.2708, −0.0142, 0.6909, −0.4858, −0.4617), which are those obtained by Hall et al. (1990, p. 523, Table 1).
Differencing Coefﬁcients: m = 25
0.97873,-0.06128,-0.05915,-0.05705,-0.05500,-0.05298,-0.05100,
-0.04906,-0.04715,-0.04528,-0.04345,-0.04166,-0.03990,-0.03818,
-0.03650,-0.03486,-0.03325,-0.03168,-0.03015,-0.02865,-0.02719,
-0.02577,-0.02438,-0.02303,-0.02171,-0.02043

Differencing Coefﬁcients: m = 50
0.98918,-0.03132,-0.03077,-0.03023,-0.02969,-0.02916,-0.02863,
-0.02811,-0.02759,-0.02708,-0.02657,-0.02606,-0.02556,-0.02507,
-0.02458,-0.02409,-0.02361,-0.02314,-0.02266,-0.02220,-0.02174,
-0.02128,-0.02083,-0.02038,-0.01994,-0.01950,-0.01907,-0.01864,
-0.01822,-0.01780,-0.01739,-0.01698,-0.01658,-0.01618,-0.01578,
-0.01539,-0.01501,-0.01463,-0.01425,-0.01388,-0.01352,-0.01316,
-0.01280,-0.01245,-0.01210,-0.01176,-0.01142,-0.01108,-0.01075,
-0.01043,-0.01011

Differencing Coefﬁcients: m = 100
0.99454083,-0.01583636,-0.01569757,-0.01555936,-0.01542178,
-0.01528478,-0.01514841,-0.01501262,-0.01487745,-0.01474289,
-0.01460892,-0.01447556,-0.01434282,-0.01421067,-0.01407914,

186

Semiparametric Regression for the Applied Econometrician

-0.01394819,-0.01381786,-0.01368816,-0.01355903,-0.01343053,
-0.01330264,-0.01317535,-0.01304868,-0.01292260,-0.01279714,
-0.01267228,-0.01254803,-0.01242439,-0.01230136,-0.01217894,
-0.01205713,-0.01193592,-0.01181533,-0.01169534,-0.01157596,
-0.01145719,-0.01133903,-0.01122148,-0.01110453,-0.01098819,
-0.01087247,-0.01075735,-0.01064283,-0.01052892,-0.01041563,
-0.01030293,-0.01019085,-0.01007937,-0.00996850,-0.00985823,
-0.00974857,-0.00963952,-0.00953107,-0.00942322,-0.00931598,
-0.00920935,-0.00910332,-0.00899789,-0.00889306,-0.00878884,
-0.00868522,-0.00858220,-0.00847978,-0.00837797,-0.00827675,
-0.00817614,-0.00807612,-0.00797670,-0.00787788,-0.00777966,
-0.00768203,-0.00758500,-0.00748857,-0.00739273,-0.00729749,
-0.00720284,-0.00710878,-0.00701532,-0.00692245,-0.00683017,
-0.00673848,-0.00664738,-0.00655687,-0.00646694,-0.00637761,
-0.00628886,-0.00620070,-0.00611312,-0.00602612,-0.00593971,
-0.00585389,-0.00576864,-0.00568397,-0.00559989,-0.00551638,
-0.00543345,-0.00535110,-0.00526933,-0.00518813,-0.00510750,
-0.00502745

Appendix D – Nonparametric
Least Squares

The results in this appendix are widely used in the spline function literature.
See particularly Wahba (1990). A collection of proofs may be found in Yatchew
and Bos (1997).
1. Sobolev Space Results
Let N be the nonnegative natural numbers. Let Q q ⊂ Rq be the unit cube,
which will be the domain of the nonparametric regression models below. (The
estimators remain valid if the domain is a rectangular cube.) Suppose α =
(α1 , . . . , αq ) ∈ Nq , deﬁne |α|∞ = max|αi |, and let x = (x1 , . . . , xq ) ∈ Rq . We
use the following conventional derivative notation D α f (x) = ∂ α1 + ··· + αq f (x)/
α
∂ x1α1 · · · ∂ xq q .
Let C m be the space of m-times continuously differentiable scalar functions, that is, C m = { f :Q q → R1 |D α f ∈ C 0 , |α|∞ ≤ m} and C 0 = { f :Q q →
R1 | f continuous on Q q }. On the space C m , deﬁne the norm,  f ∞,m =

α
m
|α|∞ ≤m maxx∈Q q |D f (x)|, in which case C is a complete, normed, linear
space, that is, a Banach space. Consider the following inner product of scalar
functions and the induced norm
! f, g" Sob =


|α|∞ ≤m


 f  Sob = 

D α f D αg
Qq

1/2



|α|∞ ≤m

[D α f ]2 
Qq

and deﬁne the Sobolev space Hm as the completion of { f ∈ C m } with respect to
 f  Sob . The following results on the Sobolev space Hm are particularly useful.
Proposition D.1: Hm is a Hilbert space.
187

188

Semiparametric Regression for the Applied Econometrician

The Hilbert space property allows one to take projections and to express Hm
as a direct sum of subspaces that are orthogonal to one another.
Proposition D.2: Given a ∈ Q q and b ∈ Nq , |b|∞ ≤ m − 1, there
exists a function rab ∈ Hm called!
a representor s.t. !rab , f " Sob = D b f (a) for all
q
m
b
f ∈ H . Furthermore, ra (x) = i=1 rabii (xi ) for all x ∈ Q q , where rabii (.) is the
representor in the Sobolev
of functions of one variable on Q 1 with inner
m space
d αf d αg
product ! f,g" Sob = α=0 Q 1 d x α d x α .
If b equals the zero vector, then we have representors of function evaluation, which we have denoted in the text as ra = ra0 . Proposition D.2 further
assures us of the existence of representors for derivative evaluation (of order
|b|∞ ≤ m − 1). The problem of solving for representors is well known (see
Wahba 1990). For the inner product above, representors of function evaluation
consist of two functions spliced together, each of which is a linear combination
of trigonometric functions. Formulas may be derived using elementary methods, in particular integration by parts and the solution of a linear differential
equation. Details may be found in Section 3 in this appendix. Finally, Proposition D.2 states that representors in spaces of functions of several variables
may be written as products of representors in spaces of functions of one variable. This particularly facilitates their implementation, for one simply calculates
one-dimensional representors and then multiplies them together.
Proposition D.3: The embedding Hm → C m−1 is compact.
Compactness of the embedding means that, given a ball of functions in
Hm (with respect to  f  Sob ), its closure is compact in C m−1 (with respect
to  f ∞,m ). This result ensures that functions in a bounded ball in Hm have all
lower-order derivatives bounded in supnorm.
Proposition D.4: Divide x into two subsets x = (xa , xb ). If f (xa , xb )
is of the form f a (xa ) + f b (xb ) and either f a = 0 or f b = 0, then  f 2Sob =
 f a 2Sob +  f b 2Sob .
This result is useful for analyzing additively separable models.
2. Nonparametric Least Squares
Computation of Estimator
Given data (y1 , x1 ), . . . , (yn , xn ) on a nonparametric regression model y =
f (x) + υ (x can be a vector). Let r x1 , . . . , r xn be the representors of function evaluation at x1 , . . . , xn , respectively, that is, !r x1, f " Sob = f (xi ) for all

Nonparametric Least Squares

189

f ∈ Hm . Let R be the n × n representor matrix whose columns (and rows)
equal the representors evaluated at x1 , . . . , xn ; that is, Rij = !r xi , r x j " Sob =
r xi (x j ) = r x j (xi ).
Proposition D.5: Let y = (y1 , . . . , yn ) and deﬁne
σ̂ 2 = min

1
[yi − f (xi )]2
n i

s 2 = min

1
[y − Rc] [y − Rc]
n

f

c

s.t.  f 2Sob ≤ L
s.t. c Rc ≤ L ,

where c is an n × 1 vector. Then σ̂ 2 = s 2 . Furthermore, there
n exists a solution
to the inﬁnite dimensional problem of the form fˆ =
1 ĉi r xi , where ĉ =
(ĉ1 , . . . , ĉn ) solves the ﬁnite dimensional problem.
This result ensures the computability of the estimator; fˆ can be expressed as
a linear combination of the representors with the number of terms equal to the
number of observations. Perfect ﬁt is precluded, except by extraordinary coincidence, since the coefﬁcients must satisfy the quadratic smoothness constraint.
Additive Separability
Partition x = (xa , xb ) with dimensions qa , qb , respectively, and x ∈ Q qa +qb =
[0, 1]qa +qb . Deﬁne
(
¯ = f (xa , xb ) ∈ Hm : f (xa , xb )
)
= f a (xa ) + f b (xb ),  f a + f b 2Sob ≤ L , f b = 0 ,
where the integral constraint is an identiﬁcation condition.
Proposition D.6: Given data (y1 , xa1 , xb1 ), . . . , (yn , xan , xbn ), let
y = (y1 , . . . , yn ) and deﬁne
σ̂ 2 = min
fa , fb

1
[yi − f a (xai ) − f b (xbi )]2
n t

s.t.  f a + f b 2Sob ≤ L ,
fb = 0

1
s 2 = min [y − Ra ca − Rb cb ] [y − Ra ca − Rb cb ]
ca ,cb n
s.t. ca Ra ca + cb Rb cb ≤ L ,



cbi = 0,

i

where ca , cb are n × 1 vectors and Ra , Rb are the representor matrices on
[0,1]qa at xa1 , . . . , xan and on [0,1]qb at xb1 , . . . , xbn , respectively. Then

190

Semiparametric Regression for the Applied Econometrician

σ̂ 2 = s 2 . Furthermore, there exists a solution
to the inﬁnite dimensional prob
lem of the form fˆ a (xa ) + fˆ b (xb ) = n1 ĉai r xai (xa ) + ĉbi r xbi (xb ), where ĉa =
(ĉa1 , . . . , ĉan ) , ĉb = (ĉb1 , . . . , ĉbn ) solve the ﬁnite dimensional problem.
The sets of functions { f a (xa )} and { f b (xb )| f b = 0} are orthogonal in the
Sobolev space Hm on Q qa +qb . Thus, using the Hilbert space property of Hm
(Proposition D.1), it can be shown that a function f a + f b satisfying the inﬁnite
dimensional optimization problem has a unique representation as a sum of
functions from the two subspaces.
For extensions to multiplicative separability and homothetic demand, see
Yatchew and Bos (1997).
3. Calculation of Representors
1
(k)
Let ! f,g" Sob = 0 m
(x)g (k) (x)d x, where bracketed superscripts dek=0 f
note derivatives. We construct a function ra ∈ Hm [0,1] such that ! f,ra " Sob =
f (a) for all f ∈ Hm [0,1]. This representor of function evaluation ra will be
of the form
(
L a (x) 0 ≤ x ≤ a
ra (x) =
,
Ra (x) a ≤ x ≤ 1
where L a and Ra are both analytic functions. For ra of this form to be an element
of Hm [0,1], it sufﬁces that L a(k) (a) = Ra(k) (a), 0 ≤ k ≤ m − 1. Now write
f (a) = !ra , f " Sob
m
a

=
0

L a(k) (x) f (k) (x)d x +

k=0

m
1
a

Ra(k) (x) f (k) (x)d x.

k=0

We ask that this be true for all f ∈ H [0,1], but by density it sufﬁces to
demonstrate the result for all f ∈ C ∞ [0,1]. Hence, assume that f ∈ C ∞ [0,1].
Thus, integrating by parts, we have
* k−1
m
m
a
a



(k)
(k)
L a (x) f (x)d x =
(−1) j L a(k+ j) (x) f (k− j−1) (x)
m

k=0

0

k=0

j=0
a

+ (−1)k
0

=

k−1
m 


a
0

* m
a 

0

L a(2k) (x) f (x)d x

(−1) j L a(k+ j) (x) f (k− j−1) (x)

k=0 j=0

+

0

+

k=0

+
(−1)k L a(2k) (x) f (x)d x.

Nonparametric Least Squares

191

If we let i = k − j − 1 in the ﬁrst sum, this may be written as
m

k=0

a
0

L a(k) (x) f (k) (x)d x =

m 
k−1


(−1)k−i−1 L a(2k−1−i) (x) f (i) (x)

0

k=1 i=0

* m
a


+
0

=

+
(−1)k L a(2k) (x)

f (x)d x

k=0

m−1 
m


(−1)k−i−1 L a(2k−1−i) (x) f (i) (x)

* m
a


0

=

m−1


+
(−1)k L a(2k) (x)

(i)

(a)

i=0

−

m−1


a

+

*

f

−

m

k=0

m−1


(−1)

*
f (i) (a)

i=0
1
a

L a(2k−1−i) (a)

(0)

+

m


(−1)

k−i−1

k=i+1

(−1)

k

L a(2k−1−i) (0)

+

L a(2k) (x)

f (x)d x.

k=0

+

m


(−1)k−i−1 Ra(2k−1−i) (a)

k=i+1

m−1


+

k−i−1

Ra(k) (x) f (k) (x) d x may be written as

i=0

+

(i)

* m


0
1
a

+

m


k=i+1

i=0

Similarly,

f (x)d x

k=0

*

f

a
0

i=0 k=i+1

+

a

*

f (i) (1)
* m


m


+
(−1)k−i−1 Ra(2k−1−i) (1)

k=i+1

(−1)

k

+

Ra(2k) (x)

f (x)d x.

k=0

Thus, since f (x) is arbitrary, we require both L a and R a to be solutions of the
constant coefﬁcient differential equation
m

k=0

(−1)k u (2k) (x) = 0.

192

Semiparametric Regression for the Applied Econometrician

Boundary conditions are obtained by setting the coefﬁcients of f (i) (a), 1 ≤
i ≤ m − 1, f (i) (0), 0 ≤ i ≤ m − 1 and f (i) (1), 0 ≤ i ≤ m − 1 to zero and the
coefﬁcient of f (a) to 1. That is,
m


,
(−1)k−i−1 L a(2k−1−i) (a) − Ra(2k−1−i) (a) = 0

1≤i ≤m−1

k=i+1
m


(−1)k−i−1 L a(2k−1−i) (0) = 0

0≤i ≤m−1

k=i+1
m


(−1)k−i−1 Ra(2k−1−i) (1) = 0

0≤i ≤m−1

k=i+1
m


,
(−1)k−1 L a(2k−1) (a) − Ra(2k−1) (a) = 1.

k=1

Furthermore, for ra ∈ Hm [0,1], we require, L a(k) (a) = Ra(k) (a), 0 ≤ k ≤ m − 1.
This results in (m − 1) + m + m + 1 + m = 4m boundary conditions. The
general solution of the preceding differential equationis obtained by ﬁnding
m
k 2k
the roots of its characteristic polynomial Pm (λ) =
k=0 (−1) λ . This is
easily done by noting that (1 + λ2 )Pm (λ) = 1 + (−1)m λ2m+2 , and thus the
characteristic roots are given by λk = eiθk , λk = ±i, where

θk =


(2k + 1)π



 2m + 2

m even



2kπ


2m + 2

m odd

.


(Re(λk ))x
The general solution is given by the linear combination
k ak e
sin(Im(λk ))x, where the sum is taken over 2m linearly independent real solutions of the differential
equation above.


Let L a (x) = 2m
a
u k (x) and Ra (x) = 2m
k
k=1
k=1 bk u k (x), where the u k , 1 ≤
k ≤ 2m are 2m basis functions of the solution space of the differential equation.
To show that ra exists and is unique, we need only demonstrate that the boundary
conditions uniquely determine the ak and bk . Because we have 4k unknowns
(2m ak ’s and 2m bk ’s) and 4m boundary conditions, the boundary conditions
constitute a square 4m × 4m linear system in the ak ’s and bk ’s. Thus, it sufﬁces
to show that the only solution of the associated homogenous system is the zero
vector. Now suppose that L ah (x) and Rah (x) are the functions corresponding to

Nonparametric Least Squares

193

solutions of the homogeneous system (i.e., with the coefﬁcient of f (a) in the
boundary conditions set to 0 instead of 1). Then, by exactly the same integration
by parts, it follows that !rah , f " Sob = 0 for all f ∈ C ∞ [0,1]. Hence, rah , L ah (x),
and Rah (x) are all identically zero and thus, by the linear independence of the
u k (x), so are the ak ’s and bk ’s.

Appendix E – Variable Deﬁnitions

Engel Curves and Equivalence Scale Estimation
Source: Living Standards Measurement Survey, http://www.worldbank.org/
lsms/
ltexp
log(total monthly household expenditure)
FoodShr share of total expenditure on food
A
number of adults in the household
K
number of children in the household
Scale Economies in Electricity Distribution
Source: Ontario municipal distributors. See Yatchew (2000).
tc
log(total cost per customer)
cust
log (number of customers)
wage
log(wage of lineman)
pcap
log(accumulated gross investment/kilometers of distribution wire)
PUC
public utility commission dummy
kwh
log(kilowatt hour sales per customer)
life
log(remaining lifetime of ﬁxed assets)
lf
log(load factor)
kmwire
log(kilometers of distribution wire per customer)
Household Gasoline Consumption
Source: National Private Vehicle Use Survey, Statistics Canada. See Yatchew
and No (2001).
dist
log (distance traveled per month)
price
log (price of liter of gasoline)
income
log (annual household income)
drivers
log (number of licensed drivers in household)
hhsize
log (number of members of household)

194

Variable Deﬁnitions

youngsingle
age
retire
urban

dummy for singles up to age 35
log(age)
dummy for households where head is over 65
dummy for urban dwellers

Weather and Electricity Demand
Source: Ontario Hydro Corporation, 1997
elec
log of monthly electricity sales
temp
heating and cooling degree days relative to 68 ◦ F
relprice
log of ratio of price of electricity to the price of natural gas
gdp
log of Ontario gross GDP
Housing Prices
Source: Ph.D. Thesis, Michael Ho (1995)
saleprice
sale price of house
frplc
dummy for ﬁreplace(s)
grge
dummy for garage
lux
dummy for luxury appointments
avginc
average neighborhood income
dwhy
distance to highway
ltarea
area of lot
nrbed
number of bedrooms
usespc
usable space
x1 ,x2
location coordinates
Option Prices and State Price Densities
Source: Simulated data. See Yatchew and Härdle (2001)
x
strike price
y
option price
X
vector of distinct strike prices

195

196

References

Aı̈t-Sahalia, Y. and J. Duarte (2000), “Nonparametric Option Pricing Under Shape
Restrictions,” manuscript, Princeton University.
Aı̈t-Sahalia, Y., P. Bickel, and T. Stoker (2001), “Goodness-of-Fit Tests for Regression
Using Kernel Methods,” Journal of Econometrics, 105, 363–412.
Anderson, T. W. (1971), The Statistical Analysis of Time Series, New York: Wiley.
Andrews, D. W. K. (1994a), “Empirical Process Methods in Econometrics,” in R. Engle
and D. McFadden (eds.), The Handbook of Econometrics, Vol. IV, Amsterdam:
North Holland, 2247–2294.
Andrews, D. W. K. (1994b), “Asymptotics for Semiparametric Econometric Models via
Stochastic Equicontinuity,” Econometrica, 62, 43–72.
Andrews, D. (2000), “Inconsistency of the Bootstrap When a Parameter is on the Boundary of the Parameter Space,” Econometrica, 68, 399–405.
Andrews, D. W. K. and M. Buchinsky (2000), “A Three-Step Method for Choosing the
Number of Bootstrap Repetitions,” Econometrica, 68, 23–51.
Anglin, P. and R. Gencay (1996), “Semiparametric Estimation of a Hedonic Price Function,” Journal of Applied Econometrics, 11, 633–648.
Azzalini, A. and A. Bowman (1993), “On the Use of Nonparametric Regression for
Checking Linear Relationships,” Journal of the Royal Statistical Society, B, 55,
549–557.
Azzalini A., A. Bowman, and W. Härdle (1989), “On the Use of Nonparametric Regression for Model Checking,” Biometrika, 76, 1–11.
Baltagi, B., J. Hidalgo, and Q. Li (1996), “A Nonparametric Test for Poolability Using
Panel Data,” Journal of Econometrics, 75, 345–367.
Barbeau E. (1995), Polynomials, New York: Springer-Verlag.
Barlow, R. E., D. J. Bartholomew, J. M. Bremner, and H. D. Brunk (1972), Statistical
Inference Under Order Restrictions, New York: Wiley.
Barnard, G. A. (1963), Discussion of “The Spectral Analysis of Point Processes,” by
M. S. Bartlett, Journal of the Royal Statistical Society, B, 25, 294.
Barry, D. (1993), “Testing for Additivity of a Regression Function,” Annals of Statistics,
21, 235–254.
Beran, R. (1997), “Diagnosing Bootstrap Success,” Annals of the Institute of Statistical
Mathematics, 49, 1–24.
197

198

Semiparametric Regression for the Applied Econometrician

Beran, R. and G. Ducharme (1991), Asymptotic Theory for Bootstrap Methods in Statistics, Centre de Recherches Mathématiques, Univerisité de Montréal.
Bierens, H. (1990), “A Consistent Conditional Moment Test of Functional Form,” Econometrica, 58, 1443–1458.
Bierens, H. and Ploberger, W. (1997), “Asymptotic Theory of Integrated Conditional
Moments,” Econometrica, 65, 1129–1151.
Bickel, P., C. Klaasen, Y. Ritov and J. Wellner (1993), Efﬁcient and Adaptive Estimation
for Semiparametric Models, Baltimore: John Hopkins University Press.
Bickel, P. and J. Kwon (2001), “Inference for Semiparametric Models: Some Questions
and an Answer,” Statistica Sinica, 11, 863–960 (with discussion).
Blackorby, C. and D. Donaldson (1989), “Adult Equivalence Scales, Interpersonal Comparisons of Well-Being and Applied Welfare Economics,” Department of Economics
Discussion Paper 89-24, University of British Columbia.
Blackorby, C. and D. Donaldson (1993), “Adult Equivalence Scales and the Economic
Implementation of Interpersonal Comparisons of Well-Being,” Social Choice and
Applied Welfare, 10, 335–361.
Blackorby, C. and D. Donaldson (1994), “Measuring the Costs of Children: A Theoretical Framework,” in R. Blundell, I. Preston, and I. Walker (eds.), The Economics of
Household Behavior, Cambridge: Cambridge University Press, 51–69.
Blundell, R. and A. Duncan (1998), “Kernel Regression in Empirical Microeconomics,”
Journal of Human Resources, 33, 62–87.
Blundell, R., A. Duncan, and K. Pendakur (1998), “Semiparametric Estimation and
Consumer Demand,” Journal of Applied Econometrics, 13, 435–461.
Bowman, A. W., M. C. Jones, and I. Gijbels (1998), “Testing Monotonicity of Regression,” Journal of Computational and Graphical Statistics, 7, 489–500.
Brooke, A., D. Kendrick, and A. Meeraus (1992), GAMS, Redwood City, CA: Scientiﬁc
Press.
Buja, A., T. Hastie, and R. Tibshirani (1989), “Linear Smoothers and Additive Models,”
Annals of Statistics, 17, 453–555 (with discussion).
Carroll, R. J., J. Fan, I. Gijbels, and M. P. Wand (1997), “Generalized Partially Linear
Single Index Models,” Journal of the American Statistical Association, 92, 477–489.
Cavanagh, C. and R. Sherman (1998), “Rank Estimator for Monotonic Index Models,”
Journal of Econometrics, 84, 351–381.
Chambers, J. M. and T. Hastie (1993), Statistical Models in S, New York: Chapman and
Hall.
Chen, H. (1988), “Convergence Rates for Parametric Components in a Partly Linear
Model,” Annals of Statistics, 16, 136–146.
Citro, C. and R. T. Michael (eds.) (1995), Measuring Poverty – A New Approach,
Washington, DC: National Academy Press.
Clark, R. (1975), “A Calibration Curve for Radiocarbon Dates,” Antiquity, 49, 251–266.
Cleveland, W. (1979), “Robust Locally Weighted Regression and Smoothing Scatterplots,” Journal of the American Statistical Association, 74, 829–836.
Cleveland, W. and S. Devlin (1988), “Locally Weighted Regression: An Approach to
Regression Analysis by Local Fitting,” Journal of the American Statistical Association, 83, 596–610.

References

199

Cosslett, S. (1987), “Efﬁciency Bounds for Distribution-Free Estimators of the Binary
Choice and Censored Regression Models,” Econometrica, 55, 559–586.
Davison, A. and D. Hinkley (1997), Bootstrap Methods and Their Applications,
Cambridge: Cambridge University Press.
Deaton, A. (1997), The Analysis of Household Surveys: A Microeconomic Approach
to Development Policy, Baltimore: Johns Hopkins University Press for the World
Bank.
DeJong, P. (1987), “A Central Limit Theorem for Generalized Quadratic Forms,” Probability Theory and Related Fields, 75, 261–277.
Delgado, M. (1993), “Testing the Equality of Nonparametric Regression Curves,” Statistics and Probability Letters, 17, 199–204.
Delgado, M. and W. G. Manteiga (2001), “Signiﬁcance Testing in Nonparametric Regression Based on the Bootstrap,” Annals of Statistics, 29, 1469–1507.
Delgado, M. and P. M. Robinson (1992), “Nonparametric and Semiparametric Methods
for Economic Research,” Journal of Economic Surveys, 6, 201–250.
Derbort, S., H. Dette, and A. Munk (2002), “A Test for Additivity in Nonparametric
Regression,” Annals of the Institute of Statistical Mathematics, 54, 60–82.
Dette, H. (1999), “A Consistent Test for the Functional Form of a Regression Based on
a Difference of Variance Estimators,” Annals of Statistics, 27, 1012–1040.
Dette, H. and C. Von Lieres und Wilkau (2001), “Testing Additivity by Kernel-Based
Methods – What Is a Reasonable Test?” Bernoulli, 7, 669–697.
Diack, C. (2000), “Sur la Convergence des Tests de Schlee et de Yatchew,” Canadian
Journal of Statistics, 28, 653–668.
Diack, C. and C. Thomas-Agnan (1998), “A Nonparametric Test of the Non-Convexity
of Regression,” Nonparametric Statistics, 9, 335–362.
DiNardo, J. and J. Tobias (2001), “Nonparametric Density and Regression Estimation,”
Journal of Economic Perspectives, 15, 11–28.
Dudley, R. M. (1984), “A Course on Empirical Processes,” Lecture Notes in Mathematics, Ecole d‘Été de Probabilités de Saint-Flour XII-1982, New York: SpringerVerlag.
Efron, B. (1979), “Bootstrap Methods: Another Look at the Jackknife,” Annals of
Statistics, 7, 1–26.
Efron, B. and R. J. Tibshirani (1993), An Introduction to the Bootstrap, New York:
Chapman and Hall.
Ellison, G. and S. Ellison (2000), “A Simple Framework for Nonparametric Speciﬁcation
Testing,” Journal of Econometrics, 96, 1–23.
Engle, R., C. Granger, J. Rice, and A. Weiss (1986), “Semiparametric Estimates of the
Relation Between Weather and Electricity Sales,” Journal of the American Statistical
Association, 81, 310–320.
Epstein, L. and A. Yatchew (1985), “Nonparametric Hypothesis Testing Procedures and
Applications to Demand Analysis,” Journal of Econometrics, 30, 150–169.
Eubank, R. (1988), Spline Smoothing and Nonparametric Regression, New York: Marcel Dekker.
Eubank, R. and J. Hart (1992), “Testing Goodness-of-Fit in Regression via Order
Selection Criteria,” Annals of Statistics, 20, 1412–1425.

200

Semiparametric Regression for the Applied Econometrician

Eubank, R. and P. Speckman (1993), “Conﬁdence Bands in Nonparametric Regression,”
Journal of the American Statistical Association, 88, 1287–1301.
Eubank, R. and C. H. Spiegelman (1990), “Testing the Goodness of Fit of a Linear Model
via Nonparametric Regression Techniques,” Journal of the American Statistical
Association, 85, 387–392.
Eubank, R., J. Hart, D. Simpson, and L. Stefanski (1995), “Testing for Additivity in
Nonparametric Regression,” Annals of Statistics, 23, 1896–1920.
Fan, J. and I. Gijbels (1996), Local Polynomial Modelling and Its Applications,
New York: Chapman and Hall.
Fan, J. and L. Huang (2001), “Goodness-of-Fit Tests for Parametric Regression Models,”
Journal of the American Statistical Association, 96, 640–652.
Fan, J. Q. and S. K. Lin (1998), “Test of Signiﬁcance When Data Are Curves,” Journal
of the American Statistical Association, 93, 1007–1021.
Fan, Y. and Q. Li (1996), “Consistent Model Speciﬁcation Tests: Omitted Variables and
Semiparametric Functional Forms,” Econometrica, 64, 865–890.
Friedman, J. and R. Tibshirani (1984), “The Monotone Smoothing of Scatterplots,”
Technometrics, 26, 243–250.
Gallant, R. (1981), “Unbiased Determination of Production Technologies,” Journal of
Econometrics, 20, 285–323.
Gijbels, I., P. Hall, M. C. Jones, and I. Koch (2000), “Tests for Monotonicity of a
Regression Mean with Guaranteed Level,” Biometrika, 87, 663–673.
Goldman, S. and P. Ruud (1992), “Nonparametric Multivariate Regression Subject to
Monotonicity and Convexity Constraints,” manuscript, University of California,
Berkeley.
Golub, G. and C. Van Loan (1989), Matrix Computations, Baltimore: Johns Hopkins.
Gozalo, P. (1993), “A Consistent Speciﬁcation Test for Nonparametric Estimation of
Regression Function Models,” Econometric Theory, 9, 451–477.
Gozalo, P. (1997), “Nonparametric Bootstrap Analysis with Applications to Demographic Effects in Demand Functions,” Journal of Econometrics, 81, 357–393.
Gozalo, P. and O. Linton (2001), “Testing Additivity in Generalized Nonparametric
Regression Models,” Journal of Econometrics, 104, 1–48.
Green, P. and B. Silverman (1994), Nonparametric Regression and Generalized Linear
Models, London: Chapman and Hall.
Greene, W. (2000), Econometric Analysis, 4th ed, Englewood Cliffs, NJ: Prentice Hall.
Groeneboom, P., G. Jongbloed, and J. A. Wellner (2001), “Estimation of a Convex
Function: Characterizations and Asymptotic Theory,” Annals of Statistics, 29, 1653–
1698.
Hall, P. (1984), “Central Limit Theorem for Integrated Square Error of Multivariate
Nonparametric Density Estimators,” Journal of Multivariate Analysis, 14, 1–16.
Hall, P. (1992), The Bootstrap and Edgeworth Expansion, New York: Springer-Verlag.
Hall, P. (1994), “Methodology and Theory for the Bootstrap,” in R. Engle and
D. McFadden (eds.), The Handbook of Econometrics, Vol. IV, Amsterdam: North
Holland, 2342–2381.
Hall, P. and J. D. Hart (1990), “Bootstrap Test for Difference Between Means in Nonparametric Regression,” Journal of the American Statistical Association, 85, 1039–1049.

References

201

Hall, P. and N. Heckman (2000), “Testing for Monotonicity of a Regression Mean by
Calibrating for Linear Functions,” Annals of Statistics, 28, 20–39.
Hall, P. and L. Huang (2001), “Nonparametric Kernel Regression Subject to Monotonicity Constraints,” Annals of Statistics, 29, 624–647.
Hall, P. and A. Yatchew (2002), “Uniﬁed Approach to Testing Functional Hypotheses in
Semiparametric Contexts” unpublished manuscript, Australian National University,
School of Mathematical Sciences.
Hall, P., J. Kay, and D. Titterington (1990), “Asymptotically Optimal DifferenceBased Estimation of Variance in Nonparametric Regression,” Biometrika, 77, 521–
528.
Härdle, W. (1990), Applied Nonparametric Regression, Econometric Society Monograph Series, 19, Cambridge University Press.
Härdle, W. and O. Linton (1994), “Applied Nonparametric Methods,” in R. Engle and
D. McFadden (eds.), The Handbook of Econometrics, Vol. IV, Amsterdam: North
Holland, 2297–2334.
Härdle, W. and E. Mammen (1993), “Comparing Nonparametric versus Parametric
Regression Fits,” Annals of Statistics, 21, 1926–1947.
Härdle, W. and J. Marron (1985), “Optimal Bandwidth Selection in Nonparametric
Regression Estimation,” Annals of Statistics, 13, 1465–1481.
Härdle, W. and J. Marron (1990), “Semiparametric Comparison of Regression Curves,”
Annals of Statistics, 18, 63–89.
Härdle, W. and T. Stoker (1989), “Investigating Smooth Multiple Regression by the
Method of Average Derivatives,” Journal of the American Statistical Association,
84, 986–995.
Härdle, W. and A. Tsybakov (1993), “How Sensitive Are Average Derivatives?” Journal
of Econometrics, 58, 31–48.
Härdle, W., P. Hall, and J. Marron (1988), “How Far Are Automatically Chosen Regression Smoothing Parameters from Their Optimum?” Journal of the American
Statistical Association, 83, 86–99 (with discussion).
Härdle, W., P. Hall, and H. Ichimura (1993), “Optimal Smoothing in Single Index
Models,” Annals of Statistics, 21, 157–178.
Härdle, W., S. Klinke, and B. Turlach (1995), XploRe: An Interactive Statistical Computing Environment, New York: Springer-Verlag.
Härdle, W., H. Liang, and J. Gao (2000), Partially Linear Models, Heidelberg: PhysicaVerlag.
Hart, J. (1997), Nonparametric Smoothing and Lack-of-Fit Tests, New York: SpringerVerlag.
Hartigan, J. A. (1969), “Using Subsample Values as Typical Values,” Journal of the
American Statistical Association, 64, 1303–1317.
Hartigan, J. A. (1971), “Error Analysis by Replaced Samples,” Journal of the Royal
Statistical Society, B, 33, 98–110.
Hastie, T. and R. Tibshirani (1987), “Generalized Additive Models: Some Applications,”
Journal of the American Statistical Association, 82, 371–386.
Hastie, T. and R. Tibshirani (1990), Generalized Additive Models, London: Chapman
and Hall.

202

Semiparametric Regression for the Applied Econometrician

Hausman, J. (1978), “Speciﬁcation Tests in Econometrics,” Econometrica, 46, 1251–
1271.
Hausman, J. and W. Newey (1995), “Nonparametric Estimation of Exact Consumer
Surplus and Deadweight Loss,” Econometrica, 63, 1445–1476.
Ho, M. (1995), Essays on the Housing Market, unpublished Ph.D. dissertation, University of Toronto.
Hoeffding, W. (1948), “A Class of Statistics with Asymptotically Normal Distribution,”
Annals of Mathematical Statistics, 19, 293–325.
Holly, A. and D. Sargan (1982), “Testing for Endogeneity in a Limited Information Framework,” Cahiers de Recherches Economiques, No. 8204, Universite de
Lausanne.
Hong, Y. and H. White (1995), “Consistent Speciﬁcation Testing via Nonparametric
Series Regression,” Econometrica, 63, 1133–1160.
Horowitz, J. (1997), “Bootstrap Methods in Econometrics: Theory and Numerical Performance,” in D. M. Kreps and K. F. Wallis (eds.), Advances in Economics and
Econometrics: Theory and Applications, Seventh World Congress, Vol. 3, Ch. 7,
Cambridge University Press, V.
Horowitz, J. (1998), Semiparametric Methods in Econometrics, Berlin: Springer-Verlag.
Horowitz, J. (2001), “The Bootstrap,” in J. Heckman and E. Leamer (eds.), Handbook
of Econometrics, Vol. 5, Ch. 52, 3159–3228.
Horowitz, J. and W. Härdle (1994), “Testing a Parametric Model Against a Semiparametric Alternative,” Econometric Theory, 10, 821–848.
Horowitz, J. and W. Härdle (1996), “Direct Semiparametric Estimation of Single-Index
Models with Discrete Covariates,” Journal of the American Statistical Association,
91, 1632–1640.
Horowitz, J. and V. Spokoiny (2001), “An Adaptive, Rate-Optimal Test of a Parametric
Mean-Regression Model Against a Nonparametric Alternative,” Econometrica, 69,
599–631.
Hristache, M., A. Juditsky, and V. Spokoiny (2001), “Direct Estimation of the Index
Coefﬁcient in a Single Index Model,” Annals of Statistics, 29, 595–623.
Ichimura, H. (1993), “Semiparametric Least Squares (SLS) and Weighted SLS Estimation of Single-Index Models,” Journal of Econometrics, 58, 71–120.
Juditsky, A. and A. Nemirovski (2002), “On Nonparametric Tests of Positivity/ Monotonicity/ Convexity,” Annals of Statistics, 30, 498–527.
Kelly, C. and J. Rice (1990), “Monotone Smoothing with Application to Dose-Response
Curves and the Assessment of Synergism,” Biometrics, 46, 1071–1085.
King, E., J. D. Hart, and T. E. Wehrly (1991), “Testing the Equality of Regression Curves
Using Linear Smoothers,” Statistics and Probability Letters, 12, 239–247.
Klein, R. and R. Spady (1993), “An Efﬁcient Semiparametric Estimator for Binary
Response Models,” Econometrica, 61, 387–422.
Koul, H. L. and A. Schick (1997), “Testing for the Equality of Two Nonparametric
Regression Curves,” Journal of Statistical Planning and Inference, 65, 293–314.
Krause, A. and M. Olson (1997), The Basic of S and S-Plus, New York: Springer-Verlag.
Kulasekera, K. B. (1995), “Comparison of Regression Curves Using Quasi-Residuals,”
Journal of the American Statistical Association, 90, 1085–1093.

References

203

Kulasekera, K. B. and J. Wang (1997), “Smoothing Parameter Selection for Power
Optimality in Testing of Regression Curves,” Journal of the American Statistical
Association, 92, 500–511.
Lavergne, P. (2001), “An Equality Test Across Nonparametric Regressions,” Journal of
Econometrics, 103, 307–344.
Lavergne, P. and Q. Vuong (2000), “Nonparametric Signiﬁcance Testing,” Econometric
Theory, 16, 576–601.
Lee, A. J. (1990), U-Statistics: Theory and Practice, New York: Marcel Dekker.
Lee, B. J. (1991), “A Model Speciﬁcation Test Against the Nonparametric Alternative,”
manuscript, Department of Economics, University of Colorado.
LePage, R. and L. Billard (1992), Exploring the Limits of the Bootstrap, New York:
Wiley.
Lewbel, A. (1989), “Household Equivalence Scales and Welfare Comparisons,” Journal
of Public Economics, 39, 377–391.
Lewbel, A. (1995), “Consistent Nonparametric Hypothesis Tests with an Application to
Slutsky Symmetry,” Journal of Econometrics, 67, 379–401.
Lewbel, A. (1997), “Consumer Demand Systems and Household Equivalence Scales,” in
M. H. Pesaran and P. Schmidt (eds.), Handbook of Applied Econometrics, Volume II:
Microeconomics, Oxford: Blackwell Handbooks in Economics, 166–201.
Lewbel, A. (2000), “Semiparametric Qualitative Response Model Estimation with Unknown Hereoscedasticity or Instrumental Variables,” Journal of Econometrics, 97,
145–177.
Li, K. (1986), “Asymptotic Optimality of CL and Generalized Cross-Validation in Ridge
Regression with Application to Spline Smoothing,” Annals of Statistics, 14, 1101–
1112.
Li, K. (1987), “Asymptotic Optimality for CP , CL , Cross-Validation and Generalized
Cross-Validation: Discrete Index Set,” Annals of Statistics, 15, 958–975.
Li, Q. (1994), “Some Simple Consistent Tests for a Parametric Regression Function
versus Semiparametric or Nonparametric Alternatives,” manuscript, Department of
Economics, University of Guelph.
Li, Q. and S. Wang (1998), “A Simple Consistent Bootstrap Test for a Parametric
Regression Function,” Journal of Econometrics, 87, 145–165.
Linton, O. (1995a), “Estimation in Semiparametric Models: A Review,” in G. S.
Maddala, P. C. B. Phillips, and T. N. Srinivasan (eds.), Advances in Econometrics and Quantitative Economics, Essays in Honor of Professor C. R. Rao, Oxford:
Blackwell, 146–171.
Linton, O. (1995b), “Second-Order Approximation in the Partially Linear Regression
Model,” Econometrica, 63, 1079–1112.
Linton, O. (1997), “Efﬁcient Estimation of Additive Nonparametric Regression Models,”
Biometrika, 84, 469–474.
Linton, O. (2000), “Efﬁcient Estimation of Generalized Additive Nonparametric Regression Models,” Econometric Theory, 16, 502–523.
Linton, O. and J. Nielsen (1995), “A Kernel Method of Estimating Structured
Nonparametric Regression Based on Marginal Integration,” Biometrika, 82, 93–
100.

204

Semiparametric Regression for the Applied Econometrician

Linton, O., E. Mammen, and J. Nielsen (1999), “The Existence and Asymptotic Properties of a Backﬁtting Projection Algorithm under Weak Conditions,” Annals of
Statistics, 27, 1443–1490.
MacKinnon, J. (1992), “Model Speciﬁcation Tests and Artiﬁcial Regressions,” Journal
of Economic Literature, 30, 102–146.
Mammen, E. (1991), “Estimating a Smooth Monotone Regression Function,” Annals of
Statistics, 19, 724–740.
Mammen, E. (1992), When Does Bootstrap Work? New York: Springer-Verlag.
Mammen, E. and C. Thomas-Agnan (1999), “Smoothing Splines and Shape Restrictions,” Scandinavian Journal of Statistics, 26, 239–252.
Mammen, E. and S. Van de Geer (1997), “Penalized Quasi-Likelihood Estimation in
Partial Linear Models,” Annals of Statistics, 25, 1014–1035.
Matzkin, R. (1994), “Restrictions of Economic Theory in Nonparametric Methods,”
in R. Engle and D. McFadden (eds.), The Handbook of Econometrics, Vol. IV,
Amsterdam: North Holland, 2524–2559.
McLeish, D. L. (1974), “Dependent Central Limit Theorems and Invariance Principles,”
Annals of Statistics, 2, 620–628.
Mukarjee, H. (1988), “Monotone Nonparametric Regression,” Annals of Statistics, 16,
741–750.
Mukarjee, H. and S. Stern (1994), “Feasible Nonparametric Estimation of Multiargument Monotone Functions,” Journal of the American Statistical Association,
89, 77–80.
Munk, A. and H. Dette (1998), “Nonparametric Comparison of Several Regression Functions: Exact and Asymptotic Theory,” Annals of Statistics, 26, 2339–
2368.
Nadaraya, E. A. (1964), “On Estimating Regression,” Theory of Probability and Its
Applications, 10, 186–190.
Newey, W. (1994a), “The Asymptotic Variance of Semiparametric Estimators,” Econometrica, 62, 1349–1382.
Newey, W. (1994b), “Kernel Estimation of Partial Sums,” Econometric Theory, 10,
233–253.
Newey, W. and K. West (1987), “A Simple Positive Semi-deﬁnite Heteroskedasticity
and Autocorrelation-Consistent Covariance Matrix,” Econometrica, 55, 703–708.
Newey, W., J. Powell, and F. Vella (1999), “Nonparametric Estimation of Triangular
Simultaneous Equations Models,” Econometrica, 67, 565–603.
Pagan, A. and A. Ullah (1999), Nonparametric Econometrics, Cambridge: Cambridge
University Press.
Pendakur, K. (1999), “Semiparametric Estimates and Tests of Base-Independent Equivalence Scales,” Journal of Econometrics, 88, 1–40.
Phillips, P. C. B. (2001), “Bootstrapping Spurious Regression,” Cowles Foundation,
Yale University.
Pinkse, C. and P. Robinson (1995), “Pooling Nonparametric Estimates of Regression
Functions with Similar Shape,” in G. S. Maddala, P. C. B. Phillips, and T. N.
Srinivasan (eds.), Advances in Econometrics and Quantitative Economics, Oxford:
Blackwell, 172–197.

References

205

Pollard, D. (1984), Convergence of Stochastic Processes, New York: SpringerVerlag.
Powell, J. (1987), “Semiparametric Estimation of Bivariate Latent Variable Models,”
working paper 8704, Social Systems Research Institute of Wisconsin, University of
Wisconsin, Madison.
Powell, J. (1994), “Estimation of Semiparametric Models,” in R. Engle and
D. McFadden (eds.), The Handbook of Econometrics, Vol. IV, Amsterdam: North
Holland, 2444–2523.
Powell, J., J. Stock, and T. Stoker (1989), “Semiparametric Estimation of Index Coefﬁcients,” Econometrica, 57, 1403–1430.
Racine, J. (1997), “Consistent Signiﬁcance Testing for Nonparametric Regression,”
Journal of Business and Economic Statistics, 15, 369–378.
Ramsay, J. (1988), “Monotone Regression Splines in Action,” Statistical Science, 3,
425–461.
Ramsay, J. (1998), “Estimating Smooth Monotone Functions,” Journal of the Royal
Statistical Society, B, 60, 365–375.
Rice, J. (1984), “Bandwidth Choice for Nonparametric Regression,” Annals of Statistics,
12, 1215–1230.
Rice, J. (1986), “Convergence Rates for Partially Splined Models,” Statistics and
Probability Letters, 4, 203–208.
Rilstone, P. and A. Ullah (1989), “Nonparametric Estimation of Response Coefﬁcients,”
Communications in Statistics, Theory and Methods, 18, 2615–2627.
Robertson, T., F. Wright, and R. L. Dykstra (1988), Order Restricted Statistical Inference, New York: Wiley.
Robinson, P. (1988), “Root-N -Consistent Semiparametric Regression,” Econometrica,
56, 931–954.
Schlee, W. (1982), “Nonparametric Tests of the Monotony and Convexity of Regression,”
in B. V. Gnedenko, M. L. Puri, and I. Vincze (eds.), Nonparametric Statistical
Inference, Vol. 2, Amsterdam: North Holland, 823–836.
Schmalensee, R. and T. Stoker (1999), “Household Gasoline Demand in the United
States,” Econometrica, 67, 645–662.
Schott, J. R. (1997), Matrix Analysis for Statistics, New York: Wiley.
Scott, D. (1992), Multivariate Density Estimation, New York: Wiley.
Seifert, B., T. Gasser, and A. Wolf (1993), “Nonparametric Estimation of Residual
Variance Revisited,” Biometrika, 80, 373–383.
Serﬂing, R. (1980), Approximation Theorems of Mathematical Statistics, New York:
Wiley.
Shao, J. and D. Tu (1995), The Jackknife and Bootstrap, New York: Springer.
Silverman, B. W. (1986), Density Estimation for Statistics and Data Analysis,
New York: Chapman and Hall.
Simonoff, J. (1996), Smoothing Methods in Statistics, New York: Springer-Verlag.
Speckman, P. (1988), “Kernel Smoothing in Partial Linear Models,” Journal of the Royal
Statistical Society, B, 50, 413–436.
Sperlich, S., D. Tjostheim, and L. Yang (1999), “Nonparametric Estimation and Testing
of Interaction in Additive Models,” Econometric Theory, 18, 197–251.

206

Semiparametric Regression for the Applied Econometrician

Stengos, T. and Y. Sun (2001), “A Consistent Model Speciﬁcation Test for a Regression
Function Based on Nonparametric Wavelet Estimation,” Econometric Reviews, 20,
41–60.
Stoker, T. (1986), “Consistent Estimation of Scaled Coefﬁcients,” Econometrica, 54,
1461–1481.
Stoker, T. (1991), Lectures on Semiparametric Econometrics, CORE Foundation,
Louvain-La-Neuve.
Stone, C. (1980), “Optimal Rates of Convergence for Nonparametric Estimators,” Annals
of Statistics, 8, 1348–1360.
Stone, C. (1982), “Optimal Global Rates of Convergence for Nonparametric Regression,” Annals of Statistics, 10, 1040–1053.
Stone, C. (1985), “Additive Regression and Other Nonparametric Models,” Annals of
Statistics, 13, 689–705.
Stone, C. (1986), “The Dimensionality Reduction Principle for Generalized Additive
Models,” Annals of Statistics, 14, 590–606.
Ullah, A. (1988), “Nonparametric Estimation and Hypothesis Testing in Econometric
Models,” Empirical Economics, 13, 223–249.
Utreras, F. (1984), “Smoothing Noisy Data under Monotonicity Constraints: Existence, Characterization and Convergence Rates,” Numerische Mathematik, 47, 611–
625.
Van de Geer, S. (1990), “Estimating a Regression Function,” Annals of Statistics, 18,
907–924.
Van Praag, B. M. S. and M. F. Warnaar (1997), “The Cost of Children and the
Use of Demographic Variables in Consumer Demand,” in M. R. Rozenzweig and
O. Stark (eds.), Handbook of Population and Family Economics, Vol. 1A, Amsterdam: Elsevier Science, North Holland, 241–273.
Varian, H. (1985), “Nonparametric Analysis of Optimizing Behaviour with Measurement Error,” Journal of Econometrics, 30, 445–458.
Varian, H. (1990), “Goodness of Fit in Optimizing Models,” Journal of Econometrics,
46, 141–163.
Varian, H. (1992), Microeconomic Analysis, 3rd ed., New York: W. W. Norton.
Venables, W. and B. Ripley (1994), Modern Applied Statistics with S-Plus, New York:
Springer-Verlag.
Villalobos, M. and G. Wahba (1987), “Inequality-Constrained Multivariate Smoothing
Splines with Application to the Estimation of Posterior Probabilities,” Journal of the
American Statistical Association, 82, 239–248.
Wahba, G. (1990), Spline Models for Observational Data, CBMS-NSF Regional Conference Series in Applied Mathematics, No. 59, Society for Industrial and Applied
Mathematics.
Wahba, G. and S. Wold (1975), “A Completely Automatic French Curve: Fitting Spline
Functions by Cross-Validation,” Communications in Statistics, Series A, 4, 1–17.
Wand, M. P. and M. C. Jones (1995), Kernel Smoothing, New York: Chapman and Hall.
Watson, G. S. (1964), “Smooth Regression Analysis,” Sankhya, Series A, 26, 359–372.
Whang, Y. and D. Andrews (1993), “Tests of Speciﬁcation for Parametric and Semiparametric Models,” Journal of Econometrics, 57, 277–318.

References

207

White, H. (1980), “A Heteroskedasticity Consistent Covariance Matrix Estimator and a
Direct Test for Heteroskedasticity,” Econometrica, 48, 817–838.
White, H. (1985), Asymptotic Theory for Econometricians, New York: Academic Press
(new edition).
Wolak, F. (1989), “Testing Inequality Constraints in Linear Econometric Models,”
Journal of Econometrics, 41, 205–235.
Wooldridge, J. (1992), “A Test for Functional Form Against Nonparametric Alternatives,” Econometric Theory, 8, 452–475.
Wright, I. and E. Wegman (1980), “Isotonic, Convex and Related Splines,” Annals of
Statistics, 8, 1023–1035.
Wu, C. (1986), “Jackknife, Bootstrap and Other Resampling Methods in Regression
Analysis,” Annals of Statistics, 14, 1261–1350 (with discussion).
Yatchew, A. (1988), “Some Tests of Nonparametric Regressions Models,” Dynamic
Econometric Modelling, Proceedings of the Third International Symposium in
Economic Theory and Econometrics, W. Barnett, E. Berndt, and H. White (eds.),
Cambridge: Cambridge University Press, 121–135.
Yatchew, A. (1992), “Nonparametric Regression Model Tests Based on Least Squares,”
Econometric Theory, 8, 435–451.
Yatchew, A. (1997), “An Elementary Estimator of the Partial Linear Model,” Economics
Letters, 57, 135–143. Additional examples contained in Economics Letters 1998,
59, 403–405.
Yatchew, A. (1998), “Nonparametric Regression Techniques in Economics,” Journal of
Economic Literature, XXXVI, 669–721.
Yatchew, A. (1999), “An Elementary Nonparametric Differencing Test of Equality of
Regression Functions,” Economics Letters, 62, 271–278.
Yatchew, A. (2000), “Scale Economies in Electricity Distribution,” Journal of Applied
Econometrics, 15, 187–210.
Yatchew, A. and L. Bos (1997), “Nonparametric Regression and Testing in
Economic Models,” Journal of Quantitative Economics, 13, 81–131, available at
http://www.chass.utoronto.ca/∼yatchew.
Yatchew, A. and W. Härdle (2001), “Dynamic State Price Density Estimation Using
Constrained Least Squares and the Bootstrap,” manuscript, University of Toronto
and Humboldt University zu Berlin.
Yatchew, A. and A. No (2001), “Household Gasoline Demand in Canada,” Econometrica, 69, 1697–1709.
Yatchew, A. and Y. Sun (2001), “Differencing versus Smoothing in Nonparametric
Regression: Monte Carlo Results,” manuscript, University of Toronto.
Yatchew, A., Y. Sun, and C. Deri (2003), “Efﬁcient Estimation of Semi-parametric
Equivalence Scales with Evidence from South Africa,” Journal of Business and
Economic Statistics, 21, 247–257. See also www.chass.utoronto.ca/∼yatchew.
Young, S. G. and A. W. Bowman (1995), “Nonparametric Analysis of Covariance,”
Biometrics, 51, 920–931.
Zheng, J. (1996), “A Consistent Test of Functional Form via Nonparametric Estimation
Techniques,” Journal of Econometrics, 75, 263–289.

208

Index

References followed by ﬁg. refer to ﬁgures. References followed by n refer to notes. References
followed by t refer to tables.
additive separability, 11, 16ﬁg., 17, 18,
102–104, 111, 118, 135, 164, 189–190
Andrews, D., 159
applications
electricity demand and weather, xvii, 11, 47,
81, 82t, 97, 195
electricity distribution, xvii, 7–8, 9ﬁg., 11,
13, 76–81, 84, 85t, 97, 166, 168t, 171,
194
endogenous nonparametric variable,
88–89
Engel’s method for estimation of
equivalence scales, 69, 140–144,
141ﬁg.
estimation of Engel curves, using kernel
smoothers, 37, 38ﬁg.
estimation of Engel curves, using local
polynomial smoothers, 42ﬁg., 42
estimation of Engel curves, using moving
average smoothers, 30–32, 31ﬁg.
gasoline demand, xvii, 11, 73–75, 88, 96,
98, 107, 109ﬁg., 110, 194
housing prices, xviii, 11, 99, 107, 108ﬁg.,
110, 195
log-linearity of Engel curves, 65ﬁg., 65–66,
171
of partial parametric model, 84 –85, 85t
option pricing, xviii, 11, 53, 129–134,
131–133ﬁg., 136, 137, 195
testing equality of Engel curves, 69–71,
70ﬁg., 96, 142
asymptotic pivots, 155n

autocorrelation, heteroskedasticity and,
51–52, 97
average derivative estimation, 53–54
backﬁtting, 102–103, 103t
base-independent equivalence scales, 140,
142, 148–149
testing of, 149–150, 164
bias
in conﬁdence intervals for nonparametric
regression functions, 29, 34 –35, 36t,
160
trade-off of variance and, 19–22, 21ﬁg.,
25–26, 28–29, 34 –35, 100
Bierens, H., 119
Bierens speciﬁcation test, 119–120, 120t
bivariate uniform kernel estimator, 100
bootstrap procedures, xviii, 26, 36, 154 –172
beneﬁts of, 157–159
conﬁdence intervals for kernel smoothers,
160–163, 161t, 162ﬁg.
Edgeworth expansions and, 157–159
goodness-of-ﬁt tests, 163–164, 164t
Härdle and Mammen speciﬁcation test, 121
in index models, 166–170, 169t
in partial linear models, 166, 167t
limitations of, 159
location scale models, 155–156
regression models, 156
residual regression tests, 164 –65, 165t
validity of, 157
box kernel, see uniform kernel

209

210

Index

Cobb-Douglas model, xvii, 7–8, 10, 77ﬁg., 97,
166, 171
concavity, xviii, 4, 15, 22, 25, 26, 36, 115, 118,
125, 128, 135–136, 164
conﬁdence intervals
for kernel smoothers, 34–36, 36t, 37, 38ﬁg.,
41, 42, 55, 100, 160–163, 161t, 162ﬁg.
for simple smoothers, 28–32
constant elasticity of substitution (CES) cost
function, 10, 84–85, 97
convergence, rate of, 8, 19, 23, 25, 29, 30, 34,
55, 67, 93–94, 102, 103, 118, 125t,
128, 134, 135
effect of monotonicity on, 126–127
convexity, 111, 125n, 128, 129, 134
cross-validation, 26, 40n, 43– 46, 56, 161t, 166
for Engel curve estimation, 46, 46ﬁg.
for kernel estimators, 43, 45ﬁg.
for nonparametric least-squares estimators,
44, 45ﬁg.
curse of dimensionality, xvii, 11, 17, 19,
105–107, 136, 138
Deaton, A., 148
derivative estimation, 52–54
differencing, 1–14, 57–98, 104–106
alternative differencing coefﬁcients,
89–90, 90t
combining smoothing with, 92–94
empirical applications of, 11–12, 73–83,
74ﬁg., 75ﬁg., 77ﬁg., 79ﬁg., 82ﬁg., 82n,
84 –85, 107, 109ﬁg.
endogenous nonparametric variable, 87–89
endogenous parametric variables in partial
linear model, 85–87
higher order, 57–64
higher dimensions, 105–107
optimal coefﬁcients, 60–61, 62, 63, 64, 67,
71, 84, 86, 90, 114, 163, 164, 183–186
partial linear model, 2–4, 71–73
partial parametric model, 83–85, 85t
smoothing and, 90–91
two dimensions, 104–105
variance estimation, 2, 58–63
differencing matrices, 57–58, 61
dimensionality, 17–19
curse of, 17, 105–107
double residual estimators, 10, 47– 49, 73–75,
82, 91, 96, 97, 101, 107, 108ﬁg., 110,
146, 166, 167t, 168n

Edgeworth expansion, 157–159, 160
Efron, B., 154
electricity demand and weather, xvii, 11, 47,
81, 82t, 97, 195
electricity distribution, xvii, 7–8, 9ﬁg., 11,
13, 76–81, 84, 85t, 97, 166, 168t,
171, 194
endogeneity
Hausman type test of, 86–87
endogenous nonparametric variables, 87–89
endogenous parametric variables, 85–87
Engel curves
cross-validation and, 46, 46ﬁg., 56
estimation of, 56
estimation of equivalence scales, 69,
140–144, 141ﬁg.
estimation of, using kernel smoothers, 37,
38ﬁg.
estimation of, using local polynomial
smoothers, 42, 42ﬁg.
estimation of, using moving average
smoothers, 30–32, 31ﬁg.
log-linearity of, 65ﬁg., 65–66, 171
testing equality of, 69–71, 70ﬁg.,
96, 142
testing speciﬁcation of, 95, 136, 171
Engle, R.C., 47, 81
Epanechnikov kernel, 33ﬁg., 36n
equality of regression functions, tests of,
4 –7, 6ﬁg., 10, 13, 63, 66–70, 70ﬁg.,
80, 92, 96, 115, 160, 164
equivalence scales
base-independent, 148–150
Engel’s method for estimation of, 69,
140–145
estimation of, 148–150, 151–153, 169–170,
171–172
estimation
constrained and unconstrained, 111–113,
113ﬁg., 129–134
derivatives, 52–54
expected mean-squared error (EMSE), 135
external (“wild”) bootstrap procedures, 121,
156, 157, 159, 161, 164, 165t, 166,
167t, 168t, 169t, 171
Fan, Y., 115
F-statistic, 113–114
functions of several variables, kernel
estimation of, 99–101

Index
Gasser, T., 89
goodness-of-ﬁt tests, xviii, 4, 26, 91, 113–115,
149
bootstrap procedures for, 163–164, 164t
165, 171
residual regression and, 128–129
Hall, P., 60, 61, 67, 89, 127, 139, 154n, 159,
160, 184–185
Härdle, W., xviii, 36, 44, 53, 126, 136, 139,
159
Härdle and Mammen speciﬁcation test,
120–121, 122t
Hausman, J., 73
Hausman test of endogeneity, 86–87
hedonic pricing, xviii, 107, 108ﬁg.,
110
heteroskedasticity, xvii, xix, 10, 36, 50–51,
56, 95, 130, 160, 161ﬁg., 163, 164,
165ﬁg.
autocorrelation and, 51–52
in partial linear model, 72–73, 77ﬁg., 97,
166, 167ﬁg., 169ﬁg.
in speciﬁcation test, 64, 121, 124, 171
wild bootstrap procedures used for, 156,
164, 166
housing prices, xviii, 11, 99, 107, 108ﬁg.,
110, 195
Ho, M., 107
Hoeffding, W., 117n
Holly, A., 87
Hong, Y., 121–122
Hong and White speciﬁcation test, 121–122,
123t
hypotheses testing, 4–7, 49, 63, 64, 66, 68, 69,
71, 81, 87, 106, 111–137, 149, 151,
156, 163, 164, 164t, 165t, 171, see also
equality of regression function tests,
goodness-of-ﬁt tests, Hausman test of
endogeneity, residual regression tests,
signiﬁcance tests, speciﬁcation tests
Ichimura, H., 139
index models, xvii, xviii, 15, 16ﬁg., 17, 18, 25,
138–144, 144ﬁg., 145t, 151–153
bootstrap procedures in, 157n, 166–170,
169t
partial linear index models, 144–150,
152, 172

211
instrumental variables, 85–86
isotonic regression, 125–126, 128
Kay, J., 60, 89
kernel estimation, 32–37, 55, 117–118
alternative kernel functions, 33ﬁg.
bootstrap conﬁdence intervals for, 160–163,
161t, 162ﬁg.
conﬁdence intervals for, 35–36, 36t
of Engel curves 37, 38ﬁg., 42, 42ﬁg.
of functions of several variables, 99–101
in partial linear model, 47
smoothing parameters for, 43– 44, 45ﬁg.
moving average smoothers compared with,
35
uniform conﬁdence bands for, 36–37
kernel regression, 41
Kolmogorov-Smirnov statistic, 155n
Li, Q., 115, 122–124, 165
Li and Zheng speciﬁcation test, 122–124,
123t
Linton, O., 166
local averaging, 19–23, 20ﬁg.
local linear regression, 40– 41, 99
local polynomial regression, 40– 42, 99
location scale models, 155–156
loess, 42, 56, 74ﬁg., 82ﬁg., 97, 101, 103, 107,
108ﬁg., 110, 151
lowess, 42, 42ﬁg., 55–56
Mammen, E., 120–121, 127, 159
monotonicity, xviii, 4, 15, 16ﬁg., 22, 25, 36,
111, 115, 118, 125n, 134, 135, 164
isotonic regression and, 125–126
nonparametric least-squares and, 127–128
rate of convergence and, 126–127
moving average differencing coefﬁcients,
61–62
moving average smoothers, 19, 27–32, 55, 56,
90, 99, 116–117
estimation of Engel curves using, 30–32,
31ﬁg.
kernel smoothers compared with, 35
Nadaraya-Watson kernel estimator, 32, 34, 36t
nearest-neighbor, 22, 27n, 41, 99, 101, 106,
152
Newey, W., 51, 52, 73
No, A., 73

212

Index

nonparametric least-squares, 24n, 25, 37– 40,
53, 94, 99, 101–102, 125ﬁg., 129–134,
135, 136, 137, 187–193
additively separable nonparametric
least-squares model, 103–104
monotonicity constraints and, 127–128
partial linear model, 48– 49
smoothing parameters for, 44, 45ﬁg.
normal kernel, 33ﬁg., 34, 36n, 121n
optimal differencing coefﬁcients, 60–61, 61t,
62, 63, 64, 67, 71, 84, 86, 90, 114, 163,
164t, 183–186
partial linear index models, 144–150, 152,
169, 170, 172
partial linear model, xvii, 1, 2– 4, 8, 9ﬁg., 10,
11, 13, 15, 16ﬁg., 17, 18, 19, 25, 40,
47–52, 68, 71–73, 79ﬁg., 81, 82ﬁg.,
83, 89, 91, 92, 94, 97, 99, 101, 106,
107, 108ﬁg., 110, 146
autocorrelation in, 51–52
bootstrap procedures in, 166, 167t, 168t,
171
differencing estimator of, 2– 4, 71–73
double residual estimator of, 47– 49
endogenous parametric variables in,
85–87
heteroskedasticity in, 50–51
modular approach to analysis of, 92
nonparametric least-squares estimator of,
48
partial parametric model, 10, 15, 16ﬁg.,
83–85, 85t
pivots, 155, 155n, 156, 158, 159, 160
product kernel, 100, 117, 125t
quartic kernel, 33ﬁg., 36n
rate of convergence, see convergence, rate of
rectangular kernel, see uniform kernel
representors, 39, 101, 102, 104, 127, 128, 134,
137, 188, 189, 190–193
residual regression tests, xviii, 26, 113,
115–118 122, 123t, 124, 125t,
128–129, 136
bootstrap procedures for, 163–165, 165t,
171
Rice, J., 1n
Rilstone, P., 52

Robinson, P., 10, 47, 73, 75
running mean (moving average) smoothers,
27, 89
estimation of Engel curves using, 30–32
sampling, in bootstrap procedures,
155–156
Sargan, D., 87
Schmalensee, R., 73
Seifert, B., 89
signiﬁcance tests, 124, 125t
smoothing
asymptotic normality and conﬁdence
intervals for, 29–30
basic asymptotic approximation, 28–29
combining differencing with, 92–94
derivative estimation, 52–54
kernel smoothers, 32–37
local polynomial smoothers, 40– 42
moving average smoothers, 27–28,
116–117
nonparametric least-squares and spline
smoothers, 37– 40
partial linear model, 47–52
relationship between differencing and,
90–91
simple smoothers, 27–32
spline smoothers, 40
smoothing matrix, 30, 90–91
smoothing parameters, see cross-validation
Sobolev space estimation, 37, 39, 40, 44, 53,
101, 187–193
speciﬁcation tests, 4, 63–66, 119–124
bootstrap procedures and, 163–165
Speckman, P., 47
spline estimators, 37– 40, 41, 43n, 44n, 46, 49,
53n, 55–56, 92n, 94, 99, 103, 110, 136,
137, 187
for Engel curve estimation, 42, 42ﬁg.
S-Plus (programming language), xviii, xviiin,
14, 46, 53n, 55, 56, 74ﬁg., 75, 77ﬁg.,
84, 85n, 101, 103, 108ﬁg., 110, 128,
137, 143, 168
Stoker, T., 53, 73
Stone, C., 25, 29, 39
super-smoother, 56
symmetric nearest-neighbor smoother, 27n
Titterington, D., 60, 89
triangular array convergence, 157

Index
triangular kernel, 33ﬁg., 36, 37, 45ﬁg., 46,
46ﬁg., 55, 56
triweight kernel, 33ﬁg., 36n, 41
Ullah, A., 52
uniform conﬁdence bands, 36–37, 38ﬁg., 55
uniform (box) kernel, 32–35, 33ﬁg., 36n, 93,
100, 110, 121, 122t, 123t, 125t,
165t
U-statistic, 116–118, 128, 136
Van de Geer, S., 128
variance

213
estimating, 2, 58–63
trade-off of bias and, 19–22, 21ﬁg., 25–26,
28–29, 34–35, 100
Wang, S., 165
West, K., 51, 52
White, H., 50, 52, 121–122
“wild” (external) bootstrap procedures, 121,
156, 157, 159, 161, 164, 165t, 166,
167t, 168t, 169t, 171
Wolf, A., 89
Zheng, J., 115, 122–124

