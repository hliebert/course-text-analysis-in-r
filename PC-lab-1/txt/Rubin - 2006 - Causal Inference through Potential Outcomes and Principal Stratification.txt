Causal Inference through Potential Outcomes and Principal Stratification: Application to
Studies with "Censoring" Due to Death
Author(s): Donald B. Rubin
Source: Statistical Science, Vol. 21, No. 3 (Aug., 2006), pp. 299-309
Published by: Institute of Mathematical Statistics
Stable URL: https://www.jstor.org/stable/27645760
Accessed: 24-10-2019 21:54 UTC
JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide
range of content in a trusted digital archive. We use information technology and tools to increase productivity and
facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org.
Your use of the JSTOR archive indicates your acceptance of the Terms & Conditions of Use, available at
https://about.jstor.org/terms

Institute of Mathematical Statistics is collaborating with JSTOR to digitize, preserve and
extend access to Statistical Science

This content downloaded from 206.253.207.235 on Thu, 24 Oct 2019 21:54:46 UTC
All use subject to https://about.jstor.org/terms

Statistical Science
2006, Vol. 21, No. 3, 299-309
DOI: 10.1214/088342306000000114
? Institute of Mathematical Statistics, 2006

Causal Inference Through Potential
Outcomes and Principal Stratification:
Application to Studies with "Censoring"

Due to Death1
Donald B. Rubin

Abstract. Causal inference is best understood using potential outcomes.
This use is particularly important in more complex settings, that is, obser
vational studies or randomized experiments with complications such as non
compliance. The topic of this lecture, the issue of estimating the causal effect
of a treatment on a primary outcome that is "censored" by death, is another
such complication. For example, suppose that we wish to estimate the effect
of a new drug on Quality of Life (QOL) in a randomized experiment, where
some of the patients die before the time designated for their QOL to be as
sessed. Another example with the same structure occurs with the evaluation
of an educational program designed to increase final test scores, which are
not defined for those who drop out of school before taking the test. A fur
ther application is to studies of the effect of job-training programs on wages,
where wages are only defined for those who are employed. The analysis of
examples like these is greatly clarified using potential outcomes to define
causal effects, followed by principal stratification on the intermediated out

comes (e.g., survival).
Key words and phrases: Missing data, quality of life, Rubin causal model,
truncation due to death.

PROLOGUE
This article is the written version of two presenta

tions that I had the privilege of giving in 2005, the
first at Carnegie Mellon University on September 16,

the Morris DeGroot Memorial Lecture, and the sec
ond at the Washington, D.C., Chapter of the Ameri
can Statistical Association's Morris Hansen Memorial

Lecture, November 2. Both were truly enjoyable and
stimulating occasions for me, not only the presenta
tions themselves, but the warm events following them.

I am extremely grateful to the two selection commit
tees for inviting me, to the relatives of both Morrises,
and to the very good friends who shared time with me
in Pittsburgh and Washington.
The basic material in the talk had been presented a
couple of previous times before the first "Morris" talk
in Pittsburgh, so it was fairly polished, I thought, and
thus worthy of memorial lectures honoring these two
wonderful statisticians and good friends. Also, the con
tent was rather broadly accessible and relevant to both

men's interests.

Donald B. Rubin is John L. Loeb Professor of Statistics,

I do not know whether Morrie (DeGroot) or Morris

Department of Statistics, Harvard University, Cambridge,

(Hansen) knew each other well or not; they tended to
travel in different statistical circles, Morrie more in the

Massachusetts 02138, USA (e-mail:
rubin @ stat. harvard, edu).

Bayesian decision theory, academic statistics world,

discussed in 10.1214/088342306000000286,
10.1214/088342306000000277 and
10.1214/088342306000000295; rejoinder
10.1214/088342306000000303.

and Morris more in the survey design, government sta

tistics world, but they were both very influential and

widely admired.

299
This content downloaded from 206.253.207.235 on Thu, 24 Oct 2019 21:54:46 UTC
All use subject to https://about.jstor.org/terms

300 D. B. RUBIN
A few words about Morrie first, partly because the

Morrie talk was first, but also because I met Morrie
first, although I was certainly familiar with both names

as a graduate student at Harvard in the late 1960s. In
1976 Morrie became Editor of Journal of the Ameri
can Statistical Association, Theory and Methods, and
he contacted me to stay on as an Associate Editor?
of course, I was thrilled and agreed. But I wondered
why because I didn't really know him at all. He ex
plained that when he had been a JASA Associate Ed
itor earlier, he had sent me various papers to review,
and he liked the reports that I wrote. In particular, one
sequence of papers that he sent me to review was on a

topic that I had felt was completely old-fashioned and
unimportant, and my reviews repeatedly said so. I had
asked Morrie at that time why he kept sending them
to me since I didn't like them. And he replied that be
sides me, there appeared to be only two kinds of pos
sible reviewers: the vast majority who refused to read
the submissions because they were negative about the

area and didn't want to waste their own time; and a
very few who loved the boring material because it was
what they did and therefore would uncritically recom

mend publication. Morrie wanted ammunition to rec
ommend rejection, which would be difficult with only
positive reviews, and I was providing that ammunition!
As Editor, he promised to use me more efficiently ex
cept when a paper on this particular topic arrived.
There are many other Morrie stories available, some

from the very early Bayesian meetings in Valencia,
a quarter of a century ago. One has Morrie explaining
at his after-dinner talk how he could manage to stay up
partying every night until the wee hours of the morn
ing, consuming alcohol and smoking cigars, and still
arise for an early breakfast, more jovial and energetic
than those half his age?he explained that it was sim
ple: practice, practice, practice. Good advice I've tried
to follow.

My experience with Morris was more limited, and
involved discussions and meetings often stimulated by
survey nonresponse issues, or by his advisory roles at
the Census, or on government committees. He was al
ways warm but principled, with a keen desire to see sta
tistics used to address important real-world problems.
In many ways, he reminded me of my wonderful Ph.D.

advisor, Bill Cochran, with respect to having similar
attitudes toward the field of statistics and the problems
it should be addressing.
I think that the topic of today's talk, and I hope, the
presentation itself, would be of interest to both of these
pillars of twentieth century statistics.

1. INTRODUCTION TO DATA THAT ARE
"CENSORED" OR "TRUNCATED" DUE TO DEATH
There are several themes in this presentation that are
quite general. First, the proper analysis of complicated
randomized experiments can often take on many of the
features of the proper analysis of nonrandomized (ob
servational) data; in both, covariates play an important
role, which is often unappreciated. Second, it is critical

to give adequate conceptual thought to any nonstan
dard statistical problem before attacking it with math
ematical analysis or available computer programs. As
Picasso said: "computers are worthless; they only give
answers." (Thanks to Stuart Baker for first pointing out
this great quote.) And third, intermediate outcome vari
ables, which arise frequently in practice but often re
main unrecognized, are not easy to handle well; in fact,
the giant of statistics, Sir Ronald Fisher, gave flawed
advice about them throughout his career (Rubin, 2005).
To be fair, however, nearly all researchers I have read
have also failed to provide good advice on this tricky
topic of intermediate outcome variables, and Fisher ap
peared never to have focused any real attention on it.

One generic example of a complicated randomized
experiment with an intermediate outcome variable is
the specific topic of this presentation, and can be la
beled as involving "censoring" or "truncation" of data
due to death. For instance, the patient in the experiment
dies after treatment assignment, but before the primary
outcome variable, say Quality of Life (QOL) two years
after assignment, can be measured. An artificial exam
ple of this will be used throughout this presentation.
Examples of such censoring also exist in other fields.
For instance, suppose that we were interested in the ef

fect of a special educational intervention in high school

on final test scores, and some of the students in a
randomized experiment evaluating this intervention do

not finish high school. Or in some economics situa
tions, interest focuses on the causal effect of a job
training program on wages (not income), which are
only well defined for those people who are employed;

thus, people who are unemployed when wages are
measured have their wage outcome data "censored" or
"truncated." Or, suppose in a study of the effects of hor

mone replacement therapy (HRT) on five-year cancer
free survival, some women die before five years, but
are cancer-free when they die, say, of heart disease at
three years. As this short list of examples makes clear,
this type of complication can and does arise in many

circumstances.

My first contact with this specific issue was in the
context of a consulting project in the early 1990s for

This content downloaded from 206.253.207.235 on Thu, 24 Oct 2019 21:54:46 UTC
All use subject to https://about.jstor.org/terms

CAUSAL INFERENCE THROUGH POTENTIAL OUTCOMES 301
AMGEN for a product for the treatment of ALS (amy

otrophic lateral sclerosis) or "Lou Gehrig's Disease"?
see a brief discussion in Rubin (2000), and prior to that

in Rubin (1998). ALS is a progressive neuromuscu
lar disease that eventually destroys motor neurons, and
death follows, typically from lungs that are unable to
operate. No good treatments were (or are) available. In
the AMGEN example, the active treatment, say prod
uct T, was to be compared to the control treatment, C,
where the primary outcome was QOL two years post
randomization, as measured by "forced vital capacity"
(FVC), essentially, how big a balloon you can blow up
when you are alive. When FVC is large, you can typi
cally get on fairly well, whereas when this is small, you
are in very bad shape. In fact, many people do not reach
the end-point of two-year post-randomization survival,
and so two-year QOL is "truncated" or "censored" by
death. I was brought into this project because, as some
times is the case, the unavailable QOL data were trying
to be fit into a "missing data" framework.
Before continuing with this example, it is helpful

to state that the general attack on this problem be
ing presented here uses the framework of "principal

stratification" (Frangakis and Rubin, 2002). The spe
cific technical work on this topic was initiated in a
Ph.D. thesis at Harvard University (Zhang, 2002), and
follow-up work appears in Zhang and Rubin (2003)
and Zhang, Rubin and Mealli (2005, 2006). These ref
erences provide discussion of other techniques that
have been proposed to attack this problem, and why

survival if assigned control treatment. This bivariate
outcome is not affected by the treatment received, even
though which of the two outcomes is actually observed
is affected by the treatment received. Thus, in our run

ning example there are four principal strata represent
ing four types of people: those who will live no matter

how treated (LL), those who will die no matter how
treated (DD), those who will live if treated but die if not
treated (LD), and those who will die if treated but live

otherwise (DL). A specific artificial case is displayed
in Table 1 and will be used for most of this article. It

is chosen to be relatively extreme to make points more
dramatically; it does not realistically represent any data
from the AMGEN trial, which originally motivated this

approach.

2. THE RUNNING EXAMPLE
Table 1 presents the hypothetical truth, and dis
plays what would happen to the group of people in
each principal stratum under both the active treatment
and the control treatment. Of course, for any person

we can only observe the "potential outcomes" under
one or the other treatment, not both?the fundamen
tal problem facing (Rubin, 1978, ? 2.4; Holland, 1986,

?3). Holland called the general perspective to causal
inference presented here the "Rubin Causal Model"
(RCM) for a series of papers written in the 1970s
expounding and expanding this perspective (Rubin,
1974, 1975, 1976, 1977, 1978, 1979, 1980); Table 1

those techniques are generally deficient relative to the

assumes "SUTVA" (Rubin, 1980, 1990), the stable

principal stratification approach presented here. We

unit-treatment-value assumption, or stability; this as
sumption is very commonly made.
The first row of Table 1 shows that 20% of the pop
ulation will live under either treatment, as indicated by

only briefly review these other deficient approaches
later, after setting up a correct framework.

The key idea of principal stratification is to stratify
on the intermediate outcome, here the indicators for
two-year survival, but not on the observed two-year
survival, which is an outcome generally affected by the
treatment received. Rather we should stratify on the bi
variate outcome: survival if assigned active treatment,

the survival potential outcomes S(T) ? L and 5(C) =
L; S(T) is the potential outcome for survival when as
signed treatment and 5(C) is the potential outcome for
survival when assigned control. For these LL people,
the average Y (i.e., QOL) if all were treated would

Table 1
Principal strata among the patients

% Principal Treatment Control Treatment effect

population stratum S?(D Yi(T) 5/(C) F;(C) on QOL
40
20

LD

20 LL L 900 L 700 200

L
600
D
*
*
20
DL
D
*
L
Z)D
D
*
D
*
*

This content downloaded from 206.253.207.235 on Thu, 24 Oct 2019 21:54:46 UTC
All use subject to https://about.jstor.org/terms

80

302 D. B. RUBIN
be 900, which is good, but would be 700 if not treated,

which is fair. Therefore, the average causal effect of
the treatment for the LL stratum is 900 - 700 = 200,
as indicated in the last column. This will be called

the SACE?the survivor average causal effect. Criti
cally, a causal effect must be a comparison of treat
ment potential outcomes, Y(T), and control potential
outcomes, 7(C), on a COMMON subset of units, here

by +200. Therefore, with no more information about
possible subgroup differences, such as differences be
tween males versus females, the treatment is preferable
for the population.

Notice also in this example that even if all four
groups had been the same size, each representing a
quarter of the population, the treatment still would have

the set of LL units.

been preferred to control. The reason is that, although
there would have been no treatment versus control dif

The LD units, displayed in the second row of Table 1,
are those who would die under control but live under

ference on overall survival, treatment would have a
positive causal effect on QOL for the only subgroup

treatment [i.e., S(T) = L and 5(C) = D], and they

where it is well defined. If, in this case, an * were im

comprise 40% of the population. If these units were
all treated, their average QOL would be 600, which is

puted with 0, the conclusion would have been that there
is no benefit to the active treatment for either survival

poor, but if they were not treated, they would die, and
their QOL would be undefined (or defined on the sam

or QOL because the last column would have averaged
to zero (200 + 600 - 800 + 0)/4 = 0, a conclusion that

ple space of the positive real numbers extended to in
clude an asterisk). To assign a particular value to QOL
when dead is to assume we know how to trade off a par

conflates facts with value judgments. This conclusion
would be especially deceptive if conclusions from this
population were to be generalized to future healthier

ticular QOL and being dead (and out of misery). Not
only do we not know how to do this, but the trade-off
could vary by individual, so we prefer simply to repre
sent the actual truth at this point, and not bring in such
extraneous value judgments.

The third row of Table 1 is for those who would

die under treatment but live under control, those in the

DL group with S(T) ? D and 5(C) = L. These sub
jects comprise 20% of the population, and their aver
age QOL under control is a quite decent 800. And the
final 20% represented in the fourth row are in the DD
group, who would die no matter which treatment they

received.

A well-defined real value for the average causal ef
fect of the active treatment versus the control treatment

on QOL exists only for the LL group. For the LD and
DL groups, the average causal effect on QOL involves

populations dominated by people like those in the LL
group; this often can occur in real-world clinical tri
als, where experimental drugs are first tried with sicker

patients, and approval is based on results with these
patients, but if approved, the drugs are used in broader
and healthier populations.
Continuing with the examination of Table 1, under

treatment the healthiest group is the LL group, fol
lowed by the LD group; the DL and DD groups both
die when treated. However, under control the DL group
is healthier than the LL group, and both of these groups

are healthier than the LD and DD groups, whose mem

bers die under control. Can this be realistic? The an

swer is "yes" for at least two reasons: First, some
drugs do have negative side effects for some subgroups

of people, and so here that would be the DL sub

the aforementioned trade-offs with death, and for the

group, who would survive if untreated. Another possi
ble reason is that the active treatment may make some

DD group there is no QOL to compare, so the causal

people feel so much better, even though it does not af

effect on QOL for them must be zero. The most that we

fect their disease progression, that they "overdo" it?

can ever hope to learn in any study of this population of
values under these two treatments is recover this table

play tennis, go to parties, have normal sex lives, and so
on. There are some drugs that can have effects like this;

of values.

Epogen, another product made by AMGEN, substan

Before considering how to do this, however, let us
examine this table a bit more. First, the active treatment
is better for survival than the control treatment because

60% (20% LL + 40% LD) would survive when treated,
whereas only 40% (20% LL + 20% DL) would survive
if not treated (control).
Thus the active treatment is better for overall sur
vival, and the active treatment is better for QOL for the

subset of people where it is well defined, the LL group,

tially increases red blood cell production and is of sub
stantial apparent benefit to dialysis and chemotherapy
patients, who can have much more energy with the ex
tra oxygen-carrying capability created by Epogen. For
example, Epogen has become an issue in recent years
in some professional sports (e.g., bicycling with Lance
Armstrong recently, and Jerome Chiotti before him).
These situations reinforce the related points made ear
lier about the trade-offs between a potentially higher

This content downloaded from 206.253.207.235 on Thu, 24 Oct 2019 21:54:46 UTC
All use subject to https://about.jstor.org/terms

CAUSAL INFERENCE THROUGH POTENTIAL OUTCOMES 303

quality of life versus an earlier death. For example,

each treatment. This is reflected in Table 2 where each

a weak 90-year-old may consider a QOL of 600 prefer
able to death, whereas an Olympic athlete who is used
to running ten miles a day may prefer death to a com
pletely sedentary and deteriorating QOL.
A related point is that Table 1 is only a summary of
the individuals' potential outcomes in this hypotheti

row in Table 1 is split in half, with the top one in each

half getting the active treatment, indicated by Z = 1,
and the other getting the control treatment, indicated
by Z = 0. Of course, we do not get to observe all the
values in Table 2, and in fact, do not know the principal

strata to which individual people actually belong.

cal population because it only gives the mean values
of the survival and QOL potential outcomes within
each principal stratum. The more complete version
of Table 1 would also provide the marginal distrib
utions of all four potential outcomes, in addition to
their means, and moreover, would provide the joint

the same observed survival are adjacent. We trivially
obtain Table 3, but of course we still do not know the

four-dimensional distribution of the potential outcomes

ple who got treated and lived, and that these comprise

within each principal stratum. Having such informa
tion would allow individuals to make the trade-off be

tween death and QOL, but it is far more difficult to
estimate such a table of joint distributions than simply
the means, because treatment and control potential out
comes are never jointly observed. We return briefly to
this topic after understanding the simpler problem of
estimating the means given in Table 1 from observable

data.

Suppose now that we permute the rows in Table 2

so that rows that have the same observed treatment and

splits between the pairs of adjacent rows. That is, for
the first pair of rows, we observe that all these are peo

30% of the people in the experiment; consequently,
the observed survival rate in the random half assigned

treatment is 60% (= 2 x 30%). The average QOL for
this group will be a 1/3 + 2/3 mixture of LL and LD,

that is, of the averages 900 and 600, and so the ob
served average for those who got assigned treatment
and lived will equal 700. We do not observe any control

potential outcomes for these people because they are
all treated. For the second pair of rows in Table 3, we
have that they are observed to be treated and die, and
comprise 20% of the population, or 40% of the treated

For now let us accept Table 1 with just Y means in
all four principal strata as truth, and consider next how
we learn about this table from observable data.

group dies. Again, no control potential outcomes are
observed for these people.

3. WHAT WOULD BE OBSERVED IN A
RANDOMIZED EXPERIMENT?

For the third pair of rows, we observe that they are

assigned to control and live, and comprise 20% of
the population, implying a survival rate in the popu
lation under control of 40%. Also they have an ob
served average QOL of 750, which arises from the
1/2 + 1/2 mixture of LL and DL with means 700 and

Suppose that we conducted a huge completely ran
domized experiment on a huge random sample from
this population: half get randomized to active treatment

and half get randomized to the control. Even though
not blocked on the unknown principal strata, in expec
tation, half of each principal stratum will be exposed to

800, respectively. In these two rows the treated poten
tial outcomes are not observed because the people were

Table 2
Principal strata among the patients, each split by treatment assignment

%
population
10

10
20

20
10
10
10

10

Principal

Assignment

stratum
LL
LL
LD
LD
DL
DL
DD
DD

T

C
T

C
T
C
T
C

Control

Treatment

Treatment effect

Si(T)

Yi(T)

Si(C)

Yi(C)

on QOL

L
L
L
L
D
D
D
D

900
900
600
600

L
L
D
D
L
L
D
D

700

200

700
*

800

800

This content downloaded from 206.253.207.235 on Thu, 24 Oct 2019 21:54:46 UTC
All use subject to https://about.jstor.org/terms

200

304

D. B. RUBIN

Table 3
Permuted table for principal strata among the patients, each split by treatment assignment

% Principal Treatment Control Treatment effect
population stratum Z? 5/(7) Yt{T) 5/(C) f/(C) on QOL
10 LL T L 900 L 700 200

20
LD
T
L
600
D
*
10
DL
T
D
*
L
800
10
DD
T
D
*
D
*

*
*
*

10 LL C L 900 L 700 200
10
DL
C
D
*
L
800
20
LD
C
L
600
D
*
10
DD
C
D
*
D
*

*
*
*

assigned
control.
Finally
What went
wrong
mean QOL
for
surv
signed
control,
and
they
comprise
30%
parison
of does
the
not
pop
es
in
the
control
group
of
comparing
treated
F(l) and Y(0)]
on a
comes
are
observed
for
LL
group),
it
compa
The
discussion
in
the

potential
outcome
summarized
in
Table
4,
1/3 + 2/3 mixture
actually
observed
in
th
served
control
pot
noteworthy.
First,
supp

causal

from
a 1/2 +
effectcomes
of
the
active

are different
grou
the
"intermediate"
outco
people
in common,
60%
survive
when
treate

just

sess

as

the

tions.
Table
1.
Next
causal
effect
of
This method of attack,
comparing QOL when it is

in

and dropping people
who died, although popusin
treatment observed
on
QOL
ular in some settings, is simply wrong in general. But QOL
we
have
observed
then what should we do?
If we knew the700
labels of the
served
average
of
fo

principal strata for all the people, we
could simply ana
average
of
750
lyze
the
data
within
each
stratum,
in
particular
compare
that,
although
the

observed

clude
survival,

7(1) and
7(0) in the bad
LL stratum, but we dofor
not have
it
is
Q
this
information.
As
Table
5
displays,
instead,
each
of
is
simply
wrong!
The
ca
our observed groups
defined by observed contr
treatment as
ment
versus
the
group,

defined.

signment Z and observed
5obs, comprises a only
which
issurvival
the
Table 5

Table 4
Observed data for the example of Tables 1-3

% Treatment Control

population Zt S/(D YiKT) S?(C) Y^C)
30 T L 700 ? ?
20
T
D
*
?
?
20 C ? ? L 750
30 C ? ? D *

Group classification based on observed treatment assignment and

observed survival indicator OBS(Z, 5 s), and associated data
pattern and possible latent principal strata

Observed group Possible latent

OBS(Z,Sobs) Z Sohs Yobs principal s
OBS(T,L) T L eR
OBS(T,D) T D *
OBS(C,L) C L eR
OBS(C,D) C D *

This content downloaded from 206.253.207.235 on Thu, 24 Oct 2019 21:54:46 UTC
All use subject to https://about.jstor.org/terms

LL, LD
DL,DD
LL, DL
LD,DD

CAUSAL INFERENCE THROUGH POTENTIAL OUTCOMES 305
mixture of people from two unobserved, or latent, prin
cipal strata.

4. POSSIBLE APPROACHES
One possible approach is to treat the problem as one
of missing data, and try to impute, or multiply impute,
the "missing data" that are "censored" by death. But

we really already rejected this idea in the discussion
of Section 2: the Y outcomes are not missing; they are
undefined, or defined to be *. Maybe adding some sim
plifying assumptions would help?
There are some assumptions that are relatively stan
dard in similar settings, in particular, where the inter

mediate outcome variable indicates compliance with
assigned drug (Angrist, Imbens and Rubin, 1996). In
Tables 1-4, the four principal strata could be called
compliers, never-takers, always-takers and defiers, cor

responding to LD, DD, LL and DL, respectively. The
analogy here is that people who would live under treat
ment but would die under control are "complying" with
the encouragement of the active treatment to help them.

The first standard assumption (after SUTVA) that is

made in the noncompliance setting is called "monoto
nicity" or the "no-defier" assumption, which rules out

the DL group. The no-defier assumption can be rea
sonable in our QOL setting, but is wrong in the context
of our numerical example because there exist both LD

and DL groups.
The next assumption that is often made in the non

so we cannot assume it to be zero! So the exclu

assumption does us absolutely no good. When b
monotonicity and exclusion hold, however, the c
cal instrumental variables estimate (IVE) can be
to estimate the "complier average causal effect"
Angrist, Imbens and Rubin (1996) for extended dis

sion. In simple settings, the IVE is the simple treatm
minus control estimate for the mean of 7 divided by
simple treatment minus control estimate for the m

of D, so here would require some imputation of

"missing" 7 values for those who are observed to d
If we imputed zero for the QOL for those who die

would have that IVE = (420 - 300)/(0.6 - 0.4) = 6

unrelated to anything real, which is not surprisin

cause the underlying assumptions justifying th
are both wrong in our example.

Another possible assumption, considered in Zh
and Rubin (2003), is "stochastic dominance," w

implies that, on average, the LL group is healthier

der control than the DL group, and the LL grou

healthier under treatment than the LD group. Aga
this condition is violated in our numerical exampl
was noted in Section 2 earlier. Large-sample bound
the Survivor Average Causal Effect (i.e., the causa
fect in the LL group) are derived in Zhang and Ru
(2003) under monotonicity and stochastic dominan

but are not very useful in our example, as displ

in Table 6, because none of the assumptions holds.
other examples, they could be quite useful.

compliance setting is called "exclusion," which as
serts that if treatment assignment cannot change the
intermediate outcome, D, it cannot change the final

outcome, Y; here, that would mean that there is no
treatment effect on Y ? QOL for either the LL group
or the DD group. But the causal effect on QOL for the

LL group is precisely what we want to estimate, and
Table 6

5. THE ROLE FOR COVARIATES

A more successful general approach is to collect a
use covariates that are predictive of both the inter
ate potential outcomes (e.g., here survival) and the

potential outcomes (e.g., here QOL). This was do
the actual AMGEN application because there were

eral measurements of baseline FVC (= baseline Q
Thus at baseline measurements of each patient's

Bounds for treatment effect on QOL in LLfor numerical example

rent FVC and the rate of deteriorating FVC were
able, and these were highly predictive of both surv
Monotonicity Stochastic
and of two-year-later QOL if surviving.
assumption dominance [Lower Bound, Upper Bound]

No No [-200, 200]
Yes No [-150, 0]
No Yes [-100, 150]
Yes Yes [-50, 0]

To amplify this point, consider Table 7, whic

identical to Table 1 except with an added left-mos

umn, labeled X, for a covariate, baseline QOL in
hypothetical example. The hypothetical means
are displayed, and we will assume that the hypo

ical variances
The first column shows whether the monotonicity assumption
(Al)
small dom
relative
is made, and the second column shows whether the stochastic

of X within each principal stratum
to the differences between the mean

inance assumption (A2) is made. The last column shows the
bounds
again
pretend
for the numerical example of Tables 1-4.

that we conduct a huge randomized
periment, with 50% treated and 50% control, to o

This content downloaded from 206.253.207.235 on Thu, 24 Oct 2019 21:54:46 UTC
All use subject to https://about.jstor.org/terms

306 D. B. RUBIN
Table 7 (= Table 1 with Key Covariate)
Principal strata among the patients

% Principal Treatment Control Treatment effect

X population stratum SiKT) YiyT) S?(C) Yi(C) on QOL
800 20 LL L 900 L 700 200

500 40 LD L 600 D * *
900 20 DL D * L 800 *
300 20 DD D * D * *

Table

8,

which

parallels
Table is
2 quite
with
900, which

trol pairs of rows. treated,
Trying to
permute
their
QOL
table to bring groups
adjacent
that are
whereas
if treated
the same (with respect
to observed
X,
and requires
follow-u

ment

assigned,

and
observed
survival
doctors
(and/or
frie

unchanged because,
for example,
althou
reasons
for their
dea
third rows are both
and
surviv
of treated
the drug,
or
over
tinguishable by their
differing
baselin
effects
on perceived

tions.
which

The observed
data
then are
per"
covariate
hasas
al
Table
1.
we reach the following conclusi

People

with

baseline
FVC
around
Notice that collecting
more measurements
of out

pretty good, comprise
20%
the pop
comes does not help
in the sameof
way as collecting
all survive no matter
how
treated;
the
covariates, because
outcome measurements
have dif

QOL

of

active

versus
control
for
ferent potential outcomes
depending on the treatment

the

+200?this conclusion
agrees
with t
exposures, and so using
outcomes to improve inference
bles 1 and 7. Next,
consider
the
40%
will require some serious modeling efforts involving
tion with baseline
QOL
around
500,
new assumptions. Some work on this topic appears in
poor.

survive
treated,
with
Zhang (2002), but is anif
important
area for statistical re
than
baseline,
but
search becauseat
it is common,
and often easy, to collect
out the active treatment,
the
such repeated measurementshowever,
post-randomization.
Another source of information
can be utilizedQOL.
is
some may prefer death
to that
poor
F
baseline FVC around
300,
which
is
very
the distributional shape of the outcome in the different
active treatment nor
control
can
groups and treatments.
For example, if we
knew that preve
nally, for the 20%
with
best
QOL measurements
werethe
approximately
normally dis basel

of

They

600,

will

better

Table 8 (= Table 2 with Key Covariate)
Principal strata among the patients, each split by treatment assignment

% Principal Treatment Control Treatment effect

X population stratum Z; S?(T) Y?(T) 5/(C) Yi(C) on QOL
800 10 LL T L 900 L 700 200
800 10 LL C L 900 L 700 200

500 20 LD T L 600 D * *
500 20 LD C L 600 D * *

900

300
300

900 10 DL T D * L 800 *
10 DL C D *
10
10

DD
DD

T
C

D
D

L

*
*

800

D
D

This content downloaded from 206.253.207.235 on Thu, 24 Oct 2019 21:54:46 UTC
All use subject to https://about.jstor.org/terms

*
*

*

*
*

CAUSAL INFERENCE THROUGH POTENTIAL OUTCOMES 307
Table 9 (= Table 4 with Key Covariate observed)

Principal Treatment Control

nonadditive effect, being more effective for some than
for others.

Nothing essential changes in Tables 1-3, except with

the addition of the standard deviation associated with
X stratum Z? Sf(D Y?(T) 5?(C) f?(C)

800
800
500
500
900
900
300
300

LL T L 900 ? ?
LL C ? ? L 700
LD T L 600 ? ?
LD C ? ? D *
DL T D * ? ?
DL C ? ? L 800
DD T D * ? ?
*
DD C ? ? D

each mean. But Table 4, giving the observed data, is
changed in an important way when the distributions are
given. First, the treated group that lives is still observed
to have mean 700, but its distribution is markedly non
normal, with one-third having mean 900 and standard
deviation 70 and two-thirds having mean 600 and stan

dard deviation 40. These components are easily ob
servable as different because of the assumed normality
and the small within-component standard deviations.

In more subtle situations, we would have to use far
more
sophisticated
analysis methods, which general
tributed across subjects within each principal
stratum

standard
mixture modeling
techniques cited earlier.
and treatment condition, this could be ize
very
helpful
be
Although,
in
this
extreme
example,
we can distinguish
cause it would allow standard mixture modeling tools
between the one-third and two-thirds mixture compo
to be used to help disentangle the normal components
nents in the treated group that lives, we do not yet know
(e.g., Dempster, Laird and Rubin, 1977; Titterington,
which is LL and which is LD, however. But we do know
Smith and Makov, 1985). This approach is used in the
that LL + LD comprise 60% of the population, because
example in Section 6.
60% live in the random half exposed to the active treat
ment; thus, either LL is one-third of the 60%, that is,
6. THE ROLE FOR DISTRIBUTIONAL

20% of the population, with a N(900,702) distribu
ASSUMPTIONS, SUCH AS NORMALITY

tion when treated, or is two-thirds of the 60%, that is,
Here we extend the example in Table40%
1 to
include
of the
population,the
with a iV(600, 402) distribution
when
treated.
distribution of QOL within each of the
four
principal
strata to illustrate how such information
canonbe
used
to QOL distribution for the
Moving
to the
observed
help recover the information in Table 40%
1. This
extension
who survive in the control group, we will ob
will also lead to a brief discussion of the
more
serve
a mean difficult
of 750 arising from a half/half mixture

issue of the role of the joint distribution
of the and
never
of iV(800,602)
N(700,502), where one compo

jointly observed potential outcomes nent
under
T and
represents
LL andun
one component represents DL,

der C.

Again, our example will be extreme to illustrate

but which is which? Before addressing this question,
we note that here, decomposing the two components

ideas, and the actual methods of analysis with real ex
amples will nearly always involve methods of analysis

within the surviving subjects in the control group is not
as obvious as in the treated group, because in the con

(e.g., Dempster, Laird and Rubin, 1977; Titterington,

one standard deviation apart?but this is a standard

based on EM or MCMC methods for mixture models

Smith and Makov, 1985; Aitkin and Rubin, 1985).
Specifically, suppose the four marginal distributions

of the QOL potential outcomes within each of three
principal strata where they are well defined are normal

(Gaussian): N(900, 702) and N(700, 502) for LL when
treated and not, N(600, 402) for LD when treated, and

N(800, 602) for LD when not treated. Suppose also
that the investigators are confident that the distribu
tions are normal, which could occur, for example, if the
QOL scores were based on the average of a large set of
test items about activities that the individuals can and
cannot perform. The hypothetical variances tend to be
larger under the active treatment because the drug has a

trol the means of the two components are only about
problem using the aforementioned mixture modeling
algorithms. Now each component in the surviving con
trol group is observed to be one-half of the 40% who
live under control; thus, from the control data, both LL
and DL comprise 20% of the population. But from the
treatment group, we know that LL is either 20% or 40%

of the population; so combining both pieces of infor
mation, LL must be 20%. Also, LL's QOL distribution
when treated must therefore be N(900, 702) and LD's
QOL distribution when treated must be N(600, 402).
Furthermore, when not actively treated, from the sur

viving control group, LL's QOL distribution must be
either N (700, 502) or N(800, 602)?the observed data

This content downloaded from 206.253.207.235 on Thu, 24 Oct 2019 21:54:46 UTC
All use subject to https://about.jstor.org/terms

308 D. B. RUBIN
cannot distinguish these two possibilities because both

in learning why those in DL die when treated (e.g., neg

LL and DL are exactly the same size, 20% of the pop

ative side effects versus enjoying a too vigorous life),
and then use this information to make a more informed

a higher mean QOL under control or which had the

choice than directly available from Table 4.
In conclusion, I think that causal inference model

ulation. Of course, if we knew which of LL or LD had
larger variance under control, we would know which

was #(800, 602). This points out a fragility in the es
timation: as the principal strata get closer to each other
in size or closer in means and variances, the estimation
becomes more difficult.

Nevertheless, we have recovered much of Table 1.
What are uncertain are the mean values of the QOL
column under control and the S ACE: under control, the

mean QOL is either 800 for LL and 700 for DL, or 700
for LL and 800 for DL; and the S ACE is either +100 or

ing using potential outcomes and principal stratifica
tion, with its explicit and transparent assumptions, has
helped clarify situations that statisticians must confront

when there exists censoring of outcomes due to death
of units, and has led to the creation of an approach to
estimation that can be quite beneficial in a variety of
difficult settings across a variety of disciplines.

+200; in either case treatment is preferable to control.

7. DISCUSSION

REFERENCES
AITKIN, M. and Rubin, D. B. (1985). Estimation and hypothesis
testing in finite mixture models. J. Roy. Statist. Soc. Ser. B 47

It is not surprising that if we have distributional in
formation and good covariates, the estimation of the

principal strata can be sharpened, even without as
sumptions such as stochastic ordering of the groups.
Of course, in general, estimation must involve Markov

chain Monte Carlo techniques, likelihoods generally
will not be regular, and models may need to be as
sessed using posterior predictive checks (Rubin, 1984;
Gelman, Meng and Stern, 1996; Gelman, Carlin, Stern

and Rubin, 2004).
It is also important to realize that the joint con
ditional distribution of treatment potential outcomes

(5?(1), F/(l)) and control potential outcomes

(5/(0), Y i (0)) given covariates is inestimable in the
sense that the likelihood is free of parameters govern
ing this joint conditional distribution (i.e., the posterior
distribution of these parameters equals their prior dis
tribution). To address this situation, sensitivity analyses
(e.g., Rosenbaum and Rubin, 1983) and the creation of

large-sample bounds (e.g., Manski, 2003; Zhang and
Rubin, 2003, in this specific problem) could well be
quite helpful and informative. This joint distribution
can be relevant to an individual's decision-making for
treatment versus control. Knowing the detail provided

by Table 1 beyond that in Table 4, however, can be
helpful for this decision, even without knowledge of
the inestimable joint distribution. For example, a per

son may decide that he is in better health than the
typical patient and is therefore more likely to have out
comes like those either in the LL stratum, who are the

healthiest group when treated, or in the LD stratum,
who are the healthiest group when not treated. Conse
quently, such a person would be particularly interested

67-75.

Angrist, J. D., Imbens, G. W. and Rubin, D. B. (1996). Identi
fication of causal effects using instrumental variables (with dis
cussion). J. Amer. Statist. Assoc. 91 444-472.

Dempster, A. P., Laird, N. and Rubin, D. B. (1977). Maximum
likelihood from incomplete data via the EM algorithm (with dis

cussion). J. Roy. Statist. Soc. Ser. B 39 1-38. MR0501537

Frangakis, C E. and Rubin, D. B. (2002). Principal stratifica
tion in causal inference. Biometrics 58 21-29. MR1891039

Gelman, A., Carlin, J. B., Stern, H. S. and Rubin, D. B.
(2004). Bayesian Data Analysis, 2nd ed. Chapman and
Hall/CRC, New York. MR2027492

Gelman, A. E., Meng, X.-L. and Stern, H. (1996). Posterior
predictive assessment of model fitness via realized discrepan
cies (with discussion). Statist. Sinica 6 733-807. MR1422404

Holland, P. W. (1986). Statistics and causal inference (with dis
cussion). J. Amer. Statist. Assoc. 81 945-970. MR0867618
MANSKI, C F. (2003). Partial Identification of Probability Distri
butions. Springer, New York. MR2151380
ROSENBAUM, P. R. and Rubin, D. B. (1983). Assessing sensitivity
to an unobserved binary covariate in an observational study with
binary outcome. /. Roy. Statist. Soc. Ser. B 45 212-218.

Rubin, D. B. (1974). Estimating causal effects of treatments in ran

domized and nonrandomized studies. J. Educational Psychol
ogy 66 688-701.
Rubin, D. B. (1975). Bayesian inference for causality: The impor
tance of randomization. In Proc. Social Statistics Section 233
239. Amer. Statist. Assoc, Alexandria, VA.
Rubin, D. B. (1976). Inference and missing data (with discussion).

Biometrika 63 581-592. MR0455196

Rubin, D. B. (1977). Assignment to treatment group on the basis
of a covariate. /. Educational Statistics 2 1-26.

Rubin, D. B. (1978). Bayesian inference for causal effects: The
role of randomization. Ann. Statist. 6 34-58. MR0472152
Rubin, D. B. (1979). Discussion of "Conditional independence in
statistical theory," by A. P. Dawid. /. Roy. Statist. Soc. Ser. B 41

27-28.

This content downloaded from 206.253.207.235 on Thu, 24 Oct 2019 21:54:46 UTC
All use subject to https://about.jstor.org/terms

CAUSAL INFERENCE THROUGH POTENTIAL OUTCOMES 309
Rubin, D. B. (1980). Comment on "Randomization analysis of ex
perimental data: The Fisher randomization test," by D. Basu. J.

Amer Statist. Assoc. 75 591-593.

Rubin, D. B. (1984). Bayesianly justifiable and relevant fre
quency calculations for the applied statistician. Ann. Statist. 12

1151-1172. MR0760681

Rubin, D. B. (1990). Neyman (1923) and causal inference in ex
periments and observational studies. Statist. Sei. 5 472-480.

MR 1092987

Rubin, D. B. (1998). More powerful randomization-based
p-values in double-blind trials with noncompliance. Statistics
in Medicine 17 371-385.

Rubin, D. B. (2000). Comment on "Causal inference without
counterfactuals," by A. P. Dawid. J. Amer. Statist. Assoc. 95

435-438.

Rubin, D. B. (2005). Causal inference using potential outcomes:
Design, modeling, decisions. 2004 Fisher Lecture. J. Amer Sta
tist. Assoc. 100 322-331. MR2166071

Titterington, D. M., Smith, A. F. M. and Makov, U. E.
(1985). Statistical Analysis of Finite Mixture Distributions. Wi

ley, New York. MR0838090
Zhang, J. L. (2002). Bayesian estimation of causal effects in the
presence of truncation by death. Ph.D. dissertation, Harvard

Univ.

Zhang, J. L. and Rubin, D. B. (2003). Estimation of causal ef
fects via principal stratification when some outcomes are trun

cated by "death." J. Educational and Behavioral Statistics 28

353-368.
Zhang, J. L., Rubin, D. B. and Mealli, F. (2005). Using the
EM algorithm to estimate the effects of job training programs
on wages. In Proc. 55th Session of the International Statistical
Institute.

Zhang, J. L., Rubin, D. B. and Mealli, F. (2006). Evaluating
the effects of training programs on wages through principal
stratification. In Modelling and Evaluating Treatment Effects in

Econometrics (D. Millimet, J. Smith and E. Vytlacil, eds.). El
sevier, Amsterdam. To appear.

This content downloaded from 206.253.207.235 on Thu, 24 Oct 2019 21:54:46 UTC
All use subject to https://about.jstor.org/terms

