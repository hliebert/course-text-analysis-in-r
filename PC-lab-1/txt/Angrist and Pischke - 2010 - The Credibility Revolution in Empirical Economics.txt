Journal of Economic Perspectives—Volume 24, Number 2—Spring 2010—Pages 3–30

The Credibility Revolution in Empirical
Economics: How Better Research Design
is Taking the Con out of Econometrics
Joshua D. Angrist and Jörn-Steffen Pischke

J

ust over a quarter century ago, Edward Leamer (1983) reflected on the state of
empirical work in economics. He urged empirical researchers to “take the con
out of econometrics” and memorably observed (p. 37): “Hardly anyone takes
data analysis seriously. Or perhaps more accurately, hardly anyone takes anyone
else’s data analysis seriously.” Leamer was not alone; Hendry (1980), Sims (1980),
and others writing at about the same time were similarly disparaging of empirical
practice. Reading these commentaries as late-1980s Ph.D. students, we wondered
about the prospects for a satisfying career doing applied work. Perhaps credible
empirical work in economics is a pipe dream. Here we address the questions of
whether the quality and the credibility of empirical work have increased since
Leamer’s pessimistic assessment. Our views are necessarily colored by the areas of
applied microeconomics in which we are active, but we look over the fence at other
areas as well.
Leamer (1983) diagnosed his contemporaries’ empirical work as suffering
from a distressing lack of robustness to changes in key assumptions—assumptions he called “whimsical” because one seemed as good as another. The remedy
he proposed was sensitivity analysis, in which researchers show how their results
vary with changes in specification or functional form. Leamer’s critique had a
refreshing emperor’s-new-clothes earthiness that we savored on first reading and
still enjoy today. But we’re happy to report that Leamer’s complaint that “hardly
anyone takes anyone else’s data analysis seriously” no longer seems justified.
Joshua D. Angrist is Ford Professor of Economics, Massachusetts Institute of Technology,
Cambridge, Massachusetts. Jörn-Steffen Pischke is Professor of Economics, London School
of Economics, London, United Kingdom. Their e-mail addresses are 〈angrist@mit.edu
angrist@mit.edu〉〉 and
〈s.pischke@lse.ac.uk
s.pischke@lse.ac.uk〉〉.
■

doi=10.1257/jep.24.2.3

4

Journal of Economic Perspectives

Empirical microeconomics has experienced a credibility revolution, with a consequent increase in policy relevance and scientific impact. Sensitivity analysis played
a role in this, but as we see it, the primary engine driving improvement has been
a focus on the quality of empirical research designs. This emphasis on research
design is in the spirit of Leamer’s critique, but it did not feature in his remedy.
The advantages of a good research design are perhaps most easily apparent in
research using random assignment, which not coincidentally includes some of the
most influential microeconometric studies to appear in recent years. For example,
in a pioneering effort to improve child welfare, the Progresa program in Mexico
offered cash transfers to randomly selected mothers, contingent on participation in
prenatal care, nutritional monitoring of children, and the children’s regular school
attendance (Gertler, 2004, and Schultz, 2004, present some of the main findings).
In the words of Paul Gertler, one of the original investigators (quoted in Ayres,
2007, p. 86), “Progresa is why now thirty countries worldwide have conditional cash
transfer programs.” Progresa is emblematic of a wave of random assignment policy
evaluations sweeping development economics (Duflo and Kremer, 2008, provide
an overview).
Closer to home, the Moving to Opportunity program, carried out by the U.S.
Department of Housing and Urban Development, randomly selected low-income
families in Baltimore, Boston, Chicago, Los Angeles, and New York City to be
offered housing vouchers specifically limited to low-poverty areas (Kling, Liebman,
and Katz, 2007). The program has produced surprising and influential evidence
weighing against the view that neighborhood effects are a primary determinant of
low earnings by the residents of poor neighborhoods.
Structural econometric parameters, such as the intertemporal substitution
elasticity (a labor supply elasticity that measures the response to transitory wage
changes), have also been the focus of randomized experiments. For example,
Fehr and Goette (2007) randomized the pay of bicycle messengers, offering one
group and then another a temporarily higher wage. This cleverly designed study
shows how wages affect labor supply in an environment where lifetime wealth is
unchanged. The result is dramatic and convincing: holding wealth constant,
workers shift hours into high-wage periods, with an implied intertemporal substitution elasticity of about unity.
Such studies offer a powerful method for deriving results that are defensible
both in the seminar room and in a legislative hearing. But experiments are time
consuming, expensive, and may not always be practical. It’s difficult to imagine a
randomized trial to evaluate the effect of immigrants on the economy of the host
country. However, human institutions or the forces of nature can step into the
breach with informative natural or quasi-experiments. For example, in an influential paper, Card (1990a) used the Mariel boatlift from Cuba to Florida, when Cuban
émigré’s increased Miami’s labor force by about 7 percent in a period of three
months, as a natural experiment to study immigration. More recently, paralleling
the Moving to Opportunity experimental research agenda, Jacob (2004) studied
the causal effects of public housing on housing project residents by exploiting the

Joshua D. Angrist and Jörn-Steffen Pischke

5

fact that public housing demolition in Chicago was scheduled in a manner unrelated to the characteristics of the projects and their residents.
Like the results from randomized trials, quasi-experimental findings have
filtered quickly into policy discussions and become part of a constructive give-andtake between the real world and the ivory tower, at least when it comes to applied
microeconomics. Progress has been slower in empirical macro, but a smattering
of design-based empirical work appears to be generating a limited though useful
consensus on key concerns, such as the causal effect of monetary policy on inflation and output. Encouragingly, the recent financial crisis has spurred an effort
to produce credible evidence on questions related to banking. Across most fields
(although industrial organization appears to be an exception, as we discuss later),
applied economists are now less likely to pin a causal interpretation of the results
on econometric methodology alone. Design-based studies are distinguished by
their prima facie credibility and by the attention investigators devote to making
both an institutional and a data-driven case for causality.
Accounting for the origins of the credibility revolution in empirical economics
is like trying to chart the birth of rock and roll. Early influences are many, and
every fan has a story. But from the trenches of empirical labor economics, we see
an important impetus for better designs and more randomized trials coming from
studies questioning the reliability of econometric evaluations of subsidized government training programs. A landmark here is Lalonde (1986), who compared the
results from an econometric evaluation of the National Supported Work demonstration with those from a randomized trial. The econometric results typically differed
quite a bit from those using random assignment. Lalonde argued that there is little
reason to believe that statistical comparisons of alternative models (specification
testing) would point a researcher in the right direction. Two observational studies
of training effects foreshadowed the Lalonde results: Ashenfelter (1978) and
Ashenfelter and Card (1985), using longitudinal data to evaluate federal training
programs without the benefit of a quasi-experimental research design, found it
difficult to construct specification-robust estimates. Ashenfelter (1987) concluded
that randomized trials are the way to go.
Younger empiricists also began to turn increasingly to quasi-experimental
designs, often exploiting variation across U.S. states to get at causal relationships
in the fields of labor and public finance. An early example of work in this spirit
is Solon (1985), who estimated the effects of unemployment insurance on the
duration of unemployment spells by comparing the change in job-finding rates in
states that had recently tightened eligibility criteria for unemployment insurance,
to the change in rates in states that had not changed their rules. Gruber’s (1994)
influential study of the incidence of state-mandated maternity benefits applies a
similar idea to a public finance question. Angrist (1990) and Angrist and Krueger
(1991) illustrated the value of instrumental variables identification strategies in
studies of the effects of Vietnam-era military service and schooling on earnings.
Meyer’s (1995) methodological survey made many applied microeconomists aware
of the quasi-experimental tradition embodied in venerable texts on social science

6

Journal of Economic Perspectives

research methods by Campbell and Stanley (1963) and Cook and Campbell (1979).
These texts, which emphasize research design and threats to validity, were well
known in some disciplines, but distinctly outside the econometric canon.1
In this essay, we argue that a clear-eyed focus on research design is at the
heart of the credibility revolution in empirical economics. We begin with an overview of Leamer’s (1983) critique and his suggested remedies, based on concrete
examples of that time. We then turn to the key factors we see contributing to
improved empirical work, including the availability of more and better data, along
with advances in theoretical econometric understanding, but especially the fact
that research design has moved front and center in much of empirical micro. We
offer a brief digression into macroeconomics and industrial organization, where
progress—by our lights—is less dramatic, although there is work in both fields
that we find encouraging. Finally, we discuss the view that the design pendulum
has swung too far. Critics of design-driven studies argue that in pursuit of clean
and credible research designs, researchers seek good answers instead of good questions. We briefly respond to this concern, which worries us little.

The Leamer Critique and His Proposed Remedies
Naive Regressions and Extreme Bounds Analysis
Leamer (1983) presented randomized trials—a randomized evaluation of
fertilizer, to be specific—as an ideal research design. He also argued that randomized experiments differ only in degree from nonexperimental evaluations of causal
effects, the difference being the extent to which we can be confident that the causal
variable of interest is independent of confounding factors. We couldn’t agree more.
However, Leamer went on to suggest that the best way to use nonexperimental
data to get closer to the experimental ideal is to explore the fragility of nonexperimental estimates. Leamer did not advocate doing randomized trials or, for that
matter, looking for credible natural experiments.
The chief target of Leamer’s (1983) essay was naive regression analysis. In fact,
none of the central figures in the Leamer-inspired debate had much to say about
research design. Rather, these authors (like McAleer, Pagan, and Volker, 1985, and
Cooley and LeRoy, 1986, among others) appear to have accepted the boundaries of
established econometric practice, perhaps because they were primarily interested
in addressing traditional macroeconomic questions using time series data.
After making the tacit assumption that useful experiments are an unattainable
ideal, Leamer (1983, but see also 1978, 1985) proposed that the whimsical nature of
key assumptions in regression analysis be confronted head-on through a process of
1

Many of the applied studies mentioned here have been the subjects of critical re-examinations. This
back and forth has mostly been constructive. For example, in an influential paper that generated
wide-ranging methodological work, Bound, Jaeger, and Baker (1995) argue that the use of many weak
instrumental variables biases some of the estimates reported in Angrist and Krueger (1991). For a
recent discussion of weak instruments problems, see our book Angrist and Pischke (2009).

The Credibility Revolution in Empirical Economics

7

sensitivity analysis. Sims (1988) threw his weight behind this idea as well. The general
heading of sensitivity analysis features an explicitly Bayesian agenda. Recognizing
the severe demands of Bayesian orthodoxy, such as a formal specification of priors
and their incorporation into an elaborate multivariate framework, Leamer also
argued for a more ad hoc but intuitive approach called “extreme bounds analysis.”
In a nutshell, extreme bounds analysis amounts to the estimation of regressions with
many different sets of covariates included as controls; practitioners of this approach
are meant to report a range of estimates for the target parameter.
The Deterrent Effect of Capital Punishment
We sympathize with Leamer’s (1983) view that much of the applied econometrics of the 1970s and early 1980s lacked credibility. To make his point, and
to illustrate the value of extreme bounds analysis, Leamer picked an inquiry into
whether capital punishment deters murder. This question had been analyzed in a
series of influential papers by Isaac Ehrlich, one exploiting time series variation
(Ehrlich, 1975a) and one using cross sections of states (Ehrlich, 1977b). Ehrlich
concluded that the death penalty had a substantial deterrent effect. Leamer (1983)
did not try to replicate Ehrlich’s work, but reported on an independent time-series
investigation of the deterrence hypothesis using extreme bounds analysis, forcefully arguing that the evidence for deterrence is fragile at best (although Ehrlich
and Liu, 1999, disputed this).
It’s hard to exaggerate the attention this topic commanded at the time. The
U.S. Supreme Court decision in Furman v. Georgia (408 U.S. 153 [1972]) had
created a de facto moratorium on the death penalty. This moratorium lasted until
Gregg v. Georgia (428 U.S. 153 [1976]), at which time the high court decided that
the death penalty might be allowable if capital trials were bifurcated into separate
guilt–innocence and sentencing phases. Gary Gilmore was executed not long after,
in January 1977. Part of the intellectual case for restoration of capital punishment
was the deterrent effect (against a backdrop of high and increasing homicide rates
at that time). Indeed, the U.S. Supreme Court cited Ehrlich’s (1975a) paper in its
Gregg v. Georgia decision reinstating capital punishment.
Ehrlich’s work was harshly criticized by a number of contemporaries in addition to Leamer, most immediately Bowers and Pierce (1975) and Passell and Taylor
(1977). Ehrlich’s results appeared to be sensitive to changes in functional form,
inclusion of additional controls, and especially to changes in sample. Specifically,
his finding of a significant deterrent effect seemed to depend on observations from
the 1960s. The critics argued that the increase in murder rates in the 1960s may
have been driven by factors other than the sharp decline in the number of executions during this period. Ehrlich (1975b, 1977a) disputed the critics’ claims about
functional form and argued that the 1960s provided useful variation in executions
that should be retained.
Ehrlich’s contemporaneous critics failed to hit on what we think of as the most
obvious flaw in Ehrlich’s analysis. Like other researchers studying deterrent effects,
Ehrlich recognized that the level of the murder rate might affect the number of

8

Journal of Economic Perspectives

executions as well as vice versa and that his results might be biased by omitted variables (especially variables with a strong trend). Ehrlich sought to address problems
of reverse causality and omitted variables bias by using instrumental variables in a
two-stage least squares procedure. He treated the probabilities of arrest, conviction,
and execution as endogenous in a simultaneous-equations set-up. His instrumental
variables were lagged expenditures on policing, total government expenditure,
population, and the fraction of the population nonwhite. But Ehrlich did not
explain why these are good instruments, or even how and why these variables are
correlated with the right-hand-side endogenous variables.2
Ehrlich’s work on capital punishment seems typical of applied work in the
period about which Leamer (1983) was writing. Most studies of this time used fairly
short time series samples with strong trends common to both dependent and independent variables. The use of panel data to control for year and fi xed effects—even
panels of U.S. states—was still rare. The use of instrumental variables to uncover
causal relationships was typically mechanical, with little discussion of why the
instruments affected the endogenous variables of interest or why they constitute
a “good experiment.” In fact, Ehrlich was ahead of many of his contemporaries in
that he recognized the need for something other than naive regression analysis. In
our view, the main problem with Ehrlich’s work was the lack of a credible research
design. Specifically, he failed to isolate a source of variation in execution rates that
is likely to reveal causal effects on homicide rates.
The Education Production Function
Other examples of poor research design from this time period come from the
literature on education production. This literature (surveyed in Hanushek, 1986)
is concerned with the causal effect of school inputs, such as class size or per-pupil
expenditure, on student achievement. The systematic quantitative study of school
inputs was born with the report by Coleman et al. (1966), which (among other
things) used regression techniques to look at the proportion of variation in student
outputs that can be accounted for in an R 2 sense by variation in school inputs.
Surprisingly to many at the time, the Coleman report found only a weak association
between school inputs and achievement. Many subsequent regression-based studies
replicated this finding.
The Coleman Report was one of the first investigations of education production in a large representative sample. It is also distinguished by sensitivity analysis,
in that it discusses results from many specifications (with and without controls
for family background, for example). The problem with the Coleman report
and many of the studies in this mold that followed is that they failed to separate
variation in inputs from confounding variation in student, school, or community
characteristics. For example, a common finding in the literature on education

2

Ehrlich’s (1977b) follow-up cross-state analysis did not use two-stage least squares. In later work,
Ehrlich (1987, 1996) discussed his choice of instruments and the associated identification problems
at greater length.

Joshua D. Angrist and Jörn-Steffen Pischke

9

production is that children in smaller classes tend to do worse on standardized
tests, even after controlling for demographic variables. This apparently perverse
finding seems likely to be at least partly due to the fact that struggling children
are often grouped into smaller classes. Likewise, the relationship between school
spending and achievement is confounded by the fact that spending is often highest
in a mixture of wealthy districts and large urban districts with struggling minority
students. In short, these regressions suffer from problems of reverse causality and
omitted variables bias.
Many education production studies from this period also ignored the fact
that inputs like class size and per-pupil expenditure are inherently linked. Because
smaller classes cannot be had without spending more on teachers, it makes little
sense to treat total expenditure (including teacher salaries) as a control variable
when estimating the causal effect of class size (a point noted by Krueger, 2003).
Finally, the fact that early authors in the education production literature explored
many alternative models was not necessarily a plus. In what was arguably one of
the better studies of the period, Summers and Wolfe (1977) report only the final
results of an exhaustive specification search in their evaluation of the effect of
school resources on achievement. To their credit, Summers and Wolfe describe the
algorithm that produced the results they chose to report, and forthrightly caution
(p. 642) that “the data have been mined, of course.” As we see it, however, the main
problem with this literature is not data mining, but rather the weak foundation for
a causal interpretation of whatever specification authors might have favored.
Other Empirical Work in the Age of Heavy Metal
The 1970s and early 1980s saw rapid growth in mainframe computer size and
power. Stata had yet to appear, but magnetic tape jockeys managed to crunch
more and more numbers in increasingly elaborate ways. For the most part,
however, increased computing power did not produce more credible estimates.
For example, the use of randomized trials and quasi-experiments to study education production was rare until fairly recently (a history traced in Angrist, 2004).
Other areas of social science saw isolated though ambitious efforts to get at key
economic relationships using random assignment. A bright spot was the RAND
Health Insurance Experiment, initiated in 1974 (Manning, Newhouse, Duan,
Keeler, and Leibowitz, 1987). This experiment looked at the effects of deductibles and copayments on health care usage and outcomes. Unfortunately, many
of the most ambitious (and expensive) social experiments were seriously flawed:
the Seattle/Denver and Gary Income Maintenance Experiments, in which the
government compared income-support plans modeled on Milton Friedman’s idea
of a negative income tax, were compromised by sample attrition and systematic
income misreporting (Ashenfelter and Plant, 1990; Greenberg and Halsey, 1983).
This fact supports Leamer’s (1983) contention that the difference between a
randomized trial and an observational study is one of degree. Indeed, we would
be the first to admit that a well-done observational study can be more credible
and persuasive than a poorly executed randomized trial.

10

Journal of Economic Perspectives

There was also much to complain about in empirical macroeconomics. An
especially articulate complaint came from Sims (1980), who pointed out that
macroeconomic models of that time, typically a system of simultaneous equations,
invoked identification assumptions (the division of variables into those that are
jointly determined and exogenous) that were hard to swallow and poorly defended.
As an alternative to the simultaneous equations framework, Sims suggested the
use of unrestricted vector autoregressions (VARs) to describe the relation between
a given set of endogenous variables and their lags. But Sims’s complaint did not
generate the same kind of response that grew out of concerns about econometric
program evaluation in the 1980s among labor economists. Macroeconomists
circled their wagons but did not mobilize an identification posse.
Sims’s argument came on the heels of a closely related and similarly influential
stab at the heart of empirical macro known as the Lucas critique. Lucas (1976) and
Kydland and Prescott (1977) argued via theoretical examples that in a world with
forward-looking optimizing agents, nothing can be learned from past policy changes.
Lucas held out the hope that we might instead try to recover the empirical response
to changes in policy rules by estimating the structural parameters that lie at the root
of economic behavior, such as those related to technology or preferences (Lucas saw
these parameters as stable or at least policy invariant). But Kydland and Prescott—
invoking Lucas—appeared willing to give up entirely on conventional empirical work
(1977, p. 487): “If we are not to attempt to select policy optimally, how should it be
selected? Our answer is, as Lucas (1976) proposed, that economic theory be used to
evaluate alternative policy rules and that one with good operating characteristics be
selected.” This view helped to lay the intellectual foundations for a sharp turn toward
theory in macro, though often informed by numbers via “calibration.”
Our overview of empirical work in the Leamer era focuses on shortcomings.
But we should also note that the best applied work from the 1970s and early 1980s
still holds up today. A well-cited example is Feldstein and Horioka (1980), which
argues that the strong link between domestic savings and investment weighs against
the notion of substantial international capital mobility. The Feldstein and Horioka
study presents simple evidence in favor of a link between domestic savings and
investment, discusses important sources of omitted variables bias and simultaneity
bias in these estimates, and tries to address these concerns. Obstfeld’s (1995)
extensive investigation of the Feldstein and Horioka (1980) framework essentially
replicates their findings for a later and longer period.

Why There’s Less Con in Econometrics Today
Improvements in empirical work have come from many directions. Better data
and more robust estimation methods are part of the story, as is a reduced emphasis
on econometric considerations that are not central to a causal interpretation of the
main findings. But the primary force driving the credibility revolution has been a
vigorous push for better and more clearly articulated research designs.

The Credibility Revolution in Empirical Economics

11

Better and More Data
Not unusually for the period, Ehrlich (1975a) analyzed a time series of 35 annual
observations. In contrast, Donohue and Wolfers (2005) investigate the capital punishment question using a panel of U.S. states from 1934 to 2000, with many more years
and richer within-state variation due to the panel structure of the data. Better data
often engenders a fresh approach to long-standing research questions. Grogger’s
(1990) investigation of the deterrent effect of executions on daily homicide rates,
inspired by sociologist Phillips (1980), is an example.3 Farther afield, improvements
have come from a rapidly expanding reservoir of micro data in many countries. The
use of administrative records has also grown.
Fewer Distractions
Bower’s and Pierce (1975) devoted considerable attention to Ehrlich’s (1975a)
use of the log transformation, as well as to his choice of sample period. Passell and
Taylor (1977) noted the potential for omitted variables bias, but worried as much
about F-tests
-tests for temporal homogeneity and logs. The methodological appendix
to Ehrlich’s (1977b) follow-up paper discusses the possibility of using a Box–Cox
transformation to implement a flexible functional form, tests for heteroskedasticity,
and uses generalized least squares. Ehrlich’s (1975b) reply to Bowers and Pierce
focused on the statistical significance of trend terms in samples of different
lengths, differences in computational procedures related to serial correlation, and
evidence for robustness to the use of logs. Ehrlich’s (1977a) reply to Passell covers
the sample period and logs, though he also reports some of his (1977b) cross-state
estimates. Ehrlich’s rejoinders devoted little attention to the core issue of whether
the sources of variation in execution used by his statistical models justify a causal
interpretation of his estimates, but Ehrlich’s contemporaneous critics did not hit
this nail on the head either. Even were the results insensitive to the sample, the
same in logs and levels, and the residuals independent and identically distributed,
we would remain unsatisfied. In the give and take that followed Ehrlich’s original
paper, the question of instrument validity rarely surfaced, while the question of
omitted variables bias took a back seat to concerns about sample break points and
functional form.4
As in the exchange over capital punishment, others writing at about the same
time often seemed distracted by concerns related to functional form and generalized least squares. Today’s applied economists have the benefit of a less dogmatic
understanding of regression analysis. Specifically, an emerging grasp of the sense
in which regression and two-stage least squares produce average effects even
when the underlying relationship is heterogeneous and/or nonlinear has made
3

The decline in the use of time series and the increase in the use of panel data and researcher-originated data are documented for the field of labor economics in Table 1 of Angrist and Krueger (1999).
4
Hoenack and Weiler’s (1980) critical re-examination of Ehrlich (1975a) centered on identification problems, but the alternative exclusion restrictions Hoenack and Weiler proposed were offered
without much justification and seem just as hard to swallow as Ehrlich’s (for example, the proportion
nonwhite is used as an instrument).

12

Journal of Economic Perspectives

functional form concerns less central. The linear models that constitute the workhorse of contemporary empirical practice usually turn out to be remarkably robust,
a feature many applied researchers have long sensed and that econometric theory
now does a better job of explaining.5 Robust standard errors, automated clustering,
and larger samples have also taken the steam out of issues like heteroskedasticity
and serial correlation. A legacy of White’s (1980a) paper on robust standard errors,
one of the most highly cited from the period, is the near death of generalized
least squares in cross-sectional applied work. In the interests of replicability, and
to reduce the scope for errors, modern applied researchers often prefer simpler
estimators though they might be giving up asymptotic efficiency.
Better Research Design
Leamer (1983) led his essay with the idea that experiments—specifically,
randomized trials—provide a benchmark for applied econometrics. He was not
alone among econometric thought leaders of the period in this view. Here is Zvi
Griliches (1986, p. 1466) at the beginning of a chapter on data in The Handbook of
Econometrics:: “If the data were perfect, collected from well-designed randomized
experiments, there would hardly be room for a separate field of econometrics.”
Since then, empirical researchers in economics have increasingly looked to the
ideal of a randomized experiment to justify causal inference. In applied micro
fields such as development, education, environmental economics, health, labor,
and public finance, researchers seek real experiments where feasible, and useful
natural experiments if real experiments seem (at least for a time) infeasible. In
either case, a hallmark of contemporary applied microeconometrics is a conceptual framework that highlights specific sources of variation. These studies can be
said to be design based in that they give the research design underlying any sort of
study the attention it would command in a real experiment.
The econometric methods that feature most prominently in quasi-experimental studies are instrumental variables, regression discontinuity methods, and
differences-in-differences-style policy analysis. These econometric methods are not
new, but their use has grown and become more self-conscious and sophisticated
since the 1970s. When using instrumental variables, for example, it’s no longer
enough to mechanically invoke a simultaneous equations framework, labeling
some variables endogenous and others exogenous, without substantially justifying
the exclusion restrictions and as-good-as-randomly-assigned assumptions that
make instruments valid. The best of today’s design-based studies make a strong
institutional case, backed up with empirical evidence, for the variation thought to
generate a useful natural experiment.

5

For this view of regression, see, for example, White (1980b), Chamberlain’s (1984) chapter in the Handbook of Econometrics, Goldberger’s (1991) econometrics text, or our book Angrist and Pischke (2009) for
a recent take. Angrist and Imbens (1995) show how conventional two-stage least squares estimates can
be interpreted as an average causal effect in models with nonlinear and heterogeneous causal effects.

Joshua D. Angrist and Jörn-Steffen Pischke

13

The Card and Krueger (1992a, b) school quality studies illustrate this and
arguably mark a turning point in the literature on education production. The
most important problem in studies of school quality is omitted variables bias. On
one hand, students who attend better-resourced schools often end up in those
schools by virtue of their ability or family background, while on the other, weaker
students may receive disproportionately more inputs (say, smaller classes). Card
and Krueger addressed this problem by focusing on variation in resources at the
state-of-birth-by-cohort level, which they link to the economic returns to education
estimated at the same level. For example, they used Census data to compare the
returns to education for residents of Northern states educated in the North with
the returns to education for residents of Northern states educated in more poorly
resourced Southern schools.
schools.
The Card and Krueger papers show that the economic returns to schooling
are higher for those from states and cohorts with more resources (controlling
for cohort and state fi xed effects and for state of residence). They implicitly use
state-level variation in education spending as a natural experiment: aggregation of individual data up to the cohort/state level is an instrumental variables
procedure where the instruments are state-of-birth and cohort dummy variables.
(In Angrist and Pischke, 2009, we show why aggregation in this way works as an
instrumental variable.) State-by-cohort variation in the returns to schooling is
unlikely to be driven by selection or sorting, because individuals do not control
these variables. State-by-cohort variation in school resources also appears unrelated to omitted factors such as family background. Finally, Card and Krueger
took advantage of the fact that school resources increased dramatically in the
South when the Southerners in their sample were school age. The Card and
Krueger school quality studies are not bulletproof (Heckman, Layne-Farrar, and
Todd, 1996 offer a critique), but their findings on class size (the strongest set of
results in Card and Krueger, 1992a) have been replicated in other studies with
good research designs.
Angrist and Lavy (1999) illustrate the regression discontinuity research design
in a study of the effects of class size on achievement. The regression discontinuity
approach can be used when people are divided into groups based on a certain
cutoff score, with those just above or just below the cutoff suddenly becoming
eligible for a different treatment. The Angrist–Lavy research design is driven by the
fact that class size in Israel is capped at 40, so a cohort of 41 is usually split into two
small classes, while a cohort of 39 is typically left in a single large class. This leads
to a series of notional experiments: comparisons of schools with enrollments just
above and below 40, 80, or 120, in which class sizes vary considerably. In this setting,
schools with different numbers of students may be quite similar in other characteristics. Thus, as school enrollment increases, a regression capturing the relationship
between number of students and academic achievement should show discontinuities at these break points. The Angrist–Lavy design is a version of what is known as
the “fuzzy” regression discontinuity design, in which the fuzziness comes from the
fact that class size is not a deterministic function of the kinks or discontinuities in

14

Journal of Economic Perspectives

the enrollment function. Regression discontinuity estimates using Israeli data show
a marked increase in achievement when class size falls.6
The key assumption that drives regression discontinuity estimation of causal
effects is that individuals are otherwise similar on either side of the discontinuity
(or that any differences can be controlled using smooth functions of the enrollment rates, also known as the “running variable,” that determine the kink points).
In the Angrist–Lavy study, for example, we would like students to have similar
family backgrounds when they attend schools with grade enrollments of 35–39 and
41–45. One test of this assumption, illustrated by Angrist and Lavy (and Hoxby,
2000) is to estimate effects in an increasingly narrow range around the kink points;
as the interval shrinks, the jump in class size stays the same or perhaps even grows,
but the estimates should be subject to less and less omitted variables bias. Another
test, proposed by McCrary (2008), looks for bunching in the distribution of student
background characteristics around the kink. This bunching might signal strategic
behavior—an effort by some families, presumably not a random sample, to sort
themselves into schools with smaller classes. Finally, we can simply look for differences in mean pre-treatment characteristics around the kink.
In a recent paper, Urqiola and Verhoogen (2009) exploit enrollment cutoffs
like those used by Angrist and Lavy in a sample from Chile. The Chilean data
exhibit an enticing first stage, with sharp drops (discontinuities) in class size at
the cutoffs (multiples of 45). But household characteristics also differ considerably across these same kinks, probably because the Chilean school system, which is
mostly privatized, offers both opportunities and incentives for wealthier students
to attend schools just beyond the cutoffs. The possibility of such a pattern is an
important caution for users of regression discontinuity methods, though Urqiola
and Verhoogen note that the enrollment manipulation they uncover for Chile is far
from ubiquitous and does not arise in the Angrist–Lavy study. A large measure of
the attraction of the regression discontinuity design is its experimental spirit and
the ease with which claims for validity of the design can be verified.
The last arrow in the quasi-experimental quiver is differences-in-differences,
probably the most widely applicable design-based estimator. Differences-in-differences policy analysis typically compares the evolution of outcomes in groups affected
more and less by a policy change. The most compelling differences-in-differencestype studies report outcomes for treatment and control observations for a period
long enough to show the underlying trends, with attention focused on how deviations from trend relate to changes in policy. Figure 1, from Donohue and Wolfers
(2005), illustrates this approach for the death penalty question. This figure plots
homicide rates in Canada and the United States for over half a century, indicating
6

Fuzzy regression discontinuity designs are most easily analyzed using instrumental variables. In the
language of instrumental variables, the relationship between achievement and kinks in the enrollment
function is the reduced form, while the change in class size at the kinks is the fi rst stage. The ratio
of reduced form to first-stage effects is an instrumental variable estimate of the causal effect of class
size on test scores. Imbens and Lemieux (2008) offer a practitioners’ guide to the use of regression
discontinuity designs in economics.

The Credibility Revolution in Empirical Economics

15

Figure 1
Homicide Rates and the Death Penalty in the United States and Canada
(U.S. and Canada rates on the left and right y-axes, respectively)
4

United States (left axis)
3

6

2

3

Death penalty in U.S.

Gregg decision

Bill C-168

Furman decision

9

Canada (right axis)
1

Abolished

Death penalty in Canada

Death penalty in U.S. reestablished

Homicides per 100,000 residents (Canada)

Homicides per 100,000 residents (U.S.)

12

Canadian death penalty abolished (some exceptions apply until 1998)

0

0

01

98

20

95

19

92

19

89

19

86

19

83

19

80

19

77

19

74

19

71

19

68

19

65

19

62

19

59

19

56

19

53

19

50

19

19

Source: Donohue and Wolfers (2005).

periods when the death penalty was in effect in the two countries. The point of the
figure is not to focus on Canada’s consistently lower homicide rate, but instead to
show that Canadian and U.S. homicide rates move roughly in parallel, suggesting
that America’s sharp changes in death penalty policy were of little consequence for
murder. The figure also suggests that the deterrent effect would have to be large to
be visible against the background noise of yearly fluctuations in homicide rates.
Paralleling the growth in quasi-experimental experiment designs, the number
and scope of real experiments has increased dramatically, with a concomitant
increase in the quality of experimental design, data collection, and statistical
analysis. While 1970s-era randomized studies of the negative income tax were
compromised by misreporting and differential attrition in treatment and control
groups, researchers today give these concerns more attention and manage them
more effectively. Such problems are often solved by a substantial reliance on
administrative data, and a more sophisticated interpretation of survey data when
administrative records are unavailable.
A landmark randomized trial related to education production is the Tennessee
STAR experiment. In this intervention, more than 10,000 students were randomly
assigned to classes of different sizes from kindergarten through third grade. Like
the negative income tax experiments, the STAR experiment had its flaws. Not all
subjects contributed follow-up data and some self-selected into smaller classes after
random assignment. A careful analysis by Krueger (1999), however, shows clear

16

Journal of Economic Perspectives

evidence of achievement gains in smaller classes, even after taking attrition and
self-selection into account.7
Economists are increasingly running their own experiments as well as
processing the data from experiments run by others. A recent randomized trial
of a microfinance scheme, an important policy tool for economic development, is
an ambitious illustration (Banerjee, Duflo, Glennerster, and Kinnan, 2009). This
study evaluates the impact of offering small loans to independent business owners
living in slums in India. The Banerjee et al. study randomizes the availability of
microcredit across over 100 Indian neighborhoods, debunking the claim that realistic and relevant policy interventions cannot be studied with random assignment.
With the growing focus on research design, it’s no longer enough to adopt
the language of an orthodox simultaneous equations framework, labeling some
variables endogenous and others exogenous, without offering strong institutional
or empirical support for these identifying assumptions. The new emphasis on a
credibly exogenous source of variation has also filtered down to garden-variety
regression estimates, in which researchers are increasingly likely to focus on sources
of omitted variables bias, rather than a quixotic effort to uncover the “true model”
generating the data.8
More Transparent Discussion of Research Design
Over 65 years ago, Haavelmo submitted the following complaint to the readers
of Econometrica (1944, p. 14): “A design of experiments (a prescription of what the
physicists call a ‘crucial experiment’) is an essential appendix to any quantitative
theory. And we usually have some such experiment in mind when we construct the
theories, although—unfortunately—most economists do not describe their design
of experiments explicitly.”
In recent years, the notion that one’s identification strategy—in other words,
research design—must be described and defended has filtered deeply into empirical practice. The query “What’s your identification strategy?” and others like it are
now heard routinely at empirical workshops and seminars. Evidence for this claim
comes from the fact that a full text search for the terms “empirical strategy,” “identification strategy,” “research design,” or “control group” gets only 19 hits in Econlit
from 1970–1989, while producing 742 hits from 1990–2009. We acknowledge that
just because the author uses the term “research design” does not mean that he
or she has a good one! Moreover, some older studies incorporate quality designs
7
A related development at the forefront of education research is the use of choice lotteries as a
research tool. In many settings where an educational option is over-subscribed, allocation among
applicants is by lottery. The result is a type of institutional random assignment, which can then be
used to study school vouchers, charter schools, and magnet schools (for example, Rouse, 1998, who
looks at vouchers).
8
The focus on omitted variables bias is reflected in a burgeoning literature using matching and the
propensity score as an alternative (or complement) to regression. In the absence of random assignment, such strategies seek to eliminate observable differences between treatment and control groups,
with little or no attention devoted to modeling the process determining outcomes. See Imbens and
Wooldridge (2009) for an introduction.

Joshua D. Angrist and Jörn-Steffen Pischke

17

without using today’s language. Still, the shift in emphasis is dramatic and reflects
a trend that’s more than semantic.
Good designs have a beneficial side effect: they typically lend themselves to
a simple explanation of empirical methods and a straightforward presentation of
results. The key findings from a randomized experiment are typically differences
in means between treatment and controls, reported before treatment (to show
balance) and after treatment (to estimate causal effects). Nonexperimental results
can often be presented in a manner that mimics this, highlighting specific contrasts.
The Donohue and Wolfers (2005) differences-in-differences study mentioned
above illustrates this by focusing on changes in American law as a source of quasiexperimental variation and documenting the parallel evolution of outcomes in
treatment and control groups in a comparison of the United States and Canada.
Whither Sensitivity Analysis?
Responding to what he saw as the fragility of naive regression analysis, Leamer
(1983) proposed extreme bounds analysis, which focuses on the distribution of
results generated by a variety of specifications. An extreme version of extreme
bounds analysis appears in Sala-i-Martin’s (1997) paper reporting two million
regressions related to economic growth. Specifically, in a variation on a procedure
first proposed in this context by Levine and Renelt (1992), Sala-i-Martin computes
two million of the many possible growth regressions that can be constructed
from 62 explanatory variables. He retains a fi xed set of three controls (GDP, life
expectancy, and the primary school enrollment rate in 1960), leaving 59 possible
“regressors of interest.” From these 59, sets of three additional controls are chosen
from 58 while the 59th is taken to be the one of interest. This process is repeated
until every one of the 59 possible regressors of interest has played this role in
equations with all possible sets of three controls, generating 30,857 regressions
per regressor of interest. The object of this exercise is to see which variables are
robustly significant across specifications.
Sala-i-Martin’s (1997) investigation of extreme bounds analysis must have been
fun. Happily, however, this kind of agnostic specification search has not emerged as
a central feature of contemporary empirical work. Although Sala-i-Martin succeeds
in uncovering some robustly significant relations (the “fraction of the population
Confucian” is a wonderfully robust predictor of economic growth), we don’t see
why this result should be taken more seriously than the naive capital punishment
specifications criticized by Leamer. Are these the right controls? Are six controls
enough? How are we to understand sources of variation in one variable when the
effects of three others, arbitrarily chosen, are partialed out? Wide-net searches of
this kind offer little basis for a causal interpretation.
Design-based studies typically lead to a focused and much narrower specification analysis, targeted at specific threats to validity. For example, when considering
results from a randomized trial, we focus on the details of treatment assignment
and the evidence for treatment-control balance in pre-treatment variables. When
using instrumental variables, we look at whether the instrument might have causal

18

Journal of Economic Perspectives

effects on the outcome in ways other than through the channel of interest (in simultaneous equations lingo, this is an examination of the exclusion restriction). With
differences-in-differences, we look for group-specific trends, since such trends can
invalidate a comparison of changes across groups. In a regression discontinuity
design, we look at factors like bunching at the cutoff point, which might suggest
that the cutoff directly influenced behavior. Since the nature of the experiment is
clear in these designs, the tack we should take when assessing validity is also clear.

Mad About Macro
In an essay read to graduating University of Chicago economics students in
1988, Robert Lucas (1988) described what, as he sees it, economists do. Lucas used
the specific question of the connection between monetary policy and economic
depression to frame his discussion, which is very much in the experimentalist spirit:
“One way to demonstrate that I understand this connection—I think the only really
convincing way—would be for me to engineer a depression in the United States by
manipulating the US money supply.”
Ruling out such a national manipulation as immoral, Lucas (1988) describes
how to create a depression by changing the money supply at Kennywood Park,
an amusement park near Pittsburgh that is distinguished by stunning river views,
wooden roller coasters, and the fact that it issues its own currency. Lucas’s story is
evocative and compelling (the Kennywood allegory is a version of Lucas, 1973). We’re
happy to see a macroeconomist of Lucas’s stature use an experimental benchmark
to define causality and show a willingness to entertain quasi-experimental evidence
on the effects of a change in the money supply. Yet this story makes us wonder why
the real world of empirical macro rarely features design-based research.
Many macroeconomists have abandoned traditional empirical work entirely,
focusing instead on “computational experiments,” as described in this journal by
Kydland and Prescott (1996). In a computational experiment, researchers choose
a question, build a (theoretical) model economy, “calibrate” the model so that its
behavior mimics the real economy along some key statistical dimensions, and then
run a computational experiment by changing model parameters (for example,
tax rates or the money supply rule) to address the original question. The last two
decades have seen countless studies in this mold, often in a dynamic stochastic
general equilibrium framework. Whatever might be said in defense of this framework as a tool for clarifying the implications of economic models, it produces no
direct evidence on the magnitude or existence of causal effects. An effort to put
reasonable numbers on theoretical relations is harmless and may even be helpful.
But it’s still theory.
Some rays of sunlight poke through the grey clouds of dynamic stochastic
general equilibrium. One strand of empirical macro has turned away from
modeling outcome variables such as GDP growth, focusing instead on the isolation
of useful variation in U.S. monetary and fiscal policy. A leading contribution here

The Credibility Revolution in Empirical Economics

19

is Romer and Romer (1989), who, in the spirit of Friedman and Schwartz (1963),
review the minutes of Federal Reserve meetings and try to isolate events that look
like good monetary policy “experiments.” Their results suggest that monetary
contractions have significant and long-lasting effects on the real economy. Later, in
Romer and Romer (2004), they produced similar findings for the effects of policy
shocks conditional on the Fed’s own forecasts.9
The Romers’ work is design based in spirit and, for the most part, in detail.
Although a vast literature models Federal Reserve decision making, until recently,
surprisingly few studies have made an institutional case for policy experiments as
the Romers’ study does. Two recent monetary policy studies in the Romer spirit,
and perhaps even closer to the sort of quasi-experimental work we read and do, are
Richardson and Troost (2009), who exploit regional differences in Fed behavior
during the Depression to study liquidity effects, and Velde (2009), who describes the
results of an extreme monetary experiment much like the one Lucas envisioned (albeit
in eighteenth-century France). Romer and Romer (2007) use methods similar to those
they used for money to study fiscal policy, as do Ramey and Shapiro (1998) and Barro
and Redlick (2009), who investigate the effects of large fiscal shocks due to wars.
The literature on empirical growth has long suffered from a lack of imagination in research design, but here too the picture has recently improved. The
most influential design-based study in this area has probably been Acemoglu,
Johnson, and Robinson (2001), who argue that good political institutions are a key
ingredient in the recipe for growth, an idea growth economists have entertained
for many decades. The difficulty here is that better institutions might be a luxury
that richer countries can enjoy more easily, leading to a vexing reverse causality
problem. Acemoglu, Johnson, and Robinson (2001) try to overcome this problem
by using the differential mortality rates of European settlers in different colonies
as an instrument for political institutions in the modern successor countries. Their
argument goes: where Europeans faced high mortality rates, they couldn’t settle,
and where Europeans couldn’t settle, colonial regimes were more extractive, with
little emphasis on property rights and democratic institutions. Where European
immigrants could settle, they frequently tried to emulate the institutional set-up
of their home countries, with stronger property rights and more democratic
institutions. This approach leads to an instrumental variables strategy where the
instrument for the effect of institutions on growth is settler mortality.10
Acemoglu, Johnson, and Robinson (2001) are in the vanguard of promising
research on the sources of economic growth using a similar style. Examples include
Bleakley (2007), who looks at the effect of hookworm eradication on income in
the American South; and Rodrik and Wacziarg (2005) and Persson and Tabellini
9
Angrist and Kuersteiner (2007) implement a version of the Romer and Romer (2004) research design
using the propensity score and an identification argument cast in the language of potential outcomes
commonly used in microeconometric program evaluation.
10
Albouy (2008) raises concerns about the settler mortality data that Acemoglu, Johnson, and
Robinson (2001) used to construct instruments. See Acemoglu, Johnson, and Robinson (2006) for a
response to earlier versions of Albouy’s critique.

20

Journal of Economic Perspectives

(2008), who investigate interactions between democracy and growth using differences-in-differences type designs.
With these examples accumulating, macroeconomics seems primed for a wave
of empirical work using better designs. Ricardo Reis, a recently tenured macroeconomist at Columbia University, observed in the wake of the 2008 financial crisis:
“Macroeconomics has taken a turn towards theory in the last 10–15 years. Most
young macroeconomists are more comfortable with proving theorems than with
getting their hands on any data or speculating on current events.”11 The charge
that today’s macro agenda is empirically impoverished comes also from older
macro warhorses like Mankiw (2006) and Solow (2008). But the recent economic
crisis, fundamentally a macroeconomic and policy-related affair, has spawned
intriguing design-based studies of the crisis’s origins in the mortgage market (Keys,
Mukherjee, Seru, and Vig, 2010; Bubb and Kaufman, 2009). The theory-centric
macro fortress appears increasingly hard to defend.

Industrial Disorganization
An important question at the center of the applied industrial organization
agenda is the effect of corporate mergers on prices. One might think, therefore,
that studies of the causal effects of mergers on prices would form the core of a
vast micro-empirical literature, the way hundreds of studies in labor economics
have looked at union relative wage effects. We might also have expected a large
parallel literature evaluating merger policy, in the way that labor economists have
looked at the effect of policies like right-to-work laws. But it isn’t so. In a recent
review, Ashenfelter, Hosken, and Weinberg (2009) found only about 20 empirical
studies evaluating the price effects of consummated mergers directly; for example,
Borenstein (1990) compares prices on airline routes out of hubs affected to
differing degrees by mergers. Research on the aggregate effects of merger policy
seems to be even more limited; see the articles by Baker (2003) and Crandall and
Winston (2003) in this journal for a review and conflicting interpretations.
The dominant paradigm for merger analysis in modern academic studies,
sometimes called the “new empirical industrial organization,” is an elaborate
exercise consisting of three steps: The fi rst estimates a demand system for the
product in question, often using the discrete choice/differentiated products
framework developed by Berry, Levinsohn, and Pakes (1995). Demand elasticities are typically identified using instrumental variables for prices; often, the
instruments are prices in other markets (as in Hausman, 1996). Next, researchers
postulate a model of market conduct, say, Bertrand–Nash price-based competition between different brands or products. In the context of this model, the
firms’ efforts to maximize profits lead to a set of relationships between prices
11
As quoted by Justin Wolfers (2008) in his New York Times column “Freakonomics” (⟨http://
freakonomics.blogs.nytimes.com/2008/03/31/more-on-the-missing-macroeconomists/〉).

Joshua D. Angrist and Jörn-Steffen Pischke

21

and marginal costs for each product, with the link provided by the substitution
matrix estimated in the initial step. Finally, industry behavior is simulated with
and without the merger of interest.
Nevo (2000) uses this approach to estimate the effect of mergers on the
price of ready-to-eat breakfast cereals in a well-cited paper. Nevo’s study is distinguished by careful empirical work, attention to detail, and a clear discussion of
the superstructure of assumptions upon which it rests. At the same time, this
elaborate superstructure should be of concern. The postulated demand system
implicitly imposes restrictions on substitution patterns and other aspects of
consumer behavior about which we have little reason to feel strongly. The validity
of the instrumental variables used to identify demand equations—prices in other
markets—turns on independence assumptions across markets that seem arbitrary.
The simulation step typically focuses on a single channel by which mergers affect
prices—the reduction in the number of competitors—when at least in theory a
merger can lead to other effects like cost reductions that make competition tougher
between remaining producers. In this framework, it’s hard to see precisely which
features of the data drive the ultimate results.
Can mergers be analyzed using simple, transparent empirical methods that
trace a shorter route from facts to findings? The challenge for a direct causal analysis of mergers is to use data to describe a counterfactual world in which the merger
didn’t occur. Hastings (2004) does this in a study of the retail gasoline market. She
analyzes the takeover of independent Thrifty stations by large vertically integrated
station owner ARCO in California, with an eye to estimating the effects of this
merger on prices at Thrifty’s competitors. Hastings’ research design specifies a
local market for each station: treatment stations are near a Thrifty station, control
stations are not. She then compares prices around the time of the merger using a
straightforward differences-in-differences framework.
A drawback of the Hastings (2004) analysis is that it captures the effects of
a merger on Thrifty’s competitors, but not on the former Thrifty stations. Still, it
seems likely that anticompetitive effects would turn up at any station operating in
affected markets. We therefore see the Hastings approach as a fruitful change in
direction. Her estimates have clear implications for the phenomenon of interest,
while their validity turns transparently on the quality of the control group, an
issue that can be assessed using pre-merger observations to compare price trends.
Hastings’s paper illustrates the power of this approach by showing almost perfectly
parallel price trends for treatment and control stations in two markets (Los Angeles
and San Diego) in pre-treatment months, followed by a sharp uptick in Thrifty
competitor pricing after the merger.12

12
As with most empirical work, Hastings’s (2004) analysis has its problems and her conclusions may
warrant qualification. Taylor, Kreisle, and Zimmerman (2007) fail to replicate Hastings’s findings
using an alternative data source. Here as elsewhere, however, a transparent approach facilitates replication efforts and constructive criticism.

22

Journal of Economic Perspectives

For policy purposes, of course, regulators must evaluate mergers before they
have occurred; design-based studies necessarily capture the effects of mergers after
the fact. Many new empirical industrial organization studies forecast counterfactual
outcomes based on models and simulations, without a clear foundation in experience. But should antitrust regulators favor the complex, simulation-based estimates
coming out of the new empirical industrial organization paradigm over a transparent analysis of past experience? At a minimum, we’d expect such a judgment to
be based on evidence showing that the simulation-based approach delivers reasonably accurate predictions. As it stands, the proponents of this work seem to favor it
as a matter of principle.
So who can you trust when it comes to antitrust? Direct Hastings (2004)–style
evidence, or structurally derived estimates as in Nevo (2000)? We’d be happy to
see more work trying to answer this question by contrasting credible quasi-experimental estimates with results from the new empirical industrial organization
paradigm. A pioneering effort in this direction is Hausman and Leonard’s (2002)
analysis contrasting “direct” (essentially, differences-in-differences) and “indirect”
(simulation-based) estimates of the equilibrium price consequences of a new brand
of toilet paper. They evaluate the economic assumptions underlying alternative
structural models (for example, Nash–Bertrand competition) according to whether
the resulting structural estimates match the direct estimates. This is reminiscent
of Lalonde’s (1986) comparison of experimental and nonexperimental training
estimates, but instead of contrasting model-based estimates with those from
a randomized trial, the direct estimates are taken to provide a benchmark that
turns on fewer assumptions than the structural approach. Hausman and Leonard
conclude that one of their three structural models produces estimates “reasonably
similar” to the direct estimates. Along the same lines, Peters (2006) looks at the
predictive value of structural analyses of airline mergers, and finds that structural
simulation methods yield poor predictions of post-merger ticket prices. Likewise,
Ashenfelter and Hosken (2008) compare differences-in-differences-type estimates
of the effects of the breakfast cereals merger to those reported by Nevo (2000).
Ashenfelter and Hoskens conclude that transparently identified design-based
results differ markedly from those produced by the structural approach.
A good structural model might tell us something about economic mechanisms
as well as causal effects. But if the information about mechanisms is to be worth
anything, the structural estimates should line up with those derived under weaker
assumptions. Does the new empirical industrial organization framework generate
results that match credible design-based results? So far, the results seem mixed at
best. Of course, the question of which estimates to prefer turns on the quality of
the relevant quasi-experimental designs and our faith in the ability of a more elaborate theoretical framework to prop up a weakly identified structural model. We
find the empirical results generated by a good research design more compelling
than the conclusions derived from a good theory, but we also hope to see industrial
organization move towards stronger and more transparent identification strategies
in a structural framework.

The Credibility Revolution in Empirical Economics

23

Has the Research Design Pendulum Swung Too Far?
The rise of the experimentalist paradigm has provoked a reaction, as revolutions do. The first counterrevolutionary charge raises the question of external
validity—the concern that evidence from a given experimental or quasi-experimental research design has little predictive value beyond the context of the original
experiment. The second charge is that experimentalists are playing small ball while
big questions go unanswered.
External Validity
A good research design reveals a particular truth, but not necessarily the
whole truth. For example, the Tennessee STAR experiment reduced class sizes
from roughly 25 to 15. Changes in this range need not reveal the effect of reductions from 40 students to 30. Similarly, the effects might be unique to the state
of Tennessee. The criticism here—made by a number of authors including
Heckman (1997); Rosenzweig and Wolpin (2000); Heckman and Urzua (2009);
and Deaton (2009)—is that in the quest for internal validity, design-based studies
have become narrow or idiosyncratic.
Perhaps it’s worth restating an obvious point. Empirical evidence on any given
causal effect is always local, derived from a particular time, place, and research
design. Invocation of a superficially general structural framework does not make
the underlying variation or setting more representative. Economic theory often
suggests general principles, but extrapolation of causal effects to new settings is
always speculative. Nevertheless, anyone who makes a living out of data analysis
probably believes that heterogeneity is limited enough that the well-understood
past can be informative about the future.
A constructive response to the specificity of a given research design is to look for
more evidence, so that a more general picture begins to emerge. For example, one of
us (Angrist) has repeatedly estimated the effects of military service, with studies of
veterans of World War II, the Vietnam era, the first Gulf War, and periods in between.
The cumulative force of these studies has some claim to external validity—that is,
they are helpful in understanding the effects of military service for those who served
in any period and therefore, hopefully, for those who might serve in the future.
In general, military service tends to depress civilian earnings, at least for whites, a
finding that is both empirically consistent and theoretically coherent. The primary
theoretical channel by which military service affects earnings is human capital,
particularly in the form of lost civilian experience. In a design-based framework,
economic theory helps us understand the picture that emerges from a constellation
of empirical findings, but does not help us paint the picture. For example, the human
capital story is not integral to the validity of instrumental variable estimates using
draft lottery numbers as instruments for Vietnam-era military service (as in Angrist,
1990). But human capital theory provides a framework that reconciles larger losses
early in a veteran’s career (when experience profiles tend to be steeper) with losses
dissipating after many years (as shown in Angrist and Chen, 2008).

24

Journal of Economic Perspectives

The process of accumulating empirical evidence is rarely sexy in the unfolding,
but accumulation is the necessary road along which results become more general
(Imbens, 2009, makes a similar point). The class size literature also illustrates this
process at work. Reasonably well-identified studies from a number of advanced countries, at different grade levels and subjects, and for class sizes ranging anywhere from
a few students to about 40, have produced estimates within a remarkably narrow
band (Krueger, 1999; Angrist and Lavy, 1999; Rivkin, Hanushek, and Kain, 2005;
Heinesen, forthcoming). Across these studies, a ten-student reduction in class size
produces about a 0.2 to 0.3 standard deviation increase in individual test scores.
Smaller classes do not always raise test scores, so the assessment of findings should
be qualified (see, for example, Hoxby, 2000). But the weight of the evidence suggests
that class size reductions generate modest achievement gains, albeit at high cost.
Applied micro fields are not unique in accumulating convincing empirical
findings. The evidence on the power of monetary policy to influence the macro
economy also seems reasonably convincing. As we see it, however, the most persuasive evidence on this point comes not from elaborate structural models, which
only tell us that monetary policy does or does not affect output depending on the
model, but from credible empirical research designs, as in some of the work we have
discussed. Not surprisingly, the channels by which monetary policy affects output
are less clear than the finding that there is an effect. Questions of why a given effect
appears are usually harder to resolve than the questions of whether it appears or
how large it is. Like most researchers, we have an interest in mechanisms as well as
causal effects. But inconclusive or incomplete evidence on mechanisms does not
void empirical evidence of predictive value. This point has long been understood
in medicine, where clinical evidence of therapeutic effectiveness has for centuries
run ahead of the theoretical understanding of disease.
Taking the “Econ” out of Econometrics too?
Related to the external validity critique is the claim that the experimentalist
paradigm leads researchers to look for good experiments, regardless of whether the
questions they address are important. In an engaging account in The New Republic,,
Scheiber (2007) argued that young economists have turned away from important
questions like poverty, inequality, and unemployment to study behavior on television game shows. Scheiber quotes a number of distinguished academic economists
who share this concern. Raj Chetty comments: “People think about the question
less than the method . . . so you get weird papers, like sanitation facilities in Native
American reservations.” James Heckman is less diplomatic: “In some quarters of our
profession, the level of discussion has sunk to the level of a New Yorker article.”
There is no shortage of academic triviality. Still, Scheiber’s (2007) critique
misses the mark because he equates triviality with narrowness of context. For
example, he picks on DellaVigna and Malmendier (2006), who look at the attendance and renewal decisions of health club members, and on Conlin, O’Donoghue,
and Vogelsang (2007), who study catalog sales of winter clothing. Both studies are
concerned with the behavioral economics notion of present-oriented biases, an

Joshua D. Angrist and Jörn-Steffen Pischke

25

issue with far-reaching implications for economic policy and theory. The market
for snow boots seems no less interesting in this context than any other retail
market, and perhaps more so if the data are especially good. We can look to these
design-based studies to validate the findings from more descriptive empirical work
on bigger-ticket items. For example, DellaVigna and Paserman (2005) look for
present-oriented biases in job search behavior.
In the empirical universe, evidence accumulates across settings and study
designs, ultimately producing some kind of consensus. Small ball sometimes wins
big games. In our field, some of the best research designs used to estimate labor
supply elasticities exploit natural and experimenter-induced variation in specific
labor markets. Oettinger (1999) analyzes stadium vendors’ reaction to wage changes
driven by changes in attendance, while Fehr and Goette (2007) study bicycle messengers in Zurich who, in a controlled experiment, received higher commission rates
for one month only. These occupations might seem small and specialized, but they
are no less representative of today’s labor market than the durable manufacturing
sector that has long been of interest to labor economists.
These examples also serve to refute the claim that design-based empirical
work focuses on narrow policy effects and cannot uncover theoretically grounded
structural parameters that many economists care about. Quasi-experimental labor
supply studies such as Oettinger (1999) and Fehr and Goette (2007) try to measure
the intertemporal substitution elasticity, a structural parameter that can be derived
from a stochastic dynamic framework. Labor demand elasticities, similarly structural, can also be estimated using quasi-experiments, as in Card (1990b), who
exploits real wage variation generated by partial indexation of union contracts.
Quasi-experimental empirical work is also well suited to the task of contrasting
competing economic hypotheses. The investigations of present-oriented biases
mentioned above focus on key implications of alternative models. In a similarly
theory-motivated study, Karlan and Zinman (2009) try to distinguish moral hazard
from adverse selection in the consumer credit market using a clever experimental
design involving two-stage randomization. First, potential borrowers were offered
different interest rates before they applied for loans. Their initial response to variation in interest rates is used to gauge adverse selection. Some of the customers who
took loans were then randomly given rates lower than the rates initially offered. This
variation is used to identify moral hazard in a sample where everyone has already
committed to borrow.
What about grand questions that affect the entire world or the march of history?
Nunn (2008) uses a wide range of historical evidence, including sailing distances
on common trade routes, to estimate the long-term growth effects of the African
slave trade. Deschênes and Greenstone (2007) use random year-to-year fluctuations
in temperature to estimate effects of climate change on energy use and mortality.
In a study of the effects of foreign aid on growth, Rajan and Subramanian (2008)
construct instruments for foreign aid from the historical origins of donor–recipient
relations. These examples and many more speak eloquently for the wide applicability
of a design-based approach. Good research designs complement good questions. At

26

Journal of Economic Perspectives

the same time, in favoring studies that feature good designs, we accept an incremental approach to empirical knowledge in which well-designed studies get the most
weight while other evidence is treated as more provisional.

Conclusion
Leamer (1983) drew an analogy between applied econometrics and classical
experimentation, but his proposal for the use of extreme bounds analysis to bring
the two closer is not the main reason why empirical work in economics has improved.
Improvement has come mostly from better research designs, either by virtue of
outright experimentation or through the well-founded and careful implementation
of quasi-experimental methods. Empirical work in this spirit has produced a credibility revolution in the fields of labor, public finance, and development economics
over the past 20 years. Design-based revolutionaries have notched many successes,
putting hard numbers on key parameters of interest to both policymakers and
economic theorists. Imagine what could be learned were a similar wave to sweep the
fields of macroeconomics and industrial organization.
■ We

thank Guido Imbens for suggesting this topic and for feedback; Daron Acemoglu, Olivier
Blanchard, John Donohue, Isaac Ehrlich, Glenn Ellison, Jeff Grogger, Radha Iyengar, Larry
Katz, Alan Krueger, Ethan Ilzetzki, Guido Lorenzoni, Albert Marcet, Aviv Nevo, Alan
Manning, Bruce Meyer, Parag Pathak, Gary Solon, Matt Weinberg, and Justin Wolfers for
helpful comments and discussions; and the JEP editors—David Autor, James Hines, Charles
Jones, and Timothy Taylor—for comments on earlier drafts. Remaining errors and omissions
are our own.

References
Acemoglu, Daron, Simon Johnson, and James
A. Robinson. 2001. “The Colonial Origins of
Comparative Development: An Empirical Investigation.” American Economic Review, 91(5): 1369–1401.
Acemoglu, Daron, Simon Johnson, and James
A. Robinson. 2006. “Reply to the Revised (May
2006) Version of David Albouy’s ‘The Colonial
Origins of Comparative Development: An Investigation of the Settler Mortality Data.’” Available at:
http://econ-www.mit.edu/faculty/acemoglu/paper.
Albouy, David Y. 2008. “The Colonial Origins
of Comparative Development: An Investigation
of the Settler Mortality Data.” NBER Working
Paper 14130.
Angrist, Joshua D. 1990. “Lifetime Earnings
and the Vietnam Era Draft Lottery: Evidence

from Social Security Administrative Records.”
American Economic Review, 80(3): 313–36.
Angrist, Joshua D. 2004. “Education Research
Changes Tack.” Oxford Review of Economic Policy,
20(2): 198–212.
Angrist, Joshua D., and Stacey Chen. 2008.
“Long-term Economic Consequences of VietnamEra Conscription: Schooling, Experience and
Earnings.” IZA Discussion Paper 3628.
Angrist, Joshua D., and Guido W. Imbens.
1995. “Two-Stage Least Squares Estimation of
Average Causal Effects in Models with Variable
Treatment Intensity.” Journal of the American Statistical Association, 90(430): 431–42.
Angrist, Joshua D., and Alan B. Krueger. 1991.
“Does Compulsory School Attendance Affect

The Credibility Revolution in Empirical Economics

Schooling and Earnings?” Quarterly Journal of
Economics, 106(4): 976–1014.
Angrist, Joshua D., and Alan B. Krueger.
1999. “Empirical Strategies in Labor Economics.”
In Handbook of Labor Economics, vol. 3, ed. O.
Ashenfelter and D. Card, 1277–1366. Amsterdam:
North-Holland.
Angrist, Joshua D., and Guido Kuersteiner.
2007. “Semiparametric Causality Tests Using
the Policy Propensity Score.” NBER Working
Paper 10975.
Angrist, Joshua D., and Victor Lavy. 1999.
“Using Maimonides’ Rule to Estimate the
Effect of Class Size on Scholastic Achievement.”
Quarterly Journal of Economics, 114(2): 533–75.
Angrist, Joshua D., and Jörn-Steffen Pischke.
2009. Mostly Harmless Econometrics: An Empiricists
Companion. Princeton: Princeton University Press.
Ashenfelter, Orley. 1978. “Estimating the
Effect of Training Programs on Earnings.” Review
of Economics and Statistics, 60(1): 47–57.
Ashenfelter, Orley. 1987. “The Case for
Evaluating Training Programs with Randomized
Trials.” Economics of Education Review, 6(4): 333–38.
Ashenfelter, Orley, and David Card. 1985.
“Using the Longitudinal Structure of Earnings to
Estimate the Effect of Training Programs.” Review
of Economics and Statistics, 67(4): 648–60.
Ashenfelter, Orley, and Daniel Hosken. 2008.
“The Effect of Mergers on Consumer Prices:
Evidence from Five Selected Case Studies.” NBER
Working Paper 13859.
Ashenfelter, Orley, Daniel Hosken, and
Matthew Weinberg. 2009. “Generating Evidence
to Guide Merger Enforcement?” NBER Working
Paper 14798.
Ashenfelter, Orley, and Mark W. Plant. 1990.
“Nonparametric Estimates of the Labor-Supply
Effects of Negative Income Tax Programs.”
Journal of Labor Economics, 8(1, Part 2): S396–S415.
Ayres, Ian. 2007. Super Crunchers. New York:
Bantam Books.
Baker, Jonathon B. 2003. “The Case for Antitrust Enforcement.” Journal of Economic Perspectives:
17(4): 27–50.
Banerjee, Abhijit, Esther Duflo, Rachel
Glennerster, and Cynthia Kinnan. 2009. “The
Miracle of Microfinance? Evidence from a
Randomized Evaluation.” Unpublished manuscript, MIT Department of Economics, May.
Barro, Robert J., and Charles J. Redlick.
2009. “Macroeconomic Effects from Government
Purchases and Taxes.” NBER Working Paper 15369.
Berry, Steven, James Levinsohn, and Ariel
Pakes. 1995. “Automobile Prices in Market Equilibrium.” Econometrica, 63(4): 841–90.
Bleakley, Hoyt. 2007. “Disease and

27

Development: Evidence from Hookworm Eradication in the American South.” Quarterly Journal
of Economics, 122(1): 73–117.
Borenstein, Severin. 1990. “Airline Mergers,
Airport Dominance, and Market Power.” American
Economic Review, 80(2): 400–404.
Bound, John, David Jaeger, and Regina Baker.
1995. “Problems with Instrumental Variables
Estimation when the Correlation between the
Instruments and the Endogenous Explanatory
Variable is Weak.” Journal of the American Statistical
Association, 90(430): 443–50.
Bowers, William J., and Glenn L. Pierce. 1975.
“The Illusion of Deterrence in Isaac Ehrlich’s
Research on Capital Punishment.” Yale Law
Journal, 85(2): 187–208.
Bubb, Ryan, and Alex Kaufman. 2009. “Securitization and Moral Hazard: Evidence from a
Lender Cutoff Rule.” Federal Reserve Bank of
Boston Public Policy Discussion Paper No. 09-5.
Campbell, Donald, and Julian Stanley. 1963.
Experimental and Quasi-Experimental Designs for
Research. Chicago: Rand McNally.
Card, David. 1990a. “The Impact of the Mariel
Boatlift on the Miami Labor Market.” Industrial
and Labor Relations Review, 43(2): 245–57.
Card, David. 1990b. “Unexpected Inflation,
Real Wages, and Employment Determination
in Union Contracts.” American Economic Review,
80(4): 669–88.
Card, David, and Alan B. Krueger. 1992a.
“Does School Quality Matter? Returns to Education and the Characteristics of Public Schools
in the United States.” Journal of Political Economy,
100(1): 1–40.
Card, David, and Alan B. Krueger. 1992b.
“School Quality and Black–White Relative Earnings: A Direct Assessment.” Quarterly Journal of
Economics, 107(1): 151–200.
Chamberlain, Gary. 1984. “Panel Data.” In
Handbook of Econometrics, vol. 2, ed. Zvi Griliches and Michael D. Intriligator, 1248–1318.
Amsterdam: North-Holland.
Coleman, James S., et al. 1966. Equality of
Educational Opportunity. Washington, DC: U.S.
Government Printing Office.
Conlin, Michael, Ted O’Donoghue, and
Timothy J. Vogelsang. 2007. “Projection Bias in
Catalog Orders.” American Economic Review, 97(4):
1217–1249.
Cook, Thomas D., and Donald T. Campbell.
1979. Quasi-Experimentation: Design and Analysis for
Field Settings. Chicago: Rand McNally.
Cooley, Thomas F., and Stephen F. LeRoy.
1986. “What Will Take the Con Out of Econometrics? A Reply to McAleer, Pagan, and Volker.”
American Economic Review, 76(3): 504–507.

28

Journal of Economic Perspectives

Crandall, Robert W., and Clifford Winston.
2003. “Does Antitrust Policy Improve Consumer
Welfare? Assessing the Evidence.” The Journal of
Economic Perspectives, 17(4): 3–26.
Deaton, Angus. 2009. “Instruments of Development: Randomization in the Tropics, and the
Search for the Elusive Keys to Economic Development.” NBER Working Paper 14690.
DellaVigna, Stefano, and Ulrike Malmendier.
2006. “Paying Not to Go to the Gym.” American
Economic Review, 96(3): 694–719.
DellaVigna, Stefano, and Daniele Paserman.
2005. “Job Search and Impatience.” Journal of
Labor Economics, 23(3): 527–88.
Deschênes, Olivier, and Michael Greenstone.
2007. “Climate Change, Mortality, and Adaptation: Evidence from Annual Fluctuations in
Weather in the US.” NBER Working Paper 13178.
Donohue, John J., and Justin Wolfers. 2005.
“Uses and Abuses of Empirical Evidence in the
Death Penalty Debate.” Stanford Law Review, vol.
58, pp. 791–845.
Duflo, Esther, and Michael Kremer. 2008.
“Use of Randomization in the Evaluation of
Development Effectiveness.” In Evaluating
Development Effectiveness, World Bank Series on
Evaluation and Development, vol. 7, pp. 93–120.
Transaction Publishers.
Ehrlich, Isaac. 1975a. “The Deterrent Effect
of Capital Punishment: A Question of Life and
Death.” American Economic Review, 65(3): 397–417.
Ehrlich, Isaac. 1975b. “Deterrence: Evidence
and Inference.” Yale Law Journal, 85(2): 209–27.
Ehrlich, Isaac. 1977a. “The Deterrent Effect
of Capital Punishment: Reply.” American Economic
Review, 67(3): 452–58.
Ehrlich, Isaac. 1977b. “Capital Punishment
and Deterrence: Some Further Thoughts and
Additional Evidence.” Journal of Political Economy,
85(4): 741–88.
Ehrlich, Isaac. 1987. “On the Issue of Causality
in the Economic Model of Crime and Law
Enforcement: Some Theoretical Considerations
and Experimental Evidence.” American Economic
Review, 77(2): 99–106.
Ehrlich, Isaac. 1996. “Crime, Punishment,
and the Market for Offenses.” Journal of Economic
Perspectives, 10(1): 43–67.
Ehrlich, Isaac, and Zhiqiang Liu. 1999. “Sensitivity Analyses of the Deterrence Hypothesis: Let’s
Keep the Econ in Econometrics.” Journal of Law &
Economics, 42(1): 455–87.
Fehr, Ernst, and Lorenz Goette. 2007. “Do
Workers Work More if Wages Are High? Evidence
from a Randomized Field Experiment.” American
Economic Review, 97(1): 298–317.
Feldstein, Martin, and Charles Horioka.

1980. “Domestic Saving and International Capital
Flows.” Economic Journal, 90(358): 314–29.
Friedman, Milton, and Anna J. Schwartz.
1963. A Monetary History of the United States,
1867–1960. Princeton: Princeton University Press
for the National Bureau of Economic Research.
Gertler, Paul. 2004. “Do Conditional Cash
Transfers Improve Child Health? Evidence from
PROGRESA’s Control Randomized Experiment.”
American Economic Review, 94(2): 336–41.
Goldberger, Arthur S. 1991. A Course in Econometrics. Cambridge, MA: Harvard University Press.
Greenberg, David, and Harlan Halsey. 1983.
“Systematic Misreporting and Effects of Income
Maintenance Experiments on Work Effort:
Evidence from the Seattle–Denver Experiment.”
Journal of Labor Economics, 1(4): 380–407.
Griliches, Zvi. 1986. “Economic Data Issues.”
In Handbook of Econometrics, vol. 3, ed. Zvi Griliches and Michael D. Intriligator, 1465–1514.
Amsterdam: North-Holland.
Grogger, Jeffrey. 1990. “The Deterrent Effect
of Capital Punishment: An Analysis of Daily
Homicide Counts.” Journal of the American Statistical Association, 85(410): 295–303.
Gruber, Jonathan. 1994. “The Incidence of
Mandated Maternity Benefits.” American Economic
Review, 84(3): 662–41.
Haavelmo, Trygve. 1944. “The Probability
Approach in Econometrics.” Econometrica, 12(Supplement): 1–115.
Hanushek, Eric A. 1986. “The Economics of
Schooling: Production and Efficiency in Public
Schools.” Journal of Economic Literature, 24(3):
1141–77.
Hastings, Justine S. 2004. “Vertical Relationships and Competition in Retail Gasoline Markets:
Empirical Evidence from Contract Changes in
Southern California.” American Economic Review,
94(1): 317–28.
Hausman, Jerry A. 1996. “Valuation of New
Goods under Perfect and Imperfect Competition.” In The Economics of New Goods, ed. Timothy
F. Bresnahan and Robert J. Gordon, 209–247.
Chicago: National Bureau of Economic Research.
Hausman, Jerry A., and Gregory K. Leonard.
2002. “The Competitive Effects of a New Product
Introduction: A Case Study.” Journal of Industrial
Economics, 50(3): 237–63.
Heckman, James J. 1997. “Instrumental Variables: A Study of Implicit Behavioral Assumptions
Used in Making Program Evaluations.” Journal of
Human Resources, 32(3): 441–62.
Heckman, James J., and Sergio Urzua. 2009.
“Comparing IV with Structural Models: What
Simple IV Can and Cannot Identify.” NBER
Working Paper 14706.

Joshua D. Angrist and Jörn-Steffen Pischke

Heckman, James J., Anne Layne-Farrar,
and Petra Todd. 1996. “Does Measured School
Quality Really Matter?” In Does Money Matter?: The
Effect of School Resources on Student Achievement and
Adult Success, ed. Gary Burtless, 192–289. Washington, DC: Brookings Institution Press.
Heinesen, Eskil. Forthcoming. “Estimating
Class-Size Effects Using Within-School Variation
in Subject-Specific Classes.” Economic Journal.
Hendry, David F. 1980. “Econometrics—
Alchemy or Science?” Economica, 47(188): 387–406.
Hoenack, Stephen A., and William C. Weiler.
1980. “A Structural Model of Murder Behavior
and the Criminal Justice System.” American
Economic Review, 70(3): 327–41.
Hoxby, Caroline M. 2000. “The Effects of
Class Size on Student Achievement: New Evidence
from Population Variation.” Quarterly Journal of
Economics, 115(4): 1239–85.
Imbens, Guido W. 2009. “Better LATE than
Nothing: Some Comments on Deaton (2009) and
Heckman and Urzua (2009).” NBER Working
Paper 14896.
Imbens, Guido W., and Thomas Lemieux.
2008. “Regression Discontinuity Designs: A Guide
to Practice.” Journal of Econometrics, 142(2): 615–35.
Imbens, Guido W., and Jeffrey M. Wooldridge.
2009. “Recent Developments in the Econometrics
of Program Development.” Journal of Economic
Literature, 47(1): 5–86.
Jacob, Brian A. 2004. “Public Housing, Housing
Vouchers and Student Achievement: Evidence
from Public Housing Demolitions in Chicago.”
American Economic Review, 94(1): 233–58.
Karlan, Dean, and Jonathan Zinman. 2009.
“Observing Unobservables: Identifying Information Asymmetries with a Consumer Credit Field
Experiment.” Econometrica, 77(6): 1993–2008.
Keys, Benjamin, Tanmoy Mukherjee, Amit
Seru, and Vikrant Vig. 2010. “Did Securitization Lead to Lax Screening? Evidence from
Subprime Loans.” Quarterly Journal of Economics,
125(1): 307–62.
Kling, Jeffrey R., Jeffrey B. Liebman, and
Lawrence F. Katz. 2007. “Experimental Analysis of
Neighborhood Effects.” Econometrica, 75(1): 83–119.
Krueger, Alan B. 1999. “Experimental Estimates of Education Production Functions.” The
Quarterly Journal of Economics, 114(2): 497–532.
Krueger, Alan B. 2003. “Economic Considerations and Class Size.” Economic Journal, 113(485):
F34–F63.
Kydland, Finn E., and Edward C. Prescott.
1977. “Rules Rather than Discretion: The Inconsistency of Optimal Plans.” Journal of Political
Economy, 85(3): 473–92.
Kydland, Finn E., and Edward C. Prescott.

29

1996. “The Computational Experiment: An
Econometric Tool.” Journal of Economic Perspectives,
10(1): 69–85.
LaLonde, Robert J. 1986. “Evaluating the
Econometric Evaluations of Training Programs
with Experimental Data.” American Economic
Review, 76(4): 604–620.
Leamer, Edward. 1978. Specification Searches: Ad
Hoc Inference with Non Experimental Data. New York:
John Wiley and Sons.
Leamer, Edward. 1983. “Let’s Take the Con
Out of Econometrics.” American Economic Review,
73(1): 31–43.
Leamer, Edward. 1985. “Sensitivity Analyses
Would Help.” American Economic Review, 75(3):
308–313.
Levine, Ross, and David Renelt. 1992. “A
Sensitivity Analysis of Cross-Country Growth
Regressions.” American Economic Review, 82(4):
942–63.
Lucas, Robert E. 1973. “Some International
Evidence on Output–Inflation Tradeoffs.”
American Economic Review, 63(3): 326–34.
Lucas, Robert E. 1976. “Econometric Policy
Evaluation: A Critique.” In Carnegie-Rochester
Conference Series on Public Policy, vol. 1, pp. 19–46.
Lucas, Robert E. 1988. “What Economists Do.”
Unpublished.
Mankiw, Gregory N. 2006. “The Macroeconomist as Scientist and Engineer.” Journal of Economic
Perspectives, 20(4): 29–46.
Manning, Willard G., Joseph P. Newhouse,
Naihua Duan, Emmett B. Keeler, and Arleen
Leibowitz. 1987. “Health Insurance and the
Demand for Medical Care: Evidence from a
Randomized Experiment.” American Economic
Review, 77(3): 251–77.
McAleer, Michael, Adrian R. Pagan, Paul A.
Volker. 1985. “What Will Take the Con Out of
Econometrics?” American Economic Review, 75(3):
293–307.
McCrary, Justin. 2008. “Manipulation of the
Running Variable in the Regression Discontinuity
Design: A Density Test.” Journal of Econometrics,
142(2): 698–714.
Meyer, Bruce D. 1995. “Natural and QuasiExperiments in Economics.” Journal of Business &
Economic Statistics, 13(2): 151–61.
Nevo, Aviv. 2000. “Mergers with Differentiated Products: The Case of the Ready-to-Eat
Cereal Industry.” The RAND Journal of Economics,
31(3): 395–42.
Nunn, Nathan. 2008. “The Long-Term Effects
of Africa’s Slave Trades.” Quarterly Journal of
Economics: 123(1): 139–76.
Obstfeld, Maurice. 1995. “International
Capital Mobility in the 1990s.” In Understanding

30

Journal of Economic Perspectives

Interdependence: The Macroeconomics of the Open
Economy, ed. Peter B. Kenen, 201–261. Princeton:
Princeton University Press.
Oettinger, Gerald S. 1999. “An Empirical Analysis of the Daily Labor Supply of Stadium Vendors.”
Journal of Political Economy, 107(2): 360–92.
Passell, Peter, and John B. Taylor. 1977. “The
Deterrent Effect of Capital Punishment: Another
View.” American Economic Review, 67(3): 445–51.
Persson Torsten, and Guido Tabellini. 2008.
“The Growth Effect of Democracy: Is it Heterogeneous and How can It be Estimated?” Chap.
13 in Institutions and Economic Performance, ed. E.
Helpman. Cambridge, MA: Harvard University
Press.
Peters, Craig. 2006. “Evaluating the Performance of Merger Simulation: Evidence from the
US Airline Industry.” Journal of Law and Economics,
49(2): 627–49.
Phillips, David P. 1980. “The Deterrent Effect
of Capital Punishment: New Evidence on an Old
Controversy.” American Journal of Sociology, 86(1):
139–48.
Rajan, Raghuram G., and Arvind Subramanian. 2008. “Aid and Growth: What Does the
Cross-Country Evidence Really Show?” Review of
Economics and Statistics, 90(4): 643–65.
Ramey, Valerie, and Matthew D. Shapiro.
1998. “Costly Capital Reallocation and the
Effects of Government Spending.” CarnegieRochester Conference Series on Public Policy, 48(1):
145–94.
Richardson, Gary, and William Troost.
2009. “Monetary Intervention Mitigated Banking
Panics during the Great Depression: QuasiExperimental Evidence from a Federal Reserve
District Border, 1929–1933.” Journal of Political
Economy, 117(6): 1031–73.
Rivkin, Steven G., Eric A. Hanushek, and John
F. Kain. 2005. “Teachers, Schools, and Academic
Achievement.” Econometrica, 73(2): 417–58.
Rodrik, Dani, and Romain Wacziarg. 2005.
“Do Democratic Transitions Produce Bad
Outcomes?” American Economic Review, 95(2):
50–55.
Romer, Christina D., and David H. Romer.
1989. “Does Monetary Policy Matter? A New Test
in the Spirit of Friedman and Schwartz.” NBER
Macroeconomics Annual, vol. 4, pp. 121–70.
Romer, Christina D., and David H. Romer.
2004. “A New Measure of Monetary Shocks:
Derivation and Implications.” American Economic
Review, 94(4): 1055–1084.
Romer, Christina D., and David H. Romer.
2007. “The Macroeconomic Effects of Tax
Changes: Estimates Based on a New Measure of

Fiscal Shocks.” NBER Working Paper 13264.
Rosenzweig, Mark R., and Kenneth I.
Wolpin. 2000. “Natural ‘Natural Experiments’
in Economics.” Journal of Economic Literature,
38(4): 827–74.
Rouse, Cecilia. 1998. “Private School Vouchers
and Student Achievement: An Evaluation of the
Milwaukee Parental Choice Program.” Quarterly
Journal of Economics, 113(2): 553–602.
Sala-i-Martin, Xavier. 1997. “I Just Ran Two
Million Regressions.” American Economic Review,
87(2): 178–83.
Scheiber, Noam. 2007. “Freaks and Geeks.
How Freakonomics Is Ruining the Dismal
Science.” The New Republic, April 2, pp. 27–31.
Schultz, T. Paul. 2004. “School Subsidies
for the Poor: Evaluating the Mexican Progresa
Poverty Program.” Journal of Development Economics,
74(1): 199–250.
Sims, Christopher A. 1980. “Macroeconomics
and Reality.” Econometrica, 48(1): 1–48.
Sims, Christopher A. 1988. “Uncertainty across
Models.” American Economic Review, 78(2): 163–67.
Solon, Gary. 1985. “Work Incentive Effects of
Taxing Unemployment Insurance.” Econometrica,
53(2): 295–306.
Solow, Robert. 2008. “The State of Macroeconomics.” Journal of Economic Perspectives, 22(1):
243–249.
Summers, Anita A., and Barbara L. Wolfe.
1977. “Do Schools Make a Difference?” American
Economic Review, 67(4): 639–52.
Taylor, Christopher, Nicholas Kreisle, and
Paul Zimmerman. 2007. “Vertical Relationships
and Competition in Retail Gasoline Markets:
Comment.” The Federal Trade Commission,
Bureau of Economics Working Paper 291.
Urquiola, Miguel, and Eric Verhoogen. 2009.
“Class-size Caps, Sorting, and the RegressionDiscontinuity Design.” American Economic Review,
99(1): 179–215.
Velde, Francois. 2009. “Chronicles of a Deflation Unforetold.” Journal of Political Economy,
117(4): 591–634.
White, Halbert. 1980a. “A HeteroskedasticityConsistent Covariance Matrix Estimator and a
Direct Test for Heteroskedasticity.” Econometrica,
48(4): 817–38.
White, Halbert. 1980b. “Using Least Squares
to Approximate Unknown Regression Functions.” International Economic Review, 21(1): 149–70.
Wolfers, Justin. 2008. “More on the Missing
Macroeconomists.” “Freakonomics” column
of the New York Times, March 31. http://
freakonomics.blogs.nytimes.com/2008/03/31
/more-on-the-missing-macroeconomists/.

This article has been cited by:
1. Yanying Chen, Yi Jin Tan. 2018. The effect of non-contributory pensions on labour supply and private
income transfers: evidence from Singapore. IZA Journal of Labor Policy 7:1. . [Crossref]
2. Hendrik Juerges, Joachim Winter. 2018. Guest Editorial – Special Issue on Empirical Health
Economics. Jahrbücher für Nationalökonomie und Statistik 238:5, 371-373. [Crossref]
3. John Sterman. 2018. System dynamics at sixty: the path forward. System Dynamics Review 33. .
[Crossref]
4. Garret Christensen, Edward Miguel. 2018. Transparency, Reproducibility, and the Credibility of
Economics Research. Journal of Economic Literature 56:3, 920-980. [Abstract] [View PDF article]
[PDF with links]
5. Alberto Abadie, Matias D. Cattaneo. 2018. Econometric Methods for Program Evaluation. Annual
Review of Economics 10:1, 465-503. [Crossref]
6. Guido Imbens. 2018. Understanding and misunderstanding randomized controlled trials: A
commentary on Deaton and Cartwright. Social Science & Medicine 210, 50-52. [Crossref]
7. Issa J. Dahabreh. 2018. Randomization, randomized trials, and analyses using observational data: A
commentary on Deaton and Cartwright. Social Science & Medicine 210, 41-44. [Crossref]
8. Emi Nakamura, Jón Steinsson. 2018. Identification in Macroeconomics. Journal of Economic
Perspectives 32:3, 59-86. [Abstract] [View PDF article] [PDF with links]
9. Donald Kenkel, Alan Mathios, Hua Wang. 2018. Advertising and Health: A Case Study of Menthol
Cigarette Advertising and Cigarette Demand. American Journal of Health Economics 4:3, 263-286.
[Crossref]
10. Liran Einav, Amy Finkelstein. 2018. Moral Hazard in Health Insurance: What We Know and How
We Know It. Journal of the European Economic Association 16:4, 957-982. [Crossref]
11. Alberto Abadie, Matthew M. Chingos, Martin R. West. 2018. Endogenous Stratification in
Randomized Experiments. The Review of Economics and Statistics . [Crossref]
12. Christian Leuz. 2018. Evidence-based policymaking: promise, challenges and opportunities for
accounting and financial markets research. Accounting and Business Research 48:5, 582-608. [Crossref]
13. Andrew M. Ryan. 2018. Well-Balanced or too Matchy-Matchy? The Controversy over Matching in
Difference-in-Differences. Health Services Research 24. . [Crossref]
14. Damien Bol. 2018. Putting Politics in the Lab: A Review of Lab Experiments in Political Science.
Government and Opposition 3, 1-24. [Crossref]
15. Laura Camfield. 2018. Rigor and Ethics in the World of Big-team Qualitative Data: Experiences
From Research in International Development. American Behavioral Scientist 24, 000276421878463.
[Crossref]
16. Fiona Burlig. 2018. Improving transparency in observational social science research: A pre-analysis
plan approach. Economics Letters 168, 56-60. [Crossref]
17. Nadia Campaniello, Matteo Richiardi. 2018. The role of museums in bilateral tourist flows: evidence
from Italy. Oxford Economic Papers 70:3, 658-679. [Crossref]
18. Simine Vazire. 2018. Implications of the Credibility Revolution for Productivity, Creativity, and
Progress. Perspectives on Psychological Science 13:4, 411-417. [Crossref]
19. Linli Xu, Jorge M. Silva-Risso, Kenneth C. Wilbur. 2018. Dynamic Quality Ladder Model Predictions
in Nonrandom Holdout Samples. Management Science 64:7, 3187-3207. [Crossref]
20. James Alm. 2018. WHAT MOTIVATES TAX COMPLIANCE?. Journal of Economic Surveys 15. .
[Crossref]

21. Luis Mireles-Flores. Recent Trends in Economic Methodology: A Literature Review 93-126.
[Crossref]
22. Timothy Powell-Jackson, Calum Davey, Edoardo Masset, Shari Krishnaratne, Richard Hayes, Kara
Hanson, James R Hargreaves. 2018. Trials and tribulations: cross-learning from the practices of
epidemiologists and economists in the evaluation of public health interventions. Health Policy and
Planning 33:5, 702-706. [Crossref]
23. Cyrus J. DiCiccio, Joseph P. Romano, Michael Wolf. 2018. Improving weighted least squares inference.
Econometrics and Statistics . [Crossref]
24. Nicolas R. Ziebarth. Social Insurance and Health 57-84. [Crossref]
25. Dalton Conley, Simone Zhang. 2018. The promise of genes for understanding cause and effect.
Proceedings of the National Academy of Sciences 115:22, 5626-5628. [Crossref]
26. Julian Reiss. 2018. Against external validity. Synthese 24. . [Crossref]
27. Chong (Alex) Wang, Xiaoquan (Michael) Zhang, Il-Horn Hann. 2018. Socially Nudged: A QuasiExperimental Study of Friends’ Social Influence in Online Product Ratings. Information Systems
Research . [Crossref]
28. Donna K. Ginther. 2018. Using Data to Inform the Science of Broadening Participation. American
Behavioral Scientist 62:5, 612-624. [Crossref]
29. Muhammad Ali Nasir, Jamie Morgan. 2018. The unit root problem: Affinities between ergodicity and
stationarity, its practical contradictions for central bank policy, and some consideration of alternatives.
Journal of Post Keynesian Economics 18, 1-25. [Crossref]
30. Giulia Mascagni. 2018. FROM THE LAB TO THE FIELD: A REVIEW OF TAX
EXPERIMENTS. Journal of Economic Surveys 32:2, 273-301. [Crossref]
31. Katrina Kosec, Hosaena Ghebru, Brian Holtemeyer, Valerie Mueller, Emily Schmidt. 2018. The
Effect of Land Access on Youth Employment and Migration Decisions: Evidence from Rural Ethiopia.
American Journal of Agricultural Economics 100:3, 931-954. [Crossref]
32. Jens Frankenreiter. 2018. Are Advocates General Political? An Empirical Analysis of the Voting
Behavior of the Advocates General at the European Court of Justice. Review of Law & Economics
14:1. . [Crossref]
33. Michihiro Kandori. 2018. Replicability of Experimental Data and Credibility of Economic Theory.
The Japanese Economic Review 69:1, 4-25. [Crossref]
34. Bernd Hayo. 2018. On Standard-Error-Decreasing Complementarity: Why Collinearity is Not the
Whole Story. Journal of Quantitative Economics 16:1, 289-307. [Crossref]
35. Scott M Swinton. 2018. Why Should I Believe Your Applied Economics?. American Journal of
Agricultural Economics 100:2, 381-391. [Crossref]
36. Marc F Bellemare, Jeffrey R Bloem. 2018. Experimental Conversations: Perspectives on Randomized
Trials in Development Economics, by T.N. Ogden. American Journal of Agricultural Economics 100:2,
642-643. [Crossref]
37. Paolo Parra Saiani. 2018. Doing Sociology in the Age of ‘Evidence-Based Research’: Scientific
Epistemology versus Political Dominance. The American Sociologist 49:1, 80-97. [Crossref]
38. Julia M. Rohrer. 2018. Thinking Clearly About Correlations and Causation: Graphical Causal Models
for Observational Data. Advances in Methods and Practices in Psychological Science 1:1, 27-42. [Crossref]
39. Timothy Brathwaite, Joan L. Walker. 2018. Causal inference in travel demand modeling (and the lack
thereof ). Journal of Choice Modelling 26, 1-18. [Crossref]

40. Elmar Gerum, Sascha H. Mölls, Chunqian Shen. 2018. Corporate governance, capital market
orientation and firm performance: empirical evidence for large publicly traded German corporations.
Journal of Business Economics 88:2, 203-252. [Crossref]
41. Jon Bingen Sande, Mrinal Ghosh. 2018. Endogeneity in survey research. International Journal of
Research in Marketing . [Crossref]
42. Jia Yan, Xiaowen Fu, Tae Hoon Oum, Kun Wang. 2018. Airline horizontal mergers and productivity:
Empirical evidence from a quasi-natural experiment in China. International Journal of Industrial
Organization . [Crossref]
43. Lauri Sääksvuori, Maria Vaalavuo, Ismo Linnosmaa. 2018. Pieces in a big puzzle: On the relationship
between health and employment. Scandinavian Journal of Public Health 46:19_suppl, 3-6. [Crossref]
44. Ben S. Meiselman. 2018. Ghostbusting in Detroit: Evidence on nonfilers from a controlled field
experiment. Journal of Public Economics 158, 180-193. [Crossref]
45. Louis Silvia. 2018. Economics and Antitrust Enforcement: The Last 25 Years. International Journal
of the Economics of Business 25:1, 119-129. [Crossref]
46. Scott Alan Carson. 2018. The weight of nineteenth century Mexicans in the Western United States.
Historical Methods: A Journal of Quantitative and Interdisciplinary History 51:1, 1-12. [Crossref]
47. Marek Gruszczyński. 71. [Crossref]
48. Rick Hangartner, Paul Cull. Inscrutable Decision Makers: Knightian Uncertainty in Machine
Learning 228-236. [Crossref]
49. Jakob Kapeller, Benjamin Ferschli. Hans Albert und die Kritik am Modell-Platonismus in den
Wirtschaftswissenschaften 1-17. [Crossref]
50. Jakob Kapeller, Benjamin Ferschli. Hans Albert und die Kritik am Modell-Platonismus in den
Wirtschaftswissenschaften 1-17. [Crossref]
51. John K. Dagsvik. 2018. Invariance axioms and functional form restrictions in structural models.
Mathematical Social Sciences 91, 85-95. [Crossref]
52. Matt Theeke, Francisco Polidoro, James W. Fredrickson. 2017. Path-dependent Routines in the
Evaluation of Novelty: The Effects of Innovators’ New Knowledge Use on Brokerage Firms’ Coverage.
Administrative Science Quarterly 10, 000183921774726. [Crossref]
53. Jens Frankenreiter. 2017. The Politics of Citations at the ECJ-Policy Preferences of E.U. Member
State Governments and the Citation Behavior of Judges at the European Court of Justice. Journal of
Empirical Legal Studies 14:4, 813-857. [Crossref]
54. Jostein Grytten. 2017. The impact of education on dental health - Ways to measure causal effects.
Community Dentistry and Oral Epidemiology 45:6, 485-495. [Crossref]
55. Isaiah Andrews, Matthew Gentzkow, Jesse M. Shapiro. 2017. Measuring the Sensitivity of Parameter
Estimates to Estimation Moments*. The Quarterly Journal of Economics 132:4, 1553-1592. [Crossref]
56. Abhijit Banerjee, Rukmini Banerji, James Berry, Esther Duflo, Harini Kannan, Shobhini Mukerji,
Marc Shotland, Michael Walton. 2017. From Proof of Concept to Scalable Policies: Challenges and
Solutions, with an Application. Journal of Economic Perspectives 31:4, 73-102. [Abstract] [View PDF
article] [PDF with links]
57. Francesco Bogliacino, Cristiano Codagnone. 2017. Microfoundations, behaviour, and evolution:
Evidence from experiments. Structural Change and Economic Dynamics . [Crossref]
58. Sebastian Jilke, Nicolai Petrovsky, Bart Meuleman, Oliver James. 2017. Measurement equivalence in
replications of experiments: when and why it matters and guidance on how to determine equivalence.
Public Management Review 19:9, 1293-1310. [Crossref]

59. John Ovretveit, Brian Mittman, Lisa Rubenstein, David A. Ganz. 2017. Using implementation tools
to design and conduct quality improvement projects for faster and more effective improvement.
International Journal of Health Care Quality Assurance 30:8, 755-768. [Crossref]
60. Roee Gutman, Orna Intrator, Tony Lancaster. 2017. A Bayesian procedure for estimating the causal
effects of nursing home bed-hold policy. Biostatistics 72. . [Crossref]
61. Anja Breljak, Felix Kersting. 2017. Performativity: moving economics further?. Journal of Economic
Methodology 24:4, 434-440. [Crossref]
62. Zacharias Maniadis, Fabio Tufano, John A. List. 2017. To Replicate or Not To Replicate? Exploring
Reproducibility in Economics through the Lens of a Model and a Pilot Study. The Economic Journal
127:605, F209-F235. [Crossref]
63. Marie-Laure Allain, Claire Chambolle, Stéphane Turolla, Sofia B. Villas-Boas. 2017. Retail Mergers
and Food Prices: Evidence from France. The Journal of Industrial Economics 65:3, 469-509. [Crossref]
64. P. Owen. 2017. Evaluating Ingenious Instruments for Fundamental Determinants of Long-Run
Economic Growth and Development. Econometrics 5:3, 38. [Crossref]
65. Emanuela Raffinetti, Elena Siletti, Achille Vernizzi. 2017. Analyzing the Effects of Negative and Nonnegative Values on Income Inequality: Evidence from the Survey of Household Income and Wealth of
the Bank of Italy (2012). Social Indicators Research 133:1, 185-207. [Crossref]
66. Donald E. Bowen, Laurent Frésard, Jérôme P. Taillard. 2017. What’s Your Identification Strategy?
Innovation in Corporate Finance Research. Management Science 63:8, 2529-2548. [Crossref]
67. Ashwini Deshpande, Alain Desrochers, Christopher Ksoll, Abu S. Shonchoy. 2017. The Impact of
a Computer-based Adult Literacy Program on Literacy and Numeracy: Evidence from India. World
Development 96, 451-473. [Crossref]
68. Eivind Tveter, Morten Welde, James Odeck. 2017. Do Fixed Links Affect Settlement Patterns: A
Synthetic Control Approach. Research in Transportation Economics 63, 59-72. [Crossref]
69. Gustav Alexandrie. 2017. Surveillance cameras and crime: a review of randomized and natural
experiments. Journal of Scandinavian Studies in Criminology and Crime Prevention 18:2, 210-222.
[Crossref]
70. Klaus E Meyer, Arjen van Witteloostuijn, Sjoerd Beugelsdijk. 2017. What?s in a p? Reassessing best
practices for conducting and reporting hypothesis-testing research. Journal of International Business
Studies 48:5, 535-551. [Crossref]
71. Marc F. Bellemare, Takaaki Masaki, Thomas B. Pepinsky. 2017. Lagged Explanatory Variables and
the Estimation of Causal Effect. The Journal of Politics 79:3, 949-963. [Crossref]
72. David I. Stern, Jeremy van Dijk. 2017. Economic growth and global particulate pollution
concentrations. Climatic Change 142:3-4, 391-406. [Crossref]
73. Karen Palmer, Margaret Walls. 2017. Using information to close the energy efficiency gap: a review
of benchmarking and disclosure ordinances. Energy Efficiency 10:3, 673-691. [Crossref]
74. Cristian Huse, Nikita Koptyug. 2017. Bailing on the Car That Was Not Bailed Out: Bounding
Consumer Reactions to Financial Distress. Journal of Economics & Management Strategy 26:2,
337-374. [Crossref]
75. Neil Seftor. 2017. Raising the Bar. Evaluation Review 41:3, 212-239. [Crossref]
76. Luke Keele, Scott Lorch, Molly Passarella, Dylan Small, Rocío Titiunik. An Overview of
Geographically Discontinuous Treatment Assignments with an Application to Children’s Health
Insurance 147-194. [Crossref]
77. Dionissi Aliprantis. 2017. Assessing the evidence on neighborhood effects from Moving to
Opportunity. Empirical Economics 52:3, 925-954. [Crossref]

78. Sebastian Strunz, Bernd Klauer, Irene Ring, Johannes Schiller. 2017. Between Scylla and Charybdis?
On the place of economic methods in sustainability science. Sustainability Science 12:3, 421-432.
[Crossref]
79. Joshua D. Angrist, Jörn-Steffen Pischke. 2017. Undergraduate Econometrics Instruction: Through
Our Classes, Darkly. Journal of Economic Perspectives 31:2, 125-144. [Abstract] [View PDF article]
[PDF with links]
80. Siri Terjesen, Pankaj C. Patel. 2017. In Search of Process Innovations: The Role of Search Depth,
Search Breadth, and the Industry Environment. Journal of Management 43:5, 1421-1446. [Crossref]
81. Arindrajit Dube. 2017. Book Review: The Long-Run Impact of Minimum Wage Research: A Case
Study of Myth and Measurement. ILR Review 70:3, 818-823. [Crossref]
82. Koki Arai. 2017. Ex-post examination of mergers: effects on retail prices. Asia-Pacific Journal of
Accounting & Economics 24:1-2, 145-162. [Crossref]
83. Marc Gurgand, Michael Rosholm. 2017. Social experiments in labour and social policies: A thriving
research field. Labour Economics 45, 1-4. [Crossref]
84. Muhammad F. Bhuiyan, Radek S. Szulga. 2017. Extreme bounds of subjective well-being: economic
development and micro determinants of life satisfaction. Applied Economics 49:14, 1351-1378.
[Crossref]
85. Sander Gerritsen, Dinand Webbink, Bas ter Weel. 2017. Sorting Around the Discontinuity Threshold:
The Case of a Neighbourhood Investment Programme. De Economist 165:1, 101-128. [Crossref]
86. Jarosław Kantorowicz. 2017. Electoral systems and fiscal policy outcomes: Evidence from Poland.
European Journal of Political Economy 47, 36-60. [Crossref]
87. Joseph P. Romano, Michael Wolf. 2017. Resurrecting weighted least squares. Journal of Econometrics
197:1, 1-19. [Crossref]
88. Melanie J. Cozad, Cole G. Chapman, John M. Brooks. 2017. Specifying a Conceptual Treatment
Choice Relationship Before Analysis Is Necessary for Comparative Effectiveness Research. Medical
Care 55:2, 94-96. [Crossref]
89. Gebhard Kirchgässner. 2017. On Estimating the Size of the Shadow Economy. German Economic
Review 18:1, 99-111. [Crossref]
90. Ling Cen, Edward L. Maydew, Liandong Zhang, Luo Zuo. 2017. Customer–supplier relationships
and corporate tax avoidance. Journal of Financial Economics 123:2, 377-394. [Crossref]
91. Adam La Caze, Mark Colyvan. 2017. A Challenge for Evidence-Based Policy. Axiomathes 27:1, 1-13.
[Crossref]
92. Per Engström, Johannes Hagen. 2017. Income underreporting among the self-employed: A permanent
income approach. European Economic Review 92, 92-109. [Crossref]
93. W. Bentley MacLeod. 2017. Viewpoint: The human capital approach to inference. Canadian Journal
of Economics/Revue canadienne d'économique 50:1, 5-39. [Crossref]
94. Thomas S. Dee, Emily K. Penner. 2017. The Causal Effects of Cultural Relevance. American
Educational Research Journal 54:1, 127-166. [Crossref]
95. Hakan Seckinelgin. Evidence-Based Policy: Randomised Controlled Trials’ Knowledge Claims to
AIDS Policy 105-124. [Crossref]
96. Julian Reiss. On the Causal Wars 45-66. [Crossref]
97. Hsiang-Ke Chao, David Teira. Model-Based Knowledge and Credible Policy Analysis 181-197.
[Crossref]
98. Holger Breinlich, Volker Nocke, Nicolas Schutz. 2017. International aspects of merger policy: A
survey. International Journal of Industrial Organization 50, 415-429. [Crossref]

99. J.M. Gueron. The Politics and Practice of Social Experiments 27-69. [Crossref]
100. W.J. Congdon, J.R. Kling, J. Ludwig, S. Mullainathan. Social Policy 389-426. [Crossref]
101. Carlianne Patrick, Amanda Ross, Heather Stephens. Designing Policies to Spur Economic Growth:
How Regional Scientists Can Contribute to Future Policy Development and Evaluation 119-133.
[Crossref]
102. Hiroshi Kanasugi, Koichi Ushijima. 2017. The impact of a high-speed railway on residential land
prices. Papers in Regional Science 24. . [Crossref]
103. 2017. OUP accepted manuscript. Cambridge Journal Of Economics . [Crossref]
104. Jeff E. Biddle, Daniel S. Hamermesh. 2017. Theory and Measurement. History of Political Economy
49:Supplement, 34-57. [Crossref]
105. Matthew T. Panhans, John D. Singleton. 2017. The Empirical Economist’s Toolkit. History of
Political Economy 49:Supplement, 127-157. [Crossref]
106. Julia Brüggemann, Kilian Bizer. 2016. Laboratory experiments in innovation research: a
methodological overview and a review of the current literature. Journal of Innovation and
Entrepreneurship 5:1. . [Crossref]
107. Duo Qin, Sophie van Huellen, Qing-Chao Wang. 2016. How Credible Are Shrinking Wage
Elasticities of Married Women Labour Supply?. Econometrics 4:4, 1. [Crossref]
108. Ryan H. Murphy. 2016. Beggaring thy neighbor at the state and local level. Journal of Financial
Economic Policy 8:4, 532-539. [Crossref]
109. Denis Cogneau. 2016. History, Data and Economics for Africa: Can We Get Them Less Wrong?:
Reply to Morten Jerven's ‘Trapped between tragedies and miracles: Misunderstanding African
economic growth’. Development Policy Review 34:6, 895-899. [Crossref]
110. Stefan Nagel. 2016. The Liquidity Premium of Near-Money Assets *. The Quarterly Journal of
Economics 131:4, 1927-1971. [Crossref]
111. Thomas R. Kubick, Daniel P. Lynch, Michael A. Mayberry, Thomas C. Omer. 2016. The Effects of
Regulatory Scrutiny on Tax Avoidance: An Examination of SEC Comment Letters. The Accounting
Review 91:6, 1751-1780. [Crossref]
112. Michael A. Clemens. 2016. Losing our minds? New research directions on skilled emigration and
development. International Journal of Manpower 37:7, 1227-1248. [Crossref]
113. Carlianne Patrick. 2016. IDENTIFYING THE LOCAL ECONOMIC DEVELOPMENT
EFFECTS OF MILLION DOLLAR FACILITIES. Economic Inquiry 54:4, 1737-1762. [Crossref]
114. Zhiguo He, Konstantin Milbradt. 2016. Dynamic Debt Maturity. Review of Financial Studies 29:10,
2677-2736. [Crossref]
115. Pierpaolo Parrotta, Dario Pozzoli, Davide Sala. 2016. Ethnic diversity and firms' export behavior.
European Economic Review 89, 248-263. [Crossref]
116. Emily Oster. 2016. Unobservable Selection and Coefficient Stability: Theory and Evidence. Journal
of Business & Economic Statistics 40, 1-18. [Crossref]
117. Mark S. Bell. 2016. Examining Explanations for Nuclear Proliferation. International Studies Quarterly
60:3, 520-529. [Crossref]
118. Christoph Engel. 2016. A random shock is not random assignment. Economics Letters 145, 45-47.
[Crossref]
119. Justin Esarey, Ahra Wu. 2016. Measuring the effects of publication bias in political science. Research
& Politics 3:3, 205316801666585. [Crossref]
120. Andrew T. Little, Thomas B. Pepinsky. 2016. Simple and Formal Models in Comparative Politics.
Chinese Political Science Review . [Crossref]

121. Alex Eble, Peter Boone, Diana Elbourne. 2016. On Minimizing the Risk of Bias in Randomized
Controlled Trials in Economics. The World Bank Economic Review lhw034. [Crossref]
122. Julian Reiss. 2016. Suppes’ probabilistic theory of causality and causal inference in economics. Journal
of Economic Methodology 23:3, 289-304. [Crossref]
123. Jonas Björnerstedt, Frank Verboven. 2016. Does Merger Simulation Work? Evidence from the Swedish
Analgesics Market. American Economic Journal: Applied Economics 8:3, 125-164. [Abstract] [View
PDF article] [PDF with links]
124. David McGrogan. 2016. THE PROBLEM OF CAUSALITY IN INTERNATIONAL HUMAN
RIGHTS LAW. International and Comparative Law Quarterly 65:03, 615-644. [Crossref]
125. Teodoro Dario Togati. 2016. How can we explain the persistence of the Great Recession? A balanced
stability approach. Cambridge Journal of Economics 40:4, 1077-1101. [Crossref]
126. Cyrus Samii. 2016. Causal Empiricism in Quantitative Research. The Journal of Politics 78:3, 941-955.
[Crossref]
127. Madhu S Mohanty. 2016. Relationship between Positive Attitude and Job Satisfaction: Evidence from
the US Data. Eastern Economic Journal 42:3, 349-372. [Crossref]
128. Carlos Arnade, Fred Kuchler, Linda Calvin. 2016. The changing role of consumers and suppliers in
a food safety event: the 2006 foodborne illness outbreak linked to spinach. Applied Economics 48:25,
2354-2366. [Crossref]
129. IAN D. GOW, DAVID F. LARCKER, PETER C. REISS. 2016. Causal Inference in Accounting
Research. Journal of Accounting Research 54:2, 477-523. [Crossref]
130. Judith Favereau. 2016. On the analogy between field experiments in economics and clinical trials in
medicine. Journal of Economic Methodology 23:2, 203-222. [Crossref]
131. Luis F. Sanchez, David I. Stern. 2016. Drivers of industrial and non-industrial greenhouse gas
emissions. Ecological Economics 124, 17-24. [Crossref]
132. Maximilian Kasy. 2016. Partial Identification, Distributional Preferences, and the Welfare Ranking of
Policies. Review of Economics and Statistics 98:1, 111-131. [Crossref]
133. Donald P Green, Michael Schwam-Baird. 2016. Mobilization, participation, and American
democracy. Party Politics 22:2, 158-164. [Crossref]
134. Òscar Jordà, Alan M. Taylor. 2016. The Time for Austerity: Estimating the Average Treatment
Effect of Fiscal Policy. The Economic Journal 126:590, 219-255. [Crossref]
135. Jan Fidrmuc, Jarko Fidrmuc. 2016. Foreign languages and trade: evidence from a natural experiment.
Empirical Economics 50:1, 31-49. [Crossref]
136. P.K. Goldberg, N. Pavcnik. The Effects of Trade Policy 161-206. [Crossref]
137. David R. Ross. Industrial Organization 1-4. [Crossref]
138. Isabel Baumann. A Tailor-Made Plant Closure Survey 35-61. [Crossref]
139. Sebastian Jilke, Steven Van de Walle, Soonhee Kim. 2016. Generating Usable Knowledge through
an Experimental Approach to Public Administration. Public Administration Review 76:1, 69-72.
[Crossref]
140. Richard A. Ashley, Christopher F. Parmeter. 2015. Sensitivity analysis for inference in 2SLS/GMM
estimation with possibly flawed instruments. Empirical Economics 49:4, 1153-1171. [Crossref]
141. Christopher K. Coombs, Robert J. Newman, Richard J. Cebula, Mary L. White. 2015. The
Bargaining Power of Health Care Unions and Union Wage Premiums for Registered Nurses. Journal
of Labor Research 36:4, 442-461. [Crossref]

142. Hannes Kröger, Eduwin Pakpahan, Rasmus Hoffmann. 2015. What causes health inequality? A
systematic review on the relative importance of social causation and health selection. The European
Journal of Public Health 25:6, 951-960. [Crossref]
143. James J. Heckman, John Eric Humphries, Gregory Veramendi. 2015. Dynamic treatment effects.
Journal of Econometrics . [Crossref]
144. Richard A. Ashley, Christopher F. Parmeter. 2015. When is it justifiable to ignore explanatory variable
endogeneity in a regression model?. Economics Letters 137, 70-74. [Crossref]
145. Thor O. Thoresen, Trine E. Vattø. 2015. Validation of the discrete choice labor supply model by
methods of the new tax responsiveness literature. Labour Economics 37, 38-53. [Crossref]
146. Anthony Fowler, B. Pablo Montagnes. 2015. College football, elections, and false-positive results in
observational research. Proceedings of the National Academy of Sciences 112:45, 13800-13804. [Crossref]
147. Holger Spamann. 2015. Empirical Comparative Law. Annual Review of Law and Social Science 11:1,
131-153. [Crossref]
148. Elliott Ash, W. Bentley MacLeod. 2015. Intrinsic Motivation in Public Service: Theory and Evidence
from State Supreme Courts. The Journal of Law and Economics 58:4, 863-913. [Crossref]
149. Attilia Ruzzene. 2015. The limits of inference without theory. Journal of Economic Methodology 22:4,
520-525. [Crossref]
150. Timo Mitze, Alfredo R. Paloyo, Björn Alecke. 2015. Is There a Purchase Limit on Regional Growth?
A Quasi-experimental Evaluation of Investment Grants Using Matching Techniques. International
Regional Science Review 38:4, 388-412. [Crossref]
151. Brian J Fogarty. 2015. Comment on Zigerell (2015): Using Poisson inverse Gaussian regression on
citation data. Research & Politics 2:4, 205316801561749. [Crossref]
152. Richard Friberg, André Romahn. 2015. Divestiture requirements as a tool for competition policy:
A case from the Swedish beer market. International Journal of Industrial Organization 42, 1-18.
[Crossref]
153. Laura Bierut, David Cesarini. 2015. How Genetic and Other Biological Factors Interact with Smoking
Decisions. Big Data 3:3, 198-202. [Crossref]
154. J. Pearl. 2015. Detecting Latent Heterogeneity. Sociological Methods & Research . [Crossref]
155. Hunt Allcott. 2015. Site Selection Bias in Program Evaluation. The Quarterly Journal of Economics
130:3, 1117-1165. [Crossref]
156. Andrew M. Ryan, James F. Burgess, Justin B. Dimick. 2015. Why We Should Not Be Indifferent
to Specification Choices for Difference-in-Differences. Health Services Research 50:4, 1211-1235.
[Crossref]
157. Luke Keele. 2015. The Statistics of Causal Inference: A View from Political Methodology: Fig. 1.
Political Analysis 23:3, 313-335. [Crossref]
158. Grant D. Jacobsen. 2015. Consumers, experts, and online product evaluations: Evidence from the
brewing industry. Journal of Public Economics 126, 114-123. [Crossref]
159. David Albouy, Bert Lue. 2015. Driving to Opportunity: Local Rents, Wages, Commuting, and SubMetropolitan Quality of Life. Journal of Urban Economics . [Crossref]
160. Adriana Di Liberto, Marco Sideri. 2015. Past dominations, current institutions and the Italian regional
economic performance. European Journal of Political Economy 38, 12-41. [Crossref]
161. Philip DeCicca, Don Kenkel. 2015. Synthesizing Econometric Evidence: The Case of Demand
Elasticity Estimates. Risk Analysis 35:6, 1073-1085. [Crossref]
162. Jennifer Gippel, Tom Smith, Yushu Zhu. 2015. Endogeneity in Accounting and Finance Research:
Natural Experiments as a State-of-the-Art Solution. Abacus 51:2, 143-168. [Crossref]

163. Audun Langørgen. 2015. A structural approach for analyzing fiscal equalization. International Tax
and Public Finance 22:3, 376-400. [Crossref]
164. Gebhard Kirchgässner. 2015. Wissenschaftlicher Fortschritt in den Wirtschaftswissenschaften: Einige
Bemerkungen. Schmollers Jahrbuch 135:2, 209-248. [Crossref]
165. David Colander, Huei-Chun Su. 2015. Making sense of economists' positive-normative distinction.
Journal of Economic Methodology 22:2, 157-170. [Crossref]
166. Marianne Zeyringer, Shonali Pachauri, Erwin Schmid, Johannes Schmidt, Ernst Worrell, Ulrich B.
Morawetz. 2015. Analyzing grid extension and stand-alone photovoltaic systems for the cost-effective
electrification of Kenya. Energy for Sustainable Development 25, 75-86. [Crossref]
167. D. Mark Anderson, Daniel I. Rees, Joseph J. Sabia. 2015. Anderson et al. Respond. American Journal
of Public Health 105:4, e8-e9. [Crossref]
168. Daisy J. Huang, Charles K. Leung, Baozhi Qu. 2015. Do bank loans and local amenities explain
Chinese urban house prices?. China Economic Review . [Crossref]
169. Marion Fourcade, Etienne Ollion, Yann Algan. 2015. The Superiority of Economists. Journal of
Economic Perspectives 29:1, 89-114. [Abstract] [View PDF article] [PDF with links]
170. Shlomo Yitzhaki. 2015. Gini’s mean difference offers a response to Leamer’s critique. METRON .
[Crossref]
171. Alexander Volokh. Law: Economics of Its Public Enforcement 570-577. [Crossref]
172. Steve Gibbons, Henry G. Overman, Eleonora Patacchini. Spatial Methods 115-168. [Crossref]
173. Seán M. Muller. 2015. Causal Interaction and External Validity: Obstacles to the Policy Relevance of
Randomized Evaluations. The World Bank Economic Review 29:suppl 1, S217-S225. [Crossref]
174. Francesca Colantuoni, Christian Rojas. 2015. THE IMPACT OF SODA SALES TAXES ON
CONSUMPTION: EVIDENCE FROM SCANNER DATA. Contemporary Economic Policy n/a-n/
a. [Crossref]
175. X. Vives. 2014. Strategic Complementarity, Fragility, and Regulation. Review of Financial Studies
27:12, 3547-3592. [Crossref]
176. Ing-Haw Cheng, Wei Xiong. 2014. Financialization of Commodity Markets. Annual Review of
Financial Economics 6:1, 419-441. [Crossref]
177. Richard J Silverwood, Michael V Holmes, Caroline E Dale, Debbie A Lawlor, John C Whittaker,
George Davey Smith, David A Leon, Tom Palmer, Brendan J Keating, Luisa Zuccolo, Juan P
Casas, Frank Dudbridge. 2014. Testing for non-linear causal effects using a binary genotype in a
Mendelian randomization study: application to alcohol and cardiovascular traits. International Journal
of Epidemiology 43:6, 1781-1790. [Crossref]
178. Donald P. Green, Dane R. Thorley. 2014. Field Experimentation and the Study of Law and Policy.
Annual Review of Law and Social Science 10:1, 53-72. [Crossref]
179. Stephen Burgess, Neil M. Davies, Simon G. Thompson. 2014. Instrumental Variable Analysis with a
Nonlinear Exposure–Outcome Relationship. Epidemiology 25:6, 877-885. [Crossref]
180. John Rust. 2014. The Limits of Inference with Theory: A Review of Wolpin (2013). Journal of
Economic Literature 52:3, 820-850. [Abstract] [View PDF article] [PDF with links]
181. Sean Tanner. 2014. QCA is of questionable value for policy research. Policy and Society 33:3, 287-298.
[Crossref]
182. Hsiang-Ke Chao. 2014. Models and Credibility. Philosophy of the Social Sciences 44:5, 588-605.
[Crossref]

183. Joerg Dietz, John Antonakis, Ulrich Hoffrage, Franciska Krings, Julian N. Marewski, Christian
Zehnder. 2014. Teaching Evidence-Based Management With A Focus on Producing Local Evidence.
Academy of Management Learning & Education 13:3, 397-414. [Crossref]
184. R. Berk, L. Brown, A. Buja, E. George, E. Pitkin, K. Zhang, L. Zhao. 2014. Misspecified Mean
Function Regression: Making Good Use of Regression Models That Are Wrong. Sociological Methods
& Research 43:3, 422-451. [Crossref]
185. Annette N. Brown, Drew B. Cameron, Benjamin D. K. Wood. 2014. Quality evidence for
policymaking: I’ll believe it when I see the replication. Journal of Development Effectiveness 6:3, 215-235.
[Crossref]
186. Michael Peneder, Martin Woerter. 2014. Competition, R&D and innovation: testing the inverted-U
in a simultaneous system. Journal of Evolutionary Economics 24:3, 653-687. [Crossref]
187. HEIDI ALLEN, BILL J. WRIGHT, KRISTIN HARDING, LAUREN BROFFMAN. 2014. The
Role of Stigma in Access to Health Care for the Poor. Milbank Quarterly 92:2, 289-318. [Crossref]
188. Jimena Hurtado. 2014. Albert O. Hirschman y la economía del desarrollo: lecciones para el presente.
Cuadernos de Economía 33:62, 7-31. [Crossref]
189. John M. de Figueiredo, Brian Kelleher Richter. 2014. Advancing the Empirical Research on Lobbying.
Annual Review of Political Science 17:1, 163-185. [Crossref]
190. William N. Faulkner. 2014. A critical analysis of a randomized controlled trial evaluation in Mexico:
Norm, mistake or exemplar?. Evaluation 20:2, 230-243. [Crossref]
191. J. Huang, F. J. Chaloupka, G. T. Fong. 2014. Cigarette graphic warning labels and smoking prevalence
in Canada: a critical examination and reformulation of the FDA regulatory impact analysis. Tobacco
Control 23:Supplement 1, i7-i12. [Crossref]
192. Christopher R. Knittel, Konstantinos Metaxoglou. 2014. Estimation of Random-Coefficient Demand
Models: Two Empiricists' Perspective. Review of Economics and Statistics 96:1, 34-59. [Crossref]
193. Michael Waterson. 2014. Structural versus Quasi-Experimental Approaches to Industrial
Organization. International Journal of the Economics of Business 21:1, 43-47. [Crossref]
194. Timo Mitze. 2014. Does regional science need an experimentalist buzz?. Regional Studies, Regional
Science 1:1, 51-59. [Crossref]
195. Adam Martin, David Ogilvie, Marc Suhrcke. 2014. Evaluating causal relationships between urban
built environment characteristics and obesity: a methodological review of observational studies.
International Journal of Behavioral Nutrition and Physical Activity 11:1, 142. [Crossref]
196. Heather M. Stephens, Mark D. Partridge, Alessandra Faggian. 2013. INNOVATION,
ENTREPRENEURSHIP AND ECONOMIC GROWTH IN LAGGING REGIONS. Journal of
Regional Science 53:5, 778-812. [Crossref]
197. R. Forrest McCluer, Martha A. Starr. 2013. Using Difference in Differences to Estimate Damages
in Healthcare Antitrust: A Case Study of Marshfield Clinic. International Journal of the Economics of
Business 20:3, 447-469. [Crossref]
198. Joachim Gassen. 2013. Causal inference in empirical archival financial accounting research. Accounting,
Organizations and Society . [Crossref]
199. Eric Stokan. 2013. Testing Rubin?s Model 25 Years Later. Economic Development Quarterly 27:4,
301-315. [Crossref]
200. Michael O'Hara, Christopher F. Parmeter. 2013. Nonparametric Generalized Least Squares in Applied
Regression Analysis. Pacific Economic Review 18:4, 456-474. [Crossref]
201. Alexander Libman, Carsten Herrmann-Pillath, Gaurav Yadav. 2013. Are human rights and economic
well-being substitutes? The evidence from migration patterns across the Indian states. European
Journal of Political Economy 31, 139-164. [Crossref]

202. Seth Gershenson. 2013. The causal effect of commute time on labor supply: Evidence from a natural
experiment involving substitute teachers. Transportation Research Part A: Policy and Practice 54,
127-140. [Crossref]
203. Dieter Pennerstorfer, Christoph Weiss. 2013. Spatial clustering and market power: Evidence from the
retail gasoline market. Regional Science and Urban Economics 43:4, 661-675. [Crossref]
204. Glenn W. Harrison. 2013. Field experiments and methodological intolerance. Journal of Economic
Methodology 20:2, 103-117. [Crossref]
205. V. Ridde, S. Haddad. 2013. Pragmatisme et réalisme pour l’évaluation des interventions de santé
publique. Revue d'Épidémiologie et de Santé Publique 61, S95-S106. [Crossref]
206. Ila Patnaik, Ajay Shah, Nirvikar Singh. 2013. Foreign Investors under Stress: Evidence From India.
International Finance 16:2, 213-244. [Crossref]
207. Andrew Healy, Neil Malhotra. 2013. Retrospective Voting Reconsidered. Annual Review of Political
Science 16:1, 285-306. [Crossref]
208. Degnet Abebaw. 2013. INFANT AND CHILD HEALTH IN ETHIOPIA: REFLECTIONS ON
REGIONAL PATTERNS AND CHANGES. Journal of International Development 25:4, 536-548.
[Crossref]
209. John Ioannidis, Chris Doucouliagos. 2013. WHAT'S TO KNOW ABOUT THE CREDIBILITY
OF EMPIRICAL ECONOMICS?. Journal of Economic Surveys n/a-n/a. [Crossref]
210. Scott Alan Carson. 2013. The Significance and Relative Contributions of Demographic, Residence,
and Socioeconomic Status in Nineteenth-Century U.S. BMI Variation. Historical Methods: A Journal
of Quantitative and Interdisciplinary History 46:2, 67-76. [Crossref]
211. T.D. Stanley. 2013. WHAT'S TO DO ABOUT EMPIRICAL ECONOMICS?. Journal of Economic
Surveys n/a-n/a. [Crossref]
212. Alfredo R. Paloyo. 2013. Editorial: On the orthography of heteros*edasticity. Journal of the Royal
Statistical Society: Series A (Statistics in Society) 176:2, 291-293. [Crossref]
213. Jan F. Kiviet. 2013. Identification and inference in a simultaneous equation under alternative
information sets and sampling schemes. The Econometrics Journal 16:1, S24-S59. [Crossref]
214. Meghana Ayyagari, Asli Demirguc-Kunt, Vojislav Maksimovic. Financing in Developing Countries
683-757. [Crossref]
215. Scott Alan Carson. 2013. Body mass, wealth, and inequality in the 19th century: Joining the debate
surrounding equality and health. Economics & Human Biology 11:1, 90-94. [Crossref]
216. Michael E Martell. 2013. Differences Do Not Matter: Exploring the Wage Gap for Same-Sex Behaving
Men. Eastern Economic Journal 39:1, 45-71. [Crossref]
217. Edward Feser. 2013. Isserman’s Impact. International Regional Science Review 36:1, 44-68. [Crossref]
218. Ila Patnaik, Ajay Shah, Nirvikar Singh. 2013. Foreign Investors Under Stress: Evidence from India.
IMF Working Papers 13:122, 1. [Crossref]
219. Philippe Choné, Laurent Linnemer. 2012. A Treatment Effect Method for Merger Analysis with an
Application to Parking Prices in Paris. The Journal of Industrial Economics 60:4, 631-656. [Crossref]
220. Nancy Cartwright. 2012. Presidential Address: Will This Policy Work for You? Predicting
Effectiveness Better: How Philosophy Helps. Philosophy of Science 79:5, 973-989. [Crossref]
221. Michael E. Martell. 2012. Do ENDAs End Discrimination for Behaviorally Gay Men?. Journal of
Labor Research . [Crossref]
222. Michael Anderson, Jeremy Magruder. 2012. Learning from the Crowd: Regression Discontinuity
Estimates of the Effects of an Online Review Database*. The Economic Journal 122:563, 957-989.
[Crossref]

223. Jean-François Houde. 2012. Spatial Differentiation and Vertical Mergers in Retail Markets for
Gasoline. American Economic Review 102:5, 2147-2182. [Abstract] [View PDF article] [PDF with
links]
224. Madhu Sudan Mohanty. 2012. Effects of positive attitude and optimism on wage and employment:
A double selection approach. The Journal of Socio-Economics 41:3, 304-316. [Crossref]
225. Gunther Capelle-Blancard, Stéphanie Monjon. 2012. Trends in the literature on socially responsible
investment: looking for the keys under the lamppost. Business Ethics: A European Review 21:3, 239-250.
[Crossref]
226. Z. He, W. Xiong. 2012. Dynamic Debt Runs. Review of Financial Studies 25:6, 1799-1843. [Crossref]
227. Stephen Gibbons, Henry G. Overman. 2012. MOSTLY POINTLESS SPATIAL
ECONOMETRICS?*. Journal of Regional Science no-no. [Crossref]
228. Malcolm B Coate, Jeffrey H Fischer. 2012. Why Can't We All Just Get Along? Structural Modelling
and Natural Experiments in Merger Analysis. European Competition Journal 8:1, 41-71. [Crossref]
229. Austin B Frakt, Steven D Pizer, Roger Feldman. 2012. The Effects of Market Structure and Payment
Rate on the Entry of Private Health Plans into the Medicare Market. Inquiry 49:1, 15-36. [Crossref]
230. Ruopeng An, Roland Sturm. 2012. School and Residential Neighborhood Food Environment and
Diet Among California Youth. American Journal of Preventive Medicine 42:2, 129-135. [Crossref]
231. Victor Murinde. 2012. Financial Development and Economic Growth: Global and African Evidence.
Journal of African Economies 21:suppl 1, i10-i56. [Crossref]
232. Nicholas Bloom, Zack Cooper, Martin Gaynor, Stephen Gibbons, Simon Jones, Alistair McGuire,
Rodrigo Moreno-Serra, Carol Propper, John Van Reenen, Stephan Seiler. 2011. In defence of our
research on competition in England's National Health Service. The Lancet 378:9809, 2064-2065.
[Crossref]
233. Allyson Pollock, Azeem Majeed, Alison Macfarlane, Ian Greener, Graham Kirkwood, Howard
Mellett, Sylvia Godden, Sean Boyle, Carol Morelli, Petra Brhlikova. 2011. In defence of our research
on competition in England's National Health Service – Authors' reply. The Lancet 378:9809,
2065-2066. [Crossref]
234. Daniel E. Ho, Donald B. Rubin. 2011. Credible Causal Inference for Empirical Legal Studies. Annual
Review of Law and Social Science 7:1, 17-40. [Crossref]
235. Jose G. Montalvo. 2011. Re-examining the evidence on the electoral impact of terrorist attacks: The
Spanish election of 2004. Electoral Studies . [Crossref]
236. Kevin Milligan. 2011. The design of tax policy in Canada: thoughts prompted by Richard
Blundell's ‘Empirical evidence and tax policy design’. Canadian Journal of Economics/Revue canadienne
d'économique 44:4, 1184-1194. [Crossref]
237. G. Andrew Karolyi. 2011. The Ultimate Irrelevance Proposition in Finance?. Financial Review 46:4,
485-512. [Crossref]
238. Richard Stanton, Nancy Wallace. 2011. The Bear's Lair: Index Credit Default Swaps and the Subprime
Mortgage Crisis. Review of Financial Studies 24:10, 3250-3280. [Crossref]
239. Ole Dahl Rasmussen, Nikolaj Malchow-Møller, Thomas Barnebeck Andersen. 2011. Walking the
talk: the need for a trial registry for development interventions. Journal of Development Effectiveness
1-18. [Crossref]
240. Valerie A. Ramey. 2011. Can Government Purchases Stimulate the Economy?. Journal of Economic
Literature 49:3, 673-685. [Abstract] [View PDF article] [PDF with links]
241. François Claveau. 2011. Evidential variety as a source of credibility for causal inference: beyond sharp
designs and structural models. Journal of Economic Methodology 18:3, 233-253. [Crossref]

242. Michael A. Clemens, Gabriel Demombynes. 2011. When does rigorous impact evaluation make a
difference? The case of the Millennium Villages. Journal of Development Effectiveness 3:3, 305-339.
[Crossref]
243. Peter Arcidiacono, Paul B. Ellickson. 2011. Practical Methods for Estimation of Dynamic Discrete
Choice Models. Annual Review of Economics 3:1, 363-394. [Crossref]
244. Jens Ludwig,, Jeffrey R. Kling,, Sendhil Mullainathan. 2011. Mechanism Experiments and Policy
Evaluations. Journal of Economic Perspectives 25:3, 17-38. [Abstract] [View PDF article] [PDF with
links]
245. Joel Slemrod, Caroline Weber. 2011. Evidence of the invisible: toward a credibility revolution in the
empirical analysis of tax evasion and the informal economy. International Tax and Public Finance .
[Crossref]
246. Jeffrey Zabel, Maurice Dalton. 2011. The impact of minimum lot size regulations on house prices in
Eastern Massachusetts. Regional Science and Urban Economics . [Crossref]
247. Allan Dafoe. 2011. Statistical Critiques of the Democratic Peace: Caveat Emptor. American Journal
of Political Science 55:2, 247-262. [Crossref]
248. Judea Pearl. 2011. Statistics and Causality: Separated to Reunite-Commentary on Bryan Dowd's
“Separated at Birth”. Health Services Research 46:2, 421-429. [Crossref]
249. Hilmar Schneider, Arne Uhlendorff, Klaus F. Zimmermann. 2011. Mit Workfare aus der Sozialhilfe?
Lehren aus einem Modellprojekt. Zeitschrift für ArbeitsmarktForschung . [Crossref]
250. Henk Folmer, Olof Johansson-Stenman. 2011. Does Environmental Economics Produce Aeroplanes
Without Engines? On the Need for an Environmental Social Science. Environmental and Resource
Economics 48:3, 337-361. [Crossref]
251. Ole Rogeberg, Hans Olav Melberg. 2011. Acceptance of unsupported claims about reality: a blind
spot in economics. Journal of Economic Methodology 18:1, 29-52. [Crossref]
252. Arnold Kling. 2011. MACROECONOMETRICS: THE SCIENCE OF HUBRIS. Critical Review
23:1, 123-133. [Crossref]
253. Orley Ashenfelter, Daniel Hosken, Michael Vita, Matthew Weinberg. 2011. Retrospective Analysis
of Hospital Mergers. International Journal of the Economics of Business 18:1, 5-16. [Crossref]
254. Gregory K. Leonard, G. Steven Olley. 2011. What Can Be Learned About the Competitive Effects of
Mergers from “Natural Experiments”?. International Journal of the Economics of Business 18:1, 103-107.
[Crossref]
255. Robert J. Sampson. 2010. Gold Standard Myths: Observations on the Experimental Turn in
Quantitative Criminology. Journal of Quantitative Criminology 26:4, 489-500. [Crossref]
256. James Fenske. 2010. THE CAUSAL HISTORY OF AFRICA: A RESPONSE TO HOPKINS.
Economic History of Developing Regions 25:2, 177-212. [Crossref]
257. Changhui Kang. 2010. Confronting the shadow education system: what government policies for what
private tutoring?. Education Economics 18:3, 373-375. [Crossref]
258. James J. Heckman. 2010. Building Bridges between Structural and Program Evaluation Approaches
to Evaluating Policy. Journal of Economic Literature 48:2, 356-398. [Abstract] [View PDF article]
[PDF with links]
259. Angus Deaton. 2010. Instruments, Randomization, and Learning about Development. Journal of
Economic Literature 48:2, 424-455. [Abstract] [View PDF article] [PDF with links]
260. Agnès Labrousse. 2010. Nouvelle économie du développement et essais cliniques randomisés : une
mise en perspective d’un outil de preuve et de gouvernement. Revue de la régulation :7. . [Crossref]
261. Judith Favereau. Expérimentations 121-125. [Crossref]

