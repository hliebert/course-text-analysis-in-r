Econometrica, Vol. 70, No. 1 (January, 2002), 357â€“368

REGRESSIONS, SHORT AND LONG

By Philip J. Cross and Charles F. Manski1
1 introduction
Suppose that each member of a population is characterized by a triple y x z. Here
y is real-valued, x takes values in a ï¬nite dimensional real space X, and z takes values in
a J -element ï¬nite set Z. Let P denote the population distribution of y x z.
This paper studies the problem of identiï¬cation of the long regression Ey  x z when
the short conditional distributions P y  x and P z  x are known but the long conditional
distribution P y  x z is not known. The nature of the problem is revealed by the Law
of Total Probability,

(1)
Prz = j  xP y  x z = j
P y  x =
jâˆˆZ

Knowledge of P y  x and P z  x restricts P y  x z = j j âˆˆ Z to J -vectors of distributions that satisfy (1). Our objective is to determine the implied restrictions on Ey  x z.
Aspects of the problem of inference on Ey  x z have been studied in several literatures with varying concerns and terminology. The classical literature on linear regression
compares the parameter estimates obtained in a least squares ï¬t of y to x with those
obtained in a least squares ï¬t of y to x z. The expected difference between the estimated coefï¬cients on x in the former and the latter ï¬ts is sometimes called â€œomitted
variable bias.â€ The ï¬ndings are speciï¬c to least squares estimation of linear regressions
and so do not directly inform the present nonparametric analysis. We do, however, borrow
the terms short regression and long regression from Goldberger (1991, Sec. 17.2).
Stimulated by Simpson (1951), statisticians have been intrigued by the fact that Ey 
x may be increasing in a scalar x and yet all J components of Ey  x z = j j âˆˆ Z
may be decreasing in x. Studies of Simpsonâ€™s Paradox have sought to characterize the
circumstances in which this phenomenon occurs. See, for example, Lindley and Novick
(1981) and Zidek (1984).
Stimulated by Robinson (1950), social scientists have studied the ecological inference
problem, often described as inference on individual behavior from aggregate data (e.g.,
King (1997)). A leading example in political science is inference on voting behavior y conditional on electoral district x and demographic attributes z, using administrative records
on voting by district and census data on the attributes of persons in each district. The
administrative records and census data reveal P y  x and P z  x respectively; the problem is to infer P y  x z. Focusing on settings in which y and z are both binary variables,
Duncan and Davis (1953) and Goodman (1953) performed informal partial analyses of
the identiï¬cation problem that we address in generality here.
Following Huber (1964), research on robust estimation under contaminated sampling
has taken the object of interest to be P y  x z = j for a speciï¬ed value of j. Values
1
We have beneï¬ted from the opportunity to present this research in seminars at Northwestern
University and the University of Wisconsinâ€”Madison, and from the comments of two anonymous
referees. This research was supported in part by National Science Foundation Grant SBR-9722846.

357

358

p. j. cross and c. f. manski

of y x z with z = j are said to be error-free, whereas those with z = j are said to be
erroneous. The researcher only observes y x pairs, not y x z triples, and so does not
know which observations are error free. The researcher is, however, assumed to know the
conditional probability Prz = j  x that an observation is error-free, or at least to know
a lower bound on this probability. Recently, Horowitz and Manski (1995) showed that
equation (1) implies a sharp bound on Ey  x z = j. The lower and upper bounds on
Ey  x z = j are the expectations on certain right-truncated and left-truncated versions
of P y  x. This ï¬nding forms the starting point for the present analysis.
Our basic ï¬ndings are developed in Section 2. We prove that the set of feasible values
of the J -vector Ey  x z = j j âˆˆ Z, its identiï¬cation region, is a bounded convex set
whose extreme points are the expectations of certain J -vectors of stacked distributions.
When P y  x has ï¬nite support or J = 2, we are able to characterize the identiï¬cation
region fully as the convex hull of these extreme points. When P y  x has inï¬nite support
and J â‰¥ 3, we show that the identiï¬cation region contains this convex hull and is contained
in another convex polytope.
Whereas the analysis in Section 2 assumes no information is available beyond knowledge of P y  x and P z  x, we entertain additional information in Section 3. Here we
study exclusion restrictions asserting that y is either mean-independent or statistically independent of some component of x, conditional on z and the other components of x. We
ï¬rst characterize abstractly the identifying power of such exclusion restrictions and then
present a simple rank condition that sufï¬ces for point identiï¬cation of long regressions.
Section 4 shows how our ï¬ndings may be used to address structural prediction problems,
in which the long regression Ey  x z is assumed to remain invariant under hypothesized
changes in the covariate distribution P x z.
2 identifying Ey  x z given knowledge of P y  x and P z  x
We proceed in three steps. Section 2.1 reviews the sharp bound on the scalar Ey  x z =
j reported in Horowitz and Manski (1995). Section 2.2 uses this bound to characterize
the identiï¬cation region for the J -vector Ey  x Â· â‰¡ Ey  x z = j j âˆˆ Z. Section 2.3
extends the analysis to Ey  Â· Â· â‰¡ Ey  x Â· x âˆˆ X.
21 Identiï¬cation of Ey  x z = j
Fix x. For p âˆˆ 0 1, let qx p denote the p-quantile of P y  x. Let Lx p and Ux p
be, respectively, the right-truncated and left-truncated distributions deï¬ned by

(2)

ï£±
Pry â‰¤ t  x
ï£´
ï£²
p
Lx pâˆ’  t â‰¡
ï£´
ï£³
1
ï£±
ï£´
ï£²0

Ux pâˆ’  t â‰¡

if t < qx p
if t â‰¥ qx p

Pry â‰¤ t  x âˆ’ 1 âˆ’ p
ï£´
ï£³
p

if t < qx 1 âˆ’ p
if t â‰¥ qx 1 âˆ’ p

Let P y  x and P z  x be known. Suppose that Ey  x exists and that xj â‰¡ Prz = j 
x > 0 for all j âˆˆ Z.

regressions, short and long

359

Horowitz and Manski (1995, Proposition 4) prove that the smallest and largest feasible
values of Ey  x z = j are the expected values of y under Lx xj  and Ux xj , respectively. Thus



Ey  x z = j âˆˆ
y dLx xj  y dUx xj  â‰¡ E xj  E xj 
(3)
Simple reasoning underlies this result. Consider the subpopulation with covariates x. The
smallest feasible value of Ey  x z = j occurs if, within this subpopulation, the persons
with z = j have the smallest values of y. Then P y  x z = j = Lx xj . The largest feasible
value occurs if the persons with z = j have the largest values of y. Then P y  x z = j =
Ux xj .
The bound (3) has a particularly simple form when y is a binary outcome variable,
taking the values 0 and 1. Then qx p = 0 if Pry = 1  x < 1 âˆ’ p and qx p = 1 otherwise.
It follows that
E xj = max 0

Pry = 1  x âˆ’ 1 âˆ’ xj 
xj



E xj = min 1

Pry = 1  x

xj

A simple direct proof of this result is given in Horowitz and Manski (1995, Corollary 1.2).
Duncan and Davis (1953) gave numerical illustrations when y and z are both binary, but
did not formalize their analysis.
The univariate bound (3) immediately implies a bound on the J -vector Ey  x Â·. That
is, Ey  x Â· must lie in the J -dimensional rectangle Cx â‰¡ Ã—jâˆˆZ E xj  E xj . The set Cx ,
however, is not the sharp bound on Ey  x Â·. The Law of Total Probability (1) implies
further restrictions, including the Law of Iterated Expectations,

Ey  x =
(4)
xj Ey  x z = j
jâˆˆZ

Hence Ey  x Â· must lie in the intersection of Cx with the hyperplane satisfying (4). In
what follows, we characterize further the identiï¬cation region for Ey  x Â·.
22 Identiï¬cation of Ey  x Â·
For each value of x, the feasible values of Ey  x Â· follow immediately, albeit
abstractly, from the Law of Total Probability (1). Let  denote the space of all probability distributions on R. Let x denote the set of all J -vectors of distributions on R that
satisfy (1). That is, j  j âˆˆ Z âˆˆ x if, and only if,

P y  x =
(5)
xj j 
jâˆˆZ

Then the identiï¬cation region for Ey  x Â· is

Dx =
y dj  j âˆˆ Z  j  j âˆˆ Z âˆˆ x 
(6)
Some properties of Dx are immediate. The set x is convex and the expectation operator is linear, so Dx is convex. Moreover, Dx is contained within the J -dimensional rectangle Cx . Hence Dx is a bounded convex set.
Our objective is to characterize Dx more precisely. Proposition 1 shows that Dx has at
most J ! distinct extreme points, these being the expectations of the stacked distributions
deï¬ned below. Following Proposition 1, we develop some immediate implications through
two Corollaries.

360

p. j. cross and c. f. manski

The Stacked Distributions
J -vectors of stacked distributions are sequences of J distributions such that the entire
probability mass of the jth distribution lies weakly to the left of that of the j + 1st
distribution. Stacked distributions are essential to characterization of Dx . It will be shown
in Step (ii) of the proof to Proposition 1 that the most extreme sequences P y  x z =
j j = 1     J  compatible with given values of P y  x and P z  x are sequences of
such distributions.
To describe these distribution sequences, we now let Z be the ordered set of integers
1     J . This set has J ! permutations, each of which generates a distinct J -vector of
stacked distributions. We label these permutations of Z as Z m  m = 1     J !, and the
corresponding J -vectors of stacked distributions as Pxjm  j = 1     J  m = 1     J !.
For each value of m, the elements of Pxjm  j = 1     J  solve a recursive set of minimization problems. In what follows, we show the construction of Pxj1  j = 1     J , which
is based on Z 1 , the original ordering of Z. The other J ! âˆ’ 1 J -vectors are generated
from Z m  m = 2 3     J !, which alters the order in which the recursion is performed.
For each j = 1     J  Pxj1 is chosen to minimize its expectation subject to the distributions earlier chosen for Pxi1  i < j, and subject to the global condition that equation (5)
must hold. The recursion is as follows: For j = 1     J  Pxj1 solves the problem
(7)

min
âˆˆ



y d

subject to
(8)

P y  x =

jâˆ’1


i=1

xi Pxi1 + xj  +

J

k=j+1

xk k 

where k âˆˆ   k = j + 1     J are unrestricted probability distributions.
This recursion yields a sequence of stacked distributions. For j = 1, equation (8) reduces
to
(9)

P y  x = x1  +

J

k=2

xk k 

Horowitz and Manski (1995, Proposition 4) show that the distribution solving (7) subject
1
to (9) is Lx x1 , the right-truncated version of P y  x deï¬ned in (2); thus Px1
= Lx x1 .
For j = 2, equation (8) has the form
(10)

P y  x = x1 Lx x1  + x2  +

J

k=3

xk k 

The proof of Horowitz and Manskiâ€™s (1995) Proposition 4 shows that
(11)

P y  x = x1 Lx x1  + 1 âˆ’ x1 Ux 1 âˆ’ x1 

where Ux 1 âˆ’ x1  is the left-truncated version of P y  x that maximizes Ey  x z > 1.
Hence (8) becomes
(12)

Ux 1 âˆ’ x1  =

J

x2
xk
+
 
1 âˆ’ x1
1
âˆ’
x1 k
k=3

regressions, short and long

361

Equation (12) has the same form as (9), with Ux 1 âˆ’ x1  replacing P y  x and
1
, the solution to (7) subject to (12), is a rightx k+1 /1 âˆ’ x1  replacing xk . Hence Px2
truncated version of Ux 1 âˆ’ x1 . By deï¬nition, Lx x1  has no mass to the right of the
1
1
and Px2
are
point qx x1  and Ux 1 âˆ’ x1  has no mass to the left of this point. Hence Px1
stacked side-by-side, with all of the mass of the former distribution lying weakly to the left
of the mass of the latter distribution. The distributions Pxj1  j = 3     J  are similarly
stacked. For each j, the mass of Pxj1 lies weakly to the left of the mass of Px1 j+1 .
Stacking implies that, for each value of j, the supremum of the support of Pxj1 may equal
the inï¬mum of the support of Px1 j+1 , but otherwise the distributions are concentrated on
disjoint intervals. If P y  x has a mass point, then Pxj1 and Px1 j+1 may share this mass
point; indeed Pxj1 and Px1 j+1 may even be degenerate at this common point. However, if
P y  x is continuous, then Pxj1 and Px1 j+1 are continuous and place their mass on disjoint
intervals.
Example: P y  x standard normal, x1 = 1/2 x2 = 1/3 and x3 = 1/6. Since J = 3
in this example, there are 3! = 6 3-vectors of stacked distributions, based on Z 1 through
Z 6 . These are illustrated by their densities in Figure 1.
Notice that the ï¬rst vector of stacked distributions in the ï¬gure is Pxj1  j = 1 2 3.
1
1
Px1
is Lx 1/2, the standard normal right-truncated at 0. Px2
is constructed by righttruncation at 097 of the distribution resulting from Lx 1/2 being removed from the
1
. The second
standard normal. And the remaining mass, which is Ux 1/6, constitutes Px3
2
vector of stacked distributions in Figure 1 is Pxj  j = 1 2 3, where we deï¬ne Z 2 =
1 3 2. And the remaining vectors of stacked distributions in the ï¬gure are derived from
the remaining permutations, Z 3 through Z 6 .
The Extreme Points of the Identiï¬cation Region
With the above as preliminary, Proposition 1 proves that the expectations of the stacked
distributions are the extreme points of Dx .
Proposition 1: Let P y  x and P z  x be known. Let Ey  x exist. Let Exm â‰¡
 y dPxjm  j = 1     J . Then the extreme points of Dx are $Exm  m = 1     J !%.
Proof: By construction, each of the J -vectors in $Exm  m = 1     J !% is a feasible
value of Ey  x Â·. Step (i) of the proof shows that these vectors are extreme points of
Dx . Step (ii) shows that Dx has no other extreme points. In what follows, we simplify the
notation by suppressing the subscript x.
Step (i). It sufï¬ces to consider E 1 . Permuting Z does not alter the argument below.
Suppose that E 1 is not an extreme point of D. Then there exist an & âˆˆ 0 1 and
distinct J -vectors '   '   âˆˆ D such that E 1 = &'  + 1 âˆ’ &'  . Suppose that E 1  '  , and ' 
differ in their ï¬rst component. Then either '1 < E11 < '1 or '1 < E11 < '1 . By construction,
however, E11 = E 1 , the global minimum of Ey  z = 1. So '1 â‰¥ E11 and '1 â‰¥ E11 . Hence it
must be the case that '1 = '1 = E11 .
Now suppose that E 1  '  , and '  differ in their second component. Then '2 < E21 < '2
or '2 < E21 < '2 . But E21 minimizes Ey  z = 2 subject to the previous minimization of
Ey  z = 1. So '2 â‰¥ E21 and '2 â‰¥ E21 . Hence '2 = '2 = E21 . Recursive application of this
reasoning shows that '  = '  = E 1 , contrary to supposition. Hence E 1 is an extreme point
of D.
Step (ii). Let ' âˆˆ D, with '  $E m  m = 1     J !%. Then ' is the expectation of some
feasible J -vector of nonstacked distributions. We want to show that ' is not an extreme

362

p. j. cross and c. f. manski

Figure 1.â€” Densities of stacked distributions for P y  x standard normal, x1  x2  x3  =
1/2 1/3 1/6 for the six permutations of Z = 1 2 3

point of D. Thus, we must show that there exists an & âˆˆ 0 1 and distinct J -vectors
'   '   âˆˆ D such that ' = &'  + 1 âˆ’ &'  .
Let the set-valued function S denote the support of any probability distribution
 on the real line. Let j  j âˆˆ Z âˆˆ  be any feasible J -vector of distributions with
expectation '. This J -vector is not stacked, so there exist components i and k such
that inf Si  sup Si  âˆ© inf Sk  sup Sk  has positive length. Thus sup Si  >
inf Sk  and sup Sk  > inf Si . For ease of exposition, henceforth let aj â‰¡ inf Sj 
and bj â‰¡ sup Sj , for j = i k.
We now construct a feasible J -vector of distributions that shifts mass, in a particular
balanced manner, between distributions i and k , while leaving the other components of
j  j âˆˆ Z unchanged. Let 0 < + < 21 bi âˆ’ ak . Then k ak  ak + + > 0 i bi âˆ’ + bi  > 0,
and ak  ak + + âˆ© bi âˆ’ + bi  = . Let
,â‰¡

k k ak  ak + +

i i bi âˆ’ + bi 

regressions, short and long

363

Now deï¬ne the new J -vector j  j âˆˆ Z as follows: Let j = j for j = i k. If , â‰¤ 1, let
ï£±

ï£´
if A âŠ‚ ak  ak + +
i A + k k A 0
ï£´
ï£´
i
ï£²

i A k A = 1 âˆ’ ,i A k A + , i i A if A âŠ‚ bi âˆ’ + bi 
ï£´

ï£´
k
ï£´
ï£³
elsewhere.
i A k A
Alternatively, if , > 1, let
ï£±
1 k
1
ï£´
ï£´
i A +
k A 1 âˆ’ k A if A âŠ‚ ak  ak + +
ï£´
ï£´
,

,
ï£²
i
i A k A = 0  A + i  A
if A âŠ‚ bi âˆ’ + bi 
ï£´
k
ï£´
k i
ï£´
ï£´
ï£³
elsewhere.
i A k A
Thus, the new J -vector shifts i mass leftward from the bi âˆ’ + bi  interval to the ak  ak +
+ interval and compensates by shifting k mass rightward to the bi âˆ’ + bi  interval from
the ak  ak + + interval. The , parameter ensures that we shift equal amounts of mass and
that i i + k k = i i + k k . Hence j  j âˆˆ Z is a feasible J -vector of distributions,
that is, an element of  . The mean of j  j âˆˆ Z is related to the mean of j  j âˆˆ Z as
follows: 'i < 'i  'k > 'k , and 'j = 'j for j = i k.
An analogous operation switching the roles of i and k produces another new J -vector
j  j âˆˆ Z. Now let 0 < + < 21 bk âˆ’ ai  and redeï¬ne , accordingly. This construction shifts
k mass leftward from the bk âˆ’ + bk  interval to the ai  ai + + interval and shifts an
equal amount of i mass rightward to the bk âˆ’ + bk  interval from the ai  ai + + interval,
while ensuring that i i + k k = i i + k k . The mean of this J -vector is related to
the mean of j  j âˆˆ Z as follows: 'i > 'i  'k < 'k , and 'j = 'j for j = i k.
It follows from the above that i 'i + k 'k = i 'i + k 'k = i 'i + k 'k . Thus, 'i  'k 
lies on the line connecting 'i  'k  and 'i  'k . Moreover, 'i > 'i > 'i and 'k > 'k > 'k .
Hence 'i  'k  is a strictly convex combination of 'i  'k  and 'i  'k . Finally recall that
'j = 'j = 'j for j = i k. Hence ' is a strictly convex combination of '  and '  . Thus ' is
not an extreme point of D.
Q.E.D.
Proposition 1 has two immediate implications that further characterize the identiï¬cation
region. Let conv$Exm  m = 1     J !% denote the convex hull of $Exm  m = 1     J !%. Then
we have the following corollaries.
Proposition 1, Corollary 1: conv$Exm  m = 1     J !% âŠ‚ Dx .
Proof: Dx is a convex set containing $Exm  m = 1     J !%. Hence Dx contains the
convex hull of these points.
Q.E.D.
Proposition 1, Corollary 2: If
conv$Exm  m = 1     J !%.

P y  x

has

ï¬nite

support,

then

Dx =

Proof: Minkowskiâ€™s Theorem (e.g., BrÃ¸ndsted (1983, Theorem 5.10)) shows that a
compact convex set in R J is the convex hull of its extreme points. We already know that
Dx is a bounded convex set, so we need only show that Dx is closed. Let Y denote the
support of P y  x and suppose that Y has ï¬nite cardinality H. For j âˆˆ Z and 0 âˆˆ Y , let

364

p. j. cross and c. f. manski

1j0 be a feasible value for Pry = 0  x z = j. Then equation (5) becomes the following
system of H linear equations in the J Ã— H unknowns 1j0 :
Pry = 0  x =


jâˆˆZ

xj 1j0 

0 âˆˆ Y

Let 2x denote the solutions to this system of equations. 2x forms a closed set in R J Ã—H .

The identiï¬cation region for Ey  x Â· is Dx = $ 0âˆˆY 0 Â· 1j0  j âˆˆ Z 1 âˆˆ 2x %, a linear
J
map from 2x to R . Hence Dx is closed.
Q.E.D.
The Identiï¬cation Region when P y  x has Inï¬nite Support
Proposition 1 and its Corollaries fully characterize the identiï¬cation region when P y 
x has ï¬nite support, but only partially so when P y  x has inï¬nite support. If Dx can
be shown to be closed, then the reasoning of Corollary 2 may be applied. Unfortunately,
it appears difï¬cult to characterize Dx topologically when P y  x has inï¬nite support.
Although we currently are not able to characterize fully the identiï¬cation region
when P y  x has inï¬nite support, we can add to the characterization given thus far.
We have already shown that Dx contains the convex polytope conv$Exm  m = 1     J !%.
Proposition 2 uses $Exm  m = 1     J !% to construct another convex polytope that contains Dx . When J = 2, this yields a full characterization of Dx .
Proposition 2: For each m = 1     J !, let Z m denote the mth permutation of Z. Let
jm k be the position in Z of the kth element of Z m . Deï¬ne the following subsets of R J :
G0x â‰¡ ' âˆˆ R J 
J
Gm
x â‰¡ ' âˆˆR 

J

j=1
n

k=1

xj 'j = Ey  x 
xjm k 'jm k â‰¥

n

k=1

m
xjm k Exjm
k  n = 1     J âˆ’ 1

m = 1     J !
and
Gx â‰¡

J!

m=0

Gm
x 

Then Gx is a convex polytope and conv$Exm  m = 1     J !% âŠ‚ Dx âŠ‚ Gx . When J = 2
conv$Exm  m = 1     J !% = Dx = Gx .
Proof: Proposition 1, Corollary 1 showed that conv$Exm  m = 1     J !% âŠ‚ Dx . It is
easy to see that Dx âŠ‚ Gx . The Law of Iterated Expectations (4) requires that every point
in Dx satisfy the equality deï¬ning G0x . For each m â‰¥ 1, the construction of Exm by recursive
minimization implies that every point in Dx must satisfy each of the J âˆ’ 1 inequalities
deï¬ning Gm
x . Hence Dx âŠ‚ Gx .
To show that Gx is a convex polytope, observe ï¬rst that G0x is a hyperplane and each
Gm
x is the intersection of J âˆ’ 1 closed half-spaces. Hence Gx is a polyhedral set. Next
observe that Gx is bounded from below. In particular, the ï¬rst inequality used to deï¬ne
each set Gm
x shows that ' âˆˆ Gx â‡’ 'j â‰¥ E xj  j âˆˆ Z. Finally, observe that this lower bound
and the equality deï¬ning G0x imply that Gx is bounded from above; in particular, ' âˆˆ

regressions, short and long

365


Gx â‡’ 'i â‰¤ Ey  x âˆ’ k=i xjm k E xk  i âˆˆ Z. Thus Gx is a bounded polyhedral set, and
hence a convex polytope. See BrÃ¸ndsted (1983, Corollary 8.7).
When J = 2 Gx is the line segment connecting the points E x1  E x2  and E x1  E x2 ,
which are the extreme points of Dx . So conv$Exm  m = 1     J !% = Dx = Gx in this special
case.
Q.E.D.
23 Identiï¬cation of EyÂ· Â·
It remains only to extend the analysis from identiï¬cation of Ey  x Â· to identiï¬cation
of EyÂ· Â·. This is straightforward. Knowledge of P y  x and P z  x implies no crossx restrictions on Ey  x Â·. Hence the identiï¬cation region for EyÂ· Â· is the Cartesian
product Ã—xâˆˆX Dx .
3 the identifying power of exclusion restrictions
Propositions 1 and 2 have characterized the restrictions on Ey  x z implied by knowledge of P y  x and P z  x. Tighter inferences may be feasible if additional information
is available. Among the many forms that such information may take, we focus on exclusion restrictions of the type that have been found useful in resolving other identiï¬cation
problems.
Let us dispose ï¬rst of one form of exclusion restriction whose implications are so
immediate as barely to require comment. Suppose it is known that y is mean-independent
of z, conditional on x; that is, Ey  x z = Ey  x. Then knowledge of P y  x identiï¬es
Ey  x z.
More interesting are exclusion restrictions connecting Ey  x z across different values
of x. Let x = v w and X = V Ã— W . One familiar form of exclusion restriction asserts
that y is mean-independent of v, conditional on w z. Thus
(13)

Ey  v w z = Ey  w z

A stronger form of exclusion asserts that y is statistically independent of v, conditional
on w z. Thus
(14)

P y  v w z = P y  w z

Restrictions of these forms are often called instrumental variable assumptions, v being the
instrumental variable.
Proposition 3 below characterizes fully, albeit abstractly, the identifying power of
assumptions (13) and (14). We then present a weaker, but much simpler, ï¬nding that
yields a straightforward rank condition for point identiï¬cation of Ey  w Â· â‰¡ Ey  w z =
j j âˆˆ Z. This rank condition indicates that, in applications, exclusion restrictions of the
form (13) and (14) often sufï¬ce to identify Ey  w Â·. We also call attention to the fact
that these exclusion restrictions are testable assumptions.
Proposition 3: Let P y  v w and P z  v w be known. Let Ey  v w exist. Let Dwâˆ—
and Dwâˆ—âˆ— denote the identiï¬cation regions for Ey  w Â· under assumptions (13) and (14)
respectively. Then
(15)

Dwâˆ— â‰¡


vâˆˆV

Dv w 

366

p. j. cross and c. f. manski

and
(16)

Dwâˆ—âˆ— â‰¡



y dj  j âˆˆ Z  j  j âˆˆ Z âˆˆ


vâˆˆV

v w âŠ‚ Dwâˆ— 

The corresponding identiï¬cation regions for Ey  Â· Â· are Ã—wâˆˆW Dwâˆ— and Ã—wâˆˆW Dwâˆ—âˆ— .
Proof: Consider assumption (13). Recall that, for each value of v w, we have
j  j âˆˆ Z âˆˆ v w if, and only if,
P y  v w =


jâˆˆZ

v wj j 

Let ' âˆˆ R J . Under (13), ' is a feasible value for Ey  w Â· if, and only if, for every v âˆˆ V
there exists an element of v w whose expectation is '. The set Dwâˆ— comprises these
feasible values of '.
Consider assumption (14). Under (14), j  j âˆˆ Z is a feasible value for P y  w j j âˆˆ
Z if, and only if, j  j âˆˆ Z satisï¬es the system of equations
P y  v w =


jâˆˆZ

v wj j 

for all v âˆˆ V 

Thus the set of feasible values for P y  w j j âˆˆ Z is âˆ©vâˆˆV v w . The set Dwâˆ—âˆ— comprises the expectations of these feasible J -vectors of distributions. That Dwâˆ—âˆ— âŠ‚ Dwâˆ— follows
from the fact that assumption (14) is stronger than (13). It can also be seen directly by
comparing (15) and (16).
Now consider Ey  Â· Â·. Neither (13) nor (14) imposes a cross-w restriction. Hence the
identiï¬cation regions for Ey  Â· Â· are the Cartesian products of the regions for Ey  w Â·
under these assumptions.
Q.E.D.
A Rank Condition for Point Identiï¬cation
Proposition 3 is general, but it is too abstract to convey a sense of the identifying power
of exclusion restrictions. A much simpler, readily applicable ï¬nding emerges if we exploit
only the Law of Iterated Expectations rather than the full force of the Law of Total
Probability.
Let Cwâˆ— âŠ‚ R J denote the set of solutions ' âˆˆ R J to the system of linear equations
(17)

Ey  v w =


jâˆˆZ

v wj 'j 

for all v âˆˆ V 


Let V  denote the cardinality of the set V . Let
denote the V  Ã— J matrix whose jth
column is v wj  v âˆˆ V . Then we have the following corollary.
Proposition 3, Corollary 1: Dwâˆ— âŠ‚ Cwâˆ— . If
= Cwâˆ— .

Dwâˆ—



has rank J , then Cwâˆ— is a singleton and

Proof: The Law of Iterated Expectations and assumption (13) require that feasible
values of Eyw Â· solve equations (17). Hence Dwâˆ— âŠ‚ Cwâˆ— . Dwâˆ— is nonempty, so (17) must

have at least one solution. If has rank J , then (17) has a unique solution and Dwâˆ— = Cwâˆ— .
Q.E.D.

regressions, short and long

367

Goodman (1953), considering problems in which y and z are binary, showed informally
that knowledge of Ey  x and P z  x combined with an exclusion restriction yields the
rank condition for point identiï¬cation developed in Proposition 3, Corollary 1. However,
the literature appears to contain no precedent for Proposition 3, nor for the general form
of Corollary 1.
Testing Exclusion Restrictions
We have thus far supposed that the speciï¬ed exclusion restriction is correct. Suppose
that an attempt to solve the system of equations (17) reveals that the solution set Cwâˆ— is
empty. Or, if Cwâˆ— is nonempty, suppose that evaluation of the identiï¬cation region Dwâˆ— or
Dwâˆ—âˆ— , as the case may be, shows the region to be empty. Any such ï¬nding implies that the
speciï¬ed exclusion restriction cannot be correct. Thus, exclusion restrictions of the form
(13) and (14) are testable assumptions.

4 applying the ï¬ndings to structural prediction problems
Applied economists often want to predict how the observed mean population outcome
Ey would change if the covariate distribution were to change from P x z to some
other distribution, say P âˆ— x z. It is common to address this prediction problem under
the assumption that the long regression Ey  Â· Â· is structural, in the sense that Ey  Â· Â·
would remain invariant under the hypothesized change in the covariate distribution. Given
this assumption, the mean outcome under covariate distribution P âˆ— x z would be
E âˆ— y â‰¡

 

Ey  x z = jPrâˆ— z = j  x dP âˆ— x

jâˆˆZ

To motivate the assumption that Ey  Â· Â· is structural, economists often pose behavioral models of the form y = f x z u, wherein an individualâ€™s outcome y is some function f of the covariates x z, and of other factors u. Then EyÂ· Â· is structural if u is
statistically independent of x z and if the distribution of u remains unchanged under
the hypothesized change in the covariate distribution.
What can be said about E âˆ— y when Ey  Â· Â· is unknown? Our ï¬ndings are applicable
if the available data reveal P y  x and P z  x. For example, a well-known problem in
poverty research is to predict participation in social welfare programs under hypothesized
changes in the geographic distribution and demographic attributes of the population. Let
y be a binary variable indicating program participation, let x be a geographic unit such
as a county, and let z denote demographic attributes. One might be willing to assume
that Ey  Â· Â· is structural in the sense deï¬ned above. One might learn Ey  Â· Â· from a
household survey, but a suitable survey may be unavailable in practice. However, administrative records may reveal program participation by county and census data may reveal
demographic attributes by county; that is, P y  x and P z  x.
In such settings our ï¬ndings yield identiï¬cation regions for Ey  Â· Â· and hence for
E âˆ— y. Section 2 showed that, in the absence of assumptions on P y x z, the identiï¬cation region for Ey  Â· Â· is Ã—xâˆˆX Dx . Section 3 showed that, under two alternative
forms of exclusion restrictions, the identiï¬cation regions for Ey  Â· Â· areÃ—wâˆˆW Dwâˆ— and
Ã—wâˆˆW Dwâˆ—âˆ— . Each of these results implies an identiï¬cation region for E âˆ— y. For example,

368

p. j. cross and c. f. manski

in the absence of assumptions on P y x z E âˆ— y must lie in the set 
j  x dP âˆ— x ' âˆˆ Dx  x âˆˆ X.



jâˆˆZ

'j Prâˆ— z =

Dept. of Economics, Georgetown University, Washington, D.C. 20057, U.S.A.; pjc22@
georgetown.edu; www.georgetown.edu/faculty/pjc22
and
Dept. of Economics and Institute for Policy Research, Northwestern University, Evanston,
IL 60208, U.S.A.; cfmanski@northwestern.edu; www.faculty.econ.northwestern.edu/faculty/
manski
Manuscript received November, 1999; ï¬nal revision received July, 2000.
REFERENCES
BrÃ¸ndsted, A. (1983): An Introduction to Convex Polytopes. New York: Springer-Verlag.
Duncan, O., and B. Davis (1953): â€œAn Alternative to Ecological Correlation,â€ American Sociological Review, 18, 665â€“666.
Goldberger, A. (1991): A Course in Econometrics. Cambridge, Mass.: Harvard University Press.
Goodman, L. (1953): â€œEcological Regressions and Behavior of Individuals,â€ American Sociological
Review, 18, 663â€“664.
Horowitz, J., and C. Manski (1995): â€œIdentiï¬cation and Robustness with Contaminated and
Corrupted Data,â€ Econometrica, 63, 281â€“302.
Huber, P. (1964): â€œRobust Estimation of a Location Parameter,â€ Annals of Mathematical Statistics,
35, 73â€“101.
King, G. (1997): A Solution to the Ecological Inference Problem: Reconstructing Individual Behavior
from Aggregate Data. Princeton: Princeton University Press.
Lindley, D., and M. Novick (1981): â€œThe Role of Exchangeability in Inference,â€ Annals of Statistics, 9, 45â€“58.
Robinson, W. (1950): â€œEcological Correlation and the Behavior of Individuals,â€ American Sociological Review, 15, 351â€“357.
Simpson, E. (1951): â€œThe Interpretation of Interaction in Contingency Tables,â€ Journal of the Royal
Statistical Society, 13, 238â€“241.
Zidek, J. (1984): â€œMaximal Simpson-disaggregations of 2 Ã— 2 Tables,â€ Biometrika, 71, 187â€“190.

