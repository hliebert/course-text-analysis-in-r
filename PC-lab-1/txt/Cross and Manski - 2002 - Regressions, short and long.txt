Econometrica, Vol. 70, No. 1 (January, 2002), 357‚Äì368

REGRESSIONS, SHORT AND LONG

By Philip J. Cross and Charles F. Manski1
1 introduction
Suppose that each member of a population is characterized by a triple y x z. Here
y is real-valued, x takes values in a Ô¨Ånite dimensional real space X, and z takes values in
a J -element Ô¨Ånite set Z. Let P denote the population distribution of y x z.
This paper studies the problem of identiÔ¨Åcation of the long regression Ey  x z when
the short conditional distributions P y  x and P z  x are known but the long conditional
distribution P y  x z is not known. The nature of the problem is revealed by the Law
of Total Probability,

(1)
Prz = j  xP y  x z = j
P y  x =
j‚ààZ

Knowledge of P y  x and P z  x restricts P y  x z = j j ‚àà Z to J -vectors of distributions that satisfy (1). Our objective is to determine the implied restrictions on Ey  x z.
Aspects of the problem of inference on Ey  x z have been studied in several literatures with varying concerns and terminology. The classical literature on linear regression
compares the parameter estimates obtained in a least squares Ô¨Åt of y to x with those
obtained in a least squares Ô¨Åt of y to x z. The expected difference between the estimated coefÔ¨Åcients on x in the former and the latter Ô¨Åts is sometimes called ‚Äúomitted
variable bias.‚Äù The Ô¨Åndings are speciÔ¨Åc to least squares estimation of linear regressions
and so do not directly inform the present nonparametric analysis. We do, however, borrow
the terms short regression and long regression from Goldberger (1991, Sec. 17.2).
Stimulated by Simpson (1951), statisticians have been intrigued by the fact that Ey 
x may be increasing in a scalar x and yet all J components of Ey  x z = j j ‚àà Z
may be decreasing in x. Studies of Simpson‚Äôs Paradox have sought to characterize the
circumstances in which this phenomenon occurs. See, for example, Lindley and Novick
(1981) and Zidek (1984).
Stimulated by Robinson (1950), social scientists have studied the ecological inference
problem, often described as inference on individual behavior from aggregate data (e.g.,
King (1997)). A leading example in political science is inference on voting behavior y conditional on electoral district x and demographic attributes z, using administrative records
on voting by district and census data on the attributes of persons in each district. The
administrative records and census data reveal P y  x and P z  x respectively; the problem is to infer P y  x z. Focusing on settings in which y and z are both binary variables,
Duncan and Davis (1953) and Goodman (1953) performed informal partial analyses of
the identiÔ¨Åcation problem that we address in generality here.
Following Huber (1964), research on robust estimation under contaminated sampling
has taken the object of interest to be P y  x z = j for a speciÔ¨Åed value of j. Values
1
We have beneÔ¨Åted from the opportunity to present this research in seminars at Northwestern
University and the University of Wisconsin‚ÄîMadison, and from the comments of two anonymous
referees. This research was supported in part by National Science Foundation Grant SBR-9722846.

357

358

p. j. cross and c. f. manski

of y x z with z = j are said to be error-free, whereas those with z = j are said to be
erroneous. The researcher only observes y x pairs, not y x z triples, and so does not
know which observations are error free. The researcher is, however, assumed to know the
conditional probability Prz = j  x that an observation is error-free, or at least to know
a lower bound on this probability. Recently, Horowitz and Manski (1995) showed that
equation (1) implies a sharp bound on Ey  x z = j. The lower and upper bounds on
Ey  x z = j are the expectations on certain right-truncated and left-truncated versions
of P y  x. This Ô¨Ånding forms the starting point for the present analysis.
Our basic Ô¨Åndings are developed in Section 2. We prove that the set of feasible values
of the J -vector Ey  x z = j j ‚àà Z, its identiÔ¨Åcation region, is a bounded convex set
whose extreme points are the expectations of certain J -vectors of stacked distributions.
When P y  x has Ô¨Ånite support or J = 2, we are able to characterize the identiÔ¨Åcation
region fully as the convex hull of these extreme points. When P y  x has inÔ¨Ånite support
and J ‚â• 3, we show that the identiÔ¨Åcation region contains this convex hull and is contained
in another convex polytope.
Whereas the analysis in Section 2 assumes no information is available beyond knowledge of P y  x and P z  x, we entertain additional information in Section 3. Here we
study exclusion restrictions asserting that y is either mean-independent or statistically independent of some component of x, conditional on z and the other components of x. We
Ô¨Årst characterize abstractly the identifying power of such exclusion restrictions and then
present a simple rank condition that sufÔ¨Åces for point identiÔ¨Åcation of long regressions.
Section 4 shows how our Ô¨Åndings may be used to address structural prediction problems,
in which the long regression Ey  x z is assumed to remain invariant under hypothesized
changes in the covariate distribution P x z.
2 identifying Ey  x z given knowledge of P y  x and P z  x
We proceed in three steps. Section 2.1 reviews the sharp bound on the scalar Ey  x z =
j reported in Horowitz and Manski (1995). Section 2.2 uses this bound to characterize
the identiÔ¨Åcation region for the J -vector Ey  x ¬∑ ‚â° Ey  x z = j j ‚àà Z. Section 2.3
extends the analysis to Ey  ¬∑ ¬∑ ‚â° Ey  x ¬∑ x ‚àà X.
21 IdentiÔ¨Åcation of Ey  x z = j
Fix x. For p ‚àà 0 1, let qx p denote the p-quantile of P y  x. Let Lx p and Ux p
be, respectively, the right-truncated and left-truncated distributions deÔ¨Åned by

(2)

Ô£±
Pry ‚â§ t  x
Ô£¥
Ô£≤
p
Lx p‚àí  t ‚â°
Ô£¥
Ô£≥
1
Ô£±
Ô£¥
Ô£≤0

Ux p‚àí  t ‚â°

if t < qx p
if t ‚â• qx p

Pry ‚â§ t  x ‚àí 1 ‚àí p
Ô£¥
Ô£≥
p

if t < qx 1 ‚àí p
if t ‚â• qx 1 ‚àí p

Let P y  x and P z  x be known. Suppose that Ey  x exists and that xj ‚â° Prz = j 
x > 0 for all j ‚àà Z.

regressions, short and long

359

Horowitz and Manski (1995, Proposition 4) prove that the smallest and largest feasible
values of Ey  x z = j are the expected values of y under Lx xj  and Ux xj , respectively. Thus



Ey  x z = j ‚àà
y dLx xj  y dUx xj  ‚â° E xj  E xj 
(3)
Simple reasoning underlies this result. Consider the subpopulation with covariates x. The
smallest feasible value of Ey  x z = j occurs if, within this subpopulation, the persons
with z = j have the smallest values of y. Then P y  x z = j = Lx xj . The largest feasible
value occurs if the persons with z = j have the largest values of y. Then P y  x z = j =
Ux xj .
The bound (3) has a particularly simple form when y is a binary outcome variable,
taking the values 0 and 1. Then qx p = 0 if Pry = 1  x < 1 ‚àí p and qx p = 1 otherwise.
It follows that
E xj = max 0

Pry = 1  x ‚àí 1 ‚àí xj 
xj



E xj = min 1

Pry = 1  x

xj

A simple direct proof of this result is given in Horowitz and Manski (1995, Corollary 1.2).
Duncan and Davis (1953) gave numerical illustrations when y and z are both binary, but
did not formalize their analysis.
The univariate bound (3) immediately implies a bound on the J -vector Ey  x ¬∑. That
is, Ey  x ¬∑ must lie in the J -dimensional rectangle Cx ‚â° √ój‚ààZ E xj  E xj . The set Cx ,
however, is not the sharp bound on Ey  x ¬∑. The Law of Total Probability (1) implies
further restrictions, including the Law of Iterated Expectations,

Ey  x =
(4)
xj Ey  x z = j
j‚ààZ

Hence Ey  x ¬∑ must lie in the intersection of Cx with the hyperplane satisfying (4). In
what follows, we characterize further the identiÔ¨Åcation region for Ey  x ¬∑.
22 IdentiÔ¨Åcation of Ey  x ¬∑
For each value of x, the feasible values of Ey  x ¬∑ follow immediately, albeit
abstractly, from the Law of Total Probability (1). Let  denote the space of all probability distributions on R. Let x denote the set of all J -vectors of distributions on R that
satisfy (1). That is, j  j ‚àà Z ‚àà x if, and only if,

P y  x =
(5)
xj j 
j‚ààZ

Then the identiÔ¨Åcation region for Ey  x ¬∑ is

Dx =
y dj  j ‚àà Z  j  j ‚àà Z ‚àà x 
(6)
Some properties of Dx are immediate. The set x is convex and the expectation operator is linear, so Dx is convex. Moreover, Dx is contained within the J -dimensional rectangle Cx . Hence Dx is a bounded convex set.
Our objective is to characterize Dx more precisely. Proposition 1 shows that Dx has at
most J ! distinct extreme points, these being the expectations of the stacked distributions
deÔ¨Åned below. Following Proposition 1, we develop some immediate implications through
two Corollaries.

360

p. j. cross and c. f. manski

The Stacked Distributions
J -vectors of stacked distributions are sequences of J distributions such that the entire
probability mass of the jth distribution lies weakly to the left of that of the j + 1st
distribution. Stacked distributions are essential to characterization of Dx . It will be shown
in Step (ii) of the proof to Proposition 1 that the most extreme sequences P y  x z =
j j = 1     J  compatible with given values of P y  x and P z  x are sequences of
such distributions.
To describe these distribution sequences, we now let Z be the ordered set of integers
1     J . This set has J ! permutations, each of which generates a distinct J -vector of
stacked distributions. We label these permutations of Z as Z m  m = 1     J !, and the
corresponding J -vectors of stacked distributions as Pxjm  j = 1     J  m = 1     J !.
For each value of m, the elements of Pxjm  j = 1     J  solve a recursive set of minimization problems. In what follows, we show the construction of Pxj1  j = 1     J , which
is based on Z 1 , the original ordering of Z. The other J ! ‚àí 1 J -vectors are generated
from Z m  m = 2 3     J !, which alters the order in which the recursion is performed.
For each j = 1     J  Pxj1 is chosen to minimize its expectation subject to the distributions earlier chosen for Pxi1  i < j, and subject to the global condition that equation (5)
must hold. The recursion is as follows: For j = 1     J  Pxj1 solves the problem
(7)

min
‚àà



y d

subject to
(8)

P y  x =

j‚àí1


i=1

xi Pxi1 + xj  +

J

k=j+1

xk k 

where k ‚àà   k = j + 1     J are unrestricted probability distributions.
This recursion yields a sequence of stacked distributions. For j = 1, equation (8) reduces
to
(9)

P y  x = x1  +

J

k=2

xk k 

Horowitz and Manski (1995, Proposition 4) show that the distribution solving (7) subject
1
to (9) is Lx x1 , the right-truncated version of P y  x deÔ¨Åned in (2); thus Px1
= Lx x1 .
For j = 2, equation (8) has the form
(10)

P y  x = x1 Lx x1  + x2  +

J

k=3

xk k 

The proof of Horowitz and Manski‚Äôs (1995) Proposition 4 shows that
(11)

P y  x = x1 Lx x1  + 1 ‚àí x1 Ux 1 ‚àí x1 

where Ux 1 ‚àí x1  is the left-truncated version of P y  x that maximizes Ey  x z > 1.
Hence (8) becomes
(12)

Ux 1 ‚àí x1  =

J

x2
xk
+
 
1 ‚àí x1
1
‚àí
x1 k
k=3

regressions, short and long

361

Equation (12) has the same form as (9), with Ux 1 ‚àí x1  replacing P y  x and
1
, the solution to (7) subject to (12), is a rightx k+1 /1 ‚àí x1  replacing xk . Hence Px2
truncated version of Ux 1 ‚àí x1 . By deÔ¨Ånition, Lx x1  has no mass to the right of the
1
1
and Px2
are
point qx x1  and Ux 1 ‚àí x1  has no mass to the left of this point. Hence Px1
stacked side-by-side, with all of the mass of the former distribution lying weakly to the left
of the mass of the latter distribution. The distributions Pxj1  j = 3     J  are similarly
stacked. For each j, the mass of Pxj1 lies weakly to the left of the mass of Px1 j+1 .
Stacking implies that, for each value of j, the supremum of the support of Pxj1 may equal
the inÔ¨Åmum of the support of Px1 j+1 , but otherwise the distributions are concentrated on
disjoint intervals. If P y  x has a mass point, then Pxj1 and Px1 j+1 may share this mass
point; indeed Pxj1 and Px1 j+1 may even be degenerate at this common point. However, if
P y  x is continuous, then Pxj1 and Px1 j+1 are continuous and place their mass on disjoint
intervals.
Example: P y  x standard normal, x1 = 1/2 x2 = 1/3 and x3 = 1/6. Since J = 3
in this example, there are 3! = 6 3-vectors of stacked distributions, based on Z 1 through
Z 6 . These are illustrated by their densities in Figure 1.
Notice that the Ô¨Årst vector of stacked distributions in the Ô¨Ågure is Pxj1  j = 1 2 3.
1
1
Px1
is Lx 1/2, the standard normal right-truncated at 0. Px2
is constructed by righttruncation at 097 of the distribution resulting from Lx 1/2 being removed from the
1
. The second
standard normal. And the remaining mass, which is Ux 1/6, constitutes Px3
2
vector of stacked distributions in Figure 1 is Pxj  j = 1 2 3, where we deÔ¨Åne Z 2 =
1 3 2. And the remaining vectors of stacked distributions in the Ô¨Ågure are derived from
the remaining permutations, Z 3 through Z 6 .
The Extreme Points of the IdentiÔ¨Åcation Region
With the above as preliminary, Proposition 1 proves that the expectations of the stacked
distributions are the extreme points of Dx .
Proposition 1: Let P y  x and P z  x be known. Let Ey  x exist. Let Exm ‚â°
 y dPxjm  j = 1     J . Then the extreme points of Dx are $Exm  m = 1     J !%.
Proof: By construction, each of the J -vectors in $Exm  m = 1     J !% is a feasible
value of Ey  x ¬∑. Step (i) of the proof shows that these vectors are extreme points of
Dx . Step (ii) shows that Dx has no other extreme points. In what follows, we simplify the
notation by suppressing the subscript x.
Step (i). It sufÔ¨Åces to consider E 1 . Permuting Z does not alter the argument below.
Suppose that E 1 is not an extreme point of D. Then there exist an & ‚àà 0 1 and
distinct J -vectors '   '   ‚àà D such that E 1 = &'  + 1 ‚àí &'  . Suppose that E 1  '  , and ' 
differ in their Ô¨Årst component. Then either '1 < E11 < '1 or '1 < E11 < '1 . By construction,
however, E11 = E 1 , the global minimum of Ey  z = 1. So '1 ‚â• E11 and '1 ‚â• E11 . Hence it
must be the case that '1 = '1 = E11 .
Now suppose that E 1  '  , and '  differ in their second component. Then '2 < E21 < '2
or '2 < E21 < '2 . But E21 minimizes Ey  z = 2 subject to the previous minimization of
Ey  z = 1. So '2 ‚â• E21 and '2 ‚â• E21 . Hence '2 = '2 = E21 . Recursive application of this
reasoning shows that '  = '  = E 1 , contrary to supposition. Hence E 1 is an extreme point
of D.
Step (ii). Let ' ‚àà D, with '  $E m  m = 1     J !%. Then ' is the expectation of some
feasible J -vector of nonstacked distributions. We want to show that ' is not an extreme

362

p. j. cross and c. f. manski

Figure 1.‚Äî Densities of stacked distributions for P y  x standard normal, x1  x2  x3  =
1/2 1/3 1/6 for the six permutations of Z = 1 2 3

point of D. Thus, we must show that there exists an & ‚àà 0 1 and distinct J -vectors
'   '   ‚àà D such that ' = &'  + 1 ‚àí &'  .
Let the set-valued function S denote the support of any probability distribution
 on the real line. Let j  j ‚àà Z ‚àà  be any feasible J -vector of distributions with
expectation '. This J -vector is not stacked, so there exist components i and k such
that inf Si  sup Si  ‚à© inf Sk  sup Sk  has positive length. Thus sup Si  >
inf Sk  and sup Sk  > inf Si . For ease of exposition, henceforth let aj ‚â° inf Sj 
and bj ‚â° sup Sj , for j = i k.
We now construct a feasible J -vector of distributions that shifts mass, in a particular
balanced manner, between distributions i and k , while leaving the other components of
j  j ‚àà Z unchanged. Let 0 < + < 21 bi ‚àí ak . Then k ak  ak + + > 0 i bi ‚àí + bi  > 0,
and ak  ak + + ‚à© bi ‚àí + bi  = . Let
,‚â°

k k ak  ak + +

i i bi ‚àí + bi 

regressions, short and long

363

Now deÔ¨Åne the new J -vector j  j ‚àà Z as follows: Let j = j for j = i k. If , ‚â§ 1, let
Ô£±

Ô£¥
if A ‚äÇ ak  ak + +
i A + k k A 0
Ô£¥
Ô£¥
i
Ô£≤

i A k A = 1 ‚àí ,i A k A + , i i A if A ‚äÇ bi ‚àí + bi 
Ô£¥

Ô£¥
k
Ô£¥
Ô£≥
elsewhere.
i A k A
Alternatively, if , > 1, let
Ô£±
1 k
1
Ô£¥
Ô£¥
i A +
k A 1 ‚àí k A if A ‚äÇ ak  ak + +
Ô£¥
Ô£¥
,

,
Ô£≤
i
i A k A = 0  A + i  A
if A ‚äÇ bi ‚àí + bi 
Ô£¥
k
Ô£¥
k i
Ô£¥
Ô£¥
Ô£≥
elsewhere.
i A k A
Thus, the new J -vector shifts i mass leftward from the bi ‚àí + bi  interval to the ak  ak +
+ interval and compensates by shifting k mass rightward to the bi ‚àí + bi  interval from
the ak  ak + + interval. The , parameter ensures that we shift equal amounts of mass and
that i i + k k = i i + k k . Hence j  j ‚àà Z is a feasible J -vector of distributions,
that is, an element of  . The mean of j  j ‚àà Z is related to the mean of j  j ‚àà Z as
follows: 'i < 'i  'k > 'k , and 'j = 'j for j = i k.
An analogous operation switching the roles of i and k produces another new J -vector
j  j ‚àà Z. Now let 0 < + < 21 bk ‚àí ai  and redeÔ¨Åne , accordingly. This construction shifts
k mass leftward from the bk ‚àí + bk  interval to the ai  ai + + interval and shifts an
equal amount of i mass rightward to the bk ‚àí + bk  interval from the ai  ai + + interval,
while ensuring that i i + k k = i i + k k . The mean of this J -vector is related to
the mean of j  j ‚àà Z as follows: 'i > 'i  'k < 'k , and 'j = 'j for j = i k.
It follows from the above that i 'i + k 'k = i 'i + k 'k = i 'i + k 'k . Thus, 'i  'k 
lies on the line connecting 'i  'k  and 'i  'k . Moreover, 'i > 'i > 'i and 'k > 'k > 'k .
Hence 'i  'k  is a strictly convex combination of 'i  'k  and 'i  'k . Finally recall that
'j = 'j = 'j for j = i k. Hence ' is a strictly convex combination of '  and '  . Thus ' is
not an extreme point of D.
Q.E.D.
Proposition 1 has two immediate implications that further characterize the identiÔ¨Åcation
region. Let conv$Exm  m = 1     J !% denote the convex hull of $Exm  m = 1     J !%. Then
we have the following corollaries.
Proposition 1, Corollary 1: conv$Exm  m = 1     J !% ‚äÇ Dx .
Proof: Dx is a convex set containing $Exm  m = 1     J !%. Hence Dx contains the
convex hull of these points.
Q.E.D.
Proposition 1, Corollary 2: If
conv$Exm  m = 1     J !%.

P y  x

has

Ô¨Ånite

support,

then

Dx =

Proof: Minkowski‚Äôs Theorem (e.g., Br√∏ndsted (1983, Theorem 5.10)) shows that a
compact convex set in R J is the convex hull of its extreme points. We already know that
Dx is a bounded convex set, so we need only show that Dx is closed. Let Y denote the
support of P y  x and suppose that Y has Ô¨Ånite cardinality H. For j ‚àà Z and 0 ‚àà Y , let

364

p. j. cross and c. f. manski

1j0 be a feasible value for Pry = 0  x z = j. Then equation (5) becomes the following
system of H linear equations in the J √ó H unknowns 1j0 :
Pry = 0  x =


j‚ààZ

xj 1j0 

0 ‚àà Y

Let 2x denote the solutions to this system of equations. 2x forms a closed set in R J √óH .

The identiÔ¨Åcation region for Ey  x ¬∑ is Dx = $ 0‚ààY 0 ¬∑ 1j0  j ‚àà Z 1 ‚àà 2x %, a linear
J
map from 2x to R . Hence Dx is closed.
Q.E.D.
The IdentiÔ¨Åcation Region when P y  x has InÔ¨Ånite Support
Proposition 1 and its Corollaries fully characterize the identiÔ¨Åcation region when P y 
x has Ô¨Ånite support, but only partially so when P y  x has inÔ¨Ånite support. If Dx can
be shown to be closed, then the reasoning of Corollary 2 may be applied. Unfortunately,
it appears difÔ¨Åcult to characterize Dx topologically when P y  x has inÔ¨Ånite support.
Although we currently are not able to characterize fully the identiÔ¨Åcation region
when P y  x has inÔ¨Ånite support, we can add to the characterization given thus far.
We have already shown that Dx contains the convex polytope conv$Exm  m = 1     J !%.
Proposition 2 uses $Exm  m = 1     J !% to construct another convex polytope that contains Dx . When J = 2, this yields a full characterization of Dx .
Proposition 2: For each m = 1     J !, let Z m denote the mth permutation of Z. Let
jm k be the position in Z of the kth element of Z m . DeÔ¨Åne the following subsets of R J :
G0x ‚â° ' ‚àà R J 
J
Gm
x ‚â° ' ‚ààR 

J

j=1
n

k=1

xj 'j = Ey  x 
xjm k 'jm k ‚â•

n

k=1

m
xjm k Exjm
k  n = 1     J ‚àí 1

m = 1     J !
and
Gx ‚â°

J!

m=0

Gm
x 

Then Gx is a convex polytope and conv$Exm  m = 1     J !% ‚äÇ Dx ‚äÇ Gx . When J = 2
conv$Exm  m = 1     J !% = Dx = Gx .
Proof: Proposition 1, Corollary 1 showed that conv$Exm  m = 1     J !% ‚äÇ Dx . It is
easy to see that Dx ‚äÇ Gx . The Law of Iterated Expectations (4) requires that every point
in Dx satisfy the equality deÔ¨Åning G0x . For each m ‚â• 1, the construction of Exm by recursive
minimization implies that every point in Dx must satisfy each of the J ‚àí 1 inequalities
deÔ¨Åning Gm
x . Hence Dx ‚äÇ Gx .
To show that Gx is a convex polytope, observe Ô¨Årst that G0x is a hyperplane and each
Gm
x is the intersection of J ‚àí 1 closed half-spaces. Hence Gx is a polyhedral set. Next
observe that Gx is bounded from below. In particular, the Ô¨Årst inequality used to deÔ¨Åne
each set Gm
x shows that ' ‚àà Gx ‚áí 'j ‚â• E xj  j ‚àà Z. Finally, observe that this lower bound
and the equality deÔ¨Åning G0x imply that Gx is bounded from above; in particular, ' ‚àà

regressions, short and long

365


Gx ‚áí 'i ‚â§ Ey  x ‚àí k=i xjm k E xk  i ‚àà Z. Thus Gx is a bounded polyhedral set, and
hence a convex polytope. See Br√∏ndsted (1983, Corollary 8.7).
When J = 2 Gx is the line segment connecting the points E x1  E x2  and E x1  E x2 ,
which are the extreme points of Dx . So conv$Exm  m = 1     J !% = Dx = Gx in this special
case.
Q.E.D.
23 IdentiÔ¨Åcation of Ey¬∑ ¬∑
It remains only to extend the analysis from identiÔ¨Åcation of Ey  x ¬∑ to identiÔ¨Åcation
of Ey¬∑ ¬∑. This is straightforward. Knowledge of P y  x and P z  x implies no crossx restrictions on Ey  x ¬∑. Hence the identiÔ¨Åcation region for Ey¬∑ ¬∑ is the Cartesian
product √óx‚ààX Dx .
3 the identifying power of exclusion restrictions
Propositions 1 and 2 have characterized the restrictions on Ey  x z implied by knowledge of P y  x and P z  x. Tighter inferences may be feasible if additional information
is available. Among the many forms that such information may take, we focus on exclusion restrictions of the type that have been found useful in resolving other identiÔ¨Åcation
problems.
Let us dispose Ô¨Årst of one form of exclusion restriction whose implications are so
immediate as barely to require comment. Suppose it is known that y is mean-independent
of z, conditional on x; that is, Ey  x z = Ey  x. Then knowledge of P y  x identiÔ¨Åes
Ey  x z.
More interesting are exclusion restrictions connecting Ey  x z across different values
of x. Let x = v w and X = V √ó W . One familiar form of exclusion restriction asserts
that y is mean-independent of v, conditional on w z. Thus
(13)

Ey  v w z = Ey  w z

A stronger form of exclusion asserts that y is statistically independent of v, conditional
on w z. Thus
(14)

P y  v w z = P y  w z

Restrictions of these forms are often called instrumental variable assumptions, v being the
instrumental variable.
Proposition 3 below characterizes fully, albeit abstractly, the identifying power of
assumptions (13) and (14). We then present a weaker, but much simpler, Ô¨Ånding that
yields a straightforward rank condition for point identiÔ¨Åcation of Ey  w ¬∑ ‚â° Ey  w z =
j j ‚àà Z. This rank condition indicates that, in applications, exclusion restrictions of the
form (13) and (14) often sufÔ¨Åce to identify Ey  w ¬∑. We also call attention to the fact
that these exclusion restrictions are testable assumptions.
Proposition 3: Let P y  v w and P z  v w be known. Let Ey  v w exist. Let Dw‚àó
and Dw‚àó‚àó denote the identiÔ¨Åcation regions for Ey  w ¬∑ under assumptions (13) and (14)
respectively. Then
(15)

Dw‚àó ‚â°


v‚ààV

Dv w 

366

p. j. cross and c. f. manski

and
(16)

Dw‚àó‚àó ‚â°



y dj  j ‚àà Z  j  j ‚àà Z ‚àà


v‚ààV

v w ‚äÇ Dw‚àó 

The corresponding identiÔ¨Åcation regions for Ey  ¬∑ ¬∑ are √ów‚ààW Dw‚àó and √ów‚ààW Dw‚àó‚àó .
Proof: Consider assumption (13). Recall that, for each value of v w, we have
j  j ‚àà Z ‚àà v w if, and only if,
P y  v w =


j‚ààZ

v wj j 

Let ' ‚àà R J . Under (13), ' is a feasible value for Ey  w ¬∑ if, and only if, for every v ‚àà V
there exists an element of v w whose expectation is '. The set Dw‚àó comprises these
feasible values of '.
Consider assumption (14). Under (14), j  j ‚àà Z is a feasible value for P y  w j j ‚àà
Z if, and only if, j  j ‚àà Z satisÔ¨Åes the system of equations
P y  v w =


j‚ààZ

v wj j 

for all v ‚àà V 

Thus the set of feasible values for P y  w j j ‚àà Z is ‚à©v‚ààV v w . The set Dw‚àó‚àó comprises the expectations of these feasible J -vectors of distributions. That Dw‚àó‚àó ‚äÇ Dw‚àó follows
from the fact that assumption (14) is stronger than (13). It can also be seen directly by
comparing (15) and (16).
Now consider Ey  ¬∑ ¬∑. Neither (13) nor (14) imposes a cross-w restriction. Hence the
identiÔ¨Åcation regions for Ey  ¬∑ ¬∑ are the Cartesian products of the regions for Ey  w ¬∑
under these assumptions.
Q.E.D.
A Rank Condition for Point IdentiÔ¨Åcation
Proposition 3 is general, but it is too abstract to convey a sense of the identifying power
of exclusion restrictions. A much simpler, readily applicable Ô¨Ånding emerges if we exploit
only the Law of Iterated Expectations rather than the full force of the Law of Total
Probability.
Let Cw‚àó ‚äÇ R J denote the set of solutions ' ‚àà R J to the system of linear equations
(17)

Ey  v w =


j‚ààZ

v wj 'j 

for all v ‚àà V 


Let V  denote the cardinality of the set V . Let
denote the V  √ó J matrix whose jth
column is v wj  v ‚àà V . Then we have the following corollary.
Proposition 3, Corollary 1: Dw‚àó ‚äÇ Cw‚àó . If
= Cw‚àó .

Dw‚àó



has rank J , then Cw‚àó is a singleton and

Proof: The Law of Iterated Expectations and assumption (13) require that feasible
values of Eyw ¬∑ solve equations (17). Hence Dw‚àó ‚äÇ Cw‚àó . Dw‚àó is nonempty, so (17) must

have at least one solution. If has rank J , then (17) has a unique solution and Dw‚àó = Cw‚àó .
Q.E.D.

regressions, short and long

367

Goodman (1953), considering problems in which y and z are binary, showed informally
that knowledge of Ey  x and P z  x combined with an exclusion restriction yields the
rank condition for point identiÔ¨Åcation developed in Proposition 3, Corollary 1. However,
the literature appears to contain no precedent for Proposition 3, nor for the general form
of Corollary 1.
Testing Exclusion Restrictions
We have thus far supposed that the speciÔ¨Åed exclusion restriction is correct. Suppose
that an attempt to solve the system of equations (17) reveals that the solution set Cw‚àó is
empty. Or, if Cw‚àó is nonempty, suppose that evaluation of the identiÔ¨Åcation region Dw‚àó or
Dw‚àó‚àó , as the case may be, shows the region to be empty. Any such Ô¨Ånding implies that the
speciÔ¨Åed exclusion restriction cannot be correct. Thus, exclusion restrictions of the form
(13) and (14) are testable assumptions.

4 applying the Ô¨Åndings to structural prediction problems
Applied economists often want to predict how the observed mean population outcome
Ey would change if the covariate distribution were to change from P x z to some
other distribution, say P ‚àó x z. It is common to address this prediction problem under
the assumption that the long regression Ey  ¬∑ ¬∑ is structural, in the sense that Ey  ¬∑ ¬∑
would remain invariant under the hypothesized change in the covariate distribution. Given
this assumption, the mean outcome under covariate distribution P ‚àó x z would be
E ‚àó y ‚â°

 

Ey  x z = jPr‚àó z = j  x dP ‚àó x

j‚ààZ

To motivate the assumption that Ey  ¬∑ ¬∑ is structural, economists often pose behavioral models of the form y = f x z u, wherein an individual‚Äôs outcome y is some function f of the covariates x z, and of other factors u. Then Ey¬∑ ¬∑ is structural if u is
statistically independent of x z and if the distribution of u remains unchanged under
the hypothesized change in the covariate distribution.
What can be said about E ‚àó y when Ey  ¬∑ ¬∑ is unknown? Our Ô¨Åndings are applicable
if the available data reveal P y  x and P z  x. For example, a well-known problem in
poverty research is to predict participation in social welfare programs under hypothesized
changes in the geographic distribution and demographic attributes of the population. Let
y be a binary variable indicating program participation, let x be a geographic unit such
as a county, and let z denote demographic attributes. One might be willing to assume
that Ey  ¬∑ ¬∑ is structural in the sense deÔ¨Åned above. One might learn Ey  ¬∑ ¬∑ from a
household survey, but a suitable survey may be unavailable in practice. However, administrative records may reveal program participation by county and census data may reveal
demographic attributes by county; that is, P y  x and P z  x.
In such settings our Ô¨Åndings yield identiÔ¨Åcation regions for Ey  ¬∑ ¬∑ and hence for
E ‚àó y. Section 2 showed that, in the absence of assumptions on P y x z, the identiÔ¨Åcation region for Ey  ¬∑ ¬∑ is √óx‚ààX Dx . Section 3 showed that, under two alternative
forms of exclusion restrictions, the identiÔ¨Åcation regions for Ey  ¬∑ ¬∑ are√ów‚ààW Dw‚àó and
√ów‚ààW Dw‚àó‚àó . Each of these results implies an identiÔ¨Åcation region for E ‚àó y. For example,

368

p. j. cross and c. f. manski

in the absence of assumptions on P y x z E ‚àó y must lie in the set 
j  x dP ‚àó x ' ‚àà Dx  x ‚àà X.



j‚ààZ

'j Pr‚àó z =

Dept. of Economics, Georgetown University, Washington, D.C. 20057, U.S.A.; pjc22@
georgetown.edu; www.georgetown.edu/faculty/pjc22
and
Dept. of Economics and Institute for Policy Research, Northwestern University, Evanston,
IL 60208, U.S.A.; cfmanski@northwestern.edu; www.faculty.econ.northwestern.edu/faculty/
manski
Manuscript received November, 1999; Ô¨Ånal revision received July, 2000.
REFERENCES
Br√∏ndsted, A. (1983): An Introduction to Convex Polytopes. New York: Springer-Verlag.
Duncan, O., and B. Davis (1953): ‚ÄúAn Alternative to Ecological Correlation,‚Äù American Sociological Review, 18, 665‚Äì666.
Goldberger, A. (1991): A Course in Econometrics. Cambridge, Mass.: Harvard University Press.
Goodman, L. (1953): ‚ÄúEcological Regressions and Behavior of Individuals,‚Äù American Sociological
Review, 18, 663‚Äì664.
Horowitz, J., and C. Manski (1995): ‚ÄúIdentiÔ¨Åcation and Robustness with Contaminated and
Corrupted Data,‚Äù Econometrica, 63, 281‚Äì302.
Huber, P. (1964): ‚ÄúRobust Estimation of a Location Parameter,‚Äù Annals of Mathematical Statistics,
35, 73‚Äì101.
King, G. (1997): A Solution to the Ecological Inference Problem: Reconstructing Individual Behavior
from Aggregate Data. Princeton: Princeton University Press.
Lindley, D., and M. Novick (1981): ‚ÄúThe Role of Exchangeability in Inference,‚Äù Annals of Statistics, 9, 45‚Äì58.
Robinson, W. (1950): ‚ÄúEcological Correlation and the Behavior of Individuals,‚Äù American Sociological Review, 15, 351‚Äì357.
Simpson, E. (1951): ‚ÄúThe Interpretation of Interaction in Contingency Tables,‚Äù Journal of the Royal
Statistical Society, 13, 238‚Äì241.
Zidek, J. (1984): ‚ÄúMaximal Simpson-disaggregations of 2 √ó 2 Tables,‚Äù Biometrika, 71, 187‚Äì190.

