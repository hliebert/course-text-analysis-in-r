Applications of Estimators of a Density and its Derivatives to Certain Statistical Problems
Author(s): R. S. Singh
Source: Journal of the Royal Statistical Society. Series B (Methodological), Vol. 39, No. 3
(1977), pp. 357-363
Published by: Wiley for the Royal Statistical Society
Stable URL: https://www.jstor.org/stable/2985096
Accessed: 21-02-2019 19:35 UTC
JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide
range of content in a trusted digital archive. We use information technology and tools to increase productivity and
facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org.
Your use of the JSTOR archive indicates your acceptance of the Terms & Conditions of Use, available at
https://about.jstor.org/terms

Royal Statistical Society, Wiley are collaborating with JSTOR to digitize, preserve and
extend access to Journal of the Royal Statistical Society. Series B (Methodological)

This content downloaded from 206.253.207.235 on Thu, 21 Feb 2019 19:35:44 UTC
All use subject to https://about.jstor.org/terms

J. R. Statist. Soc. B (1977),
39, No. 3, pp. 357-363

Applications of Estimators of a Density
and its Derivatives to Certain Statistical Problems
By R. S. SINGH
University of Guelph, Canada

[Received April 1977]
SUMMARY
Several statistical problems are considered. By reducing them to problems of
estimating functionals of a density, derivatives of a density, or both, a method of
finding partial solutions is obtained.

Keywords: ESTIMATIONS; DENSITY; DERIVATIVES; REGRESSION; FISHER INFORMATION; OPTIMAL
TESTS; OPTIMAL ESTIMATORS; EMPIRICAL BAYES

1. INTRODUCTION

ESTIMATORS of density functions or of derivatives of density functions are abundant in the
literature. However, there have been very few attempts to explore applications of these
estimators to statistical problems. The aim of this paper is to investigate such applications.
Giving the complete solutions to the problems considered here is beyond the scope of this
paper, since each problem needs serious and detailed consideration. Nevertheless, a seriousminded statistician can get a nearly complete solution to each of the problems by the method
indicated here. Our method involves first reducing a problem to one of estimating a density,
its derivatives, or both, and then making use of estimators (of these functions) already available
in the literature.
2. APPLICATIONS

2.1. Estimations of regression Curves

Let X and Y be two random variables. The general regression curve of Y on X at X = x

is given by r(x) E( YJ X = x), the conditional expectation of Y given X = x. The function

plays an important role in practice, as it tells how the average value of Ychanges with a change
in the value of X. In fact, r(x) is the maximum correlation predictor of Y based on the value
x of X in the sense that if t(x) is any function of x, then the correlation coefficient between Y
and r(X) is not smaller than the absolute value of the correlation coefficient between Y and
t(X), see Rao (1973). Thus the importance of r from a practical point of view is evident.
However, r is known only if the marginal distribution of Y is known, which is rare in practice.
This leads to the estimation of r when the marginal distribution of Y, say G, is unknown
and unspecified.
Consider the case where the conditional probability density function (p.d.f.) of X given

Y = y is of the form u(x) a(y) exp (yx) and the support of G is in {yI fu(x) exp (yx) dx < c*}.
(Note that the densities (2T)fi exp -(x-y)2/2}, - o < x, y < oo; and

(r(oa))-l(y)01x-lexp (yx), x > , y <O
are of the above form.) With f(x) = fa(y) exp (yx) dG(y), we have

f(l)(x) = d a(y) exp (yx) dG(y) = {ya(y) exp (yx) dG(y)
where the change of order of differentiation and integration is justified by Theorem 2.9 of

This content downloaded from 206.253.207.235 on Thu, 21 Feb 2019 19:35:44 UTC
All use subject to https://about.jstor.org/terms

358

SINGH

-

Density

Estimates

[No.

3,

Lehmann (1959). Thus the value of the regression curve of Y on X at x can be expressed as

r(x) = E( YI X = x) = {ya(y) exp (yx) dG(y) /{a(y) exp (yx) dG(y) J(f() * (2

Note that f(x) is a density with respect to a u-finite measure IL on the real line such t

dp(x) = u(x) dx. We have now reduced the problem of estimation of the regression curve r

to the one of estimation of the density f and its derivative f(1). Now using the mean square

consistent (m.s.c.) estimators off andf(1) given in Singh (1976), and making an application of
the lemma in the Appendix here we can get the m.s.c. estimators of r.
Next consider the case where the conditional p.d.f. of X given Y = y is of the form

v(x) b(y) exp (- x/y) and the support of G is in {y > ? l fv(x) exp (- x/y) dy < oo}. (Note that

gamma density (r(o)y)-l exp (- x/y), x,y > O is of the above form.) With

p(x) = {b(y) exp (- x/y) dG(y),
by a change of order of integration, we have
{p(t) dt = {yb(y) exp (- x/y) dG(y).

Thus in this situation the regression curve of Y on X at X= x can be expressed as

r(x) = E( YI X = x) {yb(y) exp (- x/y) dG(y)/ b(y) exp (-x/y) dG(y) = {p(t) dt/p(x).
(2.2)

Thus the problem of estimating r is reduced to estimating the density p (w.r.t. a a-finite
measure ju on the real line such that d,u(x) = v(x) dx). Using the m.s.c. estimators of p given
in Singh (1976) and the lemma of the Appendix here, we can obtain m.s.c. estimators of r.

2.2. Estimation of Fisher Information

Let X = (X1, ..., Xp,) be a random vector with p.d.f. f(x1, ..., xp). The Fisher information
functional is a p xp symmetric matrix I(f) = (Irs) r = 1, .. ., p, s = 1, .. ., p, where

I f (Df(x)/Dx) (Df(x)/Dxs) d dx (2.3)

'TS J ~~f(x) v, 23

The Fisher information functional has been used in the development of various statistical
techniques, mostly of the non-parametric type; see Hajek and Sidak (1967). Iff is known,
I(f) can be evaluated directly by (2.3). In the case wheref is unknown, I(f) can be estimated

first by estimating the density f(x) and its partial derivatives af(x)/axs, s = 1, ...,p, and th

estimating Irs by (2.3) for each pair (r,s), r = 1, ...,p; s = 1, ...,p.
Estimators of f(x) and af(x)/axs, s = 1, ...,p, are exhibited in Singh (1976). Using

and the technique used in Theorem 2 of Bhattaeharya (1967), one can get m.s.c. estimators

(say Irs) of Irs (and hence of I(f) in the sense that , , E(Irs - I*s)2 (summing for r a

from 1 to p) converges to zero as the sample size tends to large).
Throughout the remainder of this paper, M.e.l. will stand for minimum expected loss.
2.3. M.e.l. Estimation in one Parameter Scale Exponential Families

Suppose we wish to estimate the normal population variance, or the parameter 0 in a

gamma family (r(o) 0)-lxo-lexp(-x/0), x, 6 positive. More generally, suppose the pr

is to estimate the scale parameter 0 when the observable random variable X has conditional

(on 0) p.d.f. of the form v(x) b(6) exp (-x/0) and the criterion of estimation is the loss funct

This content downloaded from 206.253.207.235 on Thu, 21 Feb 2019 19:35:44 UTC
All use subject to https://about.jstor.org/terms

1977]

SINGH

-

Density

Estimates

359

L(, ) = {(- )/0}2. Suppose 0 is distributed according to an unknown and unspecified

distribution G with support in {d > f l fv(x) exp (- x/0) dx < oo}. The estimator with min

overall expected loss is the Bayes rule 'G, where AG(X) is the value of 8(x) suc

f{(6d-(x)/0}2b(6)exp(-x/6)dG(6) is a minimum. Thus it follows that

PG(X) = 0-1 b(6) exp (- x/ 0) dG(O)/f 0-2 b(6) exp (- x/ 0) dG(O). (2.4)
If we let p(x) = fb(6) exp (-x/0) dG(6), then from Theorem 2.9 of Lehmann (1959), the

numerator and the denominator in (2.4) are respectively -p(M)(x) and p(2)(x) and we get

OG(X)

=-p(2)(x)

(2.5)

When G is unknown (which is often
the minimum overall expected loss
(perhaps by using past experiences of the above problem through observations on X). By

(2.5), estimators of OG can be provided by making use of estimators of the first- and secondorder derivatives of p, which can be thought of as a density w.r.t. a a-finite measure jU on the
real line such that du(x) = v(x) dx. The m.s.c. estimators of the latter functions are given in
Singh (1976). These estimators and the lemma in the Appendix can be used to obtain m.s.c.
estimates of PG.
2.4. M.e.l. Hypothesis Testing Problem in one Parameter Scale Exponential Families

Let the observable random variable X have conditional (on 0) probability density function
of the form given in Section 2.3. Let G and p be as defined there, and 00 be a known point in
the support of G. Based on X, suppose the problem is to test

Ho: 0600 against H1: 0> 60
with loss functions

Lo: (- 00)+ if Ho is accepted
and

L1: (00 - 0)+ if H1 is accepted,
where a+ = max {a, O}.

If, based on X, e(X) is the probability of accepting Ho, then the overall expected loss due
to 6 is

{v(x) [{{(6o- 6 + e(x)f (6 6o)} b(6)exp(-x/0)dG(0)] dx.
With

TG(x) = {Ob(O) exp (- x/ 0) dG(0)/ fb(6) exp (-x/l ) dG(O),
the above expected loss is minimized if

() = 6G(X) I {Oifelsewhere.
TG(x)< O0,
The above optimal test eG (which is based on TG) is not available if G is unknown. Neverthe-

less, the problem can be tackled through the empirical Bayes (E.B.) approach introduced by
Robbins (1955), which in this case will require estimation of TG. But, with

p(x) = fb() exp (-x/6)dG(6),
16

This content downloaded from 206.253.207.235 on Thu, 21 Feb 2019 19:35:44 UTC
All use subject to https://about.jstor.org/terms

360

SINGH

TG(x)

can

-

Density

be

Estimates

written

as

the

[No.

ratio

3,

of

density p given in Singh (1976) and the lemma in the Appendix here, one can find m.s.c.

f??

estimators of TG.
2.5. M.e.l. Estimation in one Parameter Exponential Families

Consider the problem of estimating 0 when the observed random variable X has conditiona
(on 0) p.d.f. of the form u(x) a(O) exp (- xO). (Note that the normal density with mean -0
and variance unity, and the gamma density (F(o))- 6xo-l exp (- Ox) with 0 and x positive
are of the above type.) If G is the prior distribution of 0 with support in

{ ?1 {u(x) exp (- ox) dx < oo4,
then the minimum expected squared error loss estimator based on X is the Bayes response

AG(X) versus G given by the ratio of f6a(6) exp (- 6x) dG(6) to fa(6) exp (- 6x) dG(6) (=f(x),

define), and can be exhibited only if G is known. By Theorem 2.9 of Lehmann (1959),f(1)(x),

the derivative off(x), can be written as - fa(6) exp (- Ox) dG(6). Thus the optimal estimator
PG(X) can alternatively be expressed as -f(')(x)/f(x). Note that f is the marginal density of
X w.r.t. a measure tx such that d,u(x) = u(x) dx. Where G is unknown (which is often the case

in practice) estimation of G is desirable. This problem, as we see, can be solved by getting

estimators of the ratio of the derivative of the density f to f itself. For this purpose, again,
the m.s.c. estimators off and f(l) given in Singh (1976) and the lemma of the Appendix here
may be used.

2.6. M.e.l. Estimation of the Reciprocal of a Regression Coefficient
Let the model of our observation be a simple regression model
Y = ITX+ 8,

where X is an independent variable (which could be random with unknown distribution),
ir is the regression coefficient and s is the error term having normal distribution with mean
zero and variance unity. Suppose the problem is the estimation of 0 = 1/1r with loss function

L(6, 0) = {(6- &)/0}2. Estimation of such functions arises, for example, in econometric

problems; see Zellner (1975).
Let G be the distribution of ir, which we wish to determine. By a simple analysis we see
that the Bayes optimal estimator that minimizes the expected loss is

kG(x,y) = E(r I x, y)/E(7r21 x, y).
We define

f(x, y) = Jexp {xy7r - I(7TX)2} h(x) dG(rT),

where h is the conditional p.d.f. of x (given ir) and may depend on unknown parameters. Then
f(x, y) is the joint marginal density of X and Y w.r.t. a a-finite measure X- such that
di-(x, y) = (2Tr)-l exp (-y2/2) dy dx.
Now it is easy to see that

OG(X, Y) =E(7T21x Xy)-a2fi(X, y) y2

E(.7Tj x, y) = -x Df(x, y)/Day

Thus, if G is unknown, estimation of the Bayes optimal estimator (which cannot be exhibited)
amounts to the estimation of the first and second partial derivatives of f(x, y) w.r.t. y. The
m.s.c. estimators of such functions, when observations on X and Y are available, are exhibited
in Singh (1976).

This content downloaded from 206.253.207.235 on Thu, 21 Feb 2019 19:35:44 UTC
All use subject to https://about.jstor.org/terms

1977]

2.7.

SINGH

M.e.l.

-

Density

Estimates

Estimation

of

361

Regression

Consider a general multiple linear regression model

Y

=

Xp+e,

(2.6)

where Y represents a k-dimensional observable random vector, X is a k xp matrix of known

and fixed variables, P3 is p x 1 vector of unknown regression coefficients and e is the vector of

random errors having the k-variate normal distribution with mean vector 0 and covariance
matrix I, the k x k identity matrix. Let X' X be of full rank, and , be distributed according to
an unknown and unspecified prior distribution G. Consider the problem of estimating , with

loss function L(P, ) = (,-)' (p -.
If we take
=

then

,

is

the

(X'

least

X)-1

X'

Y,

squares

as

(2.7)

well

as

vector P has normal distribution
the conditional (on P) probability

f(XSt P)-l =X' X 1 (2ir)-'P exp(-2p' X' X) t( i ) (2.8)
where

t(, I P) = exp (O' X' X, - ' X' X'). (2.9)
Since , is sufficient for f3, we can base the estimation of ,1 on ,. The minimum expected loss

estimator of P3 is the Bayes estimator given by

JG(P) = E(P I f) = Pt( I P) dG(P)/ |t( I P )dG(P). (2.10)
In practice, more often than not the distribution G of P is unknown, and hence, so is the
optimal estimator kG(P). Such situations can be tackled through the empirical Bayes approach

which involves estimation of JG(P) by using past observations on Y (and hence on P).

Writing

tG(P) = It(p|p)dG(P) (2.11)
we see that

Pt(^I P dG(P) = (X' X)-S{Pt(I ) 0 }dG(P)

= (XI X)'1{btG(O)Ia-}, (2.12)
where the interchange of order of integration and differentiation is justified by Theorem (2.9)

of Lehmann (1959). Thus 0kG(P) can be written as

-) = (X' X)-1 Pt))/ta() (2.13)

and estimation of tG(P) amounts to the estimation of
atG(P-)/P. Note that tG(O) is a density w.r.t. a measu

dt)( =I X' X +(27T)-IP exp (-2,X'X,)d

The m.s.c. estimators of tG(,) and att(P)/lap can be ob

2.8. M.e.l. Estimation of Ratio of Two Regression
Consider the multiple regression model (2.6) with Y, X,

suppose X is such that X'X is a diagonal matrix. Fo

estimation of the ratio 0 = P/:j. The problem of estim
considered in the literature; see, for example, Zellne

This content downloaded from 206.253.207.235 on Thu, 21 Feb 2019 19:35:44 UTC
All use subject to https://about.jstor.org/terms

362 SINGH - Density Estimates [No. 3,

denoting an estimate of 0, and E = -j, an
iS 82 - I2-(0_ )2 as is used in Zellner (1975
estimator of 0. For the reasons explained in Section 2.7, we can base the estimation of 0
on , defined in (2.7). The posterior expected loss

is minimum when

E(| = 2 2&(p) E(/3 /3) + #2() E(fl/)

= E(fl) I )) =JPiPit(5 I3)dG( p) fl>t(, I )dG(q), (2.14)
where the last equation follows from (2.8) and (2.9) and the definition of conditional
expectation.

If G is unknown, the optimal estimator MPG() is not known. To overcome this difficulty
one may take the view of the empirical Bayes approach which involves estimation of kG(P)
by using past experiences of the above estimation problem. Since X'X is a diagonal matrix

with diagonal elements, say, Dl,..., Dp, by (2.9)
02 t(o I P)

) = D D~j ipj t(P P) for 1 < i, j]p.

Thus with tG(3) as introduced in (2.11), the optimal estimator MPG() can be written as

MPG(P) = D% - 2G(p)/:$2 (2.15)
Thus we see that the optimal estimator, which remains unknown whenever G is so, can be

estimated by getting estimators of mixed partial derivatives of tG(P) w.r.t. j2i and Pi, and

second order partial derivatives of the same w.r.t. 93j. The m.s.c. estimators of such functions
can be obtained from Singh (1976), and then the lemma in the Appendix here can be used to
get estimators of the ratio (2.15).
REFERENCES

BHATTACHARYA, P. K. (1967). Estimates of a probability density function and its derivatives. San
A, 29, 373-382.

HAJEK, J. and SIDAK, Z. (1967). Theory of Rank Tests. Prague: Academia Kiado.
LEHMANN, E. L. (1959). Testing Statistical Hypotheses. New York: Wiley.
LOEVE, M. (1963). Probability Theory, 3rd ed. Princeton, N.J. and London: Van Nostrand.
ROBBINS, H. (1955). An empirical Bayes approach to statistics. Proc. Third Berkeley Symp., 1, 157-163.
RAO, C. R. (1973). Linear Statistical Inference and its Applications, 2nd ed. New York: Wiley.
SINGH, R. S. (1974). Estimation of derivatives of average of H densities and sequence compound estimation
in exponential families. Technical Report RM 218, Dept of Statistics and Probability, Michigan State
University.

- (1975). Nonparametric estimation of derivatives of average of densities with convergence rates and
applications. Unpublished.

-- (1976). Nonparametric estimation of mixed partial derivatives of a multivariate density. J. Mult.
Analy., 6, 111-122.

- (1977). Improvement on some known nonparametric uniformly consistent estimators of derivatives
of a density. Ann. Statist., 5, 394-400.

ZELLNER, A. (1975). Estimation of functions of population means and regression coefficients including
structural coefficients: a minimum expected loss approach. Technical report, Graduate School of Business,
University of Chicago.

APPENDIX

The following lemma, given in Singh (1974), has been found quite useful in proving the
mean square consistency of estimators of the ratio of two functions when mean square
consistent estimators of both functions are at hand.

This content downloaded from 206.253.207.235 on Thu, 21 Feb 2019 19:35:44 UTC
All use subject to https://about.jstor.org/terms

1977]

SINGH

-

Density

Estimates

363

Lemma. Let y, z and L be real numbers with z #0 and L > 0. If Y and Z are two real

valued random variables, then for every y>0

E( I Y- AL) <-,2y+(Y-1)+ I Zl Ely-E Y Yl+ (|Y +2-(Y-')+L) E| Z-Zly (A. 1)
Proof. Since I(21z-ZI<IzI)?I(2IZ1?IzI), the left-hand side of (A.1) is exceeded by

E(|Y YI I(2jZj,>,zj)+LYEI(2jzZ Z|>|jz| (A.2)
Now by the Markov inequality, the second term in (A.2) is no more than (2L)Y I z - El z -Zr7.
By the triangle inequality with intermediate term y/z, and by cr inequality (see Lo6ve, 1963,

p. 155) the first term in (A.2) is bounded by 2y+(Y-')+Izl-Y{Ely- YIY+Iy/zIYEIz-ZIY}.
Putting these results together we conclude (A.1).

ACKNOWLEDGEMENT

This research was supported by the National Research Council of Canada under research
grant no. A4631.

This content downloaded from 206.253.207.235 on Thu, 21 Feb 2019 19:35:44 UTC
All use subject to https://about.jstor.org/terms

