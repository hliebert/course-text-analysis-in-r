Econometric Theory, 21, 2005, 21–59+ Printed in the United States of America+
DOI: 10+10170S0266466605050036

MODEL SELECTION AND INFERENCE:
FACTS AND FICTION
HA N N E S LE E B
Yale University

BEN ED I K T M. PÖ T S CH ER
University of Vienna

Model selection has an important impact on subsequent inference+ Ignoring the
model selection step leads to invalid inference+ We discuss some intricate aspects
of data-driven model selection that do not seem to have been widely appreciated
in the literature+ We debunk some myths about model selection, in particular the
myth that consistent model selection has no effect on subsequent inference asymptotically+ We also discuss an “impossibility” result regarding the estimation of the
finite-sample distribution of post-model-selection estimators+

1. INTRODUCTION
In this expository article we discuss some of the problems that arise if one tries
to conduct statistical inference in the presence of data-driven model selection+
The position we hence take is that a ~finite! collection of competing models is
given, typically submodels obtained from an overall model through parameter
restrictions, and that the researcher uses the data to select one of the competing
models+1 The model selection procedure used here can be based on a ~multiple!
hypothesis testing scheme ~e+g+, general-to-specific testing, thresholding as in
wavelet regression, etc+!, on the optimization of a penalized goodness-of-fit criterion ~e+g+, Akaike information criterion @AIC#, Bayesian information criterion @BIC#, final prediction error @FPE#, minimum description length @MDL#,
or any of its numerous variants!, or on cross-validation methods+ The parameters of the selected model are then estimated ~e+g+, by least squares or maximum likelihood!+ Estimators resulting from such a two-step procedure are called
“post-model-selection estimators,” the classical pretest estimators constituting
an important example+ As an illustration consider regressor selection in a linear
model followed by least-squares estimation of the coefficients of the selected
regressors+ Here the competing models are submodels of an overall linear regression model ~of fixed finite dimension!, the submodels being given by zerorestrictions on the regression coefficients+
Address correspondence to Benedikt Pötscher, Department of Statistics, University of Vienna, Universitätsstrasse
5, A-1010, Vienna, Austria; e-mail: Benedikt+Poetscher@univie+ac+at

© 2005 Cambridge University Press

0266-4666005 $12+00

21

Downloaded from https://www.cambridge.org/core. Harvard University, on 07 Feb 2020 at 21:32:08, subject to the Cambridge Core terms of
use, available at https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0266466605050036

22

HANNES LEEB AND BENEDIKT M. PÖTSCHER

In this paper we do not wish to enter into a discussion of whether or not a
two-step procedure as described previously can be justified from a purely
decision-theoretic point of view ~although we touch upon this important
question in the discussion of the mean-squared error of post-model-selection
estimators in Sections 2+1 and 2+2 and also in Remark 4+1, which follows!+ We
rather take the pragmatic position that such procedures, explicitly acknowledged or not, are prevalent in applied econometric and statistical work and
that one needs to look at their true sampling properties and related questions
of inference post model selection+ Despite the importance of this problem in
econometrics and statistics, research on this topic has been neglected for decades,
exceptions being the pretest literature as summarized in Judge and Bock ~1978!
or Giles and Giles ~1993!, on the one hand, and the contributions regarding
distributional properties of post-model-selection estimators by, e+g+, Sen
~1979!, Sen and Saleh ~1987!, Dijkstra and Veldkamp ~1988!, and Pötscher
~1991!, on the other hand+2 Only in recent years has this area seen an increase
in research activity ~e+g+, Kabaila, 1995, 1998; Pötscher, 1995; Pötscher and
Novak, 1998; Ahmed and Basu, 2000; Kapetanios, 2001; Dukić and Peña,
2002; Hjort and Claeskens, 2003; Kabaila and Leeb, 2004; Leeb and Pötscher,
2003a, 2003b, 2004; Leeb, 2003a, 2003b; Nickl, 2003; Danilov and Magnus,
2004!+
The aim of this paper is to point to some intricate aspects of data-driven
model selection that do not seem to have been widely appreciated in the literature or that seem to be viewed too optimistically+ In particular, we demonstrate innate difficulties of data-driven model selection+ Despite occasional claims
to the contrary, no model selection procedure—implemented on a machine or
not—is immune to these difficulties+ The main points we want to make and
that will be elaborated upon subsequently can be summarized as follows+3
1+ Regardless of sample size, the model selection step typically has a dramatic effect on the sampling properties of the estimators that can not be
ignored+ In particular, the sampling properties of post-model-selection estimators are typically significantly different from the nominal distributions
that arise if a fixed model is supposed+
2+ As a consequence, naive use of inference procedures that do not take into
account the model selection step ~e+g+, using standard t-intervals as if the
selected model had been given prior to the statistical analysis! can be highly
misleading+
3+ An increasingly frequently used argument in the literature is that consistent model selection procedures allow one to employ the standard asymptotic distributions that would apply if no model selection were performed
and that thus the effects of consistent model selection on inference can be
safely ignored+4 Unfortunately, at closer inspection this conclusion turns
out not to be warranted at all, and relying on it only creates an illusion of
conducting valid inference+ In the same vein, the effects of procedures
Downloaded from https://www.cambridge.org/core. Harvard University, on 07 Feb 2020 at 21:32:08, subject to the Cambridge Core terms of
use, available at https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0266466605050036

MODEL SELECTION AND INFERENCE

23

that consistently choose from a finite set of alternatives ~e+g+, procedures that consistently decide between I ~0! and I ~1! or consistently select
the number of structural breaks, etc+! on subsequent inference can not be
ignored safely+ Although it is mathematically true that the use of a consistent model selection procedure entails that the ~pointwise! asymptotic
distributions of the post-model-selection estimators coincide with the
asymptotic distributions that would arise if the selected model were treated
as fixed a priori ~see, e+g+, Pötscher, 1991, Lemma 1!, this does not justify the aforementioned conclusion ~for the reasons already outlined in
Pötscher, 1991, Sect+ 4, Remark ~iii!; and further discussed in Kabaila,
1995!+5
4+ More generally, regardless of whether a consistent or a conservative 6 model
selection procedure is used, the finite-sample distributions of a post-modelselection estimator are typically not uniformly close to the respective
~pointwise! asymptotic distributions+ Hence, regardless of sample size these
asymptotic distributions can not be safely used to replace the ~complicated! finite-sample distributions+
5+ The finite-sample distributions of post-model-selection estimators are typically complicated and depend on unknown parameters+ Estimation of these
finite-sample distributions is “impossible” ~even in large samples!+ No
resampling scheme whatsoever can help to alleviate this situation+
To facilitate a detailed analysis of the effects of selecting a model from a
collection of competitors we assume in this paper—as already noted earlier—
that one of the competing models is capable of correctly describing the data
generating process+ Of course, it can always be debated whether or not such an
assumption leads to a “test-bed” that is relevant for empirical work, but we
shall not pursue this debate here ~see, e+g+, the contribution of Phillips, 2005, in
this issue!+ The important question of the effects of model selection when selecting only from approximate models will be studied elsewhere+
The points listed previously will be exemplified in detail in Section 2 in the
context of a very simple linear regression model, although they are valid on a
much wider scope+ Because of its simplicity, this example is amenable to a
small-sample and also to a large-sample analysis, allowing one to easily get
insight into the complications that arise with post-model-selection inference;
for results in more general frameworks see Pötscher ~1991!, Leeb and Pötscher
~2003a, 2003b, 2004!, and Leeb ~2003a, 2003b!+ Consistent model selection
procedures are discussed in Section 2+1, whereas Section 2+2 deals with conservative procedures+ Section 2+3 is devoted to the question of estimating the finitesample distribution of post-model-selection estimators+ Shrinkage-type estimators
such as Lasso-type estimators, Bridge estimators, and the smoothly clipped absolute deviation ~SCAD! estimator, etc+, are briefly discussed in Section 3+ Section 4 contains some remarks, and Section 5 concludes+ Some technical results
and their proofs are collected in the Appendixes+
Downloaded from https://www.cambridge.org/core. Harvard University, on 07 Feb 2020 at 21:32:08, subject to the Cambridge Core terms of
use, available at https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0266466605050036

24

HANNES LEEB AND BENEDIKT M. PÖTSCHER

2. AN ILLUSTRATIVE EXAMPLE
In the following discussion we shall—for the sake of exposition—use a very
simple example to illustrate the issues involved in model selection and inference post model selection+ These issues, however, clearly persist also in more
complicated situations such as, e+g+, nonlinear models, time series models, etc+
Consider the linear regression model
yt ⫽ ax t1 ⫹ bx t2 ⫹ et

~1 ⱕ t ⱕ n!

(1)

under the “textbook” assumptions that the errors et are independent and identically distributed ~i+i+d+! N~0, s 2 !, s 2 ⬎ 0, and the nonstochastic n ⫻ 2 regressor matrix X has full rank and satisfies X ' X0n r Q ⬎ 0 as n r `+ For simplicity,
we shall also assume that the error variance s 2 is known+7 It will be convenient to write the matrix s 2 ~X ' X0n!⫺1 as
s 2 ~X ' X0n!⫺1 ⫽

冉

sa2

sa, b

sa, b

sb2

冊

+

The elements of this matrix depend on sample size n, but we shall suppress this
dependence in the notation+ The elements of the limit of this matrix will be
2
denoted by sa,`
, etc+ It will prove useful to define r ⫽ sa, b 0~sa sb !, i+e+, r is
the correlation coefficient between the least-squares estimators for
a and b in model ~1!+ Its limit will be denoted by r` +
Suppose now that the parameter of interest is the coefficient a in ~1! and that
we are undecided whether or not to include the regressor x t2 in the model a
priori+ ~The case where a general linear function A~a, b!' , e+g+, a predictor, rather
than a is the quantity of interest is quite similar and is briefly discussed in
Remark 4+5+! In other words, we have to decide on the basis of the data whether
to fit the unrestricted ~full! model or the restricted model with b ⫽ 0+ We shall
denote the two competing candidate models by U and R ~for unrestricted and
restricted, respectively!+ For any given value of the parameter vector ~a, b!,
the most parsimonious true model will be denoted by M0 and is given by
M0 ⫽

再

U

if b ⫽ 0

R

if b ⫽ 0+

It is important to note that M0 depends on the unknown parameters ~namely,
through b!+ The least-squares estimators for a and b in the unrestricted model
will be denoted by a~U
[
! and b~U
Z !, respectively+ The least-squares estimator
for a in the restricted model will be denoted by a~R!,
[
and we shall set b~R!
Z
⫽ 0+
We shall decide between the competing models U and R depending on whether
Z !0sb 6 ⬎ c or not, where c ⬎ 0 is a user-specified cutthe test statistic 6M n b~U
off point+ That is, we shall use the model MZ ⫽ U if 6M n b~U
Z !0sb 6 ⬎ c, and we
shall work with MZ ⫽ R otherwise+ This is a traditional pretest procedure based
on the likelihood ratio, but it is worth noting that in the simple example disDownloaded from https://www.cambridge.org/core. Harvard University, on 07 Feb 2020 at 21:32:08, subject to the Cambridge Core terms of
use, available at https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0266466605050036

MODEL SELECTION AND INFERENCE

25

cussed here it coincides exactly with Akaike’s minimum AIC rule in case c ⫽
M 2 and with Schwarz’s minimum BIC rule if c ⫽ M log n + ~We note here in
passing that there is a close connection between pretest procedures and information criteria in general; see Remark 4+2+! In fact, in the present example it
seems that there is little choice with regard to the model selection procedure
other than the choice of c, as it is hard to come up with a reasonable model
selection procedure that is not based on the likelihood ratio statistic ~at least
asymptotically!+ Now that we have defined the model selection procedure M,
Z
the resulting post-model-selection estimator for the parameter of interest a will
be denoted by aJ ⫽ a~
[ MZ !; i+e+,
aJ ⫽ a~R!1~
[
MZ ⫽ R! ⫹ a~U
[
!1~ MZ ⫽ U !+
The following simple observations will be useful: The finite-sample distribution of aJ is a convex combination of the conditional distributions, where the
conditioning is on the outcome of the model selection procedure M:
Z
Pn, a, b ~ aJ ⱕ t ! ⫽ Pn, a, b ~ aJ ⱕ t 6 MZ ⫽ R!Pn, a, b ~ MZ ⫽ R!
⫹ Pn, a, b ~ aJ ⱕ t 6 MZ ⫽ U !Pn, a, b ~ MZ ⫽ U !
[
ⱕ t 6 MZ ⫽ R!Pn, a, b ~ MZ ⫽ R!
⫽ Pn, a, b ~ a~R!
[
! ⱕ t 6 MZ ⫽ U !Pn, a, b ~ MZ ⫽ U !,
⫹ Pn, a, b ~ a~U

(2)

where Pn, a, b denotes the probability measure corresponding to the true parameters a, b and sample size n+ The model selection probabilities Pn, a, b ~ MZ ⫽ U !
and Pn, a, b ~ MZ ⫽ R! ⫽ 1 ⫺ Pn, a, b ~ MZ ⫽ U ! can be evaluated easily and are
given by
Pn, a, b ~ MZ ⫽ U ! ⫽ 1 ⫺ ~F~c ⫺ M n b0sb ! ⫺ F~⫺c ⫺ M n b0sb !!,

(3)

where F~{! denotes the standard normal cumulative distribution function ~c+d+f+!+
Cf+ Leeb and Pötscher ~2003a, Sect+ 3+1! and Leeb ~2003b, Sect+ 3+1!+
The subsequent discussion is cast in terms of consistent versus conservative
model selection procedures, because this is entrenched terminology+8 However,
despite this terminology, one should not lose sight of the fact that we are given
only one sample of fixed sample size n together with a fixed model selection
procedure ~e+g+, a particular value of the cutoff point c in the present example!
and we are interested in the finite-sample properties of this procedure+ Any
given model selection procedure can now equally well be embedded as a member into a sequence of consistent model selection procedures or into a sequence
of conservative procedures for the purpose of asymptotic analysis ~by appropriately defining the model selection procedures at the other—fictitious—
sample sizes!+ Of course, the finite-sample properties of the given model
selection procedure are unaffected by our choice of the embedding asymptotic
framework+ Hence, when talking about consistent or conservative sequences of
Downloaded from https://www.cambridge.org/core. Harvard University, on 07 Feb 2020 at 21:32:08, subject to the Cambridge Core terms of
use, available at https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0266466605050036

26

HANNES LEEB AND BENEDIKT M. PÖTSCHER

model selection procedures we are in fact not talking about different procedures but rather about different asymptotic frameworks and their comparative
~dis!advantages in revealing the finite-sample properties of a given procedure+
2.1. The Consistent Model Selection Framework
As mentioned in the introduction, proceeding with inference post model selection “as usual” ~i+e+, as if the selected model were given a priori! is often
defended by the argument that a consistent model selection procedure has been
used and hence asymptotically the selected model would coincide with the most
parsimonious true model, supposedly allowing one to use the standard asymptotic results that apply in case of an a priori fixed model+ We now look more
closely at the merit of such an argument+
We assume in this section that the cutoff point c in the definition of the model
selection procedure MZ is chosen to depend on sample size n such that c r `
and c0M n r 0 as n r `+ Then it is well known ~see Bauer, Pötscher, and
Hackl, 1988; and also Remark 4+3! that the model selection procedure is a consistent procedure in the sense that
Pn, a, b ~ MZ ⫽ M0 !

nr`

&

1

(4)

holds for every a, b; i+e+, the probability of revealing the most parsimonious
true model tends to unity as sample size increases+ Because the event $ MZ ⫽ M0 %
is clearly contained in the event $ aJ ⫽ a~M
[
0 !%, the consistency property expressed
in ~4! moreover immediately entails that
[
Pn, a, b ~ aJ ⫽ a~M
0 !!

nr`

&

1

(5)

holds for every a, b, where a~M
[
0 ! denotes the least-squares estimator in the
most parsimonious true model+ Although this latter “estimator” is infeasible as
it makes use of the unknown information whether or not b ⫽ 0, relation ~5!
shows that the post-model-selection estimator aJ is a feasible version in the sense
that both estimators coincide with probability tending to unity as sample size
increases+ An immediate consequence of ~5! is that the ~pointwise! asymptotic
distributions of aJ and a~M
[
0 ! are identical, regardless of whether M0 ⫽ U or
M0 ⫽ R+ This latter property, which is sometimes called the “oracle” property
~Fan and Li, 2001!, obviously holds for post-model-selection estimators obtained
through consistent model selection procedures in general; cf+ Pötscher ~1991,
Lemma 1! for a formal statement+9
So far the preceding discussion seems to support the argument that proceeding “as usual” with inference post consistent model selection is justified+ In
particular, it seems to suggest that the usual construction of confidence sets
remains valid post consistent model selection+ Furthermore, observe that ~5!
entails that the post-model-selection estimator aJ is asymptotically normally disDownloaded from https://www.cambridge.org/core. Harvard University, on 07 Feb 2020 at 21:32:08, subject to the Cambridge Core terms of
use, available at https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0266466605050036

MODEL SELECTION AND INFERENCE

27

tributed and is as “efficient” as the maximum likelihood estimator based on the
full model if the full model is the most parsimonious true model ~i+e+, if b ⫽ 0!,
and is more “efficient” ~namely, as “efficient” as the maximum likelihood estimator based on the restricted model! if the restricted model is the most parsimonious one ~i+e+, if b ⫽ 0!+ This seems too good to be true, and, in fact, it is!
Although the result in ~5! is mathematically correct, it is a delusion to believe
that it carries much statistical meaning+ Before we explore this in detail, a little
reflection shows that the post-model-selection estimator aJ is nothing else than
a variant of Hodges’ so-called superefficient estimator ~cf+ Lehmann and Casella,
1998, pp+ 440– 443!+10 It is remarkable that estimators such as Hodges’ estimator, which was constructed in 1951 as an artificial counterexample to the belief
that any asymptotically normally distributed estimator has an asymptotic variance that can not fall below the ~asymptotic! Cramér–Rao bound, have nowadays come to some prominence in the guise of post-model-selection estimators
based on a consistent model selection procedure ~and of other related estimators; see Section 3!+ It is equally remarkable that some of the lessons learned
from Hodges’ counterexample seem not to have been received in the model
selection literature in the intervening years: 11 The actual finite-sample behavior of aJ is not properly reflected by the ~pointwise! asymptotic results; in fact,
these results can be highly misleading regardless of the sample size and tend to
paint an overly optimistic picture of the performance of the estimator+ Mathematically speaking, the culprit is nonuniformity ~w+r+t+ the true parameter vector ~a, b !! in the convergence of the finite-sample distributions to the
corresponding asymptotic distributions; cf+ the warning already issued in Pötscher
~1991! in the discussion following Lemma 1 and also in Section 4, Remark ~iii!,
of that paper+
In the simple example discussed here even a finite-sample analysis is possible that allows us to nicely showcase the problems involved+12 We begin with a
closer look at the probability Pn, a, b ~ MZ ⫽ M0 ! of selecting the most parsimonious true model+ From ~3! this probability equals F~c! ⫺ F~⫺c! if b ⫽ 0,
which—in accordance with ~4!—goes to unity as sample size increases because
we have assumed c r ` in this section+ In case b ⫽ 0, the probability equals
1 ⫺ ~F~c ⫺ M n b0sb ! ⫺ F~⫺c ⫺ M n b0sb !! and—again in accordance with
~4!—converges to unity as n r `+ This is so because c0M n r 0, so that the
arguments of the F-functions in this formula converge either both to ⫹` or
both to ⫺`+ Nevertheless, the probability of selecting the most parsimonious
true model can be very small for any given sample size if b ⫽ 0 is close to
zero+ In that case, we see that this probability is close to 1 ⫺ ~F~c! ⫺ F~⫺c!!,
which in turn is close to zero because of c r `+ More precisely, if b ⫽ 0
equals zsb c0M n , 6z6 ⬍ 1, then—despite ~4!—the probability of selecting the
most parsimonious true model in fact converges to zero! 13 That is, the consistent model selection procedure is completely “blind” to certain deviations from
the restricted model that are of the order c0M n + In particular, this reveals that
the convergence in ~4! is decidedly nonuniform w+r+t+ b: In other words, for the
Downloaded from https://www.cambridge.org/core. Harvard University, on 07 Feb 2020 at 21:32:08, subject to the Cambridge Core terms of
use, available at https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0266466605050036

28

HANNES LEEB AND BENEDIKT M. PÖTSCHER

asymptotics to “kick in” in ~4! arbitrarily large sample sizes are needed depending on the value of the parameter b+ This means that M,
Z although being consistent for M0 , is not uniformly consistent ~not even locally!+ ~This is in fact true
for any consistent model selection procedure; see Remark 4+4+! We illustrate
this now numerically+ In the following discussion, it proves useful to write g
as shorthand for ~M n 0sb !b, i+e+, to reparameterize b as b ⫽ ~sb 0M n !g+ As a
function of g, the probability of selecting the unrestricted model ~which is the
most parsimonious true model in case b ⫽ 0! is pictured in Figure 1+ Recall
that with the choice c ⫽ M log n our model selection procedure coincides with
the minimum BIC method+
Figure 1 confirms that the probability of selecting the correct model can be
very small if b ⫽ 0 is of the order O~10M n ! and also suggests that this effect
even gets stronger as the sample size increases+ The latter observation is
explained by the fact that the probability of selecting the correct model converges to zero not only for b ⫽ 0 of the order O~10M n ! but even for b ⫽ 0 of
larger order, namely, for b of the form zsb c0M n , 6z6 ⬍ 1; cf+ Proposition A+1
in Appendix A+ Furthermore, we can also calculate, for given b ⫽ 0, how many
data points are needed such that the probability of selecting the correct ~i+e+,
the unrestricted! model is at least 0+9, say+ With c ⫽ M log n as in Figure 1, we
obtain: If b0sb ⫽ 1, then a sample of size n ⱖ 8 is needed; if b0sb ⫽ _12 , one
needs n ⱖ 42; if b0sb ⫽ _14 , one needs n ⱖ 207; and if b0sb ⫽ _18 , then n ⱖ 977
is required+ This demonstrates that the required sample size heavily depends on
the unknown b and increases without bound as b gets closer to zero+

Figure 1. Finite-sample model selection probability+ The probability of selecting the
unrestricted model as a function of g ⫽ M n b0sb for various values of n, where we have
taken c ⫽ M log n + Starting from the top, the curves show Pn, a, b ~ MZ ⫽ U ! for n ⫽ 10 k for
k ⫽ 1,2, + + + ,6+ Note that Pn, a, b ~ MZ ⫽ U ! is independent of a and symmetric around zero
in b or, equivalently, g+

Downloaded from https://www.cambridge.org/core. Harvard University, on 07 Feb 2020 at 21:32:08, subject to the Cambridge Core terms of
use, available at https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0266466605050036

MODEL SELECTION AND INFERENCE

29

The phenomenon discussed here occurs only if the parameter b ⫽ 0 is “small”
in absolute value in the sense that it goes to zero of a certain order+14 It might
then be tempting to argue that in such a case erroneously selecting the restricted
model is not necessarily detrimental as the restricted model is only “marginally” misspecified: In particular, the estimator aJ is consistent, even uniformly
consistent ~cf+ Proposition A+9 in Appendix A!, and satisfies aJ ⫺ a ⫽ OP ~n⫺102 !
as n r ` ~where OP is understood relative to Pn, a, b for fixed a and b!+ However, given that the consistent model selection procedure is “blind” to deviations from the restricted model of the order 10M n ~and even to deviations of
larger order!, it should not come as a surprise that the phenomenon discussed
previously crops up again in the distribution of M n ~ aJ ⫺ a!+ Recall that, as a
consequence of ~5!, M n ~ aJ ⫺ a! is asymptotically normally distributed with
mean zero and variance equal to the asymptotic variance of the restricted leastsquares estimator if b ⫽ 0 and equal to the asymptotic variance of the
unrestricted least-squares estimator if b ⫽ 0+ However, in finite samples—
regardless of how large—we get a completely different picture: From Leeb
~2003b!, we obtain that the finite-sample density of M n ~ aJ ⫺ a! is given by
gn, a, b ~u! ⫽ sa⫺1 ~1 ⫺ r 2 !⫺102 f ~ u~1 ⫺ r 2 !⫺1020sa ⫹ r~1 ⫺ r 2 !⫺102 M n b0sb !
⫻ D~M n b0sb , c!

冉 冉M

⫹ sa⫺1 1 ⫺ D

n b0sb ⫹ ru0sa

M1 ⫺ r

2

c
,

M 1 ⫺ r2

冊冊

f~u0sa !,

(6)

where f~{! denotes the standard normal probability density function ~p+d+f+!+
Furthermore, we have used D~a, b! as shorthand for F~a ⫹ b! ⫺ F~a ⫺ b!,
where F denotes the standard normal c+d+f+ Note that D~a, b! is symmetric in
its first argument+ The finite-sample density of M n ~ aJ ⫺ a! does not depend on
a and is the sum of two terms: The first term is the density of M n ~ a~R!
[
⫺ a!
multiplied by the probability of selecting the restricted model+ The second term
[
! ⫺ a!, where the deformais a “deformed” version of the density of M n ~ a~U
tion factor is given by the 1 ⫺ D~{,{!-term+15 Figure 2 gives an example of the
possible shapes of the density of M n ~ aJ ⫺ a!+
Two of the densities in Figure 2 are unimodal: The one with the larger mode
[
⫺ a!
arises for b0sb ⫽ 0 and is quite close to the ~normal! density of M n ~ a~R!
corresponding to the restricted model+ The reason for this is that the probability
D~0, c! of selecting the restricted model is large, namely, 0+968, and hence the
first term in ~6! is the dominant one+ The density with the smaller mode arises
[
! ⫺ a! correfor b0sb ⫽ 0+5 and closely resembles the density of M n ~ a~U
sponding to the unrestricted model+ The reason here is ~i! that the probability
of selecting the unrestricted model is large, namely, 0+998, and hence the second term in ~6! is dominant and ~ii! that this dominant term is approximately
Gaussian; more precisely, the second term in ~6! is approximately equal to
f~u!~1 ⫺ D~7 ⫹ 0+98u,3!!, which differs from f~u! in absolute value by less
Downloaded from https://www.cambridge.org/core. Harvard University, on 07 Feb 2020 at 21:32:08, subject to the Cambridge Core terms of
use, available at https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0266466605050036

30

HANNES LEEB AND BENEDIKT M. PÖTSCHER

Figure 2. Finite-sample densities+ The density gn, a, b of M n ~ aJ ⫺ a! for various values
of b0sb + For the graphs, we have taken n ⫽ 100, c ⫽ M log n , r ⫽ 0+7, and sa2 ⫽ 1+
The four curves correspond to b0sb equal to 0, 0+21, 0+25, and 0+5 and are discussed in
the text+

than 0+002+ The bimodal densities correspond to the cases b0sb ⫽ 0+21 and
b0sb ⫽ 0+25+ In both cases, the left-hand mode reflects the contribution of the
first term in ~6! whereas the right-hand mode reflects the contribution of the
second term+ The height of the left-hand mode is proportional to the probability of selecting the restricted model, which is larger for b0sb ⫽ 0+21 than
for b0sb ⫽ 0+25+ In summary, we see that the finite-sample distribution
of M n ~ aJ ⫺ a! depends heavily on the value of the unknown parameter b
~through b0sb ! and that it is far from its Gaussian large-sample limit distribution for certain values of b+ The same phenomenon is also found if we repeat
the calculations for other sample sizes n, regardless of how large n is+ In other
words: Although the distribution of M n ~ aJ ⫺ a! is approximately Gaussian for
each given ~a, b! and sufficiently large sample size, the amount of data required
to achieve a given accuracy of approximation depends on the unknown b+ In
the example presented in Figure 2, a sample size of 100 appears to be sufficient for the normal approximation predicted by pointwise asymptotic theory
to be reasonably accurate in the cases b0sb ⫽ 0 and b0sb ⫽ 0+5, whereas it is
clearly insufficient in case b0sb ⫽ 0+21 or b0sb ⫽ 0+25+
How can this be reconciled with the result mentioned earlier that M n ~ aJ ⫺ a!
has an asymptotic normal distribution with mean zero and appropriate variance? The crucial observation again is that this limit result is a pointwise one;
i+e+, it holds for each fixed value of the parameter vector ~a, b! individually but
does not hold uniformly w+r+t+ ~a, b! ~in fact, not even locally uniformly!: While
it is easy to see that for every u 僆 R the density gn, a, b ~u! given by ~6! converges to the appropriate normal density for each fixed ~a, b!, it is equally easy
to see ~cf+ Proposition A+2 in Appendix A! that ~6! has a different asymptotic
Downloaded from https://www.cambridge.org/core. Harvard University, on 07 Feb 2020 at 21:32:08, subject to the Cambridge Core terms of
use, available at https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0266466605050036

MODEL SELECTION AND INFERENCE

31

behavior if, e+g+, b ⫽ sb g0M n with g ⫽ 0+ In this case ~6! converges to a
[
⫺ a!,
shifted version of the density of the asymptotic distribution of M n ~ a~R!
the shift being controlled by g+ Yet another asymptotic behavior is obtained if
we consider b ⫽ sb gn 0M n with gn r ` ~or gn r ⫺`! but gn ⫽ o~c!+ Then
gn, a, b ~u! even converges to zero for every u 僆 R! That is, the distribution
of M n ~ aJ ⫺ a! does not “stabilize” as sample size increases but—loosely
speaking—“escapes” to ` or ⫺` ~depending on the sign of gn !; in fact,
M n ~ aJ ⫺ a! r ` or ⫺` in Pn, a, b -probability+ More complicated asymptotic
behavior is in fact possible and is described in Proposition A+2 in Appendix
A+16 ~To simplify matters the rather special case r` ⫽ 0 is excluded from the
preceding discussion; cf+ Remark 4+6 for some comments on this case+ However, note that Proposition A+2 also covers the case r` ⫽ 0+!
We are now in a position to analyze the actual coverage properties of confidence intervals that are constructed “as usual,” thereby ignoring the presence
of model selection ~this step seemingly being justified by a reference to ~5!!+
Let I denote the “naive” confidence interval that is given by the usual confidence interval in the restricted ~unrestricted! model if the restricted ~unrestricted!
model is selected+ That is,
I ⫽ @ aJ ⫺ z h n⫺102 sa ~1 ⫺ r 2 ! 102, aJ ⫹ z h n⫺102 sa ~1 ⫺ r 2 ! 102 #

(7)

if MZ ⫽ R and
I ⫽ @ aJ ⫺ z h n⫺102 sa , aJ ⫹ z h n⫺102 sa #

(8)

if MZ ⫽ U where 1 ⫺ h denotes the nominal coverage probability and z h is the
~1 ⫺ h02! quantile of a standard normal distribution+ In view of ~2!, the actual
coverage probability satisfies
Pn, a, b ~a 僆 I ! ⫽ Pn, a, b ~a 僆 I 6 MZ ⫽ R!Pn, a, b ~ MZ ⫽ R!
⫹ Pn, a, b ~a 僆 I 6 MZ ⫽ U !Pn, a, b ~ MZ ⫽ U !+

(9)

Using the remark in note 15 in the notes section, it is an elementary calculation
to obtain
Pn, a, b ~a 僆 I !
⫽ D~r~1 ⫺ r 2 !⫺102 M n b0sb , z h !D~M n b0sb , c!
⫹

冕

zh

⫺z h

~1 ⫺ D~~M n b0sb ⫹ ru!~1 ⫺ r 2 !⫺102, c~1 ⫺ r 2 !⫺102 !!f~u! du+
(10)

Note that the coverage probability does not depend on a and is symmetric around
zero as a function of b+ Because of ~5! and the attending discussion, pointwise
asymptotic theory tells us that the coverage probability Pn, a, b ~a 僆 I ! conDownloaded from https://www.cambridge.org/core. Harvard University, on 07 Feb 2020 at 21:32:08, subject to the Cambridge Core terms of
use, available at https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0266466605050036

32

HANNES LEEB AND BENEDIKT M. PÖTSCHER

Figure 3. Finite-sample coverage probabilities+ The coverage probability of the “naive”
confidence interval I with nominal confidence level 1 ⫺ h ⫽ 0+95 as a function of g ⫽
M n b0sb for various values of n, where we have taken c ⫽ M log n and r ⫽ 0+7+ The
curves are given for n ⫽ 10 k for k ⫽ 1,2, + + + ,7; larger sample sizes correspond to curves
with a smaller minimal coverage probability+

verges to 1 ⫺ h for every ~a, b!+ However, the plots of the coverage probability given in Figure 3 speak another language+
We see that the actual coverage probability of the “naive” interval I is often
far below its nominal level of 0+95, sometimes falling below 0+3+ Figure 3 also
suggests that this phenomenon gets more pronounced when sample size
increases! In fact, it is not difficult to see that the minimal coverage probability
of I converges to zero as sample size increases and not to the nominal coverage probability 1 ⫺ h as one might have hoped for ~except possibly in the relatively special case r` ⫽ 0!; cf+ also Kabaila ~1995!+ To see this, note that
min Pn, a, b ~a 僆 I ! ⱕ Pn, a, sb gn 0M n ~a 僆 I !,
a, b

where a is arbitrary and gn is chosen such that gn r ` ~or gn r ⫺`! and
gn ⫽ o~c!+ ~The r+h+s+ in the preceding inequality does actually not depend on a
in view of ~10!+! Because Pn, a, sb gn 0M n ~ MZ ⫽ U ! converges to zero as discussed
earlier ~cf+ Proposition A+1 in Appendix A!, we arrive—using ~9! and ~10!—at
lim min Pn, a, b ~a 僆 I !

nr` a, b

ⱕ lim Pn, a, sb gn 0M n ~a 僆 I 6 MZ ⫽ R!Pn, a, sb gn 0M n ~ MZ ⫽ R!
nr`

⫽ lim D~r~1 ⫺ r 2 !⫺102 gn , z h !D~gn , c! ⫽ 0,
nr`

the last equality being true because 6gn 6 r ` ~and because we have excluded
the case r` ⫽ 0!+
Downloaded from https://www.cambridge.org/core. Harvard University, on 07 Feb 2020 at 21:32:08, subject to the Cambridge Core terms of
use, available at https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0266466605050036

MODEL SELECTION AND INFERENCE

33

We finally illustrate the impact of model selection on the ~scaled! bias and
the ~scaled! mean-squared error of the estimator ~again excluding for simplicity of discussion the case r` ⫽ 0!+ Let Bias denote the expectation and MSE
the second moment of M n ~ aJ ⫺ a!+ We discuss the bias first+ An explicit
formula for the bias can be obtained from ~6! by a tedious but straightforward
computation and is given by
Bias ⫽ ⫺rsa @~M n b0sb !D~M n b0sb , c!
⫺ f~M n b0sb ⫺ c! ⫹ f~M n b0sb ⫹ c!# +

(11)

A pointwise ~i+e+, for fixed ~a, b!! asymptotic analysis tells us that this bias
vanishes asymptotically+17 In Figure 4 we have computed this bias numerically
as a function of g ⫽ M n b0sb+ Note that the bias is independent of a and antisymmetric around zero in b or, equivalently, g ~and hence is shown only for
g ⱖ 0!+
Figure 4 demonstrates that—contrary to the prediction of pointwise asymptotic theory—the bias can be quite substantial if b is of the order O~10M n ! and
that this effect gets more pronounced as the sample size increases ~the reason
for this discrepancy again being nonuniformity in the pointwise asymptotic
results!+ An asymptotic analysis of ~11! using b ⫽ sb g0M n with g ⫽ 0 shows
that the bias converges to ⫺sa r` g ~see Proposition A+4 in Appendix A for
more information!+ Note that this limit corresponds to the “envelope” of the
finite-sample bias curves ~for all n! as indicated in Figure 4+ Furthermore, if
b ⫽ sb gn 0M n with gn r ` ~or gn r ⫺`! but gn ⫽ o~c!, the asymptotic
analysis in Proposition A+4 even shows that the bias converges to 6`, the sign

Figure 4. Finite-sample bias+ The expectation of M n ~ aJ ⫺ a!, i+e+, the ~scaled! bias of
the post-model-selection estimator for a, as a function of g ⫽ M n b0sb for various values of n, where we have taken c ⫽ M log n , r ⫽ 0+7, and sa2 ⫽ 1+ The curves are given
for n ⫽ 10 k for k ⫽ 1,2, + + + ,7; larger sample sizes correspond to curves with larger maximal absolute biases+

Downloaded from https://www.cambridge.org/core. Harvard University, on 07 Feb 2020 at 21:32:08, subject to the Cambridge Core terms of
use, available at https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0266466605050036

34

HANNES LEEB AND BENEDIKT M. PÖTSCHER

depending on the sign of gn + As a consequence, the maximal absolute bias in
fact grows without bound as sample size increases!
Turning to the MSE we encounter a similar situation+ Using the fact that the
Z !0sb 6 is independent of a~R!
[
~e+g+, Leeb and Pötscher,
test statistic 6M n b~U
2003a, Proposition 3+1! and that a~R!
[
⫽ a~U
[
! ⫺ r~sa 0sb ! b~U
Z !, the MSE can
be computed explicitly to be

MSE ⫽ sa2 ⫹ sa2 r 2

冤

~c ⫹ M n b0sb !f~c ⫹ M n b0sb !
⫹ ~c ⫺ M n b0sb !f~c ⫺ M n b0sb !
⫹ ~~nb 20sb2 ! ⫺ 1!~F~c ⫺ M n b0sb !
⫺ F~⫺c ⫺ M n b0sb !!

冥

+

(12)

Alternatively, the preceding formula can also be obtained by brute force integration from the density ~6! or from Theorems 2+2 and 4+1 in Magnus ~1999!+
The MSE is independent of a+ A pointwise asymptotic analysis tells us that
2
~1 ⫺ r`2 ! of M n ~ a~R!
[
⫺ a! if
MSE converges to the asymptotic variance sa,`
2
[
! ⫺ a! if b ⫽ 0+18
b ⫽ 0 and to the asymptotic variance sa,` of M n ~ a~U
Again, however, the finite-sample mean-squared error exhibits a totally different behavior, regardless how large sample size is ~as a result of nonuniformity
in the pointwise asymptotics!+ This can be gleaned from Figure 5: The maximal mean-squared error is much larger than the mean-squared error of the
unrestricted least-squares estimator that is constant and equal to sa2 ⫽ 1+ As
Figure 5 suggests, the maximal mean-squared error diverges to infinity as sam-

Figure 5. Finite-sample mean-squared error+ The second moment of M n ~ aJ ⫺ a!,
i+e+, the ~scaled! mean-squared error of the post-model-selection estimator for a, as a
function of g ⫽ M n b0sb for various values of n, where we have taken c ⫽ M log n ,
r ⫽ 0+7, and sa2 ⫽ 1+ The curves are given for n ⫽ 10 k for k ⫽ 1,2, + + + ,7; larger sample
sizes correspond to curves with larger maximal mean-squared error+

Downloaded from https://www.cambridge.org/core. Harvard University, on 07 Feb 2020 at 21:32:08, subject to the Cambridge Core terms of
use, available at https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0266466605050036

MODEL SELECTION AND INFERENCE

35

ple size increases, whereas the mean-squared error of M n ~ a~U
[
! ⫺ a! stays
2
!+ This is well known for the Hodges estimator
bounded ~it converges to sa,`
~e+g+, Lehmann and Casella, 1998, p+ 442!+ For the mean-squared error of
M n ~ aJ ⫺ a! this follows of course immediately from the fact noted previously
that the bias diverges to 6` when setting b ⫽ sb gn 0M n with gn r ` ~or
gn r ⫺`! but gn ⫽ o~c!+ ~The phenomenon that the maximal absolute bias
and hence the maximal mean-squared error diverge to infinity holds for postmodel-selection estimators based on consistent model selection procedures in
general; see Remark 4+1, Appendix C; and Yang ~2003!+!
2.2. The Conservative Model Selection Framework
Generally speaking, post-model-selection estimators based on conservative model
selection procedures are subject to phenomena similar to the ones observed in
Section 2+1 for post-model-selection estimators based on consistent procedures+
In particular, the finite-sample behavior of both types of post-model-selection
estimators is governed by exactly the same formulas, because the finite-sample
behavior is clearly not much impressed by what we fancy about the behavior of
the model selection procedure at fictitious sample sizes other than n ~e+g+, what
we fancy about the behavior of the cutoff point c as a function of n!+ Cf+ the
discussion immediately preceding Section 2+1+ Not surprisingly, some differences arise in the asymptotic theory+
In this section we consider the same model selection procedure and postmodel-selection estimator aJ as before, except that we now assume the cutoff
point c to be independent of sample size n+19 This results in a conservative
model selection procedure ~that is not consistent!+20 As just noted, the finitesample distribution, the expectation, and the second moment of M n ~ aJ ⫺ a! are
again given by ~6!, ~11!, and ~12!, respectively+ Also, the model selection probabilities and the coverage probability of the “naive” confidence interval are given
by the same formulas as before+ As a consequence, all conclusions drawn from
the finite-sample formulas in Section 2+1 remain valid here: The finite-sample
distribution of the post-model-selection estimator is often decidedly nonnormal, and the standard asymptotic approximations derived on the presumption
of an a priori given model are inappropriate+ In particular, the actual coverage
probability of the “naive” confidence interval is often much smaller than the
nominal coverage probability+ Finally, the bias can be substantial, and the meansquared error can by far exceed the mean-squared error of the unrestricted
estimator+
We briefly discuss the asymptotic behavior next+21 A much more detailed treatment covering more general model selection procedures and more general models
can be found in Pötscher ~1991!, Leeb and Pötscher ~2003a!, and Leeb ~2003a, b!+
The pointwise limiting behavior of the model selection probabilities can be
easily read off from the finite-sample formula ~3!: lim nr` Pn, a, b ~ MZ ⫽ R! ⫽ 0
if b ⫽ 0 and lim nr` Pn, a, b ~ MZ ⫽ R! ⫽ F~c! ⫺ F~⫺c! ⬍ 1 if b ⫽ 0, reflecting
Downloaded from https://www.cambridge.org/core. Harvard University, on 07 Feb 2020 at 21:32:08, subject to the Cambridge Core terms of
use, available at https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0266466605050036

36

HANNES LEEB AND BENEDIKT M. PÖTSCHER

the fact that the model selection procedure is conservative but not consistent+
As in the case of consistent model selection procedures, this convergence is
not uniform w+r+t+ b+ In contrast to consistent model selection procedures ~cf+
Proposition A+1 in Appendix A!, the behavior under sample-size-dependent
parameters ~an , bn ! is quite simple: If M n bn 0sb r g, 6g6 ⬍ `, then
lim nr` Pn, an , bn ~ MZ ⫽ R! ⫽ F~c ⫺ g! ⫺ F~⫺c ⫺ g!+ ~If M n 6 bn 60sb r `,
then the limit is zero; i+e+, the asymptotic behavior is identical to the asymptotic
behavior under fixed b ⫽ 0+! In particular, the asymptotic analysis confirms what
we already know from the finite-sample analysis, namely, that the probability
of erroneously selecting the restricted model can be substantial, namely, if 6g6
is small+ However, in contrast to consistent model selection procedures, this probability does not converge to unity as sample size increases+ It is also interesting
to note that deviations from the restricted model such as b ⫽ zsb cn 0M n with
6z6 ⬍ 1 and cn r `, cn 0M n r 0, that can not be detected by a consistent
model selection procedure using cutoff point cn ~cf+ Proposition A+1 and note
14 in the notes section! can be detected with probability approaching unity by a
conservative procedure using a fixed cutoff point c+ Consequently and not surprisingly, conservative model selection procedures are more powerful than consistent model selection procedures in the sense that they are less likely to
erroneously select an incorrect model for large sample sizes+ ~Needless to say
this advantage of the conservative procedure is paid for by a larger probability
of selecting an overparameterized model+!
Turning to the post-model-selection estimator aJ itself, it is obvious that now
conditions ~4! and ~5! are no longer satisfied; 22 as a consequence, and in contrast to the case of consistent model selection procedures, the pointwise asymptotic distribution now captures some of the effects of model selection and no
longer coincides with the usual asymptotic distribution that applies in the absence
of model selection+ This can easily be seen from ~2!: Whereas in the case of
consistent model selection procedures, regardless of the value of b, only one of
the two terms in ~2! survives asymptotically and the corresponding conditioning event becomes a set of probability one asymptotically and hence has no
effect, for conservative procedures both terms do not vanish in the limit if b ⫽ 0+
Hence, the pointwise asymptotic limit captures some of the effects of the model
selection step, at least in the case when the restricted model is correct+ ~In that
sense the asymptotic framework that views a given model selection procedure
as embedded in a sequence of conservative procedures has some advantage
over the framework considered in Section 2+1+! More precisely, the pointwise
⫺1
f~u0sa,` ! if
asymptotic distribution of M n ~ aJ ⫺ a! has a density given by sa,`
b ⫽ 0 and given by
⫺1
~1 ⫺ r`2 !⫺102 f~u~1 ⫺ r`2 !⫺1020sa,` !D~0, c!
sa,`

冋 冉M

⫺1
1⫺D
⫹ sa,`

r` u0sa,`
1 ⫺ r`2

c
,

M1 ⫺ r

2
`

冊册

f~u0sa,` !

(13)

Downloaded from https://www.cambridge.org/core. Harvard University, on 07 Feb 2020 at 21:32:08, subject to the Cambridge Core terms of
use, available at https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0266466605050036

MODEL SELECTION AND INFERENCE

37

if b ⫽ 0+ Note that ~13! bears some resemblance to the finite-sample distribution ~6!+ However, the pointwise asymptotic distribution does not capture all
the effects present in the finite-sample distribution, especially if b ⫽ 0; in particular, the convergence is not uniform w+r+t+ b ~except in trivial cases such
as r` ⫽ 0!; cf+ Corollary 5+5 in Leeb and Pötscher ~2003a!, Remark 6+6 in
Leeb and Pötscher ~2003b!, and note 16+ A much better approximation, capturing all the essential features of the finite-sample distribution, is obtained by
the asymptotic distribution under sample-size dependent parameters ~an , bn !
with M n bn 0sb r g, 6g6 ⬍ `: This asymptotic distribution has a density of the
form
⫺1
~1 ⫺ r`2 !⫺102 f~u~1 ⫺ r`2 !⫺1020sa,` ⫹ r` ~1 ⫺ r`2 !⫺102 g!D~g, c!
sa,`

冋 冉

⫺1
1⫺D
⫹ sa,`

g ⫹ r` u0sa,`

M1 ⫺

r`2

c
,

M1 ⫺ r

2
`

冊册

f~u0sa !+

(14)

This follows either as a special case of Proposition 5+1 of Leeb ~2003b! ~cf+
also Leeb and Pötscher, 2003a, Proposition 5+3 and Corollary 5+4! or can be
gleaned directly from ~6!+ ~If M n 6 bn 60sb r `, then the limit has the form
⫺1
f~u0sa,` !+! 23 Observe that ~14! follows the same formula as the finitesa,`
sample density ~6!, except that sa and r have been replaced by their respective
limits sa,` and r` and that M n b0sb has been replaced by g+
Consider next the asymptotic behavior of the actual coverage probability of
the “naive” confidence interval I given by ~7! and ~8!+ The pointwise limit of
the actual coverage probability has been studied in Pötscher ~1991, Sect+ 3+3!+
In contrast to the case of consistent model selection procedures, it turns out to
be less than the nominal coverage probability in case the restricted model is
correct+ However, this pointwise asymptotic result, although hinting at the problem, still gives a much too optimistic picture when compared with the actual
finite-sample coverage probability+ The large-sample minimal coverage probability of the “naive” confidence interval has been studied in Kabaila and Leeb
~2004!+ Although it does not equal zero as in the case of consistent model selection procedures, it turns out to be often much smaller than the nominal coverage probability 1 ⫺ h ~as in Figure 3!; see Kabaila and Leeb ~2004! for more
details+
J Under the
We finally turn to the bias and mean-squared error of M n a+
sequence of parameters ~an , bn ! with M n bn 0sb r g, 6g6 ⬍ `, it is readily
seen from ~11! that the bias converges to
⫺ r` sa,` @gD~g, c! ⫺ f~g ⫺ c! ⫹ f~g ⫹ c!# +
The pointwise asymptotics corresponds to the cases g ⫽ 0 and g ⫽ 6` ~with
the convention that 6`D~6`, c! ⫽ 0 and f~6`! ⫽ 0! and results in a zero
limiting bias+ However, the maximal bias can be quite substantial if b is of the
order O~10M n !+ In contrast to the case of consistent model selection proceDownloaded from https://www.cambridge.org/core. Harvard University, on 07 Feb 2020 at 21:32:08, subject to the Cambridge Core terms of
use, available at https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0266466605050036

38

HANNES LEEB AND BENEDIKT M. PÖTSCHER

dures, the maximal bias does not go to infinity ~in absolute value! as n r `
but remains bounded+ ~It is perhaps somewhat ironic—although not surprising—
that consistent model selection procedures that look perfect in a pointwise
asymptotic analysis lead in fact to more heavily distorted post-model-selection
estimators than conservative model selection procedures+! The limiting meansquared error under ~an , bn ! as before is easily seen to be given by
2
2
⫹ sa,`
r`2
sa,`

冋

~c ⫹ g!f~c ⫹ g! ⫹ ~c ⫺ g!f~c ⫺ g!
⫹ ~g 2 ⫺ 1!~F~c ⫺ g! ⫺ F~⫺c ⫺ g!!

册

,

the pointwise asymptotics again corresponding to the cases g ⫽ 0 and g ⫽ 6`
~with the convention that `D~6`, c! ⫽ 0 and 6`f~6`! ⫽ 0!+ In contrast to
the case of consistent model selection procedures, the pointwise limit of MSE
captures some ~but not all! of the effects of model selection and hence no longer coincides with the asymptotic variance of the infeasible “estimator” a~M
[
0 !+
Also, in contrast to the case of consistent model selection procedures, the maximal mean-squared error does not go off to infinity as n r `, but rather it
remains bounded; cf+ also Remark 4+1+
2.3. Can One Estimate the Distribution of
Post-Model-Selection Estimators?
It transpires from the preceding discussion that the finite-sample distributions
~and also the asymptotic distributions! of post-model-selection estimators depend
on unknown parameters ~i+e+, b in the example discussed in this paper!, often
in a complicated fashion+ For inference purposes, e+g+, for the construction of
confidence sets, estimators for these distributions would be desirable+ Consistent estimators for these distributions can typically be constructed quite
easily, e+g+, by suitably replacing unknown parameters in the large-sample limit
distributions by estimators: In the case of the consistent model selection procedure discussed in Section 2+1 a consistent estimator for the finite-sample
distribution of M n ~ aJ ⫺ a! is simply given by the normal distribution
N~0, sa2 ~1 ⫺ r 2 !!, i+e+, by the distribution of M n ~a~R! ⫺ a!, if MZ ⫽ R, and
by N~0, sa2 !, i+e+, by the distribution of M n ~a~U ! ⫺ a!, if MZ ⫽ U+ However,
recall from Section 2+1 that the finite-sample distribution of the post-modelselection estimator is not uniformly close to its pointwise asymptotic limit+ Hence
the suggested estimator ~being identical with the pointwise asymptotic distri2
and r`2 by sa2 and r 2 ! will—although being
bution except for replacing sa,`
consistent—not be close to the finite-sample distribution uniformly in the
unknown parameters, thus providing a rather useless estimator+ In the case of
conservative model selection procedures consistent estimators for the finitesample distribution of the post-model-selection estimator can also be constructed from the pointwise asymptotic distribution by suitably plugging in
estimators for unknown quantities; see Leeb and Pötscher ~2003b, 2004!+ HowDownloaded from https://www.cambridge.org/core. Harvard University, on 07 Feb 2020 at 21:32:08, subject to the Cambridge Core terms of
use, available at https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0266466605050036

MODEL SELECTION AND INFERENCE

39

ever, again these estimators will be quite useless for the same reason: As discussed in Section 2+2, the convergence of the finite-sample distributions to their
~pointwise! large-sample limits is typically not uniform with respect to the underlying parameters, and there is no reason to believe that this nonuniformity will
disappear when unknown parameter values in the large-sample limit are replaced
by estimators+
A natural reaction to the preceding discussion could be to try the bootstrap
or some related resampling procedure such as, e+g+, subsampling+ Consider first
the case of a consistent model selection procedure+ Then, in view of ~4! and
~5!, the bootstrap that resamples from the residuals of the selected model certainly provides a consistent estimator for the finite-sample distribution of the
post-model-selection estimator+ Note that the consistent estimator described in
the preceding paragraph can be viewed as a ~parametric! bootstrap+ The discussion in the previous paragraph then, however, suggests that such estimators based
on the bootstrap ~or on other resampling procedures such as subsampling!,
despite being consistent, will be plagued by the nonuniformity issues discussed
earlier+ Next consider the case where the model selection procedure is conservative ~but not consistent!+ Then the bootstrap will typically not even provide
consistent estimators for the finite-sample distribution of the post-model-selection
estimator, as the bootstrap can be shown to stay random in the limit ~Kulperger
and Ahmed, 1992; Knight, 1999, Example 3!: 24 Basically the only way one can
coerce the bootstrap into delivering a consistent estimator is to resample from
a model that has been selected by an auxiliary consistent model selection procedure+ ~The construction of consistent estimators in Leeb and Pötscher, 2003b,
2004, alluded to previously basically follows this route+! In contrast, subsampling will typically deliver consistent estimators+ However, the discussion in
the preceding paragraph strongly suggests that any such estimator will again
suffer from the nonuniformity defect+
A natural question then is how estimators ~not necessarily derived from the
asymptotic distributions or from resampling considerations! can be found that
do not suffer from the nonuniformity defect+ In other words, we are asking for
estimators GZ n, a, b of the finite-sample c+d+f+ Gn, a, b of M n ~ aJ ⫺ a! that are uniformly consistent, i+e+, that satisfy for every t 僆 R and every d ⬎ 0
sup Pn, a, b ~6 GZ n, a, b ~t ! ⫺ Gn, a, b ~t !6 ⬎ d!

nr`

&

0+

a, b

However, it turns out that no estimator GZ n, a, b can satisfy this requirement ~except
possibly in the trivial case where r` ⫽ 0!+ For conservative model selection
procedures this is proved in Leeb and Pötscher ~2003a, 2004! in a more general
framework, including model selection by AIC from a quite arbitrary collection
of linear regression models+ For a consistent model selection procedure such a
result is given in Leeb and Pötscher ~2002, Sect+ 2+3!+ In fact, these papers
show that the situation is even more dramatic: For every consistent estimator
GZ n, a, b of Gn, a, b even
Downloaded from https://www.cambridge.org/core. Harvard University, on 07 Feb 2020 at 21:32:08, subject to the Cambridge Core terms of
use, available at https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0266466605050036

40

HANNES LEEB AND BENEDIKT M. PÖTSCHER

sup Pn, a, b ~6 GZ n, a, b ~t ! ⫺ Gn, a, b ~t !6 ⬎ d!

nr`

&

1

a, b

holds for suitable d ⬎ 0, and this result is even local in the sense that it holds
also if the supremum in the preceding display extends only over suitable balls
that shrink at rate 10M n + 25 ~These “impossibility” results hold for randomized
estimators of Gn, a, b also+!
The preceding “impossibility” results establish in particular that any proposal to estimate the distribution of post-model-selection estimators by whatever resampling procedure ~bootstrap, subsampling, etc+! is doomed as any such
estimator is necessarily plagued by the nonuniformity defect ~if it is consistent
at all!+ On a more general level, an implication of the preceding results is that
assessing the variability of post-model-selection estimators ~e+g+, the construction of valid confidence intervals post model selection! is a harder problem
than perhaps expected+26
3. RELATED PROCEDURES: SHRINKAGE-TYPE ESTIMATORS
AND PENALIZED LEAST-SQUARES
Post-model-selection estimators can be viewed as a discontinuous form of shrinkage estimators+ In this section we briefly discuss the relationship between postmodel-selection estimators and shrinkage-type estimators and look at the
distributional properties of such estimators+ Although estimators such as the
James–Stein estimator or ridge estimators have a long tradition in econometrics and statistics, a number of shrinkage-type estimators such as the Lasso estimator, the Bridge estimator, and the SCAD estimator are of more recent vintage+
In the context of a linear regression model Y ⫽ Xu ⫹ « many of these estimators can be cast in the form of a penalized least-squares estimator: Let uZ be the
estimator that is obtained by minimizing the penalized least-squares criterion
n

k

t⫽1

j⫽1

( ~ yt ⫺ x t+ u! 2 ⫹ l n ( 6uj 6 q,

(15)

where x t+ denotes the tth row and k the number of columns of X+ This is the
class of Bridge estimators introduced by Frank and Friedman ~1993!, the case
q ⫽ 2 corresponding to the ridge estimator+ The member of this class obtained
by setting q ⫽ 1 has been referred to as a Lasso-type estimator by Knight and
Fu ~2000!, because it is closely related to the Lasso of Tibshirani ~1996!+ Knight
and Fu ~2000! also note that in the context of wavelet regression minimizing
~15! with q ⫽ 1 is known as “basis pursuit,” cf+ Chen, Donoho, and Saunders
~1998!+ In fact, in the case of diagonal X ' X the Lasso-type estimator reduces
to soft-thresholding of the coordinates of the least-squares estimator+ ~We
note that in this case hard-thresholding, which obviously is a model selection
procedure, can also be represented as a penalized least-squares estimator+!
Downloaded from https://www.cambridge.org/core. Harvard University, on 07 Feb 2020 at 21:32:08, subject to the Cambridge Core terms of
use, available at https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0266466605050036

MODEL SELECTION AND INFERENCE

41

The SCAD estimator introduced by Fan and Li ~2001! is also a penalized
least-squares estimator but uses a different penalty term+ It is given as the minimizer of
n

k

t⫽1

j⫽1

( ~ yt ⫺ x t+ u! 2 ⫹ ( pln ~uj !

with a specific choice of pl n that we do not reproduce here+
The asymptotic distributional properties of Bridge estimators have been studied in Knight and Fu ~2000!+ Under appropriate conditions on q and on the
regularization parameter l n , the asymptotic distribution shows features similar
to the asymptotic distribution of post-model-selection estimators based on a
conservative model selection procedure ~e+g+, bimodality!+ Under other conditions on q and l n , the Bridge estimator acts more like a post-model-selection
estimator based on a consistent procedure+ In particular, such a Bridge estimator will estimate zero components of the true u exactly as zero with probability
approaching unity+ It hence satisfies an “oracle” property+ This is also true for
the SCAD estimator of Fan and Li ~2001!+ In view of the discussion in Section 2+1 and the lessons learned from Hodges’ estimator, one should, however,
not read too much into this property as it can give a highly misleading impression of the properties of these estimators in finite samples+27
Another similarity with post-model-selection estimators is the fact that the
distribution function or the risk of shrinkage-type estimators often can not be
estimated uniformly consistently+ See Leeb and Pötscher ~2002! for more on
this subject+
4. REMARKS
Remark 4+1+ In this remark we collect some decision-theoretic facts about postmodel-selection estimators+ These results could be taken as a starting point for
a discussion of whether or not model selection ~from submodels of an overall
model of fixed finite dimension! can be justified from a decision-theoretic point
of view+
1+ Sometimes model selection is motivated by arguing that allowing for the
selection of models more parsimonious than the overall model would lead
to a gain in the precision of the estimate+ However, this argument does
not hold up to closer scrunity+ For example, it is well known in the standard linear regression model Y ⫽ Xu ⫹ « that the mean-squared error of
any given pretest estimator for u exceeds the mean-squared error of the
least-squares estimator ~X ' X !⫺1 X ' Y on parts of the parameter space ~Judge
and Bock, 1978; Judge and Yancey, 1986; Magnus, 1999!+ Hence, pretesting does not lead to a global gain ~i+e+, a gain that holds over the entire
parameter space! in mean-squared error over the least-squares estimator
Downloaded from https://www.cambridge.org/core. Harvard University, on 07 Feb 2020 at 21:32:08, subject to the Cambridge Core terms of
use, available at https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0266466605050036

42

2+

3+

4+

5+

HANNES LEEB AND BENEDIKT M. PÖTSCHER

obtained from the overall model+ Cf+ also the discussion of the meansquared error in Sections 2+1 and 2+2+
For Hodges’ estimator and also for the post-model-selection estimator based
on a consistent model selection procedure considered in Section 2+1 the
maximal ~scaled! mean-squared error increases without bound as n r `,
whereas the maximal ~scaled! mean-squared error of the least-squares estimator in the overall model remains bounded+ Cf+ Section 2+1+
The unboundedness of the maximal ~scaled! mean-squared error is true
for post-model-selection estimators based on consistent procedures more
generally+ Yang ~2003! proves such a result in a normal linear regression
framework for some sort of maximal predictive risk+ A proof for the maximal @scaled# mean-squared error ~in fact for the maximal @scaled# absolute bias! as considered in the present paper is given in Appendix C+28 In
contrast, the maximal ~scaled! mean-squared error of a post-model-selection
estimator based on a conservative ~but inconsistent! procedure typically
stays bounded as sample size increases ~although it can substantially exceed
the @scaled# mean-squared error of the least-squares estimator in the
unrestricted model!+29
Kempthorne ~1984! has shown that in a normal linear regression model
no post-model-selection estimator uD ~including the trivial post-modelselection estimators that are based on a fixed model! dominates any other
post-model-selection estimator in terms of mean-squared error of Xu+D
It is well known that in a normal linear regression model Y ⫽ Xu ⫹ «
with more than two regressors the least-squares estimator ~X ' X !⫺1 X ' Y
is inadmissible as it is dominated by the Stein estimator ~and its admissible versions!+ Similarly, every pretest estimator is inadmissible as shown
by Sclove, Morris, and Radhakrishnan ~1972!+ See Judge and Yancey
~1986, p+ 33! for more information+

Remark 4+2+ That in the case of two competing models minimum AIC ~and
also BIC! reduces to a likelihood ratio test has been noted already by Söderström ~1977! and has been rediscovered numerous times+ Even in the general
case there is a closer connection between model selection based on multiple
testing procedures and model selection procedures based on information criteria such as AIC or BIC than is often recognized+ For example, the minimum
AIC or BIC method can be reexpressed as the search for that model that is not
rejected in pairwise comparisons against any other competing model, where
rejection occurs if the likelihood-ratio statistic ~corresponding to the pairwise
comparison! exceeds a critical value that is determined by the model dimensions and sample size; see Pötscher ~1991, Sect+ 4, Remark ~ii!! for more
information+
Remark 4+3+ The idea that hypothesis tests give rise to consistent ~model!
selection procedures if the significance levels of the tests approach zero at an
appropriate rate as sample size increases has already been used in Pötscher ~1981,
Downloaded from https://www.cambridge.org/core. Harvard University, on 07 Feb 2020 at 21:32:08, subject to the Cambridge Core terms of
use, available at https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0266466605050036

MODEL SELECTION AND INFERENCE

43

1983! in the context of ARMA models and in Bauer, Pötscher, and Hackl ~1988!
in the context of general ~semi!parametric models+ It has since been rediscovered numerous times, e+g+, by Andrews ~1986!, Corradi ~1999!, Altissimo and
Corradi ~2002, 2003!, and Bunea, Niu, and Wegkamp ~2003!, to mention a few+
@The editor has informed us that in the context of a linear regression model the
same idea appears also in a 1981 manuscript by Sargan, which was eventually
published as Sargan, 2001+#
Remark 4+4+
1+ If an ⫽ a ⫹ d0M n and bn ⫽ b ⫹ g0M n then Pn, an , bn is contiguous w+r+t+
Pn, a, b ~and this is more generally true in any sufficiently regular parametric model!+ If MX is an arbitrary consistent model selection procedure, i+e+,
satisfies Pn, a, b ~ MX ⫽ M0 ! r 1 as n r `, where M0 ⫽ M0 ~a, b! is the
most parsimonious true model corresponding to ~a, b !, then also
Pn, an , bn ~ MX ⫽ M0 ! r 1 as n r ` by contiguity, and hence the postmodel-selection estimator based on MX coincides with the restricted estimator with Pn, an , bn probability converging to unity if b ⫽ 0+ Hence, any
consistent model selection procedure is insensitive to deviations at least
of the order 10M n + It is obvious that this argument immediately carries
over to any class of sufficiently regular parametric models ~except if the
competing models are “well separated”!+
2+ As a consequence of the preceding contiguity argument, in general no
model selector can be uniformly consistent for the most parsimonious true
model+ Cf+ also Corollary 2+3 in Pötscher ~2002! and Corollary 3+3 in Leeb
and Pötscher ~2002! and observe that the estimand ~i+e+, the most parsimonious true model! depends discontinuously on the probability measure
underlying the data generating process ~except in the case where the competing models are “well separated”!+
Remark 4+5+ Suppose that in the context of model ~1! the parameter of interest is now not a but more generally a linear combination d1 a ⫹ d2 b, which is
estimated by d1 aJ ⫹ d2 b,D where aJ is the post-model-selection estimator as defined
in Section 2 and the post-model-selection estimator bD is defined similarly, i+e+,
bD ⫽ b~
Z MZ !+ An important example is the case where the quantity of interest is a
linear predictor+ Then appropriate analogues to the results discussed in the present
paper apply, where the rôle of r is now played by the correlation coefficient
[
! ⫹ d2 b~U
Z ! and b~U
Z !+ See Leeb ~2003a, 2003b! and Leeb and
between d1 a~U
Pötscher ~2003b, 2004! for a discussion in a more general framework+
Remark 4+6+ We have excluded the special case r` ⫽ 0 in parts of the discussion of consistent model selection procedures in Section 2+1 for the sake of
simplicity+ It is, however, included in the theoretical results presented in Appendix A+ In the following discussion we comment on this case+
1+ If r ⫽ 0 then it is easy to see that all effects from model selection disappear in the finite-sample formulas in Section 2+1+ This is not surprising
Downloaded from https://www.cambridge.org/core. Harvard University, on 07 Feb 2020 at 21:32:08, subject to the Cambridge Core terms of
use, available at https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0266466605050036

44

HANNES LEEB AND BENEDIKT M. PÖTSCHER

because r ⫽ 0 implies that the design matrix has orthogonal columns and
hence the post-model-selection estimator aJ coincides with the restricted
and also with the unrestricted least-squares estimator for a+
2+ If only r` ⫽ 0 ~i+e+, the columns of the design matrix are only asymptotically orthogonal!, then the effects of model selection need not disappear
from the asymptotic formulas; cf+ Appendix A+ However, inspection of
the results in Appendix A shows that these effects will disappear asymptotically if r converges to r` ⫽ 0 sufficiently fast ~essentially faster than
10c!+ ~In contrast, in the case of conservative model selection procedures
the condition r` ⫽ 0 suffices to make all effects from model selection
disappear from the asymptotic formulas; cf+ Section 2+2+!
3+ As noted previously, in the case of an orthogonal design ~i+e+, r ⫽ 0! all
effects from model selection on the distributional properties of aJ vanish+
However, even for orthogonal designs, effects from model selection will
nevertheless typically be present as soon as a linear combination
d1 a ⫹ d2 b other than a represents the parameter of interest because then
the correlation coefficient between d1 a~U
[
! ⫹ d2 b~U
Z ! and b~U
Z ! rather
than r governs the effects from model selection on the post-model-selection
estimator; cf+ Remark 4+5+
5. CONCLUSION
The distributional properties of post-model-selection estimators are quite intricate and are not properly captured by the usual pointwise large-sample analysis+ The reason is lack of uniformity in the convergence of the finite-sample
distributions and of associated quantities such as the bias or mean-squared error+
Although it has long been known that uniformity ~at least locally! w+r+t+ the
parameters is an important issue in asymptotic analysis, this lesson has often
been forgotten in the daily practice of econometric and statistical theory where
we are often content to prove pointwise asymptotic results ~i+e+, results that
hold for each fixed true parameter value!+ This amnesia—and the resulting
practice—fortunately has no dramatic consequences as long as only sufficiently “regular” estimators in sufficiently “regular” models are considered+30
However, because post-model-selection estimators are quite “irregular,” the
uniformity issues surface here with a vengeance+ Hajek’s ~1971, p+ 153! warning,
Especially misinformative can be those limit results that are not uniform+ Then
the limit may exhibit some features that are not even approximately true for any
finite n + + +

thus takes on particular relevance in the context of model selection: While a
pointwise asymptotic analysis paints a very misleading picture of the properties of post-model-selection estimators, an asymptotic analysis based on the fiction of a true parameter that depends on sample size provides highly accurate
insights into the finite-sample properties of such estimators+
Downloaded from https://www.cambridge.org/core. Harvard University, on 07 Feb 2020 at 21:32:08, subject to the Cambridge Core terms of
use, available at https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0266466605050036

MODEL SELECTION AND INFERENCE

45

The distinction between consistent and conservative model selection procedures is an artificial one as discussed in Section 2 and is rather a property of
the embedding framework than of the model selection procedure+ Viewing a
model selection procedure as consistent results in a completely misleading
pointwise asymptotic analysis that does not capture any of the effects of model
selection that are present in finite samples+ Viewing a model selection procedure as conservative ~but inconsistent! results in a pointwise asymptotic
analysis that captures some of the effects of model selection, although still missing others+
We would like to stress that the claim that the use of a consistent model
selection procedure allows one to act as if the true model were known in advance
is without any substance+ In fact, any asymptotic consideration based on the
so-called oracle property should not be trusted+ ~Somewhat ironically, consistent model selection procedures that seem not to affect the asymptotic distribution in a pointwise analysis at all exhibit stronger effects @e+g+, larger maximal
absolute bias or larger maximal mean-squared error# as a result of model selection in a “uniform” analysis when compared with conservative procedures+! 31
Similar warnings apply more generally to procedures that consistently choose
from a finite set of alternatives ~e+g+, procedures that consistently decide between
I ~0! and I ~1! or consistently select the number of structural breaks, etc+!+ Also,
the claim that one can come up with a model selection procedure that can always
detect the most parsimonious true model with high probability is unwarranted:
However the model selection procedure is constructed, the misclassification error
is always there and will be substantial for certain values of the true parameter,
regardless of how large sample size is+
As shown in Section 2+3, accurate estimation of the distribution of post-modelselection estimators is intrinsically a difficult problem+ In particular, it is typically impossible to estimate these distributions uniformly consistently+ Similar
results apply to certain shrinkage-type estimators as discussed in Section 3+
Although the discussion in this paper is set in the framework of a simple
linear regression model, the issues discussed are obviously relevant much more
generally+ Results on post-model-selection estimators for nonlinear models
and0or dependent data are given in Sen ~1979!, Pötscher ~1991!, Hjort and
Claeskens ~2003!, and Nickl ~2003!+
We stress that the discussion in this paper should neither be construed as a
criticism nor as an endorsement of model selection ~be it consistent or conservative!+ In this paper we take no position on whether or not model selection is
a sensible strategy+ Of course, this is an important issue, but it is not the one
we address here+ A starting point for such a discussion could certainly be the
results mentioned in Remark 4+1+
Although there is now a substantial body of literature on distributional properties of post-model-selection estimators, a proper theory of inference post model
selection is only slowly emerging and is currently the subject of intensive
research+ We hope to be able to report on this elsewhere+
Downloaded from https://www.cambridge.org/core. Harvard University, on 07 Feb 2020 at 21:32:08, subject to the Cambridge Core terms of
use, available at https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0266466605050036

46

HANNES LEEB AND BENEDIKT M. PÖTSCHER

NOTES
1+ We assume throughout that at least one of the competing models is capable of correctly
describing the data generating process+ We do not touch upon the important question of model
selection in the context of fitting only approximate models+
2+ The pretest literature as summarized in Judge and Bock ~1978! or Giles and Giles ~1993!
concentrates exclusively on second moment properties of pretest estimators and does not provide
distributional results+
3+ Some of the issues we raise here may not apply in the ~relatively trivial! case where one
selects between “well-separated” model classes, i+e+, model classes that have positive minimum
distance, e+g+, in the Kullback–Leibler sense+
4+ For example, Bunea ~2004!, Dufour, Pelletier, and Renault ~2003, Sect+ 7!; Fan and Li ~2001!,
Hall and Peixe ~2003, Theorem 3!, Hidalgo ~2002, Theorem 3+4!, and Lütkepohl ~1990, p+ 120! to
mention a few+
5+ With hindsight the second author regrets having included Lemma 1 in Pötscher ~1991! at
all, as this lemma seems to have contributed to popularizing the aforementioned unwarranted conclusion in the literature+ Given that this lemma was included, he wishes at least that he had been
more guarded in his wording in the discussion of this lemma and that he had issued a stronger
warning against an uncritical use of it+
6+ That is, a procedure that asymptotically selects only correct models but possibly overparameterized ones+
7+ Nothing substantial changes because of this convenience assumption+ The entire discussion
that follows can also be given for the unknown s 2 case+ See Leeb and Pötscher ~2003a! and Leeb
~2003a, 2003b!+
8+ In fact, it would be more precise to talk about consistent ~or conservative! sequences of
model selection procedures+
9+ This property of consistent model selection procedures has already been observed by Hannan and Quinn ~1979, p+ 191!+ It has since been rediscovered several times in special instances; cf+
Ensor and Newton ~1988, Theorem 2+1!; Bunea ~2004, Sect+ 4!+
10+ Hodges’ estimator ~with a ⫽ 0 in the notation of Lehmann and Casella, 1998! is a postmodel-selection estimator based on a model selection procedure that consistently chooses between
an N~0,1! and an N~u,1! distribution+
11+ Exceptions are Hosoya ~1984!, Shibata ~1986!, Pötscher ~1991!, and Kabaila ~1995, 1996!,
who explicitly note this problem+
12+ For a detailed treatment of the finite-sample properties of post-model-selection estimators
in linear regression models see Leeb and Pötscher ~2003a!, Leeb ~2003a, 2003b!+
13+ Slightly more general conditions under which this is true are given in Proposition A+1 in
Appendix A+
14+ It can be debated whether the b’s giving rise to this phenomenon are justifiably viewed
as “small”: The phenomenon can, e+g+, arise if b ⫽ 0 satisfies b ⫽ zsb c0M n with 6z6 ⬍ 1 ~cf+
Proposition A+1 in Appendix A!+ Although such sequences of b’s converge to zero by the assumption c ⫽ o~M n ! maintained in Section 2+1, the “nonzeroness” of any such b can be detected with
probability approaching unity by a standard test with fixed significance level or equivalently, with
fixed cutoff point, and thus such b’s could justifiably be classified as “far” from zero+ ~In more
mathematical terms, Pn, a, b is not contiguous w+r+t+ Pn, a,0 for such b’s+! By the way, this also
nicely illustrates that the consistent model selection procedure is ~not surprisingly! less powerful
in detecting b ⫽ 0 compared with the conservative procedure with a fixed value of c, the reason
being that the consistent procedure has to let the significance level of the test approach zero to
asymptotically avoid choosing a model that is too large+ ~This loss of power is not specific to the
consistent model selection procedure discussed here but is typical for consistent model selection
procedures in general+!
[
⫺ a! given the
15+ In light of ~2!, the first term is actually the conditional density of M n ~ a~R!
event that the pretest does not reject multiplied by the probability of this event+ Because the test

Downloaded from https://www.cambridge.org/core. Harvard University, on 07 Feb 2020 at 21:32:08, subject to the Cambridge Core terms of
use, available at https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0266466605050036

MODEL SELECTION AND INFERENCE

47

statistic is independent of a~R!
[
~Leeb and Pötscher, 2003a, Proposition 3+1!, this conditional density reduces to the unconditional one+ Similarly, the second term is the conditional density of
M n ~ a~U
[
! ⫺ a! given that the pretest rejects multiplied by the probability of this event+ Because
the test statistic is typically correlated with a~U
[
!, the conditional density is not normal, which is
reflected by the “deformation” factor+
16+ A quick alternative argument showing that the convergence of the finite-sample c+d+f+s of
post-model-selection estimators is typically not uniform runs as follows: Equip the space of c+d+f+s
with a suitable metric ~e+g+, a metric that generates the topology of weak convergence!+ Observe
that the finite-sample c+d+f+s typically depend continuously on the underlying parameters, whereas
their ~pointwise! limits typically are discontinuous in the underlying parameters+ This shows that
the convergence can not be uniform+
17+ Although this fits in nicely with ~5!, it is not a direct consequence of ~5!+ The crucial point
here is that Pn, a, b ~ MZ ⫽ R! ⫽ D~M n b0sb , c! converges to zero exponentially fast for fixed b ⫽ 0;
see, e+g+, Lemma B+1 in Leeb and Pötscher ~2003a!+
18+ Although this is again in line with ~5! it is again not a direct consequence of ~5! but follows
from the exponential decay of D~M n b0sb , c! for fixed b ⫽ 0; cf+ note 17+ Furthermore, the fact
that the pointwise limit of the MSE coincides with the asymptotic variance of the infeasible “estimator” a~M
[
0 ! is not particular to the consistent model selection procedure discussed here+ It is true
for consistent model selection procedures in general, provided the probability of selecting an incorrect model converges to zero sufficiently fast, which is typically the case; see Nishii ~1984! for
some results in this direction+ Of course, being only pointwise limit results, these results are subject to the criticism put forward in the present paper+
19+ We could allow more generally for a sample-size-dependent c that, e+g+, converges to a
positive real number+ See Leeb and Pötscher ~2003a, Remark 6+2!+
20+ For a detailed treatment of the finite-sample and asymptotic properties of post-modelselection estimators based on a conservative model selection procedure see Pötscher ~1991!, Leeb
and Pötscher ~2003a!, and Leeb ~2003a, 2003b!+
21+ Similar as for consistent model selection procedures in fact all accumulation points of the
model selection probabilities, the finite-sample distributions, the bias, and the mean-squared error
can be characterized by a subsequence argument similar to Remark A+8; cf+ also Leeb and Pötscher
~2003a, Remark 4+4~i!!, and Leeb ~2003b, Remark 5+5!+
22+ Nevertheless, it is easy to see that aJ is consistent ~cf+ Pötscher, 1991, Lemma 2! and, in
fact, is uniformly consistent; see Proposition B+1 in Appendix B+
23+ Here the convergence of the finite-sample distribution to the asymptotic distribution is w+r+t+
total variation distance+
24+ Kilian ~1998! claims the validity of a bootstrap procedure in the context of autoregressive
models that is based on a conservative model selection procedure+ Hansen ~2003! makes a similar
claim for a stationary bootstrap procedure in the context of a conservative model selection procedure+ The preceding discussion intimates that both these claims are at least unsubstantiated+
25+ Similar “impossibility” results apply to estimators of the model selection probabilities; see
Leeb and Pötscher ~2004! in the case of conservative procedures; for consistent procedures this
argument can be easily adapted by making use of Proposition A+1+
26+ The confidence interval suggested in Hjort and Claeskens ~2003, p+ 886! does not provide a
solution to this problem+ As pointed out in Remark 3+5 of Kabaila and Leeb ~2004!, the proposed
interval ~asymptotically! coincides with the classical confidence interval obtained from the overall
model+
27+ Although the James–Stein estimator is known to dominate the least-squares estimator in a
normal linear regression model with more than two regressors, we are not aware of any similar
result for the other shrinkage-type estimators mentioned earlier+ ~In fact, for some it is known that
they do not dominate the least-squares estimator+!
28+ This proof seems to be somewhat simpler than Yang’s proof and has the advantage of also
covering nonnormally distributed errors+ It should easily extend to Yang’s framework, but we do
not pursue this here+

Downloaded from https://www.cambridge.org/core. Harvard University, on 07 Feb 2020 at 21:32:08, subject to the Cambridge Core terms of
use, available at https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0266466605050036

48

HANNES LEEB AND BENEDIKT M. PÖTSCHER

29+ The fact that the maximal ~scaled! mean-squared error remains bounded for conservative
procedures is sometimes billed as “minimax rate optimality” of the procedure ~see, e+g+, Yang,
2003, and the references given there!+ Given that this “optimality” property is typically shared by
any post-model-selection estimator based on a conservative procedure ~including the procedure
that always selects the overall model!, this property does not seem to carry much weight here+
30+ The reason is that the asymptotic properties of such estimators typically are then in fact
“automatically” uniform, at least locally+
31+ This is not surprising+ For the particular model selection procedure considered here it is
obvious that a larger value of the cutoff point c gives more “weight” to the restricted model, which
results in a larger maximal absolute bias+

REFERENCES
Ahmed, S+E+ & A+K+ Basu ~2000! Least squares, preliminary test and Stein-type estimation in general vector AR~ p! models+ Statistica Neerlandica 54, 47– 66+
Altissimo, F+ & V+ Corradi ~2002! Bounds for inference with nuisance parameters present only
under the alternative+ Econometrics Journal 5, 494–519+
Altissimo, F+ & V+ Corradi ~2003! Strong rules for detecting the numbers of breaks in a time series+
Journal of Econometrics 117, 207–244+
Andrews, D+W+K+ ~1986! Complete consistency: A testing analogue of estimator consistency+ Review
of Economic Studies 53, 263–269+
Bauer, P+, B+M+ Pötscher, & P+ Hackl ~1988! Model selection by multiple test procedures+ Statistics
19, 39– 44+
Bunea, F+ ~2004! Consistent covariate selection and post model selection inference in semiparametric regression+ Annals of Statistics 32, 898–927+
Bunea, F+, X+ Niu, & M+H+ Wegkamp ~2003! The Consistency of the FDR Estimator+ Working
paper, Department of Statistics, Florida State University at Tallahassee+
Chen, S+S+, D+L+ Donoho, & M+A+ Saunders ~1998! Atomic decomposition by basis pursuit+ SIAM
Journal on Scientific Computing 20, 33– 61+
Corradi, V+ ~1999! Deciding between I ~0! and I ~1! via FLIL-based bounds+ Econometric Theory
15, 643– 663+
Danilov, D+ & J+R+ Magnus ~2004! On the harm that ignoring pretesting can cause+ Journal of
Econometrics 122, 27– 46+
Dijkstra, T+K+ & J+H+ Veldkamp ~1988! Data-driven selection of regressors and the bootstrap+ Lecture Notes in Economics and Mathematical Systems 307, 17–38+
Dufour, J+M+, D+ Pelletier, & E+ Renault ~2003! Short run and long run causality in time series:
Inference+ Journal of Econometrics ~forthcoming!+
Dukić, V+M+ & E+A Peña ~2002! Estimation after Model Selection in a Gaussian Model+ Manuscript,
Department of Statistics, University of Chicago+
Ensor, K+B+ & H+J+ Newton ~1988! The effect of order estimation on estimating the peak frequency
of an autoregressive spectral density+ Biometrika 75, 587–589+
Fan, J+ & R+ Li ~2001! Variable selection via nonconcave penalized likelihood and its oracle properties+ Journal of the American Statistical Association 96, 1348–1360+
Frank, I+E+ & J+H+ Friedman ~1993! A statistical view of some chemometrics regression tools ~with
discussion!+ Technometrics 35, 109–148+
Giles, J+A+ & D+E+A+ Giles ~1993! Pre-test estimation and testing in econometrics: Recent developments+ Journal of Economic Surveys 7, 145–197+
Hajek, J+ ~1971! Limiting properties of likelihoods and inference+ In V+P+ Godambe & D+A+ Sprott
~eds+!, Foundations of Statistical Inference: Proceedings of the Symposium on the Foundations
of Statistical Inference, University of Waterloo, Ontario, March 31–April 9, 1970, pp+ 142–159+
Holt, Rinehart and Winston+
Hajek, J+ & Z+ Sidak ~1967! Theory of Rank Tests+ Academic Press+
Hall, A+R+ & F+P+M+ Peixe ~2003! A consistent method for the selection of relevant instruments+
Econometric Reviews 22, 269–287+
Downloaded from https://www.cambridge.org/core. Harvard University, on 07 Feb 2020 at 21:32:08, subject to the Cambridge Core terms of
use, available at https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0266466605050036

MODEL SELECTION AND INFERENCE

49

Hannan, E+J+ & B+G+ Quinn ~1979! The determination of the order of an autoregression+ Journal of
the Royal Statistical Society, Series B 41, 190–195+
Hansen, P+R+ ~2003! Regression Analysis with Many Specifications: A Bootstrap Method for Robust
Inference+ Working paper, Department of Economics, Brown University+
Hidalgo, J+ ~2002! Consistent order selection with strongly dependent data and its application to
efficient estimation+ Journal of Econometrics 110, 213–239+
Hjort, N+L+ & G+ Claeskens ~2003! Frequentist model average estimators+ Journal of the American
Statistical Association 98, 879–899+
Hosoya, Y+ ~1984! Information criteria and tests for time series models+ In O+D+ Anderson ~ed+!,
Time Series Analysis: Theory and Practice, vol+ 5, pp+ 39–52+ North-Holland+
Judge, G+G+ & M+E+ Bock ~1978! The Statistical Implications of Pre-test and Stein-Rule Estimators in Econometrics+ North-Holland+
Judge, G+G+ & T+A+ Yancey ~1986! Improved Methods of Inference in Econometrics+ North-Holland+
Kabaila, P+ ~1995! The effect of model selection on confidence regions and prediction regions+
Econometric Theory 11, 537–549+
Kabaila, P+ ~1996! The evaluation of model selection criteria: Pointwise limits in the parameter
space+ In D+L+ Dowe, K+B+ Korb, & J+J+ Oliver ~eds+!, Information, Statistics and Induction in
Science, pp+ 114–118+ World Scientific+
Kabaila, P+ ~1998! Valid confidence intervals in regression after variable selection+ Econometric
Theory 14, 463– 482+
Kabaila, P+ & H+ Leeb ~2004! On the Large-Sample Minimal Coverage Probability of Confidence
Intervals after Model Selection+ Working paper, Department of Statistics, Yale University+
Kapetanios, G+ ~2001! Incorporating lag order selection uncertainty in parameter inference for AR
models+ Economics Letters 72, 137–144+
Kempthorne, P+J+ ~1984! Admissible variable-selection procedures when fitting regression models
by least squares for prediction+ Biometrika 71, 593–597+
Kilian, L+ ~1998! Accounting for lag order uncertainty in autoregressions: The endogenous lag order
bootstrap algorithm+ Journal of Time Series Analysis 19, 531–548+
Knight, K+ ~1999! Epi-convergence in Distribution and Stochastic Equi-semicontinuity+ Working
paper, Department of Statistics, University of Toronto+
Knight, K+ & W+ Fu ~2000! Asymptotics of lasso-type estimators+ Annals of Statistics 28, 1356–1378+
Koul, H+L+ & W+ Wang ~1984! Local asymptotic normality of randomly censored linear regression
model+ Statistics & Decisions, supplement 1, 17–30+
Kulperger, R+J+ & S+E+ Ahmed ~1992! A bootstrap theorem for a preliminary test estimator+ Communications in Statistics: Theory and Methods 21, 2071–2082+
Leeb, H+ ~2003a! The distribution of a linear predictor after model selection: Conditional finitesample distributions and asymptotic approximations+ Journal of Statistical Planning and Inference ~forthcoming!+
Leeb, H+ ~2003b! The Distribution of a Linear Predictor after Model Selection: Unconditional FiniteSample Distributions and Asymptotic Approximations+ Working paper, Department of Statistics,
University of Vienna+
Leeb, H+ & B+M+ Pötscher ~2002! Performance Limits for Estimators of the Risk or Distribution of
Shrinkage-Type Estimators, and Some General Lower Risk-Bound Results+ Working paper, Department of Statistics, University of Vienna+
Leeb, H+ & B+M+ Pötscher ~2003a! The finite-sample distribution of post-model-selection estimators and uniform versus nonuniform approximations+ Econometric Theory 19, 100–142+
Leeb, H+ & B+M+ Pötscher ~2003b! Can One Estimate the Conditional Distribution of Post-ModelSelection Estimators? Working paper, Department of Statistics, University of Vienna+ ~Also available as Cowles Foundation Discussion paper 1444+!
Leeb, H+ & B+M+ Pötscher ~2004! Can One Estimate the Unconditional Distribution of Post-ModelSelection Estimators? Manuscript, Department of Statistics, Yale University+
Lehmann, E+L+ & G+ Casella ~1998! Theory of Point Estimation+ Springer Texts in Statistics+
Springer-Verlag+
Downloaded from https://www.cambridge.org/core. Harvard University, on 07 Feb 2020 at 21:32:08, subject to the Cambridge Core terms of
use, available at https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0266466605050036

50

HANNES LEEB AND BENEDIKT M. PÖTSCHER

Lütkepohl, H+ ~1990! Asymptotic distributions of impulse response functions and forecast error
variance decompositions of vector autoregressive models+ Review of Economics and Statistics
72, 116–125+
Magnus, J+R+ ~1999! The traditional pretest estimator+ Teoriya Veroyatnost. i Primenen. 44, 401–
418; translation in Theory of Probability and Its Applications 44 ~2000!, 293–308+
Nickl, R+ ~2003! Asymptotic Distribution Theory of Post-Model-Selection Maximum Likelihood Estimators+ Master’s thesis, Department of Statistics, University of Vienna+
Nishii, R+ ~1984! Asymptotic properties of criteria for selection of variables in multiple regression+
Annals of Statistics 12, 758–765+
Phillips, P+C+B+ ~2005! Automated discovery in econometrics+ Econometric Theory ~this issue!+
Pötscher, B+M+ ~1981! Order Estimation in ARMA-Models by Lagrangian Multiplier Tests+ Research
report 5, Department of Econometrics and Operations Research, University of Technology, Vienna+
Pötscher, B+M+ ~1983! Order estimation in ARMA-models by Lagrangian multiplier tests+ Annals
of Statistics 11, 872–885+
Pötscher, B+M+ ~1991! Effects of model selection on inference+ Econometric Theory 7, 163–185+
Pötscher, B+M+ ~1995! Comment on “The effect of model selection on confidence regions and prediction regions+” Econometric Theory 11, 550–559+
Pötscher, B+M+ ~2002! Lower risk bounds and properties of confidence sets for ill-posed estimation
problems with applications to spectral density and persistence estimation, unit roots, and estimation of long memory parameters+ Econometrica 70, 1035–1065+
Pötscher, B+M+ & A+J+ Novak ~1998! The distribution of estimators after model selection: Large
and small sample results+ Journal of Statistical Computation and Simulation 60, 19–56+
Rao, C+R+ & Y+ Wu ~2001! On model selection+ IMS Lecture Notes Monograph Series 38, 1–57+
Sargan, D+J+ ~2001! The choice between sets of regressors+ Econometric Reviews 20, 171–186+
Sclove, S+L+, C+ Morris, & R+ Radhakrishnan ~1972! Non-optimality of preliminary-test estimators
for the mean of a multivariate normal distribution+ Annals of Mathematical Statistics 43,
1481–1490+
Sen, P+K ~1979! Asymptotic properties of maximum likelihood estimators based on conditional
specification+ Annals of Statistics 7, 1019–1033+
Sen, P+K & A+K+M+E+ Saleh ~1987! On preliminary test and shrinkage M-estimation in linear models+ Annals of Statistics 15, 1580–1592+
Shibata, R+ ~1986! Consistency of model selection and parameter estimation+ Journal of Applied
Probability, special volume 23A, 127–141+
Söderström, T+ ~1977! On model structure testing in system identification+ International Journal of
Control 26, 1–18+
Tibshirani, R+ ~1996! Regression shrinkage and selection via the lasso+ Journal of the Royal Statistical Society, Series B 58, 267–288+
Yang, Y+ ~2003! Can the Strengths of AIC and BIC Be Shared? Working paper, Department of
Statistics, Iowa State University+

APPENDIX A:
ASYMPTOTIC RESULTS FOR CONSISTENT
MODEL SELECTION PROCEDURES
In this Appendix we provide propositions that together with Remark A+8, which follows, characterize all possible limits ~more precisely, all accumulation points! of the
model selection probabilities, the finite-sample distribution, the ~scaled! bias, and the
~scaled! mean-squared error of the post-model-selection estimator based on a consistent
model selection procedure under arbitrary sequences of parameters ~an , bn !+ Recall
that these quantities do not depend on a and hence the behavior of a will not enter

Downloaded from https://www.cambridge.org/core. Harvard University, on 07 Feb 2020 at 21:32:08, subject to the Cambridge Core terms of
use, available at https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0266466605050036

MODEL SELECTION AND INFERENCE

51

the results in the sequel+ In the following discussion we consider the linear regression
model ~1! under the assumptions of Section 2+ Furthermore, we assume as in Section 2+1 that c r ` and c0M n r 0 as n r `+
PROPOSITION A+1+ Let ~an , bn ! be an arbitrary sequence of values for the regression parameters in (1).
1. Suppose M n bn 0~sb c! r z, 6z6 ⬍ 1, as n r `. Then lim nr` Pn, an , bn ~ MZ ⫽ R! ⫽ 1.
2. Suppose M n bn 0~sb c! r z, 1 ⬍ 6z6 ⱕ `, as n r `. Then lim nr` Pn, an , bn
~ MZ ⫽ R! ⫽ 0.
3. Suppose M n bn 0~sb c! r 1 and c ⫺ M n bn 0sb r r for some r 僆 R 艛 $⫺`,`% as
n r `. Then lim nr` Pn, an , bn ~ MZ ⫽ R! ⫽ F~r!.
4. Suppose M n bn 0~sb c! r ⫺1 and c ⫹ M n bn 0sb r s for some s 僆 R 艛 $⫺`,`%
as n r `. Then lim nr` Pn, an , bn ~ MZ ⫽ R! ⫽ F~s!.
Proof. From ~3! we have
Pn, an , bn ~ MZ ⫽ R! ⫽ F~c ⫺ M n bn 0sb ! ⫺ F~⫺c ⫺ M n bn 0sb !+
Observe that F~c ⫺ M n bn 0sb ! ⫽ F~c~1 ⫺ M n bn 0~sb c!!! and F~⫺c ⫺ M n bn 0sb ! ⫽
F~c~⫺1 ⫺ M n bn 0~sb c!!!+ The first two claims then follow immediately+ The third
claim follows because then F~c ⫺ M n bn 0sb ! trivially converges to F~r!, whereas
F~⫺c ⫺ M n bn 0sb ! ⫽ F~c~⫺1 ⫺ M n bn 0~sb c!!! converges to zero+ The fourth claim is
proved analogously+
䡲
The next proposition describes the possible limiting behavior of the finite-sample distribution of the post-model-selection estimator, which is somewhat complex+ It turns
out that the limit can, e+g+, be point-mass at ~plus or minus! infinity, or a convex combination of such a point-mass with a “deformed” normal distribution, or a convex
combination of a normal distribution with a “deformed” normal+ Let Gn, a, b ~t ! denote
the cumulative distribution function corresponding to the density gn, a, b ~u! of
M n ~ aJ ⫺ a!+ Also recall that convergence in total variation of a sequence of absolutely
continuous c+d+f+s on the real line is equivalent to convergence of the densities in the
L1 -sense+
PROPOSITION A+2+ Let ~an , bn ! be an arbitrary sequence of values for the regression parameters in (1).
1. Suppose that (i) M n bn 0~sb c! r z, 6z6 ⬍ 1, or that (ii) M n bn 0~sb c! r 1,
c ⫺ M n bn 0sb r `, or that (iii) M n bn 0~sb c! r ⫺1, c ⫹ M n bn 0sb r ` as
n r `. Assume furthermore that M n bn ~r0sb ! r x for some x 僆 R 艛 $⫺`,`%
as n r `. If x ⫽ ⫺`, then Gn, an , bn ~t ! converges to 0 for every t 僆 R; i.e.,
M n ~ aJ ⫺ an ! converges to ` in Pn, an , bn probability. If x ⫽ `, then Gn, an , bn ~t !
converges to 1 for every t 僆 R; i.e., M n ~ aJ ⫺ an ! converges to ⫺` in
2 ⫺102
Pn, an , bn probability. If 6 x6 ⬍ `, then Gn, an , bn ~t ! converges to F~~1 ⫺ r`
!
⫻
~t0sa,` ⫹ x!! in total variation distance; in fact, gn, an , bn ~u! converges to
⫺1
2 ⫺102
2 ⫺102
sa,`
~1 ⫺ r`
!
f~~1 ⫺ r`
!
~u0sa,` ⫹ x!! pointwise and hence in the L1
sense.
2. Suppose that (i) M n bn 0~sb c! r z, 1 ⬍ 6z6 ⱕ `, or that (ii) M n bn 0~sb c! r 1,
c ⫺ M n bn 0sb r ⫺`, or that (iii) M n bn 0~sb c! r ⫺1, c ⫹ M n bn 0sb r ⫺` as
n r `. Then Gn, an , bn ~t ! converges to F~t0sa,` ! in the total variation distance;
Downloaded from https://www.cambridge.org/core. Harvard University, on 07 Feb 2020 at 21:32:08, subject to the Cambridge Core terms of
use, available at https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0266466605050036

52

HANNES LEEB AND BENEDIKT M. PÖTSCHER

⫺1
in fact, gn, an , bn ~u! converges to sa,`
f~u0sa,` ! pointwise and hence in the L1
sense.
3. Suppose that M n bn 0~sb c! r 1, c ⫺ M n bn 0sb r r for some r 僆 R and
M n bn ~r0sb ! r x for some x 僆 R 艛 $⫺`,`% as n r `. If 6 x6 ⫽ `, then
Gn, an , bn ~t ! converges to

F~r!1~ x ⬎ 0! ⫹

冕

t

⫺`

⫺1
2 ⫺102
⫺1
sa,`
f~u0sa,` !F~~1 ⫺ r`
!
~⫺r ⫹ r` sa,`
u!! du

(A.1)
for every t 僆 R. The limit is a convex combination of pointmass at sign~⫺x!`
and a c.d.f. with density given by 10~1 ⫺ F~r!! times the integrand in the preceding display, the weights in the convex combination given by F~r! and 1 ⫺ F~r!,
respectively. If 6 x6 ⬍ `, then Gn, an , bn ~t ! converges to
F~r!

冕

t

⫺`

⫹

⫺1
2 ⫺102
2 ⫺102
sa,`
~1 ⫺ r`
!
f~~1 ⫺ r`
!
~u0sa,` ⫹ x!! du

冕

t

⫺`

⫺1
2 ⫺102
⫺1
f~u0sa,` !F~~1 ⫺ r`
!
~⫺r ⫹ r` sa,`
u!! du
sa,`

for every t 僆 R.
4. Suppose M n bn 0~sb c! r ⫺1 and c ⫹ M n bn 0sb r s for some s 僆 R, and
M n bn ~r0sb ! r x for some x 僆 R 艛 $⫺`,`% as n r `. If 6 x6 ⫽ `, then
Gn, an , bn ~t ! converges to
F~s!1~ x ⬎ 0! ⫹

冕

t

⫺`

⫺1
2 ⫺102
⫺1
sa,`
f~u0sa,` !~1 ⫺ F~~1 ⫺ r`
!
~s ⫹ r` sa,`
u!!! du

(A.2)
for every t 僆 R. The limit is a convex combination of pointmass at sign~⫺x!`
and a c.d.f. with density given by 10~1 ⫺ F~s!! times the integrand in the preceding display, the weights in the convex combination given by F~s! and 1 ⫺ F~s!,
respectively. If 6 x6 ⬍ `, then Gn, an , bn ~t ! converges to
F~s!

冕

t

⫺`

⫹

⫺1
2 ⫺102
2 ⫺102
sa,`
~1 ⫺ r`
!
f~~1 ⫺ r`
!
~u0sa,` ⫹ x!! du

冕

t

⫺`

⫺1
2 ⫺102
⫺1
sa,`
f~u0sa,` !~1 ⫺ F~~1 ⫺ r`
!
~s ⫹ r` sa,`
u!!! du

for every t 僆 R.
Proof. In view of ~2! we can write the density gn, a, b as
gn, a, b ~u! ⫽ gn, a, b ~u6R!Pn, a, b ~ MZ ⫽ R! ⫹ gn, a, b ~u6U !Pn, a, b ~ MZ ⫽ U !
⫽ gn, a, b ~u6R!D~M n b0sb , c! ⫹ gn, a, b ~u6U !~1 ⫺ D~M n b0sb , c!!,

(A.3)

Downloaded from https://www.cambridge.org/core. Harvard University, on 07 Feb 2020 at 21:32:08, subject to the Cambridge Core terms of
use, available at https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0266466605050036

MODEL SELECTION AND INFERENCE

53

where gn, a, b ~u6R! is the conditional density of M n ~ aJ ⫺ a! given that MZ ⫽ R and
gn, a, b ~u6U ! is defined analogously+ As mentioned in note 15,
gn, a, b ~u6R! ⫽ sa⫺1 ~1 ⫺ r 2 !⫺102 f~u~1 ⫺ r 2 !⫺1020sa ⫹ r~1 ⫺ r 2 !⫺102 M n b0sb !,
gn, a, b ~u6U ! ⫽ sa⫺1

冋冉 冉 M
1⫺D

n b0sb ⫹ ru0sa

M1 ⫺ r

2

册

c
,

M 1 ⫺ r2

~1 ⫺ D~M n b0sb , c!! f~u0sa !+

(A.4)

冊冊冒
(A.5)

To prove part 1 replace ~a, b! by ~an , bn ! in the preceding formulas and observe that
under the assumptions of this part of the proposition the probability Pn, an , bn ~ MZ ⫽ R!
converges to unity ~Proposition A+1! and hence the contribution to the total probability
mass by the second term on the far r+h+s+ of ~A+3! vanishes asymptotically+ It hence
suffices to consider the first term only+ Now M n bn ~r0sb ! r x by assumption+ Furthermore, r r r` ⫽ 6 1 ~because Q was assumed to be positive definite!, and sa r
sa,` ⬎ 0+ If x ⫽ 6`, inspection of ~A+4! immediately shows that the total probability
mass of M n ~ aJ ⫺ an ! escapes to 7`+ If x is finite, inspection of ~A+4! reveals that the
⫺1
2 ⫺102
2 ⫺102
conditional density gn, an , bn ~u6R! converges to sa,`
~1 ⫺ r`
!
f~~1 ⫺ r`
!
⫻
~u0sa,` ⫹ x!! for every u 僆 R+ Because the limit function is a density again, convergence takes place in the L1 sense in view of Scheffé’s theorem+ This establishes convergence of the corresponding c+d+f+ in the total variation distance+
To prove part 2 again replace ~a, b! by ~an , bn ! in the preceding formulas and
observe that under the assumptions of this part of the proposition the probability
Pn, an , bn ~ MZ ⫽ R! converges to zero ~Proposition A+1! and hence the contribution to the
total probability mass by the first term on the far r+h+s+ of ~A+3! vanishes asymptotically+
It hence suffices to consider the second term only+ Now, r r r` ⫽ 61, and sa r
sa,` ⬎ 0+ Inspection of ~A+5! then immediately shows that gn, an , bn ~u6U ! converges to
⫺1
sa,`
f~u0sa,` ! for every u 僆 R+
To prove part 3 observe that under the assumptions of this part of the proposition
Pn, an , bn ~ MZ ⫽ R! r F~r! ⬎ 0 and Pn, an , bn ~ MZ ⫽ U ! r 1 ⫺ F~r! ⬎ 0 hold+ The proof
that the total probability mass of gn, an , bn ~u6R! escapes to 7` if x ⫽ 6` is exactly the
same as in the proof of part 1+ In the case that x is finite, the same argument as in
⫺1
2 ⫺102
the proof of part 1 shows that gn, an , bn ~u6R! converges to sa,`
~1 ⫺ r`
!
⫻
2 ⫺102
1
f~~1 ⫺ r` !
~u0sa,` ⫹ x!! for every u 僆 R and in L + Now regarding gn, an , bn ~u6U !
⫺1
inspection of ~A+5! shows that this density converges to sa,`
f~u0sa,` !F~~1 ⫺
2 ⫺102
⫺1
r` !
~⫺r ⫹ r` sa,` u!!0~1 ⫺ F~r!! for every u 僆 R+ Because this limit is a probability density as is readily seen, the convergence is also in L 1 by an application of
Scheffé’s theorem+
The proof of part 4 is completely analogous to the proof of part 3+
䡲
Remark A.3. In the important case where r` ⫽ 0 the preceding results simplify
somewhat: If r` ⫽ 0 and z ⫽ lim nr`M n bn 0~sb c! ⫽ 0 in part 1 of the proposition,
then necessarily x ⫽ sign~r` z!`; i+e+, M n ~ aJ ⫺ an ! always converges to 6` in probability+ If r` ⫽ 0 in part 3 of the proposition, then necessarily x ⫽ sign~r` !`; i+e+,
only the distribution ~A+1! can arise+ If r` ⫽ 0 in part 4 of the proposition, then necessarily x ⫽ sign~⫺r` !`; i+e+, only the distribution ~A+2! can arise+
Downloaded from https://www.cambridge.org/core. Harvard University, on 07 Feb 2020 at 21:32:08, subject to the Cambridge Core terms of
use, available at https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0266466605050036

54

HANNES LEEB AND BENEDIKT M. PÖTSCHER

PROPOSITION A+4+ Let ~an , bn ! be an arbitrary sequence of values for the regression parameters in (1).
1. Suppose that M n bn 0~sb c! r z, 6z6 ⬍ 1, and that M n bn ~r0sb ! r x for some
x 僆 R 艛 $⫺`,`% as n r `. Then Bias r ⫺sa,` x.
2. Suppose that M n bn 0~sb c! r z, 1 ⬍ 6z6 ⱕ ` as n r `. Then Bias r 0.
3. Suppose that M n bn 0~sb c! r 1, c ⫺ M n bn 0sb r r for some r 僆 R 艛 $⫺`,`%,
and M n bn ~r0sb ! r x for some x 僆 R 艛 $⫺`,`% as n r `. If r ⬎ ⫺`, or if
r ⫽ ⫺` but x is finite, then Bias r ⫺sa,` xF~r! ⫹ sa,` r` f~r!. If r ⫽ ⫺` and
6 x6 ⫽ `, then Bias r ⫺sa,` lim nr` rM n bn ~M n bn ⫺ csb !⫺1 f~~M n bn ⫺ csb !0
sb ! provided this limit exists.
4. Suppose that M n bn 0~sb c! r ⫺1, c ⫹ M n bn 0sb r s for some s 僆 R 艛
$⫺`,`%, and M n bn ~r0sb ! r x for some x 僆 R 艛 $⫺`,`% as n r `. If
s ⬎ ⫺`, or if s ⫽ ⫺` but x is finite, then Bias r ⫺sa,` xF~s! ⫺ sa,` r` f~s!.
If s ⫽ ⫺` and 6 x6 ⫽ `, then Bias r sa,` lim nr` rM n bn ~M n bn ⫹ csb !⫺1 ⫻
f~~M n bn ⫹ csb !0sb ! provided this limit exists.
Proof. Under the assumptions of part 1 of the proposition Pn, an , bn ~ MZ ⫽ R! ⫽
D~M n bn 0sb , c! converges to unity by Proposition A+1+ Hence the first term in ~11! converges to ⫺sa,` x+ Because r r r` , sa r sa,` , and because f~M n bn 0sb ⫺ c! and
also f~M n bn 0sb ⫹ c! converge to zero, the second and third term in ~11! go to zero,
completing the proof of part 1+
To prove part 2 observe that the second and third term in ~11! again converge to zero+
Now, D~M n bn 0sb , c! converges to zero by Proposition A+1, whereas M n bn 0sb
diverges to 6`+ Because D~{,{! is symmetric in its first argument, we may assume that
z is positive+ Applying Lemma B+1 in Leeb and Pötscher ~2003a!, the limit of the first
term in ~11! is then readily seen to be zero+
We next prove part 3+ From Proposition A+1 we see that D~M n bn 0sb , c! converges
to F~r!+ Furthermore, ⫺rsaM n bn 0sb converges to ⫺sa,` x ~which may be infinite!+
This shows that the first term in ~11! converges to ⫺sa,` xF~r! provided x is finite or
F~r! is positive+ The second term obviously converges to r` sa,` f~⫺r! ⫽ r` sa,` f~r!
~which is zero in case r ⫽ ⫺`!, whereas the third term goes to zero+ If x is infinite and
F~r! is zero ~i+e+, if r ⫽ ⫺`!, Lemma B+1 in Leeb and Pötscher ~2003a! shows that the
first term in ~11! converges to the claimed limit+
Part 4 is proved analogously to part 3+
䡲
Remark A.5. In the important case where r` ⫽ 0 the following simplifications
arise: If r` ⫽ 0 and z ⫽ 0 in part 1 of the proposition, then necessarily x ⫽ sign~r` z!`+
If r` ⫽ 0 in part 3 of the proposition, then necessarily x ⫽ sign~r` !`+ If r` ⫽ 0 in
part 4 of the proposition, then necessarily x ⫽ sign~⫺r` !`+
PROPOSITION A+6+ Let ~an , bn ! be an arbitrary sequence of values for the regression parameters in (1).
1. Suppose that M n bn 0~sb c! r z, 6z6 ⬍ 1, and that M n bn ~r0sb ! r x for some
2
2
x 僆 R 艛 $⫺`,`% as n r `. Then MSE r sa,`
~1 ⫺ r`
⫹ x 2 !, which is infinite
if 6 x6 ⫽ `.
2
2. Suppose that M n bn 0~sb c! r z, 1 ⬍ 6z6 ⱕ ` as n r `. Then MSE r sa,`
.
Downloaded from https://www.cambridge.org/core. Harvard University, on 07 Feb 2020 at 21:32:08, subject to the Cambridge Core terms of
use, available at https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0266466605050036

MODEL SELECTION AND INFERENCE

55

3. Suppose that M n bn 0~sb c! r 1, c ⫺ M n bn 0sb r r for some r 僆 R 艛 $⫺`,`%,
and M n bn ~r0sb ! r x for some x 僆 R 艛 $⫺`,`% as n r `. Then MSE r
2
2
2
sa,`
~1 ⫹ r`
rf~r! ⫺ r`
F~r! ⫹ x 2 F~r!! if r ⬎ ⫺`, or if r ⫽ ⫺` but x is finite
(with the convention that rf~r! ⫽ 0 if r ⫽ 6`). If r ⫽ ⫺` and 6 x6 ⫽ `, then
2
MSE r sa,`
@1 ⫹ lim nr` r 2 sb⫺1 nbn2 ~M n bn ⫺ csb !⫺1 f~~M n bn ⫺ csb !0sb !# provided this limit exists.
4. Suppose that M n bn 0~sb c! r ⫺1, c ⫹ M n bn 0sb r s for some s 僆 R 艛 $⫺`,`%,
and M n bn ~r0sb ! r x for some x 僆 R 艛 $⫺`,`% as n r `. Then MSE r
2
2
2
sa,`
~1 ⫹ r`
sf~s! ⫺ r`
F~s! ⫹ x 2 F~s!! if s ⬎ ⫺`, or if s ⫽ ⫺` but x is finite
(with the convention that sf~s! ⫽ 0 if s ⫽ 6`). If s ⫽ ⫺` and 6 x6 ⫽ `, then
2
MSE r sa,`
@1 ⫺ lim nr` r 2 sb⫺1 nbn2 ~M n bn ⫹ csb !⫺1 f~~M n bn ⫹ csb !0sb !# provided this limit exists.
Proof. Under the assumptions of part 1 of the proposition the terms in ~12! involving the standard normal density f are readily seen to converge to zero+ By Proposition A+1, F~c ⫺ M n bn 0sb ! ⫺ F~⫺c ⫺ M n bn 0sb ! converges to unity+ Consequently,
2
2
MSE r sa,`
~1 ⫺ r`
⫹ x 2 !+
To prove part 2, observe that the terms in ~12! involving the standard normal density
f again converge to zero and that F~c ⫺ M n bn 0sb ! ⫺ F~⫺c ⫺ M n bn 0sb ! converges
to zero by Proposition A+1+ Hence we only need to show that n ~ bn 0sb ! 2 @F ~c ⫺
M n bn 0sb ! ⫺ F~⫺c ⫺ M n bn 0sb !# converges to zero+ This follows from an application
of Lemma B+1 in Leeb and Pötscher ~2003a!+
We next prove part 3+ The terms in ~12! involving the standard normal density f are
2
2
readily seen to converge to sa,`
r`
rf~r! with the convention that rf~r! ⫽ 0 if
r ⫽ 6`+ Furthermore, we see from Proposition A+1 that F~c ⫺ M n bn 0sb ! ⫺
F~⫺c ⫺ M n bn 0sb ! converges to F~r! and that sa2 r 2 ~n~ bn 0sb ! 2 ⫺ 1! converges to
2
2
sa,`
~ x 2 ⫺ r`
! ~which may be infinite!+ This proves the result provided x is finite or
F~r! is positive+ If x is infinite and F~r! is zero ~i+e+, if r ⫽ ⫺`!, Lemma B+1 in Leeb
and Pötscher ~2003a! shows that the third term in ~12! converges to the claimed limit+
Part 4 is proved analogously to part 3+
䡲
Remark A.7. In the important case where r` ⫽ 0 the following simplifications
arise: If r` ⫽ 0 and z ⫽ 0 in part 1 of the proposition, then necessarily x ⫽ sign~r` z!`,
and hence MSE converges to `+ If r` ⫽ 0 in part 3 of the proposition, then necessarily
x ⫽ sign~r` !`, and hence MSE converges to ` provided r ⬎ ⫺`+ If r` ⫽ 0 in part 4
of the proposition, then necessarily x ⫽ sign~⫺r` !`, and hence MSE converges to `
provided s ⬎ ⫺`+
Remark A.8. The preceding propositions in fact allow for a characterization of all
possible accumulation points of the model selection probabilities, the finite-sample distribution, the ~scaled! bias, and the ~scaled! mean-squared error of the post-modelselection estimator under arbitrary sequences of parameters ~an , bn !: Given any sequence
~an , bn !, compactness of R 艛 $⫺`,`% implies that every subsequence ~ni ! contains
a further subsequence ~ni ~ j ! ! such that the quantities M n bn 0~sb c!, M n bn ~r0sb !,
c ⫺ M n bn 0sb , c ⫹ M n bn 0sb , and the expressions in the limit operators in Propositions
A+4 and A+6 converge to respective limits in R 艛 $⫺`,`% along the subsequence ~ni ~ j ! !+
Applying the preceding propositions to the subsequence ~ni ~ j ! ! provides the desired characterization of all accumulation points+

Downloaded from https://www.cambridge.org/core. Harvard University, on 07 Feb 2020 at 21:32:08, subject to the Cambridge Core terms of
use, available at https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0266466605050036

56

HANNES LEEB AND BENEDIKT M. PÖTSCHER

PROPOSITION A+9+ The post-model-selection estimator aJ is uniformly consistent
for a, i.e.,
lim

sup

nr` ~a, b!僆R2

Pn, a, b ~6 aJ ⫺ a6 ⬎ «! ⫽ 0

for every « ⬎ 0.
Proof. Using Chebychev’s inequality we obtain
Pn, a, b ~6 aJ ⫺ a6 ⬎ «!
[
⫺ a6 ⬎ «, MZ ⫽ R! ⫹ Pn, a, b ~6 a~U
[
! ⫺ a6 ⬎ «, MZ ⫽ U !
⫽ Pn, a, b ~6 a~R!
[
⫺ a6 ⬎ «, MZ ⫽ R! ⫹ Pn, a, b ~6 a~U
[
! ⫺ a6 ⬎ «!
ⱕ Pn, a, b ~6 a~R!
[
⫺ a6 ⬎ «!, Pn, a, b ~ MZ ⫽ R!% ⫹ sa2 0~n« 2 !+
ⱕ min$Pn, a, b ~6 a~R!
Because sa2 0~n« 2 ! is independent of ~a, b! and converges to zero, it suffices to show
that the first term on the far r+h+s+ of the preceding display converges to zero uniformly
in ~a, b!+ Observe that a~R!
[
⫺ a is distributed normally with mean ~⫺rsa 0sb !b and
variance sa2 ~1 ⫺ r 2 !0n+ In view of ~3!, the first term on the far r+h+s+ of the preceding
display hence equals
min$1 ⫺ D~M n r~1 ⫺ r 2 !⫺102 b0sb ,M n sa⫺1 ~1 ⫺ r 2 !⫺102 «!, D~M n b0sb , c!%,

(A.6)

which clearly does not depend on the value of the parameter a+ Now
sup

lim

nr` 6 b6ⱖ2csb 0M n

D~M n b0sb , c! ⫽ 0

by an application of Proposition A+1+ Furthermore,
sup
6 b6⬍2csb 0M n

@1 ⫺ D~M n r~1 ⫺ r 2 !⫺102 b0sb ,M n sa⫺1 ~1 ⫺ r 2 !⫺102 «!#

ⱕ 2 ⫺ 2F~~1 ⫺ r 2 !⫺102 ~⫺2c6r6 ⫹ sa⫺1M n «!!,
which converges to zero because « ⬎ 0, r r r` , and because c0M n r 0+ It now follows that ~A+6! converges to zero uniformly+
䡲

APPENDIX B:
ASYMPTOTIC RESULTS FOR CONSERVATIVE
MODEL SELECTION PROCEDURES
In the following discussion we consider the linear regression model ~1! under the assumptions of Section 2+ Furthermore, we assume as in Section 2+2 that c does not depend on
sample size and satisfies 0 ⬍ c ⬍ `+

Downloaded from https://www.cambridge.org/core. Harvard University, on 07 Feb 2020 at 21:32:08, subject to the Cambridge Core terms of
use, available at https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0266466605050036

MODEL SELECTION AND INFERENCE

57

PROPOSITION B+1+ The post-model-selection estimator aJ is uniformly consistent
for a, i.e.,
sup

lim

nr` ~a, b!僆R2

Pn, a, b ~6 aJ ⫺ a6 ⬎ «! ⫽ 0

for every « ⬎ 0.
Proof. The proof is identical to the proof of Proposition A+9 up to and including
~A+6!+ Now
lim lim sup

gr`

nr`

sup
6 b6ⱖgsb 0M n

D~M n b0sb , c! ⫽ 0

as a consequence of Lemma C+3 in Leeb and Pötscher ~2003b!+ Furthermore,
sup
6 b6⬍gsb 0M n

@1 ⫺ D~M n r~1 ⫺ r 2 !⫺102 b0sb ,M n sa⫺1 ~1 ⫺ r 2 !⫺102 «!#

ⱕ 2 ⫺ 2F~~1 ⫺ r 2 !⫺102 ~⫺g6r6 ⫹ sa⫺1M n «!!,
which converges to zero for every given g 僆 R because « ⬎ 0 and r r r` + It then
follows that ~A+6! converges to zero uniformly+
䡲

APPENDIX C:
THE MAXIMAL ABSOLUTE BIAS AND
THE MAXIMAL MSE ARE UNBOUNDED
FOR GENERAL CONSISTENT MODEL
SELECTION PROCEDURES
We give here a simple proof of the fact that the ~scaled! maximal absolute bias and
hence the ~scaled! maximal mean-squared error of a post-model-selection estimator
diverges to infinity if an arbitrary consistent model selection procedure is employed+
This is a variant of the result of Yang ~2003!, who uses a predictive mean-square risk
measure instead+ Our proof is based on the contiguity argument discussed in Remark 4+4+
An advantage of this proof is that—contrary to Yang’s proof—it does not rely on a
normality assumption for the errors+
We assume the simple linear regression model ~1! under the basic assumptions made
in Section 2, except that the errors et only need to be i+i+d+ with mean zero and ~finite!
variance s 2 ⬎ 0+ ~The assumption that s 2 is known is inessential here+ If s 2 is unknown,
and hence f depends on the scale parameter s, Proposition C+1 holds for every value of
s 2 +! Furthermore, we assume that et has a density f that possesses an absolutely continuous derivative f ' satisfying
0⬍

冕

`

⫺`

~ f ' ~ x!0f ~ x!! 2 f ~ x! dx ⬍ `+

Downloaded from https://www.cambridge.org/core. Harvard University, on 07 Feb 2020 at 21:32:08, subject to the Cambridge Core terms of
use, available at https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0266466605050036

58

HANNES LEEB AND BENEDIKT M. PÖTSCHER

Note that the conditions on f guarantee that the information of f is finite and positive+
~These conditions are obviously satisfied in the special case of normally distributed
errors+! Let MX now be an arbitrary model selection procedure that consistently selects
between the models R and U+ Furthermore, let aY denote the corresponding post-modelselection estimator ~i+e+, aY ⫽ a~R!
[
if MX ⫽ R and aY ⫽ a~U
[
! if MX ⫽ U !+ In the following
En, a, b denotes the expectation operator w+r+t+ Pn, a, b + Recall that r` is less than unity in
absolute value because the limit Q of X ' X0n has been assumed to be positive definite+
PROPOSITION C+1+ Suppose that r` ⫽ 0. Then the maximal absolute bias
supa, b 6En, a, b @M n ~ aY ⫺ a!#6, and hence the maximal mean-squared error
supa, b En, a, b @n~ aY ⫺ a! 2 #, goes to infinity for n r `.
Proof. Clearly, it suffices to prove the result for the maximal absolute bias+ The following elementary relations hold:
En, a, b @M n ~ aY ⫺ a!#
[
⫺ a!1~ MX ⫽ R!# ⫹ En, a, b @M n ~ a~U
[
! ⫺ a!1~ MX ⫽ U !#
⫽ En, a, b @M n ~ a~R!
[
⫺ a!# ⫹ En, a, b @M n ~ a~U
[
! ⫺ a~R!!1~
[
MX ⫽ U !#
⫽ En, a, b @M n ~ a~R!
[
⫺ a!# ⫹ r~sa 0sb !En, a, b @M n b~U
Z !1~ MX ⫽ U !# +
⫽ En, a, b @M n ~ a~R!
Furthermore,
n

[
⫺ a!# ⫽ M n b ( x t1 x t2
En, a, b @M n ~ a~R!
t⫽1

冒(
n

x t12 ⫽ ⫺M n brsa sb⫺1 +

t⫽1

Consequently, for every a and every r 僆 R we have
lim inf sup 6En, a, b @M n ~ aY ⫺ a!#6
nr`

b僆R

ⱖ lim inf 6En, a, r0M n @M n ~ aY ⫺ a!#6
nr`

⫺1
ⱖ lim inf 6En, a, r0M n @M n ~ a~R!
[
⫺ a!#6 ⫽ 6r66r` 6sa,` sb,`
,

(C.1)

nr`

provided we can show that
Z !!1~ MX ⫽ U !#6 ⫽ 0
lim sup 6En, a, r0M n @M n ~ b~U

(C.2)

nr`

for every r 僆 R+ We apply the Cauchy–Schwartz inequality to obtain
Z !! 2 # @Pn, a, r0M n ~ MX ⫽ U !# 102+
Z !!1~ MX ⫽ U !#6 ⱕ En,102
6En, a, r0M n @M n ~ b~U
a, r0M n @n~ b~U
(C.3)
The first term on the r+h+s+ in ~C+3! is easily seen to satisfy
Z !! 2 # ⫽ ~sb2 ⫹ r 2 !102+
En,102
a, r0M n @n~ b~U

Downloaded from https://www.cambridge.org/core. Harvard University, on 07 Feb 2020 at 21:32:08, subject to the Cambridge Core terms of
use, available at https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0266466605050036

MODEL SELECTION AND INFERENCE

59

To prove ~C+2! it hence suffices to show that lim supnr` Pn, a, r0M n ~ MX ⫽ U ! ⫽ 0+
Because the model is locally asymptotically normal ~Koul and Wang, 1984, Theorem 2+1 and Remark 1; Hajek and Sidak, 1967, p+ 213!, the sequence of probability
measures Pn, a, r0M n is contiguous w+r+t+ the sequence Pn, a,0 ~for every r 僆 R!+ Because
lim supnr` Pn, a,0 ~ MX ⫽ U ! ⫽ 0 by the assumed consistency of the model selection procedure, contiguity implies
lim sup Pn, a, r0M n ~ MX ⫽ U ! ⫽ 0
nr`

for every r 僆 R, cf+ Remark 4+4+ This establishes ~C+2! and hence ~C+1!+ Letting 6r6 go
to infinity in ~C+1! then completes the proof ~note that 6r` 6 and sa,` are positive and
⫺1
sb,`
is finite!+
䡲
Remark C.2.
1+ The proof in fact shows that this result holds for fixed a and any bounded neighborhood of b ⫽ 0, i+e+, sup6 b6ⱕs 6En, a, b @M n ~ aY ⫺ a!#6 and sup6 b6ⱕs En, a, b @n~ aY ⫺ a! 2 #
diverge to infinity as n r ` for each fixed a and s ⬎ 0+
2+ The preceding proposition is formulated for the simple regression model with two
regressors and only two competing models from which to choose+ It can easily be
extended to more general cases+ The preceding proof should also easily extend to
the risk measure used in Yang ~2003!+ We do not pursue these issues here+

Downloaded from https://www.cambridge.org/core. Harvard University, on 07 Feb 2020 at 21:32:08, subject to the Cambridge Core terms of
use, available at https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0266466605050036

