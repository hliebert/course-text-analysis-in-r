Stat Papers (2010) 51:629â€“650
DOI 10.1007/s00362-008-0150-3
REGULAR ARTICLE

Statistical inference for fixed-effects partially linear
regression models with errors in variables
Haibo Zhou Â· Jinhong You Â· Bin Zhou

Received: 6 December 2005 / Accepted: 20 May 2008 / Published online: 18 June 2008
Â© Springer-Verlag 2008

Abstract Fixed-effects partially linear regression models are useful tools to analyze
data from economic, genetic and other fields. In this paper, we consider estimation
and inference procedures when some of the covariates are measured with errors. The
previously proposed estimations, including difference-based series estimation (Baltagi
and Li in Ann Econ Finan 3:103â€“116, 2002) and profile least squares estimation
(Fan et al. in J Am Stat Assoc 100:781â€“813, 2005) are no longer consistent because
of the attenuation. We propose a new estimation by taking the measurement errors
into account. Our proposed estimators are shown to be consistent and asymptotically
normal. Consistent estimations of the error variance are also developed. In addition,
we propose a variable-selection procedure to variable selection in the parametric part.
The procedure is an extension of the nonconcave penalized likelihood (Fan and Li in
J Am Stat Assoc 85:1348â€“1360, 2001), which simultaneously selects the important
variables and estimates the unknown parameters. The resulting estimate is shown to
possess an oracle property. Extensive simulation studies are conducted to illustrate the
finite sample performance of the proposed procedures.

The paper was supported by a grant from National Institute of Health (CA 79949).
H. Zhou (B)
Department of Biostatistics, University of North Carolina at Chapel Hill,
Chapel Hill, NC 27599-7420, USA
e-mail: zhou@bios.unc.edu
J. You
Department of Statistics, Shanghai University of Finance and Economics,
Shanghai 200433, Peopleâ€™s Republic of China
e-mail: jinhongyou@gmail.com
B. Zhou
Department of Management and Insurance, School of Finance and Statistics,
East China Normal University, Shanghai 200062, Peopleâ€™s Republic of China
e-mail: bin.zhou@afgtrust.com

123

630

H. Zhou et al.

Keywords Fixed effect Â· Partially linear Â· Measurement errors Â· Profile least
squares Â· Variable selection
Mathematics Subject Classification (2000)

Primary 62H12 Â· Secondary 62A10

1 Introduction
Because mis-specification in fully parametric models could lead to severe modeling
biases, much interests and efforts in statistical research have been devoted to nonparametric models. However, a fully nonparametric approach could be too flexible to draw
concise conclusions, and if there exists a large number of covariates, it will make a fully
nonparametric analysis infeasible due to the curse of dimensionality. Semiparametric
models, which are a compromise between a general nonparametric specification and
a fully parametric specification, have been gaining much attention in statistical literature and have broad applications in medical research, biology, genomics, economics
and education. Semiparametric models have various forms including: partially linear
regression models (Engle et al. 1986), partially nonlinear regression models (Andrews
1996), single-index regression model (Delecroix et al. 2003), varying-coefficient index
regression model (Fan et al. 2003), etc. In this paper, we will consider the fixed-effects
partially linear regression model (Baltagi and Li 2002; Fan et al. 2004).
A fixed-effects partially linear regression model can be written as
Yi = BiT Î± + XiT Î² + m(Ui ) + Îµi , i = 1, . . . , n

(1.1)

where Yi â€™s are the responses, Xi and Ui â€™s are the design points, Î² = (Î²1 , . . . , Î² p )T
is an unknown parameter vector, m(Â·) is an unknown function and Îµi â€™s are random
errors. In addition, B = (B1 , . . . , Bn )T = IG âŠ— 1 I with âŠ— the Kronecker product,
IG the G Ã— G identity matrix and 1 I the vector of length I with all elements 1, and
Î± = (Î±1 , . . . , Î±G )T is an unknown
 G Ã— 1 vector representing the fixed effects. For
identification, we assume that G
g=1 Î±g = 0.
Model (1.1) is different from the usual partially linear regression model since G
is dependent on n through G = n/I . In many practical designs, I is usually a small
number in the single digit while G can be much larger, in an order of hundreds or
larger. For example, for the cDNA microarray data used by Fan et al. (2004, 2005), I
is 2. The number of the parameters Î±g , on the other hand, grows with the sample size.
Throughout this paper, we will assume that I is kept fixed while n â†’ âˆž.
Model (1.1) can be viewed as an extension of the usual panel data parametric
regression model with fixed effects to the semiparametric context. The panel data
regression model with fixed effects would be an appropriate specification if one were
interested in a specific set of subjects. Such a model has been widely used in econometric analysis. The details can be found in the research of Baltagi (1995) and
Ahn and Schmidt (2000). Recently, the fixed-effects model has also found its application in the genomics. For instance, Fan et al. (2004, 2005) used model (1.1) to conduct
the microarray analysis of the neuroblastoma cell in response to macrophage migration
inhibitory factor (MIF).

123

Statistical inference for fixed-effects partially linear regression models with errors in variables

631

It has been well documented in the literature that there often exist covariate measurement errors in many practical problems (Carroll et al. 1995). Handling the measurement errors in covariates is generally a challenge for statistical analysis. For the
past two decades, regression analysis with measurement errors had much progress.
A detailed study can be found in the research of Fuller (1987); Cheng and Van Ness
(1999); Carroll et al. (1995); Fan (1991, 1993); Fan and Truong (1993); Liang et al.
(1999); Zhou and Wang (2000) and so on.
There are few results about the model (1.1), however, when some or all of the
covariates are measured with errors. We will fill this gap in this paper. Specifically,
we consider the case where Xi â€™s are measured with an additive error, and Ui â€™s are
error-free. That is we can not observe Xi directly but instead we observe Wi with
Wi = Xi + Î¶ i ,

(1.2)

Here Î¶ i is a measurement error and independent of (XiT , Ui , Îµi )T . We assume that
Cov(Î¶ i ) =  Î¶ with  Î¶ assumed to be known, as in the papers of Hwang (1986);
Zhu and Cui (2003), and others. When  Î¶ is unknown, we can estimate it by partial
replication (Carroll et al. 1995, Chap. 3). Because of the attenuation the previously
proposed estimators, such as difference-based series estimator (DSE) (Baltagi and Li
2002) and profile least squares estimator (PLSE) (Fan et al. 2005), are no longer consistent. We propose a new estimation by correcting the PLSE. The resulting estimator
is shown to be consistent and asymptotically normal. Consistently estimating the error
variance is also considered.
As in parametric regression models, variable selection is also important for using the
model (1.1). The number of variables in the parametric part of model (1.1) can easily
be large when nonlinear terms and interactions between covariates are introduced to
reduce possible modeling biases. We develop a class of variable procedures to select
significant variables in the parametric part of model (1.1). The proposed procedures are
based on the combination of the nonconcave penalty and corrected profile least squares
techniques. The asymptotic normality of the resulting estimators is derived. Same as
the procedure of Fan and Li (2001), with a proper choice of regularization parameters
and penalty functions, the proposed variable selection procedures are shown to perform
as well as an oracle estimate.
The layout of the remainder of this paper is as follows. In Sect. 2, we propose
new estimators for the parametric, nonparametric components and error variance by
taking the measurement error into account. A large sample theory is developed for these
estimators in Sect. 3. In Sect. 4, we propose a variable selection method for the covariate
of the parametric part. Some simulation studies are presented in Sect. 5. Conclusions
are given in Sect. 6. All proofs of main results are relegated to the Appendix.
2 The proposed estimations
When Xi â€™s are observable we can apply the DSE or PLSE to estimate the parametric
and nonparametric components of the model (1.1). According to You and Zhou (2006),
the DSE is not efficient when I â‰¥ 3. Therefore, we construct new estimators based

123

632

H. Zhou et al.

on the PLSE. For convenience, we first introduce the PLSE. Suppose that {Yi , Xi , Ui }
is an i.i.d. sample from model (1.1). For any given Î± and Î², the model (1.1) can be
written as
Yi âˆ’ BiT Î± âˆ’ XiT Î² = m(Ui ) + Îµi , i = 1, . . . , n.

(2.1)

This transforms the model (1.1) into the usual nonparametric model. Now, applying a
local linear regression technique in a small neighborhood of u 0 , one can approximate
m(u) locally by a linear function


m(u) â‰ˆ m(u 0 ) + m (u 0 )(u âˆ’ u 0 ) â‰¡ a + b(u âˆ’ u 0 )


with m (Â·) = âˆ‚m(Â·)/du. This leads to the following weighted local least squares
problem: find {(a, b)} to minimize
n



Yi âˆ’ BiT Î± âˆ’ XiT Î² âˆ’ a âˆ’ b(Ui âˆ’ u 0 )

2

K h (Ui âˆ’ u 0 ),

(2.2)

i=1

where K (Â·) is a kernel function, h is a bandwidth and K h (Â·) = K (Â·/ h)/ h.
The solution to problem (2.2) is given by
(
a (u 0 ), h
b(u 0 ))T = (DuT 0 Ï‰u 0 Du 0 )âˆ’1 DuT 0 Ï‰u 0 (Y âˆ’ BÎ± âˆ’ XÎ²)
where Y = (Y1 , . . . , Yn )T , X = (X1 , . . . , Xn )T , B is defined in previous section,
âŽ›

Du 0

1
âŽœ ..
=âŽ.
1

U1 âˆ’u 0
h

..
.

âŽž
âŽŸ
âŽ  , and Ï‰u 0 = diag(K h (U1 âˆ’ u 0 ), . . . , K h (Un âˆ’ u 0 )).

Un âˆ’u 0
h

In (2.1) m(Ui ) is replaced with 
a (Ui ). Then we have
T
T
Yi âˆ’ BiT Î± âˆ’ XiT Î² â‰ˆ (1, 0)(DU
Ï‰ D )âˆ’1 DU
Ï‰ (Y âˆ’ BÎ± âˆ’ XÎ²) + Îµi , (2.3)
i Ui Ui
i Ui

for i = 1, . . . , n. Denote
i = Yi âˆ’ (1, 0)(DT Ï‰Ui DUi )âˆ’1 DT Ï‰Ui Y,
Y
Ui
Ui
T
T

XiT = XiT âˆ’ (1, 0)(DU
Ï‰ D )âˆ’1 DU
Ï‰ X,
i Ui Ui
i Ui

and
T
T

Bi = Bi âˆ’ (1, 0)(DU
Ï‰ D )âˆ’1 DU
Ï‰ B.
i Ui Ui
i Ui

123

Statistical inference for fixed-effects partially linear regression models with errors in variables

633

Model (2.3) can be written as
i â‰ˆ 
BiT Î± + 
XiT Î² + Îµi , i = 1, . . . , n.
Y

(2.4)

By (2.4), the least squares technique and partialing out formula, we have an estimator
of Î² as follows:
âˆ—
 âˆ’1 T

XT (In âˆ’ P
Î²Ìƒ n = (
B )X) X (In âˆ’ P
B )Y

(2.5)

where 
X = (In âˆ’ S)X with
âŽž
T
T
Ï‰ D )âˆ’1 DU
Ï‰
(1, 0)(DU
1 U1 U1
1 U1
âŽŸ
âŽœ
..
S=âŽ
âŽ 
.
T
âˆ’1 DT Ï‰
Ï‰
D
)
(1, 0)(DU
Un Un
n Un Un
âŽ›

âˆ’
 T  âˆ’T
and similarly for all the other â€œhattedâ€ matrices, P
B = B(B B) B , and A denotes
the Moore Penrose inverse. Î²Ìƒ n is the PLSE.
However, Xi â€™s can not be observed in our case and we just have Wi . If we ignore
the measurement error and replace Xi with Wi in (2.5), (2.5) can be used to show that
the resulting estimate is inconsistent. The form of (2.5) suggests even more. It is well
known that in linear regression or partially linear regression, inconsistency caused
by the measurement error can be overcome by applying the so-called â€œcorrection
for attenuationâ€ (Fuller 1987; Liang et al. 1999). In the context of semiparametric
regression model (1.1), this suggests that we use the estimator


 T (In âˆ’ P)W
 âˆ’ (n âˆ’ G) Î¶
Î²n = W
B

âˆ’1

 T (In âˆ’ P)
W
B Y

 has the same definition as 
where W
X.
Moreover, the fact that m(Ui ) = E(Yi âˆ’BiT Î± âˆ’XiT Î²|Ui ) = E(Yi âˆ’BiT Î± âˆ’WiT Î²|Ui )
and the result of Fan et al. (2005) suggest that the estimate of the nonparametric
components M = (m(U1 ), . . . , m(Un ))T can be achieved with the following equation


In PÌ„B
S In



BÎ±
M




=


PÌ„B
(Y âˆ’ W
Î²n)
S

where PÌ„B = B(IG âˆ’ 1G 1TG /G)(BT B)âˆ’1 BT and W = (W1 , . . . , Wn )T . An estimator
of M has the following form


 n = In âˆ’ (In âˆ’ SPÌ„B )âˆ’1 (In âˆ’ S) (Y âˆ’ W
Î² n ).
M

(2.6)

Sometimes, it is also necessary to estimate the error variance Ïƒ 2 = E(Îµ12 )
for such tasks as the construction of confidence regions, model-based tests, model
selection procedures, single-to-noise ratio determination and so on. In this event, since

123

634

H. Zhou et al.

E(Yi âˆ’ BiT Î± âˆ’ XiT Î² âˆ’ m(Ui ))2 = Ïƒ 2 and E(Yi âˆ’ BiT Î± âˆ’ WiT Î² âˆ’ m(Ui ))2 =
Ïƒ 2 + Î² T  Î¶ Î², we define an estimator of Ïƒ 2 as

Ïƒn2 =

I
 n )T (In âˆ’PB )(Yâˆ’W
 n )âˆ’
Î² n âˆ’M
Î² nT  Î¶ 
Î²n
(Yâˆ’W
Î²n âˆ’M
n(I âˆ’ 1)

(2.7)

with PB = B(BT B)âˆ’1 BT = 1/I BBT .
 n (Â·)
In the next section, we will investigate the asymptotic properties of 
Î²n, M
2
and 
Ïƒn .
3 Asymptotic properties of the proposed estimators
In this section we will be establishing the asymptotic properties of the proposed estimators from the previous section. We begin with the following assumptions. These
assumptions, while looking a little lengthy, are actually quite simple and can be easily
satisfied. They are also used by Fan et al. (2005).
Assumption 1 The function m(Â·) has a bounded second derivative.


Assumption 2  = E {X1 âˆ’ E(X1 |U1 )}âŠ—2 is non-singular and E(X1 |U1 = u) is
Lipschitz continuous where AâŠ—2 means AAT .
Assumption 3 The random variable U1 has a bounded support . Its density function
f (Â·) is Lipschitz continuous and bounded away from 0 on .
Assumption 4 The function K (Â·) is a symmetric density function with compact support.
Assumption 5 The bandwidth h satisfies that nh 8 â†’ 0 and nh 2 /(log n)2 â†’ âˆž as
n â†’ âˆž.
With these assumptions, we are ready to establish the main results. The following
theorem gives the asymptotic normality of 
Î²n.
Theorem 1 Suppose that Assumptions 1 to 5 hold. Then the proposed estimator 
Î²n
of Î² is asymptotically normal, namely
âˆš
n(
Î² n âˆ’ Î²) â†’ D N (0, I /(I âˆ’ 1) âˆ’1  1  âˆ’1 ) as n â†’ âˆž,


where  1 = E(Îµ1 âˆ’ Î¶ 1T Î²)2  + Ïƒ 2  Î¶ + E{(Î¶ 1 Î¶ 1T âˆ’  Î¶ )Î²}âŠ—2 .
âˆ’1 
1 
âˆ’1 is a consistent estimator of  âˆ’1  1  âˆ’1 where
Further, 
=


I
 T (I âˆ’ P)W
 âˆ’ Î¶
W
B
n(I âˆ’ 1)

and


 T âŠ—2 .

T
T
1 âˆ’ W

1 = 1 Wdiag(
Y

1 Î² n , . . . , Yn âˆ’ Wn Î² n ) + 1n ( Î¶ Î² n )
n

123

Statistical inference for fixed-effects partially linear regression models with errors in variables

635

 n and 
The following two theorems provide the asymptotic properties of M
Ïƒn2 ,
respectively.
n
Theorem 2 Suppose that Assumptions 1 to 5 hold. Then the risk of the estimator M
is bounded as follows:
 n |U1 , . . . , Un }
MSE{M


âˆš


Î¼22 h 4
(Ïƒ 2 + Î² T  Î¶ Î²)Î½0 ||
I
1

2
â‰¤ âˆš
E{m (U )} +
+ o p h4 +
4
nh
nh
( I âˆ’ 1)2
where Î¼ j =



u j K (u)du, Î½ j =



 n = (
u j K 2 (u)du, M
m n (U1 ), . . . , m
n (Un ))T ,

n
1

E{
m n (Ui ) âˆ’ m(Ui )|U1 , . . . , Un }2 .
MSE{Mn |U1 , . . . , Un } =
n
i=1

Theorem 3 Suppose that Assumptions 1 to 5 hold. Then it holds that
âˆš
n(
Ïƒn2 âˆ’ Ïƒ 2 ) â†’ D N (0, Îº) as n â†’ âˆž
where Îº = E(Îµ1 âˆ’ Î¶ 1T Î²)4 + (3 âˆ’ I )/(I âˆ’ 1)(Ïƒ 2 + Î² T  Î¶ Î²)2 . Define
n)
 = (Ïˆ
n )T = (In âˆ’ PB )(Y âˆ’ W
1 , . . . , Ïˆ
Î²n âˆ’ M

and


6
3
4

Îºn = 1 + 2 âˆ’ 3 âˆ’
I
I
I



âˆ’1  

n
1
9
I
âˆ’
1
T
i4 âˆ’
6âˆ’
(
Ïƒn2 + 
Ïˆ
Î² n Î¶ 
Î²n)
n
I2
I
i=1

3âˆ’ I
(
Ïƒ2 + 
+
Î² nT  Î¶ 
Î²n)
I âˆ’1 n
then 
Îºn is a consistent estimator of Îº.

4 Selecting the covariates of the parametric part
Model selection is an indispensable tool for statistical data analysis. However, the
problem has not been studied for model (1.1) when some covariates are measured
with errors. Fan and Li (2001) proposed a variable selection via nonconcave penalized
likelihood and found some oracle properties. In this section, we will apply their method
to our setting.

123

636

H. Zhou et al.

Suppose that Xi consists of p variables, and some of these are not significant. A
penalized corrected profile least squares takes the form
L(Î²) â‰¡

1   T
n(I âˆ’ 1) T
 
(Y âˆ’ WÎ²) (In âˆ’ P
Î² Î¶ Î²
B )(Y âˆ’ WÎ²) âˆ’
2
2I
p

Î» j p j (|Î² j |)
+n

(4.1)

j=1

where p j (Â·)â€™s are penalty function, Î» j â€™s are tuning parameters and may dependent on
n, which control the model complexity. For ease of presentation, we denote Î» j p j (Â·)
by pÎ» j (Â·).
Many penalty functions, such as the family of L q -penalty (q â‰¥ 0) (Antoniadis
and Fan 2001), have been used for penalized least squares and penalized likelihood in
various parametric models. Antoniadis and Fan (2001) and Fan and Li (2001) provided
various insights into how a penalty function should be chosen. They advocated that a
good penalty function should yield an estimator with the following three properties:
unbiasedness for a large true coefficient to avoid unnecessary estimation bias, sparsity
(estimating a small coefficient as zero) to reduce model complexity, and continuity to
avoid unnecessary variation in model prediction. Necessary conditions are given in
Antoniadis and Fan (2001). However, none of the L q -penalties produces estimates
that satisfy, simultaneously, the above three properties. A simple penalty function,
which results in an estimator with the three desired properties, is the smoothly clipped
absolute deviation (SCAD) penalty. Its first derivative is defined by
pÎ» (Î²) = Î»{I (Î² â‰¤ Î») +

(aÎ» âˆ’ Î²)+
I (Î² > Î»)} for some a > 2, where Î² > 0,
(a âˆ’ 1)Î»

and pÎ» (0) = 0. The SCAD involves two unknown parameters, Î» and a. Fan and Li
(2001) suggested using a = 3.7 from a Bayesian point of view.
Now, we study the asymptotic properties of the resulting estimate of the penalized
corrected profile least squares (4.1). First we establish the convergence rate of the
penalized corrected profile least squares estimator. Assume that all penalty functions
pÎ» j (Â·) are nonnegative, non-decreasing with pÎ» j (0) = 0. Denote by Î² 0 the true value
of Î², and


an = max{| pÎ» j (|Î²0 j |)| : Î²0 j = 0}, and bn = max{| pÎ» j (|Î²0 j |)| : Î²0 j = 0}.
j

j

Then, we have the following theorem.
Theorem 4 Suppose that Assumptions 1 to 5 hold. If an and bn tend to zero as n â†’ âˆž,
then with probability tending to 1 as n â†’ âˆž, there exists a local minimizer Î²Ìƒ n of
L(Î²) such that ||Î²Ìƒ n âˆ’ Î² 0 || = O p (n âˆ’1/2 + an ).
Theorem 4 demonstrates how the rate of convergence of the penalized corrected
profile least squares estimator Î²Ìƒ n depends on Î» j . To achieve the root n convergence

123

Statistical inference for fixed-effects partially linear regression models with errors in variables

637

rate, Î» j must be small enough so that an = O p (n âˆ’1/2 ). Next we establish the oracle
property for the penalized corrected profile least squares estimator. For ease of presentation, we assume, without loss of generality, that all of the first q components of
Î² 0 are not equal to 0, and all other components equal to 0. Let the first q components
of Î² 0 be Î² 01 . Denote

 

 = diag pÎ»1 (|Î²01 |, . . . , pÎ»q (|Î²0q |) and
 


b = pÎ»1 (|Î²01 |)sgn(Î²01 ), . . . , pÎ»q (|Î²0q |)sgn(Î²0q ) .
Further, let Î²Ìƒ n1 consist of the first q components of Î²Ìƒ n and Î²Ìƒ n2 consist of the last
p âˆ’ q components of Î²Ìƒ n .
Theorem 5 (Oracle property) Assume that for j = 1, . . . , q, Î» j â†’ 0,
and the penalty function pÎ» j (|Î²0 j |) satisfies that
lim inf lim inf pÎ» j (Î²0 j )/Î» j > 0.
nâ†’âˆž Î²0 j â†’0+

âˆš

nÎ» j â†’ âˆž

(4.2)

If an = O p (n âˆ’1/2 ), then under the conditions of Theorem 4, with probability tending
T
T
to 1 as n â†’ âˆž, the root n consistent local minimizer Î²Ìƒ n = (Î²Ìƒ n1 , Î²Ìƒ n2 )T in Theorem 4
must satisfy
(i) (Sparsity) Î²Ìƒ n2 = 0;
(ii) (Asymptotic normality)
âˆš  (1)
n  +



Î²Ìƒ n1 âˆ’ Î² 01 +  (1) +


âˆ’1 
b â†’ D N 0,

I
(1)

(I âˆ’ 1) 1



(1)

where  (1) and  1 consist of the first q rows and columns of  and  1 , respectively.
âˆš
From Theorem 5 if Î» j â†’ 0, nÎ» j â†’ âˆž for j = 1, . . . , d, an = O p (n âˆ’1/2 ),
and the condition (4.2) is satisfied, then the resulting estimate possesses an oracle
property. This implies that the resulting procedure correctly specifies the true model
and estimates the unknown regression coefficients as efficiently as if we knew the
sub-model. If all the penalty functions are SCAD, then an = 0 when n is sufficiently
large, and hence the resulting estimate possesses the oracle property. However, this is
not true forâˆšthe L 1 -penalty, since the condition an = max j Î» j = O p (n âˆ’1/2 ) and the
conditions nÎ» j â†’ âˆž cannot be satisfied simultaneously.
It is challenging to find the solution of the penalized corrected profile least squares
of (4.1) because the penalty function pÎ» j (|Î² j |), such as the SCAD penalty, is irregular
at the origin and may not have a second derivative at some points. Following Fan
and Li (2001), we locally approximate the penalty functions by quadratic functions
as follows. Given an initial value Î² (0) that is close to the minimizer of (4.1), when
(0)
|Î² j | â‰¥ Î· (a prescribed value), the penalty pÎ» j (|Î² j |) can be locally approximated by

123

638

H. Zhou et al.

the quadratic function as
 


(0)
(0)
[ pÎ» j (|Î² j |)] = pÎ» j (|Î² j |)sgn(Î² j ) â‰ˆ pÎ» j (|Î² j |)/|Î² j | Î² j
With the local quadratic approximation, the Newton-Raphson algorithm can be implemented directly for minimizing L(Î²).
To implement the methods described above, it is desirable to have an automatic
data-driven method for estimating the tuning parameters Î»1 , . . . , Î» p . Similar to Fan
and Li (2001) we can estimate Î» = (Î»1 , . . . , Î» p )T by minimizing an approximate
generalized cross validation (GCV) score.
5 Simulation results
In this section we carry out some simulation studies to demonstrate the finite sample
performance of the proposed procedures.
Example 1 The data are generated from the following fixed-effects partially linear
regression model
yi = BiT Î± + xi1 Î²1 + xi2 Î²2 + m(u i ) + Îµi and wi j = xi j + Î¶i j
for j = 1, 2 and i = 1, . . . , n, where Î±g are i.i.d. U (âˆ’0.5, 0.5); xi j â€™s are i.i.d.
N (0.5, 1); (Î²1 , Î²2 ) = (1.5, 1); Î¶i j â€™s are i.i.d. N (0, ÏƒÎ¶2 ) with ÏƒÎ¶2 = 0.5 and 0.25;
m(u) = sin(2Ï€ u); u i â€™s are i.i.d. U (0, 1), Îµi â€™s are i.i.d. N (0, 1). Moreover, we take
n = 300 and 600 and I = 2 and 5. We consider two cases: one is ÏƒÎ¶2 known and the
other ÏƒÎ¶2 unknown with two replications for each wi j .
The number of simulated realizations is 1, 000. The kernel function is the Gaussian
kernel and the bandwidth is selected by Cross-Validation. We calculate the sample
means and standard deviations (SDs) of the proposed estimators for the parametric
components Î²1 and Î²2 , and the error variance Ïƒ 2 in Sect. 2. We also calculate the sample
means and SDs of the naive estimators (neglecting the measurement errors) and the
benchmark estimators (xi1 and xi2 can be observed exactly). The naive estimators for
(Î²1 , Î²2 )T and Ïƒ 2 are defined as
 T (In âˆ’ P)W

Î²ÌŒ n = W
B

âˆ’1

 T (In âˆ’ P)
W
B Y

and
ÏƒÌŒn2 =

I
(Y âˆ’ WiT Î²ÌŒ n âˆ’ MÌŒn )T (In âˆ’ PB )(Y âˆ’ WiT Î²ÌŒ n âˆ’ MÌŒn )
n(I âˆ’ 1)

where


MÌŒn = In âˆ’ (In âˆ’ SPÌ„B )âˆ’1 (In âˆ’ S) (Y âˆ’ WÎ²ÌŒ n ).

123

Statistical inference for fixed-effects partially linear regression models with errors in variables

639

âˆ—

The benchmark estimators Î²Ìƒ n and ÏƒÌƒnâˆ—2 have the same definitions as those of Î²ÌŒ n and
ÏƒÌŒn2 except that W is replaced with X.
Simulation results are summarized in the following Tables 1 and 2.
From Tables 1 and 2 we make the following observations:
1. The naive estimators of the parametric component and error variance are biased.
These biases do not decrease as the sample size increases, and increase as the
variance of measurement errors increases.
2. The benchmark estimators and proposed estimators of the parametric component
and error variance are asymptotically unbiased.
3. There is a clear difference between the SDs of the benchmark estimators and those
of the proposed estimators. We think this difference is caused by the measurement
errors.
4. Whether ÏƒÎ¶2 is known or unknown has almost no influence on the results.
In addition, we also calculate the estimators of the nonparametric component m(Â·)
in which n = 300, I = 5 and ÏƒÎ¶2 = 0.5. Results are summarized in the following
Fig. 1. From Fig. 1, we see that the proposed estimator of the nonparametric component
outperforms the naive estimator. The latter is biased.
Example 2 The data are generated from the following fixed-effects partially linear
regression model

Table 1 The finite sample performance of the estimators for the parametric components Î²1 = 1.5, Î²2 = 1
and error variance Ïƒ 2 = 1 with ÏƒÎ¶2 known

ÏƒÎ¶2 = 0.5

âˆ—
Î²Ìƒn1

âˆ—
Î²Ìƒn2

n1
Î²

n2
Î²

Î²ÌŒn1

Î²ÌŒn2

ÏƒÌƒnâˆ—2


Ïƒn2

ÏƒÌŒn2

Mean

1.502

1.000

1.616

1.087

0.998

0.670

0.950

0.815

1.964

SD

0.085

0.088

0.206

0.204

0.097

0.100

0.113

0.299

0.229

Mean

1.498

0.999

1.570

1.049

0.995

0.664

0.981

0.869

1.998

SD

0.066

0.065

0.157

0.149

0.078

0.075

0.091

0.246

0.190

Mean

1.500

0.996

1.552

1.029

0.999

0.664

0.987

0.923

2.034

SD

0.059

0.059

0.137

0.128

0.068

0.068

0.079

0.212

0.165

Mean

1.501

0.999

1.539

1.023

1.001

0.666

0.986

0.922

2.035

SD

0.046

0.044

0.099

0.094

0.053

0.051

0.063

0.172

0.138

Mean

1.502

0.999

1.562

1.035

1.207

0.799

0.948

0.877

1.551

SD

0.082

0.088

0.145

0.143

0.099

0.100

0.108

0.201

0.179

5

Mean

1.499

0.999

1.536

1.022

1.199

0.800

0.965

0.900

1.567

SD

0.067

0.066

0.105

0.105

0.075

0.075

0.088

0.162

0.146

2

Mean

1.500

1.000

1.531

1.016

1.203

0.800

0.972

0.933

1.596

SD

0.057

0.060

0.094

0.093

0.066

0.068

0.078

0.140

0.125

Mean

1.502

0.998

1.517

1.011

1.201

0.799

0.979

0.951

1.610

SD

0.046

0.047

0.074

0.071

0.053

0.053

0.062

0.111

0.101

n

I

300

2
5

600

2
5

ÏƒÎ¶2 = 0.25

300

600

2

5

123

640

H. Zhou et al.

Table 2 The finite sample performance of the estimators for the parametric components Î²1 = 1.5, Î²2 = 1
and error variance Ïƒ 2 = 1 with ÏƒÎ¶2 unknown

ÏƒÎ¶2 = 0.5

n

I

300

2
5

600

2
5

ÏƒÎ¶2 = 0.25

300

2
5

600

2
5

âˆ—
Î²Ìƒn1

âˆ—
Î²Ìƒn2

n1
Î²

n2
Î²

Î²ÌŒn1

Î²ÌŒn2

ÏƒÌƒnâˆ—2


Ïƒn2

ÏƒÌŒn2

Mean

1.497

1.000

1.549

1.028

1.194

0.795

0.967

0.915

1.578

SD

0.086

0.084

0.154

0.135

0.100

0.095

0.110

0.215

0.183

Mean

1.499

0.999

1.533

1.029

1.198

0.803

0.969

0.909

1.577

SD

0.068

0.066

0.112

0.109

0.076

0.076

0.091

0.176

0.154

Mean

1.497

1.000

1.524

1.018

1.198

0.801

0.973

0.936

1.597

SD

0.056

0.061

0.092

0.096

0.065

0.069

0.081

0.149

0.134

Mean

1.500

1.001

1.517

1.015

1.200

0.801

0.980

0.951

1.611

SD

0.046

0.046

0.078

0.074

0.053

0.053

0.063

0.124

0.102

Mean

1.500

0.998

1.523

1.017

1.331

0.891

0.950

0.916

1.283

SD

0.085

0.086

0.112

0.114

0.093

0.096

0.111

0.159

0.149

Mean

1.500

0.999

1.517

1.010

1.334

0.888

0.968

0.939

1.304

SD

0.066

0.069

0.088

0.089

0.072

0.076

0.089

0.128

0.120

Mean

1.499

0.998

1.512

1.008

1.333

0.889

0.978

0.960

1.325

SD

0.058

0.060

0.076

0.079

0.064

0.067

0.079

0.117

0.112

Mean

1.498

0.999

1.508

1.004

1.332

0.887

0.986

0.974

1.338

SD

0.046

0.047

0.060

0.060

0.050

0.051

0.061

0.092

0.087

yi = BiT Î± +

8


xi j Î² j + m(Ui ) + Îµi and wi j = xi j + Î¶i j ,

j=1

for j = 1, . . . , 8 and i = 1, . . . , n where xi j â€™s are i.i.d. N (0.5, 1), Î¶i j â€™s are i.i.d.
N (0, ÏƒÎ¶2 ) and (Î²1 , . . . , Î²8 )T = (0, 0, 0, 0.5, 1.5, 2.0, 0, 0)T . Other symbols are same
as those in Example 1.
The means and SDs of RGMSEs over 1,000 simulated data are summarized in
Table 3. Here, the RGMSE means the Relative GMSE, i.e. the ratio of GMSE of
an underlying procedure to that of the proposed estimator without penalization. For
estimator Î²Ì„ n , the GMSE is defined as

GMSE = (Î²Ì„ n âˆ’ Î²)T


1 T
 âˆ’ I âˆ’ 1 ÏƒÎ¶2 I p (Î²Ì„ n âˆ’ Î²).
W (In âˆ’ 
P
)
W
B
n
I

In addition, the average number of zero coefficients is also reported in Table 3, in which
the column labeled â€œCâ€ presents the average, restricted only to the true zero coefficients, while the column labeled â€œEâ€ depicts the average of coefficients erroneously
set to 0.
From Table 3, we can see that the proposed covariate selection procedure performs
very well. Whether ÏƒÎ¶2 is known or unknown has almost no influence on the results.

123

Statistical inference for fixed-effects partially linear regression models with errors in variables

641

1.5

1

0.5

0

âˆ’0.5

âˆ’1

âˆ’1.5

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

1.5

1

0.5

0

âˆ’0.5

âˆ’1

âˆ’1.5

Fig. 1 The estimators of the nonparametric component m(Â·) with n = 300, I = 5 and ÏƒÎ¶2 = 0.5, ÏƒÎ¶2
known (left panel) and ÏƒÎ¶2 unknown (right panel). sin(2Ï€ u) (solid curve), the benchmark estimator (dotted

curve), the proposed estimator (dash-dotted curve) and the naive estimator (dashed curve)

123

642

H. Zhou et al.

Table 3 The finite sample performance of the covariate selection
ÏƒÎ¶2 (known) n

I Mean SD

C

ÏƒÎ¶2 (unknown) n

E

I Mean SD

C

E

0.5

300 2 0.522 0.246 4.642 0.010

0.5

300 2 0.521 0.250 4.645 0.015

0.5

300 5 0.532 0.241 4.657 0.001

0.5

300 5 0.519 0.247 4.671 0

0.5

600 2 0.526 0.247 4.652 0

0.5

600 2 0.520 0.246 4.688 0

0.5

600 5 0.490 0.246 4.762 0

0.5

600 5 0.497 0.243 4.751 0

0.25

300 2 0.426 0.231 4.916 0.003

0.25

300 2 0.411 0.228 4.929 0.004

0.25

300 5 0.415 0.233 4.919 0

0.25

300 5 0.426 0.231 4.913 0

0.25

600 2 0.404 0.230 4.919 0

0.25

600 2 0.420 0.235 4.928 0

0.25

600 5 0.394 0.226 4.958 0

0.25

600 5 0.414 0.230 4.951 0

6 Concluding remarks
We have studied the estimation and inference of the fixed-effects partially linear regression models when some of the covariates are measured with errors. We constructed
new estimators for the parametric, nonparametric and error variance by taking the
measurement errors into account, and showed that they were consistent and asymptotically normal. In addition, we proposed a variable selection procedure to select
the significant variables in the parametric part. The resulting estimate was shown to
possess an oracle property.
We have focused on the case where the covariates in regressors were measured with
additive errors. In some situations, this additive property of the measurement errors
may not be true (Hwang 1986). Future research will be needed for dealing with other
type of error structure.
Appendix: Proof of main results
In order to prove the main results, we first present several lemmas.
Lemma 1 Suppose that Assumptions 1 to 5 hold. Then
1 T

W (In âˆ’ P
B )W âˆ’ (I âˆ’ 1)/I  Î¶ â†’ p (I âˆ’ 1)/I  as n â†’ âˆž
n
where  is defined in Assumption 2.
Proof The proof is the same as that of Lemma A.2 in Fan et al. (2005).
Lemma 2 Under the conditions of Theorem 5, with probability tending to 1 as n â†’
âˆž, for any given estimator Î² âˆ—1 satisfying ||Î² âˆ—1 âˆ’ Î² 01 || = O p (n âˆ’1/2 ) and any constant
c,
L{(Î² âˆ—1T , 0)T )T } =

123

min

||Î² âˆ—2 ||â‰¤cn âˆ’1/2

L{(Î² âˆ—1T , Î² âˆ—2T )T }.

Statistical inference for fixed-effects partially linear regression models with errors in variables

643

Proof The proof is same as that of Lemma A.1 in Fan et al. (2004). We here omit the
detail.
Îµ, by the definition of 
Î²n
Proof of Theorem 1 Since 
Y=
BÎ± + 
XÎ² + (In âˆ’ S)M + 
it holds that

Î²n âˆ’ Î²


 T (In âˆ’ P)W
 âˆ’ (n âˆ’ G) Î¶ âˆ’1
(n âˆ’ G) Î¶ Î² + W
B


 T (In âˆ’ P) (
 + (In âˆ’ S)M + 
 T (In âˆ’ P)W
 âˆ’ (n âˆ’ G) Î¶ âˆ’1
Â·W
X âˆ’ W)Î²
Îµ = W
B
B

 T (In âˆ’ P)(

T
T
Â· (n âˆ’ G) Î¶ Î² + W
Îµ
B X âˆ’ W)Î² + W (In âˆ’ P
B )(In âˆ’ S)M + W (In âˆ’ P
B )

 âˆ’ (n âˆ’ G) Î¶
 T (In âˆ’ P)W
= W
B

âˆ’1

According to the proof of Theorem 1 of Fan et al. (2005) it holds that
T
 T (In âˆ’ P)(

2
W
B X âˆ’ W)Î² = âˆ’(D + Î¶ ) (In âˆ’ P
B )Î¶ Î² + o p (n )
1
1

= âˆ’(D + Î¶ )T (In âˆ’ PB )Î¶ Î² + o p (n 2 )
 T (In âˆ’ P)(In âˆ’ S)M = o p (n 2 )
W
B
1

and
T
T
 T (In âˆ’ P)
2
2
W
B Îµ = (D + Î¶ ) (In âˆ’ P
B )Îµ + o p (n ) = (D + Î¶ ) (In âˆ’ PB )Îµ + o p (n )
1

1

where D = (D1 , . . . , Dn )T = (X1 âˆ’ E(X1 |U1 ), . . . , Xn âˆ’ E(Xn |Un ))T and Î¶ =
(Î¶ 1 , . . . , Î¶ n )T .
Therefore, applying Lemma 1 and the fact (A + aB)âˆ’1 â†’ Aâˆ’1 as a â†’ 0 we have

âˆ’1
âˆš
(I âˆ’1)
n(
Î² n âˆ’Î²) =

I
 
1  T
Â· âˆš D (Inâˆ’PB )(Îµâˆ’Î¶ Î²)+Î¶ T (In âˆ’PB )Îµ+ (nâˆ’G) Î¶ âˆ’Î¶ T (In âˆ’PB )Î¶ Î² +o p (1).
n
Since
Cov(DT (In âˆ’ PB )(Îµ âˆ’ Î¶ Î²), Î¶ T (In âˆ’ PB )Îµ) = 0,

Cov(D (In âˆ’ PB )(Îµ âˆ’ Î¶ Î²), (n âˆ’ G) Î¶ âˆ’ Î¶ T (In âˆ’ PB )Î¶ Î²) = 0
T

and

Cov(Î¶ T (In âˆ’ PB )Îµ, (n âˆ’ G) Î¶ âˆ’ Î¶ T (In âˆ’ PB )Î¶ Î²) = 0
we have


 
1 
lim Cov âˆš DT (In âˆ’PB )(Îµ âˆ’ Î¶ Î²) + Î¶ T (In âˆ’PB )Îµ + (n âˆ’ G) Î¶ âˆ’ Î¶ T (In âˆ’PB )Î¶ Î²
nâ†’âˆž
n
I âˆ’1
1.
=
I

123

644

H. Zhou et al.

Then, applying the central limit theorem the proof is complete.
âˆ’1 
1 
âˆ’1 being a consistent estimator of  âˆ’1  1  âˆ’1 is straightThe proof of 
forward. We here omit the details.

Proof of Theorem 2 The proof of Theorem 2 is the same as that of Theorem 3 in Fan
et al. (2005). We here omit the details.

Proof of Theorem 3 According to the definition, 
Ïƒn2 can be decomposed as

I
T
T




(Îµâˆ’Î¶ Î² n ) (In âˆ’PB )(Îµâˆ’Î¶ Î² n )âˆ’ Î² n  Î¶ Î² n
n(I âˆ’1)


I
 n T (In âˆ’PB ) X(Î² âˆ’
n
X(Î² âˆ’
Î² n )+Mâˆ’ M
+
Î² n )+Mâˆ’ M
n(I âˆ’1)

2I
 n T (In âˆ’PB )(Îµâˆ’Î¶ 
X(Î² âˆ’
Î² n )+Mâˆ’ M
+
Î² n ) = J1 + J2 + J3 . say
n(I âˆ’1)



Ïƒn2 =

By the root-n consistency of 
Î²n,
J1 âˆ’ Ïƒ 2 =

1
I
(Îµ âˆ’ Î¶ Î²)T (In âˆ’ PB )(Îµ âˆ’ Î¶ Î²) âˆ’ (Ïƒ 2 + Î² T  Î¶ Î²) + o p (n âˆ’ 2 ).
n(I âˆ’ 1)

Let ÎµÌ„g = I âˆ’1

I

i=1 Îµ(gâˆ’1)I +i

and similarly for all the other â€œbarredâ€ variables.

I
(Îµ âˆ’ Î¶ Î²)T (In âˆ’ PB )(Îµ âˆ’ Î¶ Î²)
n(I âˆ’ 1)


I
1
T
T
(Îµ âˆ’ Î¶ Î²) In âˆ’ IG âŠ— 1 I 1 I (Îµ âˆ’ Î¶ Î²)
=
n(I âˆ’ 1)
I
n
G 
2


I
I2
T
=
ÎµÌ„g âˆ’ Î¶Ì„ Tg Î²
(Îµi âˆ’ Î¶ i Î²) âˆ’
n(I âˆ’ 1)
n(I âˆ’ 1)
g=1

i=1

=

I
n(I âˆ’ 1)

G 
I 


T
T
Îµ(gâˆ’1)I +i âˆ’ Î¶ (gâˆ’1)I
+i Î² âˆ’ ÎµÌ„g + Î¶Ì„ g Î²

2

.

g=1 i=1

2
I 
T
T
Îµ(gâˆ’1)I +i âˆ’ Î¶ (gâˆ’1)I
Let Î¶g = i=1
+i Î² âˆ’ ÎµÌ„g + Î¶Ì„ g Î²) . Obviously, Î¶g â€™s are i.i.d.
random variables with

EÎ¶g =

âŽ§
I âŽ¨

(I âˆ’1)2
i=1

âŽ©

I2

âŽ«
I
âŽ¬

1
2
T
T
E(Îµ(gâˆ’1)I+1 âˆ’Î¶ (gâˆ’1)I+i
Î²)2 + 2
E(Îµ(gâˆ’1)I+i1 âˆ’Î¶ (gâˆ’1)I+i
Î²)
1
âŽ­
I

= (I âˆ’1) (Ïƒ +Î² T  Î¶ Î²).
2

123

i 1 =i

Statistical inference for fixed-effects partially linear regression models with errors in variables

645

Then, by some simple calculation, we have
$ %2
$ %
Var Î¶g = E Î¶g âˆ’(EÎ¶g )2
1
6I (I âˆ’1)
= I E(Îµ1 âˆ’Î¶ 1T Î²)4 + I (I âˆ’1)(Ïƒ 2 +Î² T  Î¶ Î²)2 + E(Îµ1 âˆ’Î¶ 1T Î²)4 +
I
2I 2


2
I E(Îµ1 âˆ’Î¶ 1T Î²)4 + I (I âˆ’1)(Ïƒ 2 +Î² T  Î¶ Î²)2
Ã—(Ïƒ 2 +Î² T  Î¶ Î²)2 âˆ’
I
âˆ’ (I âˆ’1)2 (Ïƒ 2 +Î² T  Î¶ Î²)2




1
3
= I + âˆ’2 E(Îµ1 âˆ’Î¶ 1T Î²)4 +(I âˆ’1) âˆ’1+
(Ïƒ 2 +Î² T  Î¶ Î²)2 .
I
I
Therefore,



I
Var âˆš
Î¶g
n(I âˆ’ 1)
G



i=1

=




1
âˆ’ 2 E(Îµ1 âˆ’ Î¶ 1T Î²)4
I



3
âˆ’ 1 (Ïƒ 2 + Î² T  Î¶ Î²)2 .
+ (I âˆ’ 1)
I
I
(I âˆ’ 1)2

I+

Moreover, according to Theorems 1 and 2 we have


1
I
 n T X(Î² âˆ’ 
 n = o p (n âˆ’ 2 ).
X(Î² âˆ’ 
Î²n) + M âˆ’ M
Î²n) + M âˆ’ M
n(I âˆ’ 1)

J2 â‰¤

So in order to complete the proof, we just need to show that
J3 =


1
2I
 n T (In âˆ’ PB )(Îµ âˆ’ Î¶ 
X(Î² âˆ’ 
Î²n) + M âˆ’ M
Î² n ) = o p (n âˆ’ 2 ). (A1)
n(I âˆ’ 1)

Denote
T
T

(gâˆ’1)I +i = (Îµ(gâˆ’1)I +i âˆ’ Î¶ (gâˆ’1)I
+i Î² n ) âˆ’ ÎµÌ„g + Î¶Ì„ g Î² n .

J3 can be further decomposed as



2I
T
(gâˆ’1)I +i X(gâˆ’1)I
(Î² âˆ’ 
Î² n ) âˆ’ XÌ„gT (Î² âˆ’ 
Î²n)
+i
n(I âˆ’ 1)
I

G

J3 =

g=1 i=1


2I
(gâˆ’1)I +i {m(U(gâˆ’1)I +i ) âˆ’ m
n (U(gâˆ’1)I +i )}
n(I âˆ’ 1)
G

+

I

g=1 i=1



2I
Â¯ n (U )g
(gâˆ’1)I +i mÌ„(U )g âˆ’ m

n(I âˆ’ 1)
G

âˆ’

I

g=1 i=1

= J31 + J32 âˆ’ J33 say.

123

646

H. Zhou et al.

Since
I
G 


1

(gâˆ’1)I +i X(gâˆ’1)I +i = O p (n 2 ) and

g=1 i=1

I
G 


1

(gâˆ’1)I +i XÌ„g = O p (n 2 ),

g=1 i=1

combining the root-n consistency of 
Î² n we obtain J31 = o p (n âˆ’1/2 ).
Let di denote an n dimensional column vector with the ith element being 1 and
others 0. According to the definition of m
n (Â·), we have


m
n (Ui ) = diT In âˆ’ (In âˆ’ SPÌ„B )âˆ’1 (In âˆ’ S) X(Î² âˆ’ 
Î²n)


+diT In âˆ’ (In âˆ’ SPÌ„B )âˆ’1 (In âˆ’ S) M


+diT In âˆ’ (In âˆ’ SPÌ„B )âˆ’1 (In âˆ’ S) (Îµ âˆ’ Î¶ 
Î²n)
+diT (In âˆ’ SPÌ„B )âˆ’1 S(In âˆ’ PÌ„B )BÎ±.
Since
have

G

i=1 Î±i

= 0, it is easy to show that (In âˆ’ SPÌ„B )âˆ’1 S(In âˆ’ PÌ„B )BÎ± = 0. Thus we




2I
T
In âˆ’(In âˆ’SPÌ„B )âˆ’1 (In âˆ’S) X(Î² âˆ’
(gâˆ’1)I+i d(gâˆ’1)I+i
Î²n)
n(I âˆ’1)
G

J32 =

I

g=1 i=1


2I
âˆ’1
T
(gâˆ’1)I +i d(gâˆ’1)I
+i (In âˆ’ SPÌ„B ) (In âˆ’ S)M
n(I âˆ’ 1)
G

âˆ’

I

g=1 i=1




2I
T
In âˆ’(In âˆ’SPÌ„B )âˆ’1 (In âˆ’S) (Îµâˆ’Î¶ 
(gâˆ’1)I+i d(gâˆ’1)I+i
Î²n)
n(I âˆ’ 1)
G

+

I

g=1 i=1

= J321 + J322 + J323 say.
According to the root-n consistency of 
Î² n and the proof of Theorem 3 of Fan et al.
(2005) it holds that

2
J321

=

2I
n(I âˆ’1)

&
&2
2 &
I
&

& G 
&
âˆ’1
T
&

(gâˆ’1)I +i In âˆ’d(gâˆ’1)I +i (In âˆ’SPÌ„B ) (In âˆ’S) &
&
& Â· X(Î² âˆ’ Î² n )
&g=1 i=1
&

= O(n âˆ’1 ) Â· O p (h 4 + 1/nh) = o p (n âˆ’1 )

This implies that J321 = o p (n âˆ’1/2 ). On the other hand, since
(In âˆ’ S)M =

Î¼2 

(m (U1 ), . . . , m (Un ))T h 2 + o p (h 2 ),
2

we can show that J322 = o p (n âˆ’1/2 ). By the same argument as proving Lemma 4(2)
of You and Chen (2006) we can show that J323 = o p (n âˆ’1/2 ). Thus (A1) holds. This

123

Statistical inference for fixed-effects partially linear regression models with errors in variables

647

implies that J32 = o p (n âˆ’1/2 ). Following the same line, we can show that J33 =
o p (n âˆ’1/2 ).
We now show the consistency of 
Îºn . To facilitate the notations we let
T
T


âˆ‡(gâˆ’1)I +i = X(gâˆ’1)I
+i (Î² âˆ’ Î² n ) âˆ’ XÌ„g (Î² âˆ’ Î² n )

Â¯ n (U )g
n (U(gâˆ’1)I +i ) âˆ’ mÌ„(U )g + m

+m(U(gâˆ’1)I +i ) âˆ’ m
T
T
and Ï‘g,i = Îµ(gâˆ’1)I +i âˆ’ Î¶ (gâˆ’1)I
+i Î² âˆ’ (ÎµÌ„g âˆ’ Î¶Ì„ g Î²). Then it holds that

I
I
I
n
G
G 
G 


1  4  4
4
3
i =
Ïˆ
Ï‘g,i +
âˆ‡(gâˆ’1)I
+
4
Ï‘g,i
âˆ‡(gâˆ’1)I +i
+i
n
g=1 i=1

i=1

+6

G 
I


g=1 i=1

2
2
Ï‘g,i
âˆ‡(gâˆ’1)I
+i + 4

g=1 i=1

g=1 i=1

G 
I


3
Ï‘g,i âˆ‡(gâˆ’1)I
+i

g=1 i=1

= Q1 + Q2 + Q3 + Q4 + Q5.
For Q 1 , we have
âŽ§
âŽ«4
I
I
âŽ¬

1 âŽ¨
1
Q1 =
E Îµi âˆ’Î¶ iT Î² âˆ’
(Îµi1 âˆ’Î¶ iT1 Î²) +o p (1)
âŽ©
âŽ­
I
I
i=1
i 1 =1
âŽ¡
âŽ«2 âŽ¤
âŽ§
I
I
âŽ¬
âŽ¨


6
âŽ¥
âŽ¢
E âŽ£(Îµi âˆ’ Î¶ iT Î²)2
(Îµi1 âˆ’ Î¶ iT1 Î²) âŽ¦
= o p (1)+ E(Îµ1 âˆ’ Î¶ iT Î²)4 + 3
âŽ­
âŽ©
I
i=1

âŽ§
âŽ¨

âŽ«
âŽ¬

i 1 =1

.4
I
I
I

3 
4 
3
T
T
T
E (Îµi âˆ’ Î¶i Î²)
(Îµi1 âˆ’ Î¶ i1 Î²) âˆ’ 4
(Îµi âˆ’ Î¶ i Î²)
âˆ’ 2
âŽ©
âŽ­ I
I
i=1
i 1 =1
i=1




9
6
3
4
(I âˆ’ 1)
4
T
6âˆ’
E(Îµ1 âˆ’ Î¶ i Î²) +
(Ïƒ 2 + Î² T  Î¶ Î²)2 .
= 1+ 2 âˆ’ 3 âˆ’
I
I
I
I2
I
-

I

4
Moreover, according to HÃ¶lder inequality and the fact that G
g=1
i=1 âˆ‡(gâˆ’1)I +i =
o p (1) we can show that Q s = o p (1) for s = 2, 3, 4 and 5. Thus, the consistency result
of 
Îºn follows.
Proof of Theorem 4 Denote Î±n = n âˆ’1/2 + an . It is sufficient to show that for any
given Î¾ > 0, there exists a large constant c such that

P


inf L(Î² 0 + Î±n u) â‰¥ L(Î² 0 ) â‰¥ 1 âˆ’ Î¾.

||u||=c

(A2)

This implies, with probability at least 1 âˆ’ Î¾ , that there exists a local minimizer in
the ball {Î² 0 + Î±n u : ||u|| â‰¤ c}. Define Dn (u) = L(Î² 0 + Î±n u) âˆ’ L(Î² 0 ). Note that

123

648

H. Zhou et al.

pÎ»s (0) = 0 and pÎ»s (|Î²0s |) is nonnegative. Therefore, it holds that
n âˆ’1 Dn (u)
âŠ—2
 0
1 /  
 0 )T (In âˆ’ P) âŠ—2
(Y âˆ’ W(Î² 0 + Î±n u))T (In âˆ’ P
=
âˆ’ (
Y âˆ’ WÎ²
B)
B
2n

I âˆ’1
âˆ’
(Î² 0 + Î±n u)T  Î¶ (Î² 0 + Î±n u) âˆ’ Î² 0T  Î¶ Î² 0
2I
q

+
{ pÎ»s (|Î²0s + Î±n u s |) âˆ’ pÎ»s (|Î²0s |)}
s=1

= J1 +

q


pÎ»s (|Î²0s + Î±n u s |) âˆ’ pÎ»s (|Î²0s |)



say.

s=1

Obviously,
1 2 T T
T

 

{Î± u W (In âˆ’ P
B )Wu âˆ’ 2Î±n (Y âˆ’ WÎ² 0 ) (In âˆ’ P
B )Wu}
2n n

I âˆ’1 2 T
Î±n u  Î¶ u âˆ’ 2Î±n uT  Î¶ Î² 0
âˆ’
2I


Î±n2 T 1  T
 âˆ’ I âˆ’ 1 Î¶ u
u
W (In âˆ’ P
=
)
W
B
2
n
I


1
 + I âˆ’ 1 uT  Î¶ Î² 0
(Îµ âˆ’ Î¶ Î² 0 + M)T (In âˆ’ S)T (In âˆ’ P
âˆ’Î±n
)
Wu
B
n
I


2
Î±
1 T
âˆ’ 21

= n uT
Î±n ||u||) = J2 + J3 say
W (In âˆ’ P
B )W âˆ’ (I âˆ’ 1)/I  Î¶ u+ O p (n
2
n

J1 =

By Lemma 1 it holds that
1
1 T
I âˆ’1
I âˆ’1

W (In âˆ’ P
Î¶ =
Cov(X1 âˆ’ E(X1 |U1 )) + O p (n âˆ’ 2 ) > 0,
B )W âˆ’
n
I
I

for n large enough, and hence J2 is of the order c2 Î±n2 . Note that n âˆ’1/2 Î±n = O p (Î±n2 )
because Î±n = n âˆ’1/2 + an . By choosing a sufficiently large c, J2 will dominate J3 ,
uniformly in ||u|| = c. Moreover,
q


pÎ»s (|Î²0s + Î±n u s |) âˆ’ pÎ»s (|Î²0s |)



s=1

âˆš
âˆš
is bounded by qÎ±n an ||u|| + Î±n2 bn ||u||2 = O p {cÎ±n2 ( q + bn c)} by the Taylor expansion and Cauchy-Schwarz inequality, where q is the number of components of Î² 01 .
âˆš
cÎ±n2 ( q +bn c) is dominated by J1 as bn â†’ 0, by taking c sufficiently large. Therefore,
(A2) holds for sufficiently large c. This completes the proof of Theorem 4.


123

Statistical inference for fixed-effects partially linear regression models with errors in variables

649

Proof of Theorem 5 Part (i) directly follows from Lemma 2. Now we prove Part (ii).
Using arguments similar to that of the proof of Theorem 4, it can be shown that
there exists a Î²Ìƒn1 in Theorem 5 that is a root-n consistent minimizer of L{(Î² 1T , 0T )T },
satisfying the penalized corrected profile least squares equations:
1
âˆ‚L (Î² 1T , 0T )T 1
1
= 0.
1
âˆ‚Î² 1
Î² 1 =Î²Ìƒ n1
Further, we have
1
âˆ‚ L (Î² 1T , 0T )T 1
1
1
âˆ‚Î² 1
Î² 1 =Î²Ìƒ n1


q
n(I âˆ’1) T (1)
1   (1)
1
  (1)
âˆ‚ 2 (Yâˆ’ W Î² 1 )T (In âˆ’P
B )(Yâˆ’ W Î² 1 )âˆ’ 2I Î² 1  Î¶ Î² 1 +n
j=1 Î» j p j (|Î² j |) 1
1
=
1
âˆ‚Î² 1
Î² 1=Î²Ìƒ n1
 (1)T (In âˆ’ P)(
 (1)
= âˆ’W
B Y âˆ’ W Î² 01 )


n(I âˆ’ 1) (1)
 (1)T (In âˆ’ P)W
 (1) âˆ’ n(I âˆ’ 1)  (1) (Î²Ìƒ n1 âˆ’ Î² 01 )
 Î¶ Î² 01 + W
âˆ’
B
Î¶
I
I
/
0
+ n b + { + o p (1)}(Î²Ìƒ n1 âˆ’ Î² 01 )
(1)

 (1) consists of the first q columns of W,
  consists of the first q rows and
where W
Î¶
columns of  Î¶ . Therefore, it holds that
/
 (1)T (In âˆ’ P)W
 (1) âˆ’
(W
B

n(I âˆ’1) (1)
 Î¶ ) + n{
I

0
+ o p (1)} (Î²Ìƒ n1 âˆ’ Î² 01 ) + nb

 (1)T (In âˆ’ P)(
 (1)
=W
B Y âˆ’ W Î² 01 ) +

n(I âˆ’1) (1)
 Î¶ Î² 01 .
I

Similar to Theorem 1, we can show that




1  (1)T

 (1) Î² 01 )+ n(I âˆ’1)  (1) Î² 01 â†’ D N 0, I âˆ’1  (1)
(In âˆ’P
)(
Yâˆ’
W
âˆš W
B
Î¶
1
I
I
n
as n â†’ âˆž
(1)

where  1 consists of the first q rows and columns of  1 . Thus, combining Lemma 1,
it follows that
âˆš  (1)
n  +


Î²Ìƒ n1 âˆ’ Î² 01 + ( (1) +


)âˆ’1 b â†’ D N (0,

I
 (1) ) as n â†’ âˆž
I âˆ’1 1

where  (1) consists of the first q rows and columns of . This completes the proof of
Theorem 5.

123

650

H. Zhou et al.

References
Ahn SC, Schmidt P (2000) Estimation of linear panel data models using GMM. In: MÃ¡tyÃ¡s L (ed) Generalized
method of moments estimation, Chap. 8. Cambridge university press, Cambridge
Andrews DWK (1996) Nonparametric kernel estimation for semiparametric models. Econ Theory 11:560â€“
596
Antoniadis A, Fan J (2001) Regularization of wavelet approximations. With discussion and a rejoinder by
the authors. J Am Stat Assoc 96:939â€“967
Baltagi BH (1995) Econometric analysis of panel data. Wiley
Baltagi BH, Li D (2002) Series estimation of partially linear panel data models with fixed effects. Ann Econ
Finan 3: 103â€“116
Carroll RJ, Ruppert D, Stefanski LA (1995) Measurement error in nonlinear models. Chapman & Hall,
London
Cheng CL, Van Ness J (1999) Statistical regression with measurement error. Arnold, London
Delecroix M, HÃ¤rdle W, Hristache M (2003) Efficient estimation in conditional single-index regression.
J Multivar Anal 86:213â€“226
Engle RF, Granger WJ, Rice J, Weiss A (1986) Semiparametric estimates of the relation between weather
and electricity sales. J Am Stat Assoc 80:310â€“319
Fan J (1991) On the optimal rates of convergence for nonparametric deconvolution problems. Ann Stat
19:1257â€“1272
Fan J (1993) Adaptively local one-dimensional subproblems with applications to a deconvolution problem.
Ann Stat 21:600â€“610
Fan J, Huang T (2005) Profile likelihood inferences on semiparametric varying-coefficient partially linear
models. Bernoulli 11:1031â€“1057
Fan J, Li R (2001) Variable selection via penalized likelihood. J Am Stat Assoc 85:1348â€“1360
Fan J, Li R (2004) New estimation and model selection procedures for semiparametric modeling in longitudinal data analysis. J Am Stat Assoc 99:710â€“723
Fan J, Truong YK (1993) Nonparametric regression with errors-in-variables. Ann Stat 21:1900â€“1925
Fan J, Yao Q, Cai Z (2003) Adaptive varying-coefficient linear models. J R Stat Soc Ser B 65:57â€“80
Fan J, Tam P, Vande Woude G, Ren Y (2004) Normalization and analysis of cDNA micro-arrays using
within-array replications applied to neuroblastoma cell response to a cytokine. Proc Natl Acad Sci
USA 105:1135â€“1140
Fan J, Peng H, Huang T (2005) Semilinear high-dimensional model for normalization of microarray data:
a theoretical analysis and partial consistency. J Am Stat Assoc 100:781â€“813
Fuller WA (1987) Measurement error models. Wiley, New York
Hwang JT (1986) Multiplicative errors-in-variables models with applications to the recent data released by
the US department of energy. J Am Stat Assoc 81:680â€“688
Liang H, HÃ¤rdle W, Carroll RJ (1999) Estimation in a semiparametric partially linear errors-in-variables
model. Ann Stat 27:1519â€“1535
You J, Chen G (2006) Estimation in a semiparametric varying-coefficient partially linear errors-in-variables
model. J Multivar Anal 97:324â€“341
You J, Zhou H (2006) Weighted difference-based series estimation for partially linear in-slide models.
Submitted to Statist. Sinica
Zhou H, Wang CY (2000) Failure time regression analysis with measurement error in covariates. J R Stat
Soc B 62:657â€“665
Zhu L, Cui H (2003) A semiparametric regression model with errors in variables. Scan J Stat 30:429â€“442

123

