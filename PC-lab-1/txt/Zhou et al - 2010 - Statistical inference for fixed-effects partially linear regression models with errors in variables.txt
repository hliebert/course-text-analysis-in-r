Stat Papers (2010) 51:629–650
DOI 10.1007/s00362-008-0150-3
REGULAR ARTICLE

Statistical inference for fixed-effects partially linear
regression models with errors in variables
Haibo Zhou · Jinhong You · Bin Zhou

Received: 6 December 2005 / Accepted: 20 May 2008 / Published online: 18 June 2008
© Springer-Verlag 2008

Abstract Fixed-effects partially linear regression models are useful tools to analyze
data from economic, genetic and other fields. In this paper, we consider estimation
and inference procedures when some of the covariates are measured with errors. The
previously proposed estimations, including difference-based series estimation (Baltagi
and Li in Ann Econ Finan 3:103–116, 2002) and profile least squares estimation
(Fan et al. in J Am Stat Assoc 100:781–813, 2005) are no longer consistent because
of the attenuation. We propose a new estimation by taking the measurement errors
into account. Our proposed estimators are shown to be consistent and asymptotically
normal. Consistent estimations of the error variance are also developed. In addition,
we propose a variable-selection procedure to variable selection in the parametric part.
The procedure is an extension of the nonconcave penalized likelihood (Fan and Li in
J Am Stat Assoc 85:1348–1360, 2001), which simultaneously selects the important
variables and estimates the unknown parameters. The resulting estimate is shown to
possess an oracle property. Extensive simulation studies are conducted to illustrate the
finite sample performance of the proposed procedures.

The paper was supported by a grant from National Institute of Health (CA 79949).
H. Zhou (B)
Department of Biostatistics, University of North Carolina at Chapel Hill,
Chapel Hill, NC 27599-7420, USA
e-mail: zhou@bios.unc.edu
J. You
Department of Statistics, Shanghai University of Finance and Economics,
Shanghai 200433, People’s Republic of China
e-mail: jinhongyou@gmail.com
B. Zhou
Department of Management and Insurance, School of Finance and Statistics,
East China Normal University, Shanghai 200062, People’s Republic of China
e-mail: bin.zhou@afgtrust.com

123

630

H. Zhou et al.

Keywords Fixed effect · Partially linear · Measurement errors · Profile least
squares · Variable selection
Mathematics Subject Classification (2000)

Primary 62H12 · Secondary 62A10

1 Introduction
Because mis-specification in fully parametric models could lead to severe modeling
biases, much interests and efforts in statistical research have been devoted to nonparametric models. However, a fully nonparametric approach could be too flexible to draw
concise conclusions, and if there exists a large number of covariates, it will make a fully
nonparametric analysis infeasible due to the curse of dimensionality. Semiparametric
models, which are a compromise between a general nonparametric specification and
a fully parametric specification, have been gaining much attention in statistical literature and have broad applications in medical research, biology, genomics, economics
and education. Semiparametric models have various forms including: partially linear
regression models (Engle et al. 1986), partially nonlinear regression models (Andrews
1996), single-index regression model (Delecroix et al. 2003), varying-coefficient index
regression model (Fan et al. 2003), etc. In this paper, we will consider the fixed-effects
partially linear regression model (Baltagi and Li 2002; Fan et al. 2004).
A fixed-effects partially linear regression model can be written as
Yi = BiT α + XiT β + m(Ui ) + εi , i = 1, . . . , n

(1.1)

where Yi ’s are the responses, Xi and Ui ’s are the design points, β = (β1 , . . . , β p )T
is an unknown parameter vector, m(·) is an unknown function and εi ’s are random
errors. In addition, B = (B1 , . . . , Bn )T = IG ⊗ 1 I with ⊗ the Kronecker product,
IG the G × G identity matrix and 1 I the vector of length I with all elements 1, and
α = (α1 , . . . , αG )T is an unknown
 G × 1 vector representing the fixed effects. For
identification, we assume that G
g=1 αg = 0.
Model (1.1) is different from the usual partially linear regression model since G
is dependent on n through G = n/I . In many practical designs, I is usually a small
number in the single digit while G can be much larger, in an order of hundreds or
larger. For example, for the cDNA microarray data used by Fan et al. (2004, 2005), I
is 2. The number of the parameters αg , on the other hand, grows with the sample size.
Throughout this paper, we will assume that I is kept fixed while n → ∞.
Model (1.1) can be viewed as an extension of the usual panel data parametric
regression model with fixed effects to the semiparametric context. The panel data
regression model with fixed effects would be an appropriate specification if one were
interested in a specific set of subjects. Such a model has been widely used in econometric analysis. The details can be found in the research of Baltagi (1995) and
Ahn and Schmidt (2000). Recently, the fixed-effects model has also found its application in the genomics. For instance, Fan et al. (2004, 2005) used model (1.1) to conduct
the microarray analysis of the neuroblastoma cell in response to macrophage migration
inhibitory factor (MIF).

123

Statistical inference for fixed-effects partially linear regression models with errors in variables

631

It has been well documented in the literature that there often exist covariate measurement errors in many practical problems (Carroll et al. 1995). Handling the measurement errors in covariates is generally a challenge for statistical analysis. For the
past two decades, regression analysis with measurement errors had much progress.
A detailed study can be found in the research of Fuller (1987); Cheng and Van Ness
(1999); Carroll et al. (1995); Fan (1991, 1993); Fan and Truong (1993); Liang et al.
(1999); Zhou and Wang (2000) and so on.
There are few results about the model (1.1), however, when some or all of the
covariates are measured with errors. We will fill this gap in this paper. Specifically,
we consider the case where Xi ’s are measured with an additive error, and Ui ’s are
error-free. That is we can not observe Xi directly but instead we observe Wi with
Wi = Xi + ζ i ,

(1.2)

Here ζ i is a measurement error and independent of (XiT , Ui , εi )T . We assume that
Cov(ζ i ) =  ζ with  ζ assumed to be known, as in the papers of Hwang (1986);
Zhu and Cui (2003), and others. When  ζ is unknown, we can estimate it by partial
replication (Carroll et al. 1995, Chap. 3). Because of the attenuation the previously
proposed estimators, such as difference-based series estimator (DSE) (Baltagi and Li
2002) and profile least squares estimator (PLSE) (Fan et al. 2005), are no longer consistent. We propose a new estimation by correcting the PLSE. The resulting estimator
is shown to be consistent and asymptotically normal. Consistently estimating the error
variance is also considered.
As in parametric regression models, variable selection is also important for using the
model (1.1). The number of variables in the parametric part of model (1.1) can easily
be large when nonlinear terms and interactions between covariates are introduced to
reduce possible modeling biases. We develop a class of variable procedures to select
significant variables in the parametric part of model (1.1). The proposed procedures are
based on the combination of the nonconcave penalty and corrected profile least squares
techniques. The asymptotic normality of the resulting estimators is derived. Same as
the procedure of Fan and Li (2001), with a proper choice of regularization parameters
and penalty functions, the proposed variable selection procedures are shown to perform
as well as an oracle estimate.
The layout of the remainder of this paper is as follows. In Sect. 2, we propose
new estimators for the parametric, nonparametric components and error variance by
taking the measurement error into account. A large sample theory is developed for these
estimators in Sect. 3. In Sect. 4, we propose a variable selection method for the covariate
of the parametric part. Some simulation studies are presented in Sect. 5. Conclusions
are given in Sect. 6. All proofs of main results are relegated to the Appendix.
2 The proposed estimations
When Xi ’s are observable we can apply the DSE or PLSE to estimate the parametric
and nonparametric components of the model (1.1). According to You and Zhou (2006),
the DSE is not efficient when I ≥ 3. Therefore, we construct new estimators based

123

632

H. Zhou et al.

on the PLSE. For convenience, we first introduce the PLSE. Suppose that {Yi , Xi , Ui }
is an i.i.d. sample from model (1.1). For any given α and β, the model (1.1) can be
written as
Yi − BiT α − XiT β = m(Ui ) + εi , i = 1, . . . , n.

(2.1)

This transforms the model (1.1) into the usual nonparametric model. Now, applying a
local linear regression technique in a small neighborhood of u 0 , one can approximate
m(u) locally by a linear function


m(u) ≈ m(u 0 ) + m (u 0 )(u − u 0 ) ≡ a + b(u − u 0 )


with m (·) = ∂m(·)/du. This leads to the following weighted local least squares
problem: find {(a, b)} to minimize
n



Yi − BiT α − XiT β − a − b(Ui − u 0 )

2

K h (Ui − u 0 ),

(2.2)

i=1

where K (·) is a kernel function, h is a bandwidth and K h (·) = K (·/ h)/ h.
The solution to problem (2.2) is given by
(
a (u 0 ), h
b(u 0 ))T = (DuT 0 ωu 0 Du 0 )−1 DuT 0 ωu 0 (Y − Bα − Xβ)
where Y = (Y1 , . . . , Yn )T , X = (X1 , . . . , Xn )T , B is defined in previous section,
⎛

Du 0

1
⎜ ..
=⎝.
1

U1 −u 0
h

..
.

⎞
⎟
⎠ , and ωu 0 = diag(K h (U1 − u 0 ), . . . , K h (Un − u 0 )).

Un −u 0
h

In (2.1) m(Ui ) is replaced with 
a (Ui ). Then we have
T
T
Yi − BiT α − XiT β ≈ (1, 0)(DU
ω D )−1 DU
ω (Y − Bα − Xβ) + εi , (2.3)
i Ui Ui
i Ui

for i = 1, . . . , n. Denote
i = Yi − (1, 0)(DT ωUi DUi )−1 DT ωUi Y,
Y
Ui
Ui
T
T

XiT = XiT − (1, 0)(DU
ω D )−1 DU
ω X,
i Ui Ui
i Ui

and
T
T

Bi = Bi − (1, 0)(DU
ω D )−1 DU
ω B.
i Ui Ui
i Ui

123

Statistical inference for fixed-effects partially linear regression models with errors in variables

633

Model (2.3) can be written as
i ≈ 
BiT α + 
XiT β + εi , i = 1, . . . , n.
Y

(2.4)

By (2.4), the least squares technique and partialing out formula, we have an estimator
of β as follows:
∗
 −1 T

XT (In − P
β̃ n = (
B )X) X (In − P
B )Y

(2.5)

where 
X = (In − S)X with
⎞
T
T
ω D )−1 DU
ω
(1, 0)(DU
1 U1 U1
1 U1
⎟
⎜
..
S=⎝
⎠
.
T
−1 DT ω
ω
D
)
(1, 0)(DU
Un Un
n Un Un
⎛

−
 T  −T
and similarly for all the other “hatted” matrices, P
B = B(B B) B , and A denotes
the Moore Penrose inverse. β̃ n is the PLSE.
However, Xi ’s can not be observed in our case and we just have Wi . If we ignore
the measurement error and replace Xi with Wi in (2.5), (2.5) can be used to show that
the resulting estimate is inconsistent. The form of (2.5) suggests even more. It is well
known that in linear regression or partially linear regression, inconsistency caused
by the measurement error can be overcome by applying the so-called “correction
for attenuation” (Fuller 1987; Liang et al. 1999). In the context of semiparametric
regression model (1.1), this suggests that we use the estimator


 T (In − P)W
 − (n − G) ζ
βn = W
B

−1

 T (In − P)
W
B Y

 has the same definition as 
where W
X.
Moreover, the fact that m(Ui ) = E(Yi −BiT α −XiT β|Ui ) = E(Yi −BiT α −WiT β|Ui )
and the result of Fan et al. (2005) suggest that the estimate of the nonparametric
components M = (m(U1 ), . . . , m(Un ))T can be achieved with the following equation


In P̄B
S In



Bα
M




=


P̄B
(Y − W
βn)
S

where P̄B = B(IG − 1G 1TG /G)(BT B)−1 BT and W = (W1 , . . . , Wn )T . An estimator
of M has the following form


 n = In − (In − SP̄B )−1 (In − S) (Y − W
β n ).
M

(2.6)

Sometimes, it is also necessary to estimate the error variance σ 2 = E(ε12 )
for such tasks as the construction of confidence regions, model-based tests, model
selection procedures, single-to-noise ratio determination and so on. In this event, since

123

634

H. Zhou et al.

E(Yi − BiT α − XiT β − m(Ui ))2 = σ 2 and E(Yi − BiT α − WiT β − m(Ui ))2 =
σ 2 + β T  ζ β, we define an estimator of σ 2 as

σn2 =

I
 n )T (In −PB )(Y−W
 n )−
β n −M
β nT  ζ 
βn
(Y−W
βn −M
n(I − 1)

(2.7)

with PB = B(BT B)−1 BT = 1/I BBT .
 n (·)
In the next section, we will investigate the asymptotic properties of 
βn, M
2
and 
σn .
3 Asymptotic properties of the proposed estimators
In this section we will be establishing the asymptotic properties of the proposed estimators from the previous section. We begin with the following assumptions. These
assumptions, while looking a little lengthy, are actually quite simple and can be easily
satisfied. They are also used by Fan et al. (2005).
Assumption 1 The function m(·) has a bounded second derivative.


Assumption 2  = E {X1 − E(X1 |U1 )}⊗2 is non-singular and E(X1 |U1 = u) is
Lipschitz continuous where A⊗2 means AAT .
Assumption 3 The random variable U1 has a bounded support . Its density function
f (·) is Lipschitz continuous and bounded away from 0 on .
Assumption 4 The function K (·) is a symmetric density function with compact support.
Assumption 5 The bandwidth h satisfies that nh 8 → 0 and nh 2 /(log n)2 → ∞ as
n → ∞.
With these assumptions, we are ready to establish the main results. The following
theorem gives the asymptotic normality of 
βn.
Theorem 1 Suppose that Assumptions 1 to 5 hold. Then the proposed estimator 
βn
of β is asymptotically normal, namely
√
n(
β n − β) → D N (0, I /(I − 1) −1  1  −1 ) as n → ∞,


where  1 = E(ε1 − ζ 1T β)2  + σ 2  ζ + E{(ζ 1 ζ 1T −  ζ )β}⊗2 .
−1 
1 
−1 is a consistent estimator of  −1  1  −1 where
Further, 
=


I
 T (I − P)W
 − ζ
W
B
n(I − 1)

and


 T ⊗2 .

T
T
1 − W

1 = 1 Wdiag(
Y

1 β n , . . . , Yn − Wn β n ) + 1n ( ζ β n )
n

123

Statistical inference for fixed-effects partially linear regression models with errors in variables

635

 n and 
The following two theorems provide the asymptotic properties of M
σn2 ,
respectively.
n
Theorem 2 Suppose that Assumptions 1 to 5 hold. Then the risk of the estimator M
is bounded as follows:
 n |U1 , . . . , Un }
MSE{M


√


μ22 h 4
(σ 2 + β T  ζ β)ν0 ||
I
1

2
≤ √
E{m (U )} +
+ o p h4 +
4
nh
nh
( I − 1)2
where μ j =



u j K (u)du, ν j =



 n = (
u j K 2 (u)du, M
m n (U1 ), . . . , m
n (Un ))T ,

n
1

E{
m n (Ui ) − m(Ui )|U1 , . . . , Un }2 .
MSE{Mn |U1 , . . . , Un } =
n
i=1

Theorem 3 Suppose that Assumptions 1 to 5 hold. Then it holds that
√
n(
σn2 − σ 2 ) → D N (0, κ) as n → ∞
where κ = E(ε1 − ζ 1T β)4 + (3 − I )/(I − 1)(σ 2 + β T  ζ β)2 . Define
n)
 = (ψ
n )T = (In − PB )(Y − W
1 , . . . , ψ
βn − M

and


6
3
4

κn = 1 + 2 − 3 −
I
I
I



−1  

n
1
9
I
−
1
T
i4 −
6−
(
σn2 + 
ψ
β n ζ 
βn)
n
I2
I
i=1

3− I
(
σ2 + 
+
β nT  ζ 
βn)
I −1 n
then 
κn is a consistent estimator of κ.

4 Selecting the covariates of the parametric part
Model selection is an indispensable tool for statistical data analysis. However, the
problem has not been studied for model (1.1) when some covariates are measured
with errors. Fan and Li (2001) proposed a variable selection via nonconcave penalized
likelihood and found some oracle properties. In this section, we will apply their method
to our setting.

123

636

H. Zhou et al.

Suppose that Xi consists of p variables, and some of these are not significant. A
penalized corrected profile least squares takes the form
L(β) ≡

1   T
n(I − 1) T
 
(Y − Wβ) (In − P
β ζ β
B )(Y − Wβ) −
2
2I
p

λ j p j (|β j |)
+n

(4.1)

j=1

where p j (·)’s are penalty function, λ j ’s are tuning parameters and may dependent on
n, which control the model complexity. For ease of presentation, we denote λ j p j (·)
by pλ j (·).
Many penalty functions, such as the family of L q -penalty (q ≥ 0) (Antoniadis
and Fan 2001), have been used for penalized least squares and penalized likelihood in
various parametric models. Antoniadis and Fan (2001) and Fan and Li (2001) provided
various insights into how a penalty function should be chosen. They advocated that a
good penalty function should yield an estimator with the following three properties:
unbiasedness for a large true coefficient to avoid unnecessary estimation bias, sparsity
(estimating a small coefficient as zero) to reduce model complexity, and continuity to
avoid unnecessary variation in model prediction. Necessary conditions are given in
Antoniadis and Fan (2001). However, none of the L q -penalties produces estimates
that satisfy, simultaneously, the above three properties. A simple penalty function,
which results in an estimator with the three desired properties, is the smoothly clipped
absolute deviation (SCAD) penalty. Its first derivative is defined by
pλ (β) = λ{I (β ≤ λ) +

(aλ − β)+
I (β > λ)} for some a > 2, where β > 0,
(a − 1)λ

and pλ (0) = 0. The SCAD involves two unknown parameters, λ and a. Fan and Li
(2001) suggested using a = 3.7 from a Bayesian point of view.
Now, we study the asymptotic properties of the resulting estimate of the penalized
corrected profile least squares (4.1). First we establish the convergence rate of the
penalized corrected profile least squares estimator. Assume that all penalty functions
pλ j (·) are nonnegative, non-decreasing with pλ j (0) = 0. Denote by β 0 the true value
of β, and


an = max{| pλ j (|β0 j |)| : β0 j = 0}, and bn = max{| pλ j (|β0 j |)| : β0 j = 0}.
j

j

Then, we have the following theorem.
Theorem 4 Suppose that Assumptions 1 to 5 hold. If an and bn tend to zero as n → ∞,
then with probability tending to 1 as n → ∞, there exists a local minimizer β̃ n of
L(β) such that ||β̃ n − β 0 || = O p (n −1/2 + an ).
Theorem 4 demonstrates how the rate of convergence of the penalized corrected
profile least squares estimator β̃ n depends on λ j . To achieve the root n convergence

123

Statistical inference for fixed-effects partially linear regression models with errors in variables

637

rate, λ j must be small enough so that an = O p (n −1/2 ). Next we establish the oracle
property for the penalized corrected profile least squares estimator. For ease of presentation, we assume, without loss of generality, that all of the first q components of
β 0 are not equal to 0, and all other components equal to 0. Let the first q components
of β 0 be β 01 . Denote

 

 = diag pλ1 (|β01 |, . . . , pλq (|β0q |) and
 


b = pλ1 (|β01 |)sgn(β01 ), . . . , pλq (|β0q |)sgn(β0q ) .
Further, let β̃ n1 consist of the first q components of β̃ n and β̃ n2 consist of the last
p − q components of β̃ n .
Theorem 5 (Oracle property) Assume that for j = 1, . . . , q, λ j → 0,
and the penalty function pλ j (|β0 j |) satisfies that
lim inf lim inf pλ j (β0 j )/λ j > 0.
n→∞ β0 j →0+

√

nλ j → ∞

(4.2)

If an = O p (n −1/2 ), then under the conditions of Theorem 4, with probability tending
T
T
to 1 as n → ∞, the root n consistent local minimizer β̃ n = (β̃ n1 , β̃ n2 )T in Theorem 4
must satisfy
(i) (Sparsity) β̃ n2 = 0;
(ii) (Asymptotic normality)
√  (1)
n  +



β̃ n1 − β 01 +  (1) +


−1 
b → D N 0,

I
(1)

(I − 1) 1



(1)

where  (1) and  1 consist of the first q rows and columns of  and  1 , respectively.
√
From Theorem 5 if λ j → 0, nλ j → ∞ for j = 1, . . . , d, an = O p (n −1/2 ),
and the condition (4.2) is satisfied, then the resulting estimate possesses an oracle
property. This implies that the resulting procedure correctly specifies the true model
and estimates the unknown regression coefficients as efficiently as if we knew the
sub-model. If all the penalty functions are SCAD, then an = 0 when n is sufficiently
large, and hence the resulting estimate possesses the oracle property. However, this is
not true for√the L 1 -penalty, since the condition an = max j λ j = O p (n −1/2 ) and the
conditions nλ j → ∞ cannot be satisfied simultaneously.
It is challenging to find the solution of the penalized corrected profile least squares
of (4.1) because the penalty function pλ j (|β j |), such as the SCAD penalty, is irregular
at the origin and may not have a second derivative at some points. Following Fan
and Li (2001), we locally approximate the penalty functions by quadratic functions
as follows. Given an initial value β (0) that is close to the minimizer of (4.1), when
(0)
|β j | ≥ η (a prescribed value), the penalty pλ j (|β j |) can be locally approximated by

123

638

H. Zhou et al.

the quadratic function as
 


(0)
(0)
[ pλ j (|β j |)] = pλ j (|β j |)sgn(β j ) ≈ pλ j (|β j |)/|β j | β j
With the local quadratic approximation, the Newton-Raphson algorithm can be implemented directly for minimizing L(β).
To implement the methods described above, it is desirable to have an automatic
data-driven method for estimating the tuning parameters λ1 , . . . , λ p . Similar to Fan
and Li (2001) we can estimate λ = (λ1 , . . . , λ p )T by minimizing an approximate
generalized cross validation (GCV) score.
5 Simulation results
In this section we carry out some simulation studies to demonstrate the finite sample
performance of the proposed procedures.
Example 1 The data are generated from the following fixed-effects partially linear
regression model
yi = BiT α + xi1 β1 + xi2 β2 + m(u i ) + εi and wi j = xi j + ζi j
for j = 1, 2 and i = 1, . . . , n, where αg are i.i.d. U (−0.5, 0.5); xi j ’s are i.i.d.
N (0.5, 1); (β1 , β2 ) = (1.5, 1); ζi j ’s are i.i.d. N (0, σζ2 ) with σζ2 = 0.5 and 0.25;
m(u) = sin(2π u); u i ’s are i.i.d. U (0, 1), εi ’s are i.i.d. N (0, 1). Moreover, we take
n = 300 and 600 and I = 2 and 5. We consider two cases: one is σζ2 known and the
other σζ2 unknown with two replications for each wi j .
The number of simulated realizations is 1, 000. The kernel function is the Gaussian
kernel and the bandwidth is selected by Cross-Validation. We calculate the sample
means and standard deviations (SDs) of the proposed estimators for the parametric
components β1 and β2 , and the error variance σ 2 in Sect. 2. We also calculate the sample
means and SDs of the naive estimators (neglecting the measurement errors) and the
benchmark estimators (xi1 and xi2 can be observed exactly). The naive estimators for
(β1 , β2 )T and σ 2 are defined as
 T (In − P)W

β̌ n = W
B

−1

 T (In − P)
W
B Y

and
σ̌n2 =

I
(Y − WiT β̌ n − M̌n )T (In − PB )(Y − WiT β̌ n − M̌n )
n(I − 1)

where


M̌n = In − (In − SP̄B )−1 (In − S) (Y − Wβ̌ n ).

123

Statistical inference for fixed-effects partially linear regression models with errors in variables

639

∗

The benchmark estimators β̃ n and σ̃n∗2 have the same definitions as those of β̌ n and
σ̌n2 except that W is replaced with X.
Simulation results are summarized in the following Tables 1 and 2.
From Tables 1 and 2 we make the following observations:
1. The naive estimators of the parametric component and error variance are biased.
These biases do not decrease as the sample size increases, and increase as the
variance of measurement errors increases.
2. The benchmark estimators and proposed estimators of the parametric component
and error variance are asymptotically unbiased.
3. There is a clear difference between the SDs of the benchmark estimators and those
of the proposed estimators. We think this difference is caused by the measurement
errors.
4. Whether σζ2 is known or unknown has almost no influence on the results.
In addition, we also calculate the estimators of the nonparametric component m(·)
in which n = 300, I = 5 and σζ2 = 0.5. Results are summarized in the following
Fig. 1. From Fig. 1, we see that the proposed estimator of the nonparametric component
outperforms the naive estimator. The latter is biased.
Example 2 The data are generated from the following fixed-effects partially linear
regression model

Table 1 The finite sample performance of the estimators for the parametric components β1 = 1.5, β2 = 1
and error variance σ 2 = 1 with σζ2 known

σζ2 = 0.5

∗
β̃n1

∗
β̃n2

n1
β

n2
β

β̌n1

β̌n2

σ̃n∗2


σn2

σ̌n2

Mean

1.502

1.000

1.616

1.087

0.998

0.670

0.950

0.815

1.964

SD

0.085

0.088

0.206

0.204

0.097

0.100

0.113

0.299

0.229

Mean

1.498

0.999

1.570

1.049

0.995

0.664

0.981

0.869

1.998

SD

0.066

0.065

0.157

0.149

0.078

0.075

0.091

0.246

0.190

Mean

1.500

0.996

1.552

1.029

0.999

0.664

0.987

0.923

2.034

SD

0.059

0.059

0.137

0.128

0.068

0.068

0.079

0.212

0.165

Mean

1.501

0.999

1.539

1.023

1.001

0.666

0.986

0.922

2.035

SD

0.046

0.044

0.099

0.094

0.053

0.051

0.063

0.172

0.138

Mean

1.502

0.999

1.562

1.035

1.207

0.799

0.948

0.877

1.551

SD

0.082

0.088

0.145

0.143

0.099

0.100

0.108

0.201

0.179

5

Mean

1.499

0.999

1.536

1.022

1.199

0.800

0.965

0.900

1.567

SD

0.067

0.066

0.105

0.105

0.075

0.075

0.088

0.162

0.146

2

Mean

1.500

1.000

1.531

1.016

1.203

0.800

0.972

0.933

1.596

SD

0.057

0.060

0.094

0.093

0.066

0.068

0.078

0.140

0.125

Mean

1.502

0.998

1.517

1.011

1.201

0.799

0.979

0.951

1.610

SD

0.046

0.047

0.074

0.071

0.053

0.053

0.062

0.111

0.101

n

I

300

2
5

600

2
5

σζ2 = 0.25

300

600

2

5

123

640

H. Zhou et al.

Table 2 The finite sample performance of the estimators for the parametric components β1 = 1.5, β2 = 1
and error variance σ 2 = 1 with σζ2 unknown

σζ2 = 0.5

n

I

300

2
5

600

2
5

σζ2 = 0.25

300

2
5

600

2
5

∗
β̃n1

∗
β̃n2

n1
β

n2
β

β̌n1

β̌n2

σ̃n∗2


σn2

σ̌n2

Mean

1.497

1.000

1.549

1.028

1.194

0.795

0.967

0.915

1.578

SD

0.086

0.084

0.154

0.135

0.100

0.095

0.110

0.215

0.183

Mean

1.499

0.999

1.533

1.029

1.198

0.803

0.969

0.909

1.577

SD

0.068

0.066

0.112

0.109

0.076

0.076

0.091

0.176

0.154

Mean

1.497

1.000

1.524

1.018

1.198

0.801

0.973

0.936

1.597

SD

0.056

0.061

0.092

0.096

0.065

0.069

0.081

0.149

0.134

Mean

1.500

1.001

1.517

1.015

1.200

0.801

0.980

0.951

1.611

SD

0.046

0.046

0.078

0.074

0.053

0.053

0.063

0.124

0.102

Mean

1.500

0.998

1.523

1.017

1.331

0.891

0.950

0.916

1.283

SD

0.085

0.086

0.112

0.114

0.093

0.096

0.111

0.159

0.149

Mean

1.500

0.999

1.517

1.010

1.334

0.888

0.968

0.939

1.304

SD

0.066

0.069

0.088

0.089

0.072

0.076

0.089

0.128

0.120

Mean

1.499

0.998

1.512

1.008

1.333

0.889

0.978

0.960

1.325

SD

0.058

0.060

0.076

0.079

0.064

0.067

0.079

0.117

0.112

Mean

1.498

0.999

1.508

1.004

1.332

0.887

0.986

0.974

1.338

SD

0.046

0.047

0.060

0.060

0.050

0.051

0.061

0.092

0.087

yi = BiT α +

8


xi j β j + m(Ui ) + εi and wi j = xi j + ζi j ,

j=1

for j = 1, . . . , 8 and i = 1, . . . , n where xi j ’s are i.i.d. N (0.5, 1), ζi j ’s are i.i.d.
N (0, σζ2 ) and (β1 , . . . , β8 )T = (0, 0, 0, 0.5, 1.5, 2.0, 0, 0)T . Other symbols are same
as those in Example 1.
The means and SDs of RGMSEs over 1,000 simulated data are summarized in
Table 3. Here, the RGMSE means the Relative GMSE, i.e. the ratio of GMSE of
an underlying procedure to that of the proposed estimator without penalization. For
estimator β̄ n , the GMSE is defined as

GMSE = (β̄ n − β)T


1 T
 − I − 1 σζ2 I p (β̄ n − β).
W (In − 
P
)
W
B
n
I

In addition, the average number of zero coefficients is also reported in Table 3, in which
the column labeled “C” presents the average, restricted only to the true zero coefficients, while the column labeled “E” depicts the average of coefficients erroneously
set to 0.
From Table 3, we can see that the proposed covariate selection procedure performs
very well. Whether σζ2 is known or unknown has almost no influence on the results.

123

Statistical inference for fixed-effects partially linear regression models with errors in variables

641

1.5

1

0.5

0

−0.5

−1

−1.5

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

1.5

1

0.5

0

−0.5

−1

−1.5

Fig. 1 The estimators of the nonparametric component m(·) with n = 300, I = 5 and σζ2 = 0.5, σζ2
known (left panel) and σζ2 unknown (right panel). sin(2π u) (solid curve), the benchmark estimator (dotted

curve), the proposed estimator (dash-dotted curve) and the naive estimator (dashed curve)

123

642

H. Zhou et al.

Table 3 The finite sample performance of the covariate selection
σζ2 (known) n

I Mean SD

C

σζ2 (unknown) n

E

I Mean SD

C

E

0.5

300 2 0.522 0.246 4.642 0.010

0.5

300 2 0.521 0.250 4.645 0.015

0.5

300 5 0.532 0.241 4.657 0.001

0.5

300 5 0.519 0.247 4.671 0

0.5

600 2 0.526 0.247 4.652 0

0.5

600 2 0.520 0.246 4.688 0

0.5

600 5 0.490 0.246 4.762 0

0.5

600 5 0.497 0.243 4.751 0

0.25

300 2 0.426 0.231 4.916 0.003

0.25

300 2 0.411 0.228 4.929 0.004

0.25

300 5 0.415 0.233 4.919 0

0.25

300 5 0.426 0.231 4.913 0

0.25

600 2 0.404 0.230 4.919 0

0.25

600 2 0.420 0.235 4.928 0

0.25

600 5 0.394 0.226 4.958 0

0.25

600 5 0.414 0.230 4.951 0

6 Concluding remarks
We have studied the estimation and inference of the fixed-effects partially linear regression models when some of the covariates are measured with errors. We constructed
new estimators for the parametric, nonparametric and error variance by taking the
measurement errors into account, and showed that they were consistent and asymptotically normal. In addition, we proposed a variable selection procedure to select
the significant variables in the parametric part. The resulting estimate was shown to
possess an oracle property.
We have focused on the case where the covariates in regressors were measured with
additive errors. In some situations, this additive property of the measurement errors
may not be true (Hwang 1986). Future research will be needed for dealing with other
type of error structure.
Appendix: Proof of main results
In order to prove the main results, we first present several lemmas.
Lemma 1 Suppose that Assumptions 1 to 5 hold. Then
1 T

W (In − P
B )W − (I − 1)/I  ζ → p (I − 1)/I  as n → ∞
n
where  is defined in Assumption 2.
Proof The proof is the same as that of Lemma A.2 in Fan et al. (2005).
Lemma 2 Under the conditions of Theorem 5, with probability tending to 1 as n →
∞, for any given estimator β ∗1 satisfying ||β ∗1 − β 01 || = O p (n −1/2 ) and any constant
c,
L{(β ∗1T , 0)T )T } =

123

min

||β ∗2 ||≤cn −1/2

L{(β ∗1T , β ∗2T )T }.

Statistical inference for fixed-effects partially linear regression models with errors in variables

643

Proof The proof is same as that of Lemma A.1 in Fan et al. (2004). We here omit the
detail.
ε, by the definition of 
βn
Proof of Theorem 1 Since 
Y=
Bα + 
Xβ + (In − S)M + 
it holds that

βn − β


 T (In − P)W
 − (n − G) ζ −1
(n − G) ζ β + W
B


 T (In − P) (
 + (In − S)M + 
 T (In − P)W
 − (n − G) ζ −1
·W
X − W)β
ε = W
B
B

 T (In − P)(

T
T
· (n − G) ζ β + W
ε
B X − W)β + W (In − P
B )(In − S)M + W (In − P
B )

 − (n − G) ζ
 T (In − P)W
= W
B

−1

According to the proof of Theorem 1 of Fan et al. (2005) it holds that
T
 T (In − P)(

2
W
B X − W)β = −(D + ζ ) (In − P
B )ζ β + o p (n )
1
1

= −(D + ζ )T (In − PB )ζ β + o p (n 2 )
 T (In − P)(In − S)M = o p (n 2 )
W
B
1

and
T
T
 T (In − P)
2
2
W
B ε = (D + ζ ) (In − P
B )ε + o p (n ) = (D + ζ ) (In − PB )ε + o p (n )
1

1

where D = (D1 , . . . , Dn )T = (X1 − E(X1 |U1 ), . . . , Xn − E(Xn |Un ))T and ζ =
(ζ 1 , . . . , ζ n )T .
Therefore, applying Lemma 1 and the fact (A + aB)−1 → A−1 as a → 0 we have

−1
√
(I −1)
n(
β n −β) =

I
 
1  T
· √ D (In−PB )(ε−ζ β)+ζ T (In −PB )ε+ (n−G) ζ −ζ T (In −PB )ζ β +o p (1).
n
Since
Cov(DT (In − PB )(ε − ζ β), ζ T (In − PB )ε) = 0,

Cov(D (In − PB )(ε − ζ β), (n − G) ζ − ζ T (In − PB )ζ β) = 0
T

and

Cov(ζ T (In − PB )ε, (n − G) ζ − ζ T (In − PB )ζ β) = 0
we have


 
1 
lim Cov √ DT (In −PB )(ε − ζ β) + ζ T (In −PB )ε + (n − G) ζ − ζ T (In −PB )ζ β
n→∞
n
I −1
1.
=
I

123

644

H. Zhou et al.

Then, applying the central limit theorem the proof is complete.
−1 
1 
−1 being a consistent estimator of  −1  1  −1 is straightThe proof of 
forward. We here omit the details.

Proof of Theorem 2 The proof of Theorem 2 is the same as that of Theorem 3 in Fan
et al. (2005). We here omit the details.

Proof of Theorem 3 According to the definition, 
σn2 can be decomposed as

I
T
T




(ε−ζ β n ) (In −PB )(ε−ζ β n )− β n  ζ β n
n(I −1)


I
 n T (In −PB ) X(β −
n
X(β −
β n )+M− M
+
β n )+M− M
n(I −1)

2I
 n T (In −PB )(ε−ζ 
X(β −
β n )+M− M
+
β n ) = J1 + J2 + J3 . say
n(I −1)



σn2 =

By the root-n consistency of 
βn,
J1 − σ 2 =

1
I
(ε − ζ β)T (In − PB )(ε − ζ β) − (σ 2 + β T  ζ β) + o p (n − 2 ).
n(I − 1)

Let ε̄g = I −1

I

i=1 ε(g−1)I +i

and similarly for all the other “barred” variables.

I
(ε − ζ β)T (In − PB )(ε − ζ β)
n(I − 1)


I
1
T
T
(ε − ζ β) In − IG ⊗ 1 I 1 I (ε − ζ β)
=
n(I − 1)
I
n
G 
2


I
I2
T
=
ε̄g − ζ̄ Tg β
(εi − ζ i β) −
n(I − 1)
n(I − 1)
g=1

i=1

=

I
n(I − 1)

G 
I 


T
T
ε(g−1)I +i − ζ (g−1)I
+i β − ε̄g + ζ̄ g β

2

.

g=1 i=1

2
I 
T
T
ε(g−1)I +i − ζ (g−1)I
Let ζg = i=1
+i β − ε̄g + ζ̄ g β) . Obviously, ζg ’s are i.i.d.
random variables with

Eζg =

⎧
I ⎨

(I −1)2
i=1

⎩

I2

⎫
I
⎬

1
2
T
T
E(ε(g−1)I+1 −ζ (g−1)I+i
β)2 + 2
E(ε(g−1)I+i1 −ζ (g−1)I+i
β)
1
⎭
I

= (I −1) (σ +β T  ζ β).
2

123

i 1 =i

Statistical inference for fixed-effects partially linear regression models with errors in variables

645

Then, by some simple calculation, we have
$ %2
$ %
Var ζg = E ζg −(Eζg )2
1
6I (I −1)
= I E(ε1 −ζ 1T β)4 + I (I −1)(σ 2 +β T  ζ β)2 + E(ε1 −ζ 1T β)4 +
I
2I 2


2
I E(ε1 −ζ 1T β)4 + I (I −1)(σ 2 +β T  ζ β)2
×(σ 2 +β T  ζ β)2 −
I
− (I −1)2 (σ 2 +β T  ζ β)2




1
3
= I + −2 E(ε1 −ζ 1T β)4 +(I −1) −1+
(σ 2 +β T  ζ β)2 .
I
I
Therefore,



I
Var √
ζg
n(I − 1)
G



i=1

=




1
− 2 E(ε1 − ζ 1T β)4
I



3
− 1 (σ 2 + β T  ζ β)2 .
+ (I − 1)
I
I
(I − 1)2

I+

Moreover, according to Theorems 1 and 2 we have


1
I
 n T X(β − 
 n = o p (n − 2 ).
X(β − 
βn) + M − M
βn) + M − M
n(I − 1)

J2 ≤

So in order to complete the proof, we just need to show that
J3 =


1
2I
 n T (In − PB )(ε − ζ 
X(β − 
βn) + M − M
β n ) = o p (n − 2 ). (A1)
n(I − 1)

Denote
T
T

(g−1)I +i = (ε(g−1)I +i − ζ (g−1)I
+i β n ) − ε̄g + ζ̄ g β n .

J3 can be further decomposed as



2I
T
(g−1)I +i X(g−1)I
(β − 
β n ) − X̄gT (β − 
βn)
+i
n(I − 1)
I

G

J3 =

g=1 i=1


2I
(g−1)I +i {m(U(g−1)I +i ) − m
n (U(g−1)I +i )}
n(I − 1)
G

+

I

g=1 i=1



2I
¯ n (U )g
(g−1)I +i m̄(U )g − m

n(I − 1)
G

−

I

g=1 i=1

= J31 + J32 − J33 say.

123

646

H. Zhou et al.

Since
I
G 


1

(g−1)I +i X(g−1)I +i = O p (n 2 ) and

g=1 i=1

I
G 


1

(g−1)I +i X̄g = O p (n 2 ),

g=1 i=1

combining the root-n consistency of 
β n we obtain J31 = o p (n −1/2 ).
Let di denote an n dimensional column vector with the ith element being 1 and
others 0. According to the definition of m
n (·), we have


m
n (Ui ) = diT In − (In − SP̄B )−1 (In − S) X(β − 
βn)


+diT In − (In − SP̄B )−1 (In − S) M


+diT In − (In − SP̄B )−1 (In − S) (ε − ζ 
βn)
+diT (In − SP̄B )−1 S(In − P̄B )Bα.
Since
have

G

i=1 αi

= 0, it is easy to show that (In − SP̄B )−1 S(In − P̄B )Bα = 0. Thus we




2I
T
In −(In −SP̄B )−1 (In −S) X(β −
(g−1)I+i d(g−1)I+i
βn)
n(I −1)
G

J32 =

I

g=1 i=1


2I
−1
T
(g−1)I +i d(g−1)I
+i (In − SP̄B ) (In − S)M
n(I − 1)
G

−

I

g=1 i=1




2I
T
In −(In −SP̄B )−1 (In −S) (ε−ζ 
(g−1)I+i d(g−1)I+i
βn)
n(I − 1)
G

+

I

g=1 i=1

= J321 + J322 + J323 say.
According to the root-n consistency of 
β n and the proof of Theorem 3 of Fan et al.
(2005) it holds that

2
J321

=

2I
n(I −1)

&
&2
2 &
I
&

& G 
&
−1
T
&

(g−1)I +i In −d(g−1)I +i (In −SP̄B ) (In −S) &
&
& · X(β − β n )
&g=1 i=1
&

= O(n −1 ) · O p (h 4 + 1/nh) = o p (n −1 )

This implies that J321 = o p (n −1/2 ). On the other hand, since
(In − S)M =

μ2 

(m (U1 ), . . . , m (Un ))T h 2 + o p (h 2 ),
2

we can show that J322 = o p (n −1/2 ). By the same argument as proving Lemma 4(2)
of You and Chen (2006) we can show that J323 = o p (n −1/2 ). Thus (A1) holds. This

123

Statistical inference for fixed-effects partially linear regression models with errors in variables

647

implies that J32 = o p (n −1/2 ). Following the same line, we can show that J33 =
o p (n −1/2 ).
We now show the consistency of 
κn . To facilitate the notations we let
T
T


∇(g−1)I +i = X(g−1)I
+i (β − β n ) − X̄g (β − β n )

¯ n (U )g
n (U(g−1)I +i ) − m̄(U )g + m

+m(U(g−1)I +i ) − m
T
T
and ϑg,i = ε(g−1)I +i − ζ (g−1)I
+i β − (ε̄g − ζ̄ g β). Then it holds that

I
I
I
n
G
G 
G 


1  4  4
4
3
i =
ψ
ϑg,i +
∇(g−1)I
+
4
ϑg,i
∇(g−1)I +i
+i
n
g=1 i=1

i=1

+6

G 
I


g=1 i=1

2
2
ϑg,i
∇(g−1)I
+i + 4

g=1 i=1

g=1 i=1

G 
I


3
ϑg,i ∇(g−1)I
+i

g=1 i=1

= Q1 + Q2 + Q3 + Q4 + Q5.
For Q 1 , we have
⎧
⎫4
I
I
⎬

1 ⎨
1
Q1 =
E εi −ζ iT β −
(εi1 −ζ iT1 β) +o p (1)
⎩
⎭
I
I
i=1
i 1 =1
⎡
⎫2 ⎤
⎧
I
I
⎬
⎨


6
⎥
⎢
E ⎣(εi − ζ iT β)2
(εi1 − ζ iT1 β) ⎦
= o p (1)+ E(ε1 − ζ iT β)4 + 3
⎭
⎩
I
i=1

⎧
⎨

⎫
⎬

i 1 =1

.4
I
I
I

3 
4 
3
T
T
T
E (εi − ζi β)
(εi1 − ζ i1 β) − 4
(εi − ζ i β)
− 2
⎩
⎭ I
I
i=1
i 1 =1
i=1




9
6
3
4
(I − 1)
4
T
6−
E(ε1 − ζ i β) +
(σ 2 + β T  ζ β)2 .
= 1+ 2 − 3 −
I
I
I
I2
I
-

I

4
Moreover, according to Hölder inequality and the fact that G
g=1
i=1 ∇(g−1)I +i =
o p (1) we can show that Q s = o p (1) for s = 2, 3, 4 and 5. Thus, the consistency result
of 
κn follows.
Proof of Theorem 4 Denote αn = n −1/2 + an . It is sufficient to show that for any
given ξ > 0, there exists a large constant c such that

P


inf L(β 0 + αn u) ≥ L(β 0 ) ≥ 1 − ξ.

||u||=c

(A2)

This implies, with probability at least 1 − ξ , that there exists a local minimizer in
the ball {β 0 + αn u : ||u|| ≤ c}. Define Dn (u) = L(β 0 + αn u) − L(β 0 ). Note that

123

648

H. Zhou et al.

pλs (0) = 0 and pλs (|β0s |) is nonnegative. Therefore, it holds that
n −1 Dn (u)
⊗2
 0
1 /  
 0 )T (In − P) ⊗2
(Y − W(β 0 + αn u))T (In − P
=
− (
Y − Wβ
B)
B
2n

I −1
−
(β 0 + αn u)T  ζ (β 0 + αn u) − β 0T  ζ β 0
2I
q

+
{ pλs (|β0s + αn u s |) − pλs (|β0s |)}
s=1

= J1 +

q


pλs (|β0s + αn u s |) − pλs (|β0s |)



say.

s=1

Obviously,
1 2 T T
T

 

{α u W (In − P
B )Wu − 2αn (Y − Wβ 0 ) (In − P
B )Wu}
2n n

I −1 2 T
αn u  ζ u − 2αn uT  ζ β 0
−
2I


αn2 T 1  T
 − I − 1 ζ u
u
W (In − P
=
)
W
B
2
n
I


1
 + I − 1 uT  ζ β 0
(ε − ζ β 0 + M)T (In − S)T (In − P
−αn
)
Wu
B
n
I


2
α
1 T
− 21

= n uT
αn ||u||) = J2 + J3 say
W (In − P
B )W − (I − 1)/I  ζ u+ O p (n
2
n

J1 =

By Lemma 1 it holds that
1
1 T
I −1
I −1

W (In − P
ζ =
Cov(X1 − E(X1 |U1 )) + O p (n − 2 ) > 0,
B )W −
n
I
I

for n large enough, and hence J2 is of the order c2 αn2 . Note that n −1/2 αn = O p (αn2 )
because αn = n −1/2 + an . By choosing a sufficiently large c, J2 will dominate J3 ,
uniformly in ||u|| = c. Moreover,
q


pλs (|β0s + αn u s |) − pλs (|β0s |)



s=1

√
√
is bounded by qαn an ||u|| + αn2 bn ||u||2 = O p {cαn2 ( q + bn c)} by the Taylor expansion and Cauchy-Schwarz inequality, where q is the number of components of β 01 .
√
cαn2 ( q +bn c) is dominated by J1 as bn → 0, by taking c sufficiently large. Therefore,
(A2) holds for sufficiently large c. This completes the proof of Theorem 4.


123

Statistical inference for fixed-effects partially linear regression models with errors in variables

649

Proof of Theorem 5 Part (i) directly follows from Lemma 2. Now we prove Part (ii).
Using arguments similar to that of the proof of Theorem 4, it can be shown that
there exists a β̃n1 in Theorem 5 that is a root-n consistent minimizer of L{(β 1T , 0T )T },
satisfying the penalized corrected profile least squares equations:
1
∂L (β 1T , 0T )T 1
1
= 0.
1
∂β 1
β 1 =β̃ n1
Further, we have
1
∂ L (β 1T , 0T )T 1
1
1
∂β 1
β 1 =β̃ n1


q
n(I −1) T (1)
1   (1)
1
  (1)
∂ 2 (Y− W β 1 )T (In −P
B )(Y− W β 1 )− 2I β 1  ζ β 1 +n
j=1 λ j p j (|β j |) 1
1
=
1
∂β 1
β 1=β̃ n1
 (1)T (In − P)(
 (1)
= −W
B Y − W β 01 )


n(I − 1) (1)
 (1)T (In − P)W
 (1) − n(I − 1)  (1) (β̃ n1 − β 01 )
 ζ β 01 + W
−
B
ζ
I
I
/
0
+ n b + { + o p (1)}(β̃ n1 − β 01 )
(1)

 (1) consists of the first q columns of W,
  consists of the first q rows and
where W
ζ
columns of  ζ . Therefore, it holds that
/
 (1)T (In − P)W
 (1) −
(W
B

n(I −1) (1)
 ζ ) + n{
I

0
+ o p (1)} (β̃ n1 − β 01 ) + nb

 (1)T (In − P)(
 (1)
=W
B Y − W β 01 ) +

n(I −1) (1)
 ζ β 01 .
I

Similar to Theorem 1, we can show that




1  (1)T

 (1) β 01 )+ n(I −1)  (1) β 01 → D N 0, I −1  (1)
(In −P
)(
Y−
W
√ W
B
ζ
1
I
I
n
as n → ∞
(1)

where  1 consists of the first q rows and columns of  1 . Thus, combining Lemma 1,
it follows that
√  (1)
n  +


β̃ n1 − β 01 + ( (1) +


)−1 b → D N (0,

I
 (1) ) as n → ∞
I −1 1

where  (1) consists of the first q rows and columns of . This completes the proof of
Theorem 5.

123

650

H. Zhou et al.

References
Ahn SC, Schmidt P (2000) Estimation of linear panel data models using GMM. In: Mátyás L (ed) Generalized
method of moments estimation, Chap. 8. Cambridge university press, Cambridge
Andrews DWK (1996) Nonparametric kernel estimation for semiparametric models. Econ Theory 11:560–
596
Antoniadis A, Fan J (2001) Regularization of wavelet approximations. With discussion and a rejoinder by
the authors. J Am Stat Assoc 96:939–967
Baltagi BH (1995) Econometric analysis of panel data. Wiley
Baltagi BH, Li D (2002) Series estimation of partially linear panel data models with fixed effects. Ann Econ
Finan 3: 103–116
Carroll RJ, Ruppert D, Stefanski LA (1995) Measurement error in nonlinear models. Chapman & Hall,
London
Cheng CL, Van Ness J (1999) Statistical regression with measurement error. Arnold, London
Delecroix M, Härdle W, Hristache M (2003) Efficient estimation in conditional single-index regression.
J Multivar Anal 86:213–226
Engle RF, Granger WJ, Rice J, Weiss A (1986) Semiparametric estimates of the relation between weather
and electricity sales. J Am Stat Assoc 80:310–319
Fan J (1991) On the optimal rates of convergence for nonparametric deconvolution problems. Ann Stat
19:1257–1272
Fan J (1993) Adaptively local one-dimensional subproblems with applications to a deconvolution problem.
Ann Stat 21:600–610
Fan J, Huang T (2005) Profile likelihood inferences on semiparametric varying-coefficient partially linear
models. Bernoulli 11:1031–1057
Fan J, Li R (2001) Variable selection via penalized likelihood. J Am Stat Assoc 85:1348–1360
Fan J, Li R (2004) New estimation and model selection procedures for semiparametric modeling in longitudinal data analysis. J Am Stat Assoc 99:710–723
Fan J, Truong YK (1993) Nonparametric regression with errors-in-variables. Ann Stat 21:1900–1925
Fan J, Yao Q, Cai Z (2003) Adaptive varying-coefficient linear models. J R Stat Soc Ser B 65:57–80
Fan J, Tam P, Vande Woude G, Ren Y (2004) Normalization and analysis of cDNA micro-arrays using
within-array replications applied to neuroblastoma cell response to a cytokine. Proc Natl Acad Sci
USA 105:1135–1140
Fan J, Peng H, Huang T (2005) Semilinear high-dimensional model for normalization of microarray data:
a theoretical analysis and partial consistency. J Am Stat Assoc 100:781–813
Fuller WA (1987) Measurement error models. Wiley, New York
Hwang JT (1986) Multiplicative errors-in-variables models with applications to the recent data released by
the US department of energy. J Am Stat Assoc 81:680–688
Liang H, Härdle W, Carroll RJ (1999) Estimation in a semiparametric partially linear errors-in-variables
model. Ann Stat 27:1519–1535
You J, Chen G (2006) Estimation in a semiparametric varying-coefficient partially linear errors-in-variables
model. J Multivar Anal 97:324–341
You J, Zhou H (2006) Weighted difference-based series estimation for partially linear in-slide models.
Submitted to Statist. Sinica
Zhou H, Wang CY (2000) Failure time regression analysis with measurement error in covariates. J R Stat
Soc B 62:657–665
Zhu L, Cui H (2003) A semiparametric regression model with errors in variables. Scan J Stat 30:429–442

123

