Statistical Science
2007, Vol. 22, No. 4, 513–515
DOI: 10.1214/07-STS242A
Main article DOI: 10.1214/07-STS242
© Institute of Mathematical Statistics, 2007

Comment: Boosting Algorithms:
Regularization, Prediction and Model
Fitting
Trevor Hastie
We congratulate the authors (hereafter BH) for an interesting take on the boosting technology, and for developing a modular computational environment in R
for exploring their models. Their use of low-degree-offreedom smoothing splines as a base learner provides
an interesting approach to adaptive additive modeling.
The notion of “Twin Boosting” is interesting as well;
besides the adaptive lasso, we have seen the idea applied more directly for the lasso and Dantzig selector
(James, Radchenko and Lv, 2007).
In this discussion we elaborate on the connections
between L2 -boosting of a linear model and infinitesimal forward stagewise linear regression. We then take
the authors to task on their definition of degrees of freedom.

(2)



Despite the difference, they both have the same limit,
which is computed exactly for squared-error loss by
the type="forward.stagewise" option in the
package lars. As ν gets very small, initially the same
coefficient tends to get continuously updated by infinitesimal amounts (hence linearly). Eventually a second variable ties with the first for coefficient updates,
which they share in a balanced way while remaining
tied. Then a third joins in, and so on. Using simple
least-squares computations, the LARS algorithm computes the entire iFSLR path with the same cost as a
single multiple-least-squares fit. Note that in this limiting case, we can no longer index the sequence by stepnumber m as in (1) or (2), but must resort to some other
measure, such as the L1 -arc-length of the coefficient
profile (Hastie, Taylor, Tibshirani and Walther, 2007).
Lasso and iFSLR are not always the same. In highdimensional problems with correlated predictors, lasso
profiles become wiggly quickly, whereas iFSLR profiles tend to be much smoother and monotone (Hastie
et al., 2007). Efron et al. (2004) establish sufficient positive cone conditions on the model matrix X which effectively limit the amount of correlation between the
variables and guarantee that lasso and iFSLR are the
same; in particular, if the lasso profiles are monotone,
all three algorithms are identical.

1. L2 -BOOST AND INFINITESIMAL FORWARD
STAGEWISE LINEAR REGRESSION

Motivated by a version of L2 -boosting in Chapter 10
of Hastie, Tibshirani and Friedman (2001), Efron,
Hastie, Johnstone and Tibshirani (2004) proposed the
LARS algorithm. The intent was to:
• develop a limiting version of L2 -boost in which the
step-length ν went to zero;
• show that this limiting version gave paths identical
to the lasso, as was hinted in that chapter.
The result was three very similar varieties of the
LARS algorithm, namely lasso, LAR and infinitesimal forward stagewise (iFSLR) (package lars for R,
available from CRAN). iFSLR is indeed the limit of
L2 -boost as ν ↓ 0, with piecewise-linear coefficient
profiles, but is not always the same as the lasso.
On a slight technical note, the version of L2 -boost
proposed in BH is slightly different from that in Hastie,
Tibshirani and Friedman (2001). Compare
(1)



[HTF] β̂ [m] = β̂ [m−1] + ν · sign β̂ (Ŝm ) .

2. DEGREES OF FREEDOM

The authors propose a simple formula for the degrees of freedom for an L2 -boosted model. They construct the hat matrix Bm that computes the fit at iteration m, and then use df(m) = trace(Bm ). They are
in effect treating the model at stage m as if it were
computed by a predetermined sequence of linear updates. If this were the case, their formula would be
spot on, by the accepted definitions for effective degrees of freedom for linear operators (Hastie et al.,
2001; Efron et al., 2004). They acknowledge that this
is an approximation (since the sequence was not predetermined, but rather adaptively chosen), but do not

[BH] β̂ [m] = β̂ [m−1] + ν · β̂ (Ŝm ) ,

Trevor Hastie is Professor, Department of Statistics,
Stanford University, Starford, California 94305, USA
(e-mail: hastie@stanford.edu).
513

514

T. HASTIE

F IG . 1. The effective degrees of freedom for L2 -boost computed using the trace formula (vertical axis) vs. the exact degrees of freedom.
The left plot is for the prostate cancer data example; the right plot is for a simulated univariate smoothing problem. In both cases df(m)
underestimates the true degrees of freedom quite dramatically.

elaborate. In fact this approximation can be very badly
off. Figure 1 shows the true degrees of freedom dfT (k)
plotted against df(k) for two examples. We see that
df(k) always underestimates dfT (k). We now discuss
the details of these examples, and the basis for these
claims.
The left example is the prostate data (Hastie et al.,
2001, Figure 10.12) and has 67 observations and 9 predictors (including intercept). The right example fits a
univariate piecewise-constant spline model of the form

f (x) = 50
j =1 βj hj (x), where the hj (x) = I (x ≥ cj )
are a sequence of Haar basis functions with predefined
knots cj at the unique values of the input values xi .
There are 50 observations and 50 predictors. In both
problems we fit the limiting L2 -boost model iFSLR,
using the lars/forward.stagewise procedure.
Figure 2 shows the coefficient profiles.

In this case, using the results in Efron et al. (2004),
it can be deduced that the equivalent limiting version
of the hat matrix (5.6) of BH simplifies to a similar but
more compact expression:
(3)

Bk = I − (I − γk Hk )
· (I − γk−1 Hk−1 ) · · · (I − γ1 H1 ).

Here k indexes the step number in the lars algorithm, where the steps delineate the breakpoints in
the piecewise-linear path. Hj is the hat matrix corresponding to the variables involved in the j th portion
of the piecewise linear path, and γj is the relative distance in arc-length traveled along this piece until the
next variable joins the active set (relative to the arclength of the step that went all the way to the least
squares fit). Using the BH definition, we would compute df(k) = trace(Bk ) (vertical axis in Figure 1).

F IG . 2. Coefficient profiles for the iFSLR algorithm for the two examples. Both profiles are monotone, and are identical to the lasso profiles
on these examples. In this case the df increment by 1 exactly at every vertical break-point line.

515

COMMENT

These two examples were chosen carefully, for they
both satisfy the positive cone condition mentioned
above. In particular, the iFSLR path is the lasso path
in both cases, and the active set grows by one at each
step. More importantly, it is under these conditions that
Efron et al. (2004) established that dfT (k) = k + 1 exactly (horizontal axis in Figure 1). The +1 takes care
of the intercept.
Consider the first step. The dominant variable enters
the model, and gets its coefficient incremented until we
reach the point that the next competitor is about to enter. At this point the df is exactly 2, while the formula
df(1) = trace(B1 ) = 1.48 for the first example in Figure 1; this is off by 25%.
The exact df satisfies our intuition as well. If the first
variable is far more significant than the rest, we will almost fit it entirely (γ1 ≈ 1) before the next one enters,
and at that point the model has 2df. There is virtually
no price for searching, because searching was not really needed. On the other hand, if many variables are
competing for the first slot, shortly after the chosen one
enters, another might appear, long before the first is
fit completely (γ1  1). Here the model also has 2df,
despite the fact that the first variable has hardly progressed at all. This is the price paid for selection.
Even when the positive cone conditions are not satisfied, it can be shown that the size of the active set is
an unbiased estimate of the true df (Zou, Hastie and
Tibshirani, 2007).

It is possible that the authors can devise a correction
for their df(k) formula, based on the insights learned
here. In some cases it may be possible to calibrate the
formula to match the size of the active set. Failing that,
one can use bootstrap methods to estimate df. But if the
main purpose for estimating df is for model selection,
K-fold cross-validation is a useful alternative.

ACKNOWLEDGMENTS

This research was supported by NSF Grant DMS05-05676 and NIH Grant 2R01 CA 72028-07.
REFERENCES
E FRON , B., H ASTIE , T., J OHNSTONE , I. and T IBSHIRANI , R.
(2004). Least angle regression (with discussion). Ann. Statist.
32 407–499. MR2060166
H ASTIE , T., TAYLOR , J., T IBSHIRANI , R. and WALTHER , G.
(2007). Forward stagewise regression and the monotone lasso.
Electron. J. Statist. 1 1–29. MR2312144
H ASTIE , T., T IBSHIRANI , R. and F RIEDMAN , J. (2001). The Elements of Statistical Learning. Data Mining, Inference, and Prediction. Springer, New York. MR1851606
JAMES , G., R ADCHENKO , P. and LV, J. (2007). The dasso algorithm for fitting the dantzig selector and the lasso. Technical
report, Marshall School of Business, Univ. Southern California.
Z OU , H., H ASTIE , T. and T IBSHIRANI , R. (2007). On the “degrees of freedom” of the lasso. Ann. Statist. 35 2173–2192.

