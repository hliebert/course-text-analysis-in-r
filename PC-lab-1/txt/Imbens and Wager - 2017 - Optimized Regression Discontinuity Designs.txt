Optimized Regression Discontinuity Designs∗

arXiv:1705.01677v3 [stat.ME] 7 Jun 2018

Guido Imbens
imbens@stanford.edu

Stefan Wager
swager@stanford.edu

Current version June 2018

Abstract
The increasing popularity of regression discontinuity methods for causal inference
in observational studies has led to a proliferation of different estimating strategies,
most of which involve first fitting non-parametric regression models on both sides of a
treatment assignment boundary and then reporting plug-in estimates for the effect of
interest. In applications, however, it is often difficult to tune the non-parametric regressions in a way that is well calibrated for the specific target of inference; for example,
the model with the best global in-sample fit may provide poor estimates of the discontinuity parameter. In this paper, we propose an alternative method for estimation
and statistical inference in regression discontinuity designs that uses numerical convex optimization to directly obtain the finite-sample-minimax linear estimator for the
regression discontinuity parameter, subject to bounds on the second derivative of the
conditional response function. Given a bound on the second derivative, our proposed
method is fully data-driven, and provides uniform confidence intervals for the regression discontinuity parameter with both discrete and continuous running variables. The
method also naturally extends to the case of multiple running variables.

Keywords: Convex optimization, discrete running variable, multiple running variables,
partial identification, uniform asymptotic inference.

1

Introduction

Regression discontinuity designs, first developed in the sixties [Thistlethwaite and Campbell,
1960], often allow for simple and transparent identification of treatment effects from observational data [Hahn, Todd, and Van der Klaauw, 2001, Imbens and Lemieux, 2008, Trochim,
1984], and their statistical properties have been the subject of recent interest [Armstrong
and Kolesár, 2018, Calonico, Cattaneo, and Titiunik, 2014, Cheng, Fan, and Marron, 1997,
Kolesár and Rothe, 2018]. The sharp regression discontinuity design assumes a treatment
assignment generated by a running variable X ∈ Rk , such that individuals get treated if
and only X ∈ A for some set A ⊂ Rk . For example, in epidemiology, X ∈ R could be a
severity index and patients are assigned a medical intervention whenever X ≥ c for some
threshold c (i.e., A = {x ∈ R : x ≥ c}); or, in political science, X ∈ R2 could denote the
∗ We are grateful for helpful comments from Timothy Armstrong, Max Farrell, Michal Kolesár, Christoph
Rothe, Cun-Hui Zhang, seminar participants at Berkeley, Stanford and the University of Chicago, as well
as the editor and four anonymous referees.

1

latitude and longitude of a household, and A could be an administrative region that has
enacted a specific policy.
Given appropriate assumptions, we can identify a causal effect by comparing subjects
i with Xi barely falling within the treatment region A to those with Xi just outside of it.
Variants of this identification strategy have proven to be useful in education [Angrist and
Lavy, 1999, Black, 1999, Jacob and Lefgren, 2004], political science [Caughey and Sekhon,
2011, Keele and Titiunik, 2014, Lee, 2008], criminal justice [Berk and Rauma, 1983], program
evaluation [Lalive, 2008, Ludwig and Miller, 2007], and other areas. As discussed in more
detail below, standard methods for inference in the regression discontinuity design rely on
plugin-in estimates from local linear regression.
In this paper, motivated by a large literature on minimax linear estimation [Armstrong
and Kolesár, 2018, Cai and Low, 2003, Donoho, 1994, Donoho and Liu, 1991, Ibragimov
and Khas’minskii, 1985, Johnstone, 2011, Juditsky and Nemirovski, 2009], we study an
alternative approach based on directly minimizing finite sample error bounds via numerical
optimization, under an assumption that the second derivative of the response surface is
bounded away from the boundary of the treatment region.1 This approach has several
advantages relative to local regression. Our estimator is well defined regardless of the shape
of the treatment region A, whether it be a half line as in the standard univariate regression
discontinuity specification or an oddly shaped region as might appear with a geographic
regression discontinuity; moreover, our inference is asymptotically valid for both discrete and
continuous running variables. Finally, even with univariate designs, our approach strictly
dominates local linear regression in terms of minimax mean-squared error.
For simplicity of exposition, we start by presenting our method in the context of classical
univariate regression discontinuity designs with a single treatment cutoff, i.e., with Xi ∈
R and A = {x ∈ R : x ≥ c}; a solution to the more general problem will then follow by
direct extension. A software implementation, optrdd for R, is available on CRAN and from
github.com/swager/optrdd.

1.1

Optimized Inference with Univariate Discontinuities

We have access to i = 1, ..., n independent pairs (Xi , Yi ) where Xi ∈ R is the running variable and Yi ∈ R is our outcome of interest; the treatment is assigned as Wi = 1 ({Xi ≥ c}).
Following the potential outcomes model [Imbens and Rubin, 2015, Neyman, 1923, Rubin,
1974], we posit potential outcomes Yi (w), for w ∈ {0, 1} corresponding to the outcome
subject i would have experienced had they received treatment w, and define the conditional
average treatment effect τ (x) in terms of the conditional response functions µw (x):


µw (x) = E Yi (w) Xi = x , τ (x) = µ1 (x) − µ0 (x).
(1)
1 Of these papers, our work is most closely related to that of Armstrong and Kolesár [2018], who explicitly
consider minimax linear estimation in the regression discontinuity model for an “approximately linear” model
in the sense of Sacks and Ylvisaker [1978] that places restrictions on second differences relative to the response
surface at the threshold. In contrast, we assume bounded second derivatives away from the threshold. An
advantage of their approach is that it allows for a closed form solution. However, a disadvantage is that
they allow for discontinuities in the response surface away from the threshold, which implies that given the
same value for our bound on the second derivative and their bound on second differences, our confidence
intervals can be substantially shorter (moreover, allowing for discontinuities in the response surface does
not seem conceptually attractive given that the assumption of continuity of the conditional expectation at
the threshold is fundamental to the regression discontinuity design). We discuss this comparison further in
Section 1.3.

2

Provided the functions µw (x) are both continuous at c, the regression discontinuity identifies
the conditional average treatment effect at the threshold c,2




τ (c) = lim E Yi Xi = x − lim E Yi Xi = x .
(2)
x↓c

x↑c

Given this setup, local linear regression is a popular strategy for estimating τ (c) [Hahn
et al., 2001, Porter, 2003]:
(
)
n
 
2
1 X
K |∆i | hn Yi − a − τ Wi − β− (∆i )− − β+ (∆i )+
τ̂ = argmin
, (3)
nhn i=1
where K(·) is some weighting function, hn is a bandwidth, ∆i = Xi − c, and a and β± are
nuisance parameters. When we do not observe data right at the boundary c (e.g., when Xi
has discrete support), then τ (c) is not point identified; however, given smoothness assumptions on µw (x), Kolesár and Rothe [2018] propose an approach to local linear regression that
can still be used to construct partial identification intervals for τ (c) in the sense of Imbens
and Manski [2004] (see Section 2.1 for a discussion).
The behavior of regression discontinuity estimation via local linear regression is fairly
well understood. When the running variable X is continuous (i.e., X has a continuous
positive density at c) and µw (x) is twice differentiable with a bounded second derivative
in a neighborhood of c, Cheng, Fan, and Marron [1997] show that the triangular kernel
K(t) = (1 − t)+ minimizes worst-case asymptotic mean-squared error among all possible
choices of K, Imbens and Kalyanaraman [2012] provide a data-adaptive choice of hn to
minimize the mean-squared error of the resulting estimator, and Calonico, Cattaneo, and
Titiunik [2014] propose a method for removing bias effects due to the curvature of µw (x) to
allow for asymptotically unbiased estimation. Meanwhile, given a second-derivative bound
|µ00w (x)| ≤ B, Armstrong and Kolesár [2018] and Kolesár and Rothe [2018] construct confidence intervals centered at the local linear estimator τ̂ that attain uniform asymptotic
coverage, even when the running variable X may be discrete.
Despite its ubiquity, however, local linear regression still has some shortfalls. First
of all, under the bounded second derivative assumption often used to justify local linear
regression (i.e., that µw (x) is twice differentiable and |µ00w (x)| ≤ B in a neighborhood of
c), local linear regression is not the minimax optimal linear estimator for τ (c)—even with a
continuous running variable. Second, and perhaps even more importantly, all the motivating
theory for local linear regression relies on X having a continuous distribution; however, in
practice, X often has a discrete distribution with a modest number of points of support.
When the running variable is discrete there is no compelling reason to expect local linear
regression to be particularly effective in estimating the causal effect of interest.3 In spite of
these limitations, local linear regression is still the method of choice, largely because of its
intuitive appeal.
2 In the fuzzy regression discontinuity design where the probability of receiving the treatment changes
discontinuously at x = c, but not necessarily from zero to one, the estimand can be written as the ratio of
two such differences. The issues we address in this paper also arise in that setting, and the present discussion
extends naturally to it; see Section 5 for a discussion.
3 One practical inconvenience that can arise in local linear regression with discrete running variables is
that, if we use a data-driven rule to pick the bandwidth ĥ (e.g., the one of Imbens and Kalyanaraman
[2012]), we may end up with no data inside the specified range (i.e., there may be no observations with
|Xi − c| ≤ h); the practitioner is then forced to select a different bandwidth ad-hoc. Ideally, methods for
regression discontinuity analysis should be fully data-driven, even when X is discrete.

3

As discussed above, the goal of this paper is to show that we can systematically do
better. Regardless of the shape of the kernel K(·)
Pn in (3), local linear regression yields a
linear estimator 4 for τ , i.e., one of the form τ̂ = i=1 γ̂i Yi for weights γ̂i that depend only
on the distances Xi −c. Here, we find that if we are willing to rely on numerical optimization
tools,Pthen minimax linear estimation of τ (c), i.e., minimax among all estimators of the form
n
under
τ̂ = i=1 γ̂i Yi conditionally on Xi , is both simple and methodologically transparent

natural assumptions on the regularity of µ(w) (x). If we know that Var Yi Xi = σi2 and
that |µ00w (x)| ≤ B, we propose estimating τ (c) as follows:
( n
)
n
X
X
2 2
2
τ̂ =
γ̂i Yi , γ̂ = argminγ
γi σi + IB (γ) ,
i=1

IB (γ) :=

i=1

sup
µ0 (·),µ1 (·)

( n
X

)
γi µWi (Xi ) − (µ1 (c) − µ0 (c)) :

|µ00w (x)|

(4)

≤ B for all w, x .

i=1

Because this estimator is minimax among the class of linear estimators, it is at least as
accurate
as any local linear regression estimator in a minimax sense over all problems with

Var Yi Xi = σi2 and |µ00w (x)| ≤ B. For further discussion of related estimators, see Cai and
Low [2003], Donoho [1994], Donoho and Liu [1991], and Juditsky and Nemirovski [2009].
When Xi has a discrete distribution, the parameter τ (c) is usually not point identified
because there may not be any observations Xi in a small neighborhood of c. However,
we can get meaningful partial identification of τ (c) thanks to our bounds on the second
derivative of µw (x). Moreover, because our approach controls for bias in finite samples, the
estimator (4) is still justified in the partially identified setting and, as discussed further in
Section 2.1, provides valid confidence intervals for τ (c) in the sense of Imbens and Manski
[2004]. We view the fact that our estimator can seamlessly move between the point and
partially identified settings as an important feature.
To motivate the above estimator qualitatively, note that the first term in the minimization problem corresponds to the conditional variance of τ̂ given {Xi }, while the second term is the worst-case conditional squared bias given that |µ00w (x)| ≤ B; thus, given
our regularity assumptions, the estimator τ̂ minimizes the worst-case conditional meansquared error among all linear estimators. Because no constraints are placed
P on µw (c)
0
or
µ
(c),
the
optimization
in
(4)
also
automatically
enforces
the
constraints
i Wi γ̂i = 1,
P w
P
P
5
(1
−
W
)γ̂
=
−1,
W
(X
−
c)γ̂
=
0,
and
(1
−
W
)(X
−
c)γ̂
=
0.
This
is a coni i
i
i
i
i
i
i
i
i
i
vex program, and can be efficiently solved using readily available software described in, e.g.,
Boyd and Vandenberghe [2004].
Although this estimator depends explicitly on knowledge of σi2 and B, we note that all
practical methods for estimation in the regression discontinuity model, including Calonico
et al. [2014], Imbens and Kalyanaraman [2012] and Kolesár and Rothe [2018], require estimating related quantities in order to tune the algorithm. Then, once estimators for these
parameters have been specified, the procedure (4) is fully automatic—in particular, there is
no need to ask whether the running variable is discrete or continuous, as the optimization
is conditional on {Xi }—whereas the baseline procedures still have other choices to make,
4 We note the unfortunate terminological overlap between “local linear regression” estimators of τ and
P
“linear” estimators of type τ̂ = n
i=1 γ̂i Yi . The word linear in these two contexts refers to different things.
All “local linear regression” estimators are “linear” in the latter sense, but not vice-versa.
5 At values of γ for which these constraints are not all satisfied, we can choose µ (x) and µ (x) with
1
0
second derivative bounded by B such as to make the conditional bias arbitrarily bad, i.e., IB (γ) = +∞.
Thus, the solution γ̂ to (4) must satisfy the constraints.

4

●
●

●

●
●
●

●

●

●

●
●

●

●

●
●

●●●

●

●● ●● ● ●

●
●
●

●

●
●
●

●●

●
●

●

●●

●●●

●● ●● ● ●

●●

●
●
●
●

●

●● ●● ● ●

●●

●

●
●
●●

●

●●●

●● ● ●

●

●

●

●

●

●●●

●
●
●
●

●
●
●

−0.6

−0.4

−0.2

0.0

0.2

n = 1, 000

0.4

0.6

−0.6

−0.4

−0.2

0.0

0.2

0.4

0.6

−0.6

n = 3, 000

−0.4

−0.2

0.0

0.2

n = 9, 000

0.4

0.6

−0.6

−0.4

−0.2

0.0

0.2

0.4

0.6

n = 27, 000

Figure 1: Optimized regression discontinuity design obtained via (4), for different values of
n and two different X distributions. The red dots show the learned weighting function in a
case where the running variable X is discrete, and different support points are sampled with
different probabilities (the probability mass function is shown in the left panel of Figure 2);
the blue line shows γ(Xi ) for standard Gaussian X. We plot n4/5 γ̂i , motivated by the fact
that, with a continuous running variable, the optimal bandwidth for local linear regression
scales as hn ∼ n−1/5 . The weights γ̂i were computed with B = 5 and σ 2 = 1.

e.g., what weight function K(·) to use, or whether to debias the resulting τ̂ -estimator. See
Sections 3 and 4 for discussions on how to choose B in practice.
Figure 1 compares the weights γ̂i obtained via (4) in two different settings: one with
a discrete, asymmetric running variable X depicted in the left panel of Figure 2, and the
other with a standard Gaussian running variable. We see that, for n = 1, 000, the resulting
weighting functions look fairly similar, and are also comparable to the implicit weighting
function generated by local linear regression with a triangular kernel. However, as n grows
and the discreteness becomes more severe, our method changes both the shape and the scale
of the weights, and the discrepancy between the optimized weighting schemes for discrete
versus continuous running variables becomes more pronounced.
In the right panel of Figure 2, we also compare the worst-case conditional mean-squared
error of our method relative to that of optimally tuned local linear regression, both with
a rectangular and triangular kernel. For the smallest sample size we consider, n = 333,
the discreteness of the running variable has a fairly mild effect on estimation and—as one
might have expected—the triangular kernel is noticeably better than the rectangular kernel,
while our method is slightly better than the triangular kernel. However, as the sample size
increases, the performance of local linear regression relative to our method ebbs and flows
rather unpredictably.6

1.2

Optimized Inference with Generic Discontinuities

The methods presented above extend naturally to the general case, where Xi ∈ Rk may be
multivariate and A is unrestricted. The problem of regression discontinuity inference with
6 As a matter of intellectual curiosity, it is intriguing to ask whether there exist discrete distributions
for which the rectangular kernel may work substantially better than the triangular kernel, or whether
additional algorithmic tweaks—such as using different bandwidths on different sides of the threshold—may
have helped (in the above example, we used the same bandwidth for local linear regression on both sides of
the boundary). However, from a practical perspective, the estimator (4) removes the need to consider such
questions in applied data analysis, and automatically adapts to the structure of the data at hand.

5

1.15
1.10
1.00

1.05

Relative Excess Error

1.20

1.25

0.10
0.08
0.06
0.04

Probability Mass

0.02
0.00

Rectangular Kernel
Triangular Kernel

−0.6 −0.4 −0.2

0.0

0.2

0.4

0.6

500

X

5000

50000

n

Figure 2: Left panel: Probability mass function underlying the example in Figure 1 and the
right panel of the present figure. Right panel: Comparison of our procedure (4) with local
linear regression, both using a rectangular (K(t) = 1 ({t ≤ 1})) and triangular (K(t) =
(1 − t)+ ) kernel. We compare methods in terms of their worst-case mean-squared error
conditional on {Xi }; for local linear regression, we always chose the bandwidth to make this
quantity as small as possible. We depict performance relative to our estimator (4).
multiple running variables is considerably richer than the corresponding problem with a
single running variable, because an investigator could now plausibly hope to identify many
different treatment effects along the boundary of the treated region A. Most of the existing
literature on this setup, including Papay et al. [2011], Reardon and Robinson [2012] and
Wong et al. [2013], have focused on these questions of identification, while using some form
of local linear regression for estimation.
In the multivariate case, however, questions about how to tune local linear regression
are exacerbated, as the problems of choosing the kernel function K(·) and the bandwidth h
are now multivariate. Perhaps for this reason, it is still popular to use univariate methods
to estimate treatment effects in the multivariate setting by, e.g., using shortest distance to
the boundary of the treatment region A as a univariate running variable [Black, 1999], or
only considering a subset of the data where univariate methods are appropriate [Jacob and
Lefgren, 2004, Matsudaira, 2008].
Here, we show how our optimization-based method can be used to side-step the problem
of choosing a multivariate kernel function by hand. In addition to providing a simple-toapply algorithm, our method lets us explicitly account for the curvature of the mean-response
function µw (x) for statistical inference, thus strengthening formal guarantees relative to prior
work.
Relative to the univariate case, the multivariate case has two additional subtleties we
need to address. First, in (4) it is natural to impose a constraint |µ00w (x)| ≤ B to ensure
smoothness; in the multivariate case, however, we have more choices to make. For example,
do we constrain µw (x) to be an additive function, or do we allow for interactions? Here, we
opt for the more flexible specification, and simply require that ∇2 µw (x) ≤ B, where k·k
denotes the operator norm (i.e., the largest absolute eigenvalue of the second derivative).

6

Moreover, as emphasized by Papay et al. [2011], whereas the univariate design only enables us to identify the conditional average treatment effect at the threshold c, the multivariate design enables us to potentially identify a larger family of treatment effect functionals.
Here, we focus onPthe following two causal estimands. First, writing c for a focal point of
n
interest, let τ̂c = i=1 γ̂c,i Yi with

( n
)!2 
n
X

X
γ̂c = argminγ
γi2 σi2 +
sup
γi µWi (Xi ) − (µ1 (c) − µ0 (c))
(5)


k∇2 µw (x)k≤B
i=1

i=1

denote an estimator for the conditional average treatment effect at c. The upside of this
approach is that it gives us an estimand that is easy to interpret; the downside is that, when
curvature is non-negligible, (5) can effectively only make use of data near the specified focal
point c, thus resulting in relatively long confidence intervals.
In order to potentially improve precision, we also study weighted conditional average
treatment effect estimation with weights greedily chosen such as to make the inference as
precise as possible: In the
Pnspirit of Crump et al. [2009], Li et al. [2017] or Robins et al.
[2008], we consider τ̂∗ = i=1 γ̂∗,i Yi , with


( n
)!2 n
n

X
X
X
γi2 σi2 +
sup
γi µ0 (Xi )
:
γi W i = 1 .
(6)
γ̂∗ = argminγ


k∇2 µ0 (x)k≤B
i=1

i=1

i=1

In other words, we seek to pick weights γi that are nearly immune to bias due to curvature
of the baseline response surface µ0 (x). By construction, this estimator satisfies
( n
)
n
X
X


E τ̂∗ {Xi } − τ̄ (γ̂∗ ) ≤
sup
γ̂∗,i µ0 (Xi ) , τ̄ (γ̂∗ ) :=
Wi γ̂∗,i τ (Xi ). (7)
k∇2 µ0 (x)k≤B

i=1

i=1

P

Because
Wi γ̂∗,i = 1, we see that τ̄ (γ̂∗ ) is in fact a weighted average of the conditional
average treatment effect function τ (·) over the treated sample. If we ignored the curvature
of τ (·),
Pwe could interpret τ̂∗ as an estimate for the conditional average treatment effect at
x∗ =
γ̂∗,i Wi Xi .
In some cases, it is helpful to consider other interpretations of the estimand underlying
(6). It we are willing to assume a constant treatment effect τ (x) = τ , then τ̄ = τ , and τ̂∗ is
the minimax linear estimator for τ . Relatedly, we can always use the confidence intervals
from Section 2.1 built around τ̂∗ to test the global null hypothesis τ (x) = 0 for all x.
To gain intuition for the multivariate version of our method, we outline a simple example
building on the work of Keele and Titiunik [2014] on the effect of television advertising on
voter turnout in presidential elections. To estimate this effect, Keele and Titiunik [2014]
examine a school district in New Jersey, half of which belongs to the Philadelphia media
market and the other half to the New York media market. Before the 2008 presidential
elections, the Philadelphia half was subject to heavy campaign advertising whereas the New
York half was not, thus creating a natural experiment for the effect of television advertising
assuming the media market boundary didn’t coincide with other major boundaries within
the school district. Keele and Titiunik [2014] use this identification strategy to build a
regression discontinuity design, comparing sets of households straddling the media market
boundary.
However, despite the multivariate identification strategy, Keele and Titiunik [2014] then
reduce the problem to a univariate regression discontinuity problem for estimation: They
7

−74.66

−74.62

latitude

40.30

●
●
●●
●●●●
●●●
●
●
●●
●●
●●
●
●
●
●●
●
●●
●● ●
●●● ●●●●● ●●● ●
● ●●●
●
●
●
●●
●●● ●
●●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●●●●
●●●●●
●●
●
●
●●
●●● ●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●●●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●●●
●●● ●●●
●
●●
●●
●● ●●
●● ●
●●
●●●
●●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●● ●
●●
●
●●
●
●
●●
●●
●
●●●●●●● ●●
●●
●●●
●●●
●
●●● ●
●●
●●
●
●●
●●
●
●●
●
●●
●●
● ●●●
●●●●●
●●●
●●●●●
●●●
●●
●
●●●
●
●●
●●
●● ●●●●
●●
●●
●●
●
●●
●●
●
●●●
●
●●●
●● ● ●
●●
●●
●●●●●●
● ●
●●
●●
●● ●
●●
●
●●
●●
●
●●
●●
●●●
●
●
●
●
●
●
●
●
●●
●
●
●●●●
●
● ●
●
●●●
●
●
●
●●● ●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
● ●
●
●●
●● ●
●
●● ●●
●
●●
●●
●
●●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●● ●
●●
●●●
●●
●
●
●●●
●●●
●●●
●●●
● ●●
●●
●●
●●●●
●
●
●●
●●
●●
●●
●●●●
●●●
●
●●
●●
●●
●●●
●●
●●●
●● ●●●
●
●●● ●
●
●●
●●
●●
●
●
●
●●
●●
●
●
●
●●
●●
●●
● ●
●●
●
●
●●
●●●●
●●
●●● ●●
●●●
●
●●
●
●
●
●
●
●
●●
●
●●
●●
●●●
●●
●●
●●●●●● ● ●
●
●
●
●●
●●
●●●
●
●●
●
●
●●●●
●
●●●
●●
●
●
●
●
●● ●●●
●
●
●
●
●●●
●●
●●
●●
●
●●
●
●
●
●
●●
●● ●●
●
●
●
●
●
●
●
●
●●
● ●●
●●
●●
●●
●●
●●
●●
●●
●●
●●
●●
●●
●●
●●
●●●
●●●
●●●
●●
●●
●●
●●
●
●●
●● ●
●
●
●
●
●
●
●
●
●
● ●●
●●●● ● ●●●
●●●
●●
●●
● ●●●
●●
●●
●●●
●●●●●
●●●●
●●
●
●●●
●
●
●●●
●
●●●
●●●
●● ● ● ●
●
●
●●●
● ●● ●●
●
●●●
●●●
●●
●●
●●
●●
●●
●●
●●●
●●
●●
●●
●●
●●
●● ●
●●●● ●
●● ●
●●
●●
●●● ●
●●
●● ●●
●●
●●●
●●
●
● ●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●● ●●
●●●
●●
●●
●●●●
●● ● ●●●
●● ●●
●●
● ●●
● ●●●
●●
●
● ●●
●●
●●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●●● ●●●
●●
●●
●●
●●
● ●●● ●● ●● ●●
● ●●
●●●●
●●●●●●
●●
●●
●●● ●●
●●
●●
●●
●●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●● ●
●●
●●
● ● ●●● ● ●●● ●● ●
●●●●
●●
●●
●● ●
●●
●●
●●●● ●●
●●●
●●
●●●●
●●
●●
●
●●●
●●
●●●●
●●● ●●● ●
●
●
●●
●●
●● ●
●●
●●
●● ●●
●●
●●
●●
●●
●●●
●●
● ●●
●●
●●●
●●
●●
●●●
●●
●●
●●
●●
●●
●●
●●
● ●●●● ●●
●●●
●●
●
●
●
●●
●
●●
●●
●
●
●●●●
●●
●●●
● ●●●●●
●●
●●
●●
●●●
●●
●●●●●
●●
●●●●●
●●
●●
●●
●●
●●
●●●
●●
●●
●●
●●●●●
●●●
●
●
●
●
●
●
●
● ●●●
●
●●
●● ●
●●
●
●
●
●
●●●
●●●
●
●●
●●
●●
●●
●●●
●●
●●●●
●●●
●
●●●
●
●●
● ●●
●● ●●●
●●
●●
●
● ●●●●●
●●
●●
●●
●●●●
●●
●●
●●●
●●●●●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
● ●●● ●●
●●
●●
●●
●●●● ●●
●●
●●
●●
●● ●●●● ●●●●
●
●
●●●●●
●
●●●
●●
●●●
●
●
●●
●●
●●
●●
●
●●
●
●●
●●●
●●●
●
●
●
●●
●●
●●●●
●
●●● ●●
●●●●
●●
●
●●
● ●●
●●
●●●●●
●
●
●
●
●
●
●
●
●
●
●●●● ●● ●
● ●
●
●
●
●
●
●
●●●●●●●
●
●

40.34

●● ●
●
●●●●●
●●
●
●
●●●
●
●●
●●
●
●●
●●
●
●
●●
●
●
●
●
●●
●●
●●●
●●
●●
●●●
●●
● ●●

40.26

40.34
latitude

40.30
40.26

●●
●
●●●
●●
●●
●
●● ●●●
●●●●
● ●●●
●
●
●
●
●
●●
●●●
●
●
●
●
●●

−74.58

●●
●
●●●
●●
●●
●
●● ●●●
●●●●
● ●●●
●
●
●
●
●
●●
●●●
●
●
●
●
●●

●
●
●●
●●●●
●●●
●
●
●●
●●
●●
●
●
●
●●
●
●●
●● ●
●●● ●●●●● ●●● ●
● ●●●
●
●
●
●●
●●● ●
●●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●●●●
●●●●●
●●
●
●
●●
●●● ●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●●●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●●●
●●● ●●●
●
●●
●●
●● ●●
●● ●
●●
●●●
●●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●● ●
●●
●
●●
●
●
●●
●●
●
●●●●●●● ●●
●●
●●●
●●●
●
●●● ●
●●
●●
●
●●
●●
●
●●
●
●●
●●
● ●●●
●●●●●
●●●
●●●●●
●●●
●●
●
●●●
●
●●
●●
●● ●●●●
●●
●●
●●
●
●●
●●
●
●●●
●
●●●
●● ● ●
●●
●●
●●●●●●
● ●
●●
●●
●● ●
●●
●
●●
●●
●
●●
●●
●●●
●
●
●
●
●
●
●
●
●●
●
●
●●●●
●
● ●
●
●●●
●
●
●
●●● ●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
● ●
●
●●
●● ●
●
●● ●●
●
●●
●●
●
●●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●● ●
●●
●●●
●●
●
●
●●●
●●●
●●●
●●●
● ●●
●●
●●
●●●●
●
●
●●
●●
●●
●●
●●●●
●●●
●
●●
●●
●●
●●●
●●
●●●
●● ●●●
●
●●● ●
●
●●
●●
●●
●
●
●
●●
●●
●
●
●
●●
●●
●●
● ●
●●
●
●
●●
●●●●
●●
●●● ●●
●●●
●
●●
●
●
●
●
●
●
●●
●
●●
●●
●●●
●●
●●
●●●●●● ● ●
●
●
●
●●
●●
●●●
●
●●
●
●
●●●●
●
●●●
●●
●
●
●
●
●● ●●●
●
●
●
●
●●●
●●
●●
●●
●
●●
●
●
●
●
●●
●● ●●
●
●
●
●
●
●
●
●
●●
● ●●
●●
●●
●●
●●
●●
●●
●●
●●
●●
●●
●●
●●
●●
●●●
●●●
●●●
●●
●●
●●
●●
●
●●
●● ●
●
●
●
●
●
●
●
●
●
● ●●
●●●● ● ●●●
●●●
●●
●●
● ●●●
●●
●●
●●●
●●●●●
●●●●
●●
●
●●●
●
●
●●●
●
●●●
●●●
●● ● ● ●
●
●
●●●
● ●● ●●
●
●●●
●●●
●●
●●
●●
●●
●●
●●
●●●
●●
●●
●●
●●
●●
●● ●
●●●● ●
●● ●
●●
●●
●●● ●
●●
●● ●●
●●
●●●
●●
●
● ●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●● ●●
●●●
●●
●●
●●●●
●● ● ●●●
●● ●●
●●
● ●●
● ●●●
●●
●
● ●●
●●
●●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●●● ●●●
●●
●●
●●
●●
● ●●● ●● ●● ●●
● ●●
●●●●
●●●●●●
●●
●●
●●● ●●
●●
●●
●●
●●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●● ●
●●
●●
● ● ●●● ● ●●● ●● ●
●●●●
●●
●●
●● ●
●●
●●
●●●● ●●
●●●
●●
●●●●
●●
●●
●
●●●
●●
●●●●
●●● ●●● ●
●
●
●●
●●
●● ●
●●
●●
●● ●●
●●
●●
●●
●●
●●●
●●
● ●●
●●
●●●
●●
●●
●●●
●●
●●
●●
●●
●●
●●
●●
● ●●●● ●●
●●●
●●
●
●
●
●●
●
●●
●●
●
●
●●●●
●●
●●●
● ●●●●●
●●
●●
●●
●●●
●●
●●●●●
●●
●●●●●
●●
●●
●●
●●
●●
●●●
●●
●●
●●
●●●●●
●●●
●
●
●
●
●
●
●
● ●●●
●
●●
●● ●
●●
●
●
●
●
●●●
●●●
●
●●
●●
●●
●●
●●●
●●
●●●●
●●●
●
●●●
●
●●
● ●●
●● ●●●
●●
●●
●
● ●●●●●
●●
●●
●●
●●●●
●●
●●
●●●
●●●●●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
● ●●● ●●
●●
●●
●●
●●●● ●●
●●
●●
●●
●● ●●●● ●●●●
●
●
●●●●●
●
●●●
●●
●●●
●
●
●●
●●
●●
●●
●
●●
●
●●
●●●
●●●
●
●
●
●●
●●
●●●●
●
●●● ●●
●●●●
●●
●
●●
● ●●
●●
●●●●●
●
●
●
●
●
●
●
●
●
●
●●●● ●● ●
● ●
●
●
●
●
●
●
●●●●●●●
●
●

−74.66

longitude

●● ●
●
●●●●●
●●
●
●
●●●
●
●●
●●
●
●●
●●
●
●
●●
●
●
●
●
●●
●●
●●●
●●
●●
●●●
●●
● ●●

−74.62

−74.58

longitude

Figure 3: Weighting functions for the geographic regression discontinuity example of Keele
and Titiunik [2014], where points depict potential voters within a single school district and
the solid black line is a media market boundary. The left panel depicts an optimal weighting
function for the conditional average treatment effect at the point c marked with a boldface-×
as in (5), while the right one allows for a weighted treatment effect as in (6) or, equivalently,
shows the optimal weighting function for a constant effect. Households below the line are
treated (i.e., in the Philadelphia media market), whereas those above it are controls (i.e.,
in the New York media market). The color of the point depicts the γ-weight: Red points
receive positive weight and blue points receive negative weight, while the shading indicates
absolute value of the weight (darker is larger).
first compute Euclidean distances Di = kXi − ck2 to a focal point c, and then use Di as
a univariate running variable. In contrast, our approach allows for transparent inference
without needing to rely on such a reduction. Figure 3 depicts γ-weights
P generated by our
optimized approach; the resulting treatment effect estimator is then i γi Yi . Qualitatively,
we replicate the “no measurable effect” finding of Keele and Titiunik [2014], while directly
and uniformly controlling for spatial curvature effects. We discuss details, including placebo
diagnostics and the choice of tuning parameter, in Section 4.2.
We also see that, at least here, standard heuristics used to reduce the multivariate
regression discontinuity problem to a univariate one are not sharp. In the setup of the
left panel of Figure 3, where we seek to estimate the treatment effect at a focal point c,
some treated points due west of c get a positive weight, whereas points the same distance
south from c get a mildly negative weight, thus violating the heuristic of Keele and Titiunik
[2014] that weights should only depend on Di = kXi − ck2 . Meanwhile, we can compare the
approach in the right panel of Figure 3, where we allow for averaging of treatment effects
along the boundary, to the popular heuristic of using shortest distance to the boundary of
the treatment region as a univariate running variable [Black, 1999]. But this reduction again
does not capture the behavior of our optimized estimator: There are some points at the
eastern edge of the treated region that are very close to the boundary, but get essentially zero
weight (presumably because there are no nearby units on the control side of the boundary).

8

1.3

Related Work

The idea of constructing estimators of the type (4) that are minimax with respect to a
regularity class for the underlying data-generating process has a long history in statistics.
In early work, Legostaeva and Shiryaev [1971] and Sacks and Ylvisaker [1978] independently
studied inference in “almost” linear models that arise from taking a Taylor expansion around
a point; see also Cheng et al. [1997]. For a broader discussion of minimax linear estimation
over non-parametric function classes, see Cai and Low [2003], Donoho [1994], Ibragimov
and Khas’minskii [1985], Johnstone [2011], Juditsky and Nemirovski [2009], and references
therein. An important result in this literature that, for many problems of interest, minimax
linear estimators are within a small explicit constant of being minimax among all estimators
[Donoho and Liu, 1991].
Armstrong and Kolesár [2018] apply these methods to regression discontinuity designs,
resulting in an estimator of the form (4), except with weights7
( n
)
X
2 2
2
γ̂ = argminγ
γi σi + AB (γ) ,
(8)
i=1

AB (γ) =

sup
µ0 (·),µ1 (·)

( n
X

γi µWi (Xi ) − τ (c) : |µw (x) − µw (c) −

i=1

µ0w (c)(x

B
− c)| ≤ (x − c)2
2

)
.

Now, although this class of functions is cosmetically quite similar to the bounded-secondderivative class used in (4), we note that the class of weights allowed for in (8) is substantially
larger, even if the value of B is the same. This is because the functions µw (·) underlying the
above weighting scheme need not be continuous, and can in fact have jumps of magnitude
B(x − c)2 /2. Given that the key assumption underlying regression discontinuity designs
is continuity of the conditional means of the potential outcomes at the threshold for the
running variable, it would appear to be reasonable to impose continuity away from the
threshold as well. Allowing for jumps through the condition (8) can make the resulting confidence intervals for τ (c) substantially larger than they are under the smoothness condition
with bounded second derivatives. One key motivation for the weighting scheme (8) rather
than our proposed one (4) appears to be that the optimization problem induced by (8) is
substantially easier, and allows for closed-form solutions for γ̂i . Conversely, we are aware of
no closed-form solution for (4), and instead need to rely on numeric convex optimization.
In the special case where the running variable X is assumed to have a continuous density around the threshold c, there has been a considerable number of recent proposals for
asymptotic confidence intervals while imposing smoothness assumptions on µw (x). Calonico,
Cattaneo, and Titiunik [2014] propose a bias correction to the local linear regression estimator that allows for valid inference, and Calonico, Cattaneo, and Farrell [2018] provide
further evidence that such bias corrections may be preferable to undersmoothing. Meanwhile, Armstrong and Kolesár [2016] show that when µw (x) is twice differentiable and X has
a continuous density around c, we can use local linear regression with a bandwidth chosen
to optimize mean-squared error as the basis for bias-adjusted confidence intervals, provided
we inflate confidence intervals by an appropriate, universal constant (e.g., to build 95%
confidence intervals, one should use a critical threshold of 2.18 instead of 1.96). Gao [2017]
characterizes the asymptotically optimal kernel for the regression discontinuity parameter
7 Armstrong and Kolesár [2018] also consider a more general setting where we assume accuracy of the
k-th order Taylor expansion of µw (x) around c; and, in fact, our method also extends to this setting. Here,
however, we focus on second-derivative bounds, which are by far the most common in applications.

9

under the bounded second derivative assumption with a continuous running variable. As
discussed above, the value of our approach relative to this literature is that we allow for
considerably more generality in the specification of the regression discontinuity design: the
running variable X may be discrete and/or multivariate, and the treatment boundary may
be irregularly shaped.
Optimal inference with multiple running variables is less developed than in the univariate
case. Papay et al. [2011] and Reardon and Robinson [2012] study local linear regression
with a “small” bandwidth, but do not account for finite sample bias due to curvature.
Zajonc [2012] extends the analysis of Imbens and Kalyanaraman [2012] to the multivariate
case, and studies optimal bandwidth selection for continuous running variables given second
derivative bounds; the inference, however, again requires undersmoothing. Keele, Titiunik,
and Zubizarreta [2015] consider an approach to geographic regression discontinuity designs
based on matching. To our knowledge, the approach we present below is the first to allow
for uniform, bias-adjusted inference in the multivariate regression discontinuity setting.
Finally, although local methods for inference in the regression discontinuity design have
desirable
theoretical
properties, many practitioners also seek to estimate τ (c) by fitting


E Yi Xi = x using a global polynomial expansion along with a jump at c; see Lee and
Lemieux [2010] for a review and examples. However, as argued by Gelman and Imbens
[2017], this approach is not recommended, as the model with the best in-sample fit may
provide poor estimates of the discontinuity parameter. For example, high-order polynomials
may give large influence to samples i for which Xi is far from the decision boundary c, and
thus lead to unreliable performance.
Another approach to regression discontinuity designs (including in the discrete case)
builds on randomization inference; see Cattaneo, Frandsen, and Titiunik [2015], Cattaneo,
Idrobo, and Titiunik [2017] and Li, Mattei, and Mealli [2015] for a discussion. The problem of specification testing for regression discontinuity designs is considered by Cattaneo,
Jansson, and Ma [2016], Frandsen [2017] and McCrary [2008].

2
2.1

Formal Considerations
Uniform Asymptotic Inference

Our main result verifies that optimized designs can be used for valid asymptotic inference
about τ (c). We here consider the problem of estimating a conditional average treatment
effect at a point c as in (4) or (5); similar arguments extend directly to the averaging case
as in (6). Following, e.g., Robins and van der Vaart [2006], we seek confidence intervals Iα
that attain uniform coverage over the whole regularity set under consideration:

lim inf inf P [µ1 (c) − µ0 (c) ∈ Iα ] : ∇2 µw (x) ≤ B for all w, x ≥ 1 − α.
(9)
n→∞

As in Armstrong and Kolesár [2018], our approach to building such confidence intervals
relies on an explicit characterization of the bias of τ̂ rather than on undersmoothing. Our
key result is as follows.


Theorem 1. Suppose that we have a moment bound E[(Yi − E Yi Xi )q Xi = x] ≤ C
uniformly over all x ∈ Rk , for some exponent q > 2 and constant C ≥ 0. Suppose, moreover,
that 0 < σmin ≤ σi for all i = 1, ..., n for a deterministic value σmin , and that none of the

10

weights γ̂i derived in (4) or (5) dominates all the others, i.e.,8
max

1≤i≤n

n
 2 .X
γ̂i
γ̂i2 →p 0.

(10)

i=1

Then, our estimator τ̂ from (4) or (5) is asymptotically Gaussian,
(τ̂ − b (γ̂))



s (γ̂) ⇒ N (0, 1) , b(γ̂) =

n
X

γ̂i µWi (Xi ) − τ (c), s2 (γ̂) :=

i=1

n
X

γ̂i2 σi2 ,

(11)

i=1

where b (γ̂) denotes the conditional bias, and s2 (γ̂) →p 0.
Now, in solving the optimization problem (4), we also obtain an explicit bound t̂ on
the conditional bias, b(γ̂) ≤ t̂, and so can use the following natural construction to obtain
confidence intervals for τ (c) [Imbens and Manski, 2004],

τ (c) ∈ τ̂ ± lα , lα = min l : P [|b + s (γ̂) Z| ≤ l] ≥ α for all |b| ≤ t̂ , Z ∼ N (0, 1) , (12)
where α is the significance level. These confidence intervals are asymptotically uniformly
valid in the sense of (9), i.e., for any α0 < α, there is a threshold nα0 for which, if n ≥ nα0 , the
confidence intervals (12) achieve α0 -level coverage for any functions µw (·) in our regularity
class.
Finally, whenever Xi does not have support arbitrarily close to c, e.g., in the case where
Xi has a discrete distribution, the parameter τ (c) is not point identified. Rather, even with
infinite data, the strongest statement we could make is that
n
τ (c) ∈ I ∗ , I ∗ = range µ(1) (c) − µ(0) (c) : ∇2 µ(w) (x) ≤ B, and
o
(13)


µ(w) (x) = E Yi Xi = x, Wi = w for all x, w ∈ supp {Xi , Wi } , ,
where supp {Xi , Wi } denotes the support of (Xi , Wi ). In this case, our confidence intervals
(12) remain valid for τ (c); however, they may not cover the whole optimal identification
interval I ∗ . In partially identified settings, this type of confidence intervals (i.e., ones that
cover the parameter of interest but not necessarily the whole identification interval) are
advocated by Imbens and Manski [2004]. From the perspective of the practitioner, an
advantage of our approach is that intervals for τ (c) have the same interpretation whether
or not τ (c) is point identified, i.e., uniformly in large samples, τ (c) will be covered with
probability 1 − α. Then, asymptotically, intervals (12) will converge to a point if and only
if τ (c) is point identified. For a further discussion of regression discontinuity inference with
discrete running variables, see Kolesár and Rothe [2018].

2.2

Implementation via Convex Optimization

In our presentation so far, we have discussed several non-parametric convex optimization
problems, and argued that solving them was feasible given advances in the numerical optimization literature over the past few decades [e.g., Boyd and Vandenberghe, 2004]. Here,
8 The bound on the relative contribution of any single γ̂ is needed to obtain a Gaussian limit distribution
i
for τ̂ . In related literature, Armstrong and Kolesár [2018] follow Donoho [1994], and side-step this issue by
assuming Gaussian errors Yi (w) − µw (Xi ), in which case no central limit theorem is needed. Conversely,
Athey, Imbens, and Wager [2018] adopt an approach more similar to ours, and explicitly bound γ̂i from
above during the optimization.

11

we present a concrete solution strategy via quadratic programming over a discrete grid, and
show that the resulting discrete solutions converge uniformly to the continuous solution as
the grid size becomes small.9
To do so, we start by writing the optimization problems underlying (4), (5) and (6) in a
unified form. Given a specified focal point c, we seek to solve
minimize
γ, t

n
X

γi2 σi2 + B 2 t2 subject to

i=1

n
X

γi (f0 (Xi ) + ψ w(Xi ) (f1 (Xi ) − f0 (Xi ))) ≤ t,

i=1

for all fw (c) = 0, ∇fw (c) = 0, ∇2 fw (x) ≤ 1 with w ∈ {0, 1}
n
n
X
X
w(Xi )γi = 1,
(1 − w(Xi ))γi = −1,
i=1

(14)

i=1

n
X

γi (Xi − c) = 0, ψ

i=1

n
X

(2w(Xi ) − 1) γi (Xi − c) = 0,

i=1

where w(x) denotes the treatment assignment function, and ψ lets us toggle between different
problem types. If we want to estimate the CATE at c as in (5) we set ψ = 1, whereas if we
want an optimally weighted CATE estimator as in (6) we set ψ = 0.
To further characterize the solution to this problem, we can use Slater’s constraint qualification [e.g., Ponstein, 2004, Theorem 3.11.2] to verify that strong duality holds, and that
the optimum of (14) matches the optimum of the following problem:
( n
X
maximize inf
γi2 σi2 + B 2 t2
γ, t

f (·), λ

+ λ1
+ λ2

n
X
i=1
n
X

i=1

!
γi (f0 (Xi ) + ψ w(Xi ) (f1 (Xi ) − f0 (Xi ))) − t
!
γi w(Xi ) − 1

+ λ3

i=1

+

n
X

n
X

!
γi (1 − w(Xi )) + 1

(15)

i=1

)
γi (λ4 + ψ λ5 (2w(Xi ) − 1)) (Xi − c)

i=1

subject to fw (c) = 0, ∇fw (c) = 0,

∇2 fw (x) ≤ 1 for w ∈ {0, 1} ,

λ1 , ≥ 0, λ2 , λ3 ∈ R, λ4 , λ5 ∈ Rk ,
where k is the number of running variables. Here, we also implicitly used von Neumann’s
minimax theorem to move the maximization over f outside the inf γ, t {} statement.
The advantage of this dual representation is that, by examining first order conditions in
the inf γ, t {} term, we can analytically solve for γ and t in the dual objective, e.g.,



− 2σi2 γ̂i = λ̂1 fˆ0 (Xi ) + ψ w(Xi ) fˆ1 (Xi ) − fˆ0 (Xi ) + λ̂2 w(Xi ) + . . . ,
(16)
9 In the case where X is univariate, the resulting optimization problem is a classical one, and arguments
made by Karlin [1973] imply that the weights γ̂i can be written as γ̂i = g(Xi ) where g is a perfect spline;
and our proposed optimization strategy reflects this fact. However, in the multivariate case, we are not
aware of a similar simple characterization.

12

where fˆ0 (·), fˆ1 (·), λ̂1 , etc., are the maximizers of (15). Carrying out the substitution results
in a more tractable optimization problem over the space of twice differentiable functions f ,
along with a finite number of Lagrange parameters λj :
n

1 λ21
1 X G2i
+ λ2 − λ3
+
minimize
2
4 i=1 σi
4 B2
f˜(·), λ
subject to Gi = f˜(Xi ) + λ2 w(Xi ) + λ3 (1 − w(Xi ))
+ λ4 (Xi − c) + ψ λ5 (2w(Xi ) − 1)(Xi − c)


f˜(x) = f˜0 (x) + ψ w(x) f˜1 (x) − f˜0 (x) , λ1 ≥ 0, λ2 , λ3 ∈ R, λ4 , λ5 ∈ Rk ,
f˜w (c) = 0, ∇f˜w (c) = 0,

(17)

∇2 f˜w (x) ≤ λ1 for w ∈ {0, 1} ,

where f˜w (x) in the above problem corresponds to λ1 fw (x) in (15), and we can recover our
b i /2 and t̂ = λ̂1 /(2B 2 ). The upshot of these manipulations
weights of interest via γ̂i = −σi−2 G
is that (17) can be approximated via a finite-dimensional quadratic program. In our software
implementation optrdd, we use this type of a finite dimensional approximation to obtain γ̂i
following the construction described in the proof of Proposition 2.
Proposition 2. Suppose that Xi ∈ X belong to some compact, convex set X ⊂ Rk . Then,
for any tolerance level η > 0, there exists a finite-dimensional quadratic program that can
recover the solution γ̂ to (17) with L2 -error at most η.

2.3

Minimizing Confidence Interval Length

As formulated in (4), our estimator seeks to minimize the worst-case mean-squared error
over the specified bounded-second-derivative class. However, in some applications, we may
be more interested in making the confidence intervals (12) as short as possible; and our
approach can easily be adaptedPfor this objective. To do so, consider the minimization
n
objective in (14). Writing v̂ 2 = i=1 γ̂i2 σi2 , we see that both the worst-case mean-squared
2
2 2
error, v̂ + B t̂ , and the confidence interval length in (12) are monotone increasing functions
of v̂ and t̂; the only difference is in how they weight these two quantities at the optimum.
Now, to derive the full Pareto frontier of pairs (v̂, t̂), we can simply re-run (14) with the
term B 2 t2 in the objective replaced with λB 2 t2 , for some λ > 0. A practitioner wanting
to minimize the length of confidence intervals could consider computing this whole solution
path to (14), and then using the value of λ that yields the best intervals; this construction
provides minimax linear fixed-length confidence intervals [Donoho, 1994]. Since this procedure never looks at the responses Yi , the inferential guarantees for the resulting confidence
intervals remain valid.
In our applications, however, we did not find a meaningful gain from optimizing over λ
instead of just minimizing worst-case mean-squared error as in (14), and so did not pursue
this line of investigation further. This observation is in line with the analytic results of
Armstrong and Kolesár [2016] who showed that, when X has a continuous density and
µw (x) is twice differentiable, using the mean-squared error optimal bandwidth for local
linear regression is over 99% efficient relative to using a bandwidth that minimizes the
length of bias-adjusted confidence intervals.
Finally, although it is beyond the scope of the present paper, it is interesting to ask
whether we can generalize our approach to obtain asymptotically quasi-minimax estimators
13

for τ (c) when the per-observation noise-scale σi needs to be estimated from the data. The
resulting question is closely related to the classical issue of when feasible generalized least
squares can emulate generalized least squares; see Romano and Wolf [2017] for a recent
discussion.

3

Univariate Optimized Designs in Practice

P 2 2
To use this result in practice, we of course need to estimate the sum
γ̂i σi and choose
a bound B on curvature. Estimating the former is relatively routine; and we recommend
the following. First, we estimate µw (x) globally, or over a large plausible relevant interval
around the threshold, and average the square of the residuals Ri = Yi − µ̂Wi (Xi ) to obtain
an estimate σ̂ 2 of the average value of σi2 . Then, we optimize weights γ̂i using (4), with
σi2 ← σ̂ 2 . Finally, once we have chosen the weights γi , we estimate the sampling error of τ̂
as below, noting that the estimator will be consistent under standard conditions
ŝ2 (γ̂) =

n
X

2

γ̂i2 (Yi − µ̂Wi (Xi )) , ŝ2 (γ̂)

X

γ̂i2 σi2 ≥ 1 − oP (1).

(18)

i=1

Conceptually, this strategy is comparable to first running local linear regression without
heteroskedasticity adjustments to get a point estimate, but then ensuring that the uncertainty quantification is heteroskedasticity-robust [White, 1980]. We summarize the resulting
method as Procedure 1. We always encourage plotting the weights γ̂i against Xi when applying our method.
Conversely, obtaining good bounds on the curvature B is more difficult, and requires
problem specific insight. In particular, adapting to the true curvature µw (x) without apriori bounds for B is not always possible; see Armstrong and Kolesár [2018], Bertanha
and Moreira [2016], and references therein. In applications, we recommend considering a
range of plausible values of B that could be obtained, e.g., from subject-matter expertise
or from considering the mean-response function globally. For example, we could estimate
µw (x) using a quadratic function globally, or over a large plausible relevant interval around
the threshold, and then multiply maximal curvature of the fitted model by a constant, e.g.,
2 or 4. The larger the value of B we use the more conservative the resulting inference. In
practice, it is often helpful to conduct a sensitivity analysis for the robustness of confidence
intervals to changing B; see Figure 6 for an example.10
10 An interesting wrinkle is that if we are able to bound B in large samples—but not uniformly—then
confidence intervals built using estimated values of B will have asymptotic but not uniform coverage.
11 Here, the algorithm assumes that all observations are of roughly the same quality (i.e., we do not know
that σi2 is lower for some observations than others). If we have a-priori information about the relative
magnitudes of the conditional variances of different observations, e.g., some pairs outcomes Yi are actually
aggregated over many observations, then we should run steps 2 and 3 below using appropriate inversevariance weights. Our software allows for such weighting.
12 Only considering data over an a-priori specified “large plausible relevant interval” around c that safely
contains all the data relevant to fitting τ (c) can also be of computational interest. Our method relies
on estimating a smooth non-parametric function over the whole range of x; and being able to reduce the
relevant range of x a-priori reduces the required computation. Although defining such plausibility intervals
is of course heuristic, our method ought not be too sensitive to how exactly the interval was chosen. For
example, in the setup considered in Section 3.1, the optimal bandwidth for local linear regression is around
3 or 6 years depending on the amount of assumed smoothness (and choosing a good bandwidth is very
important); conversely, using plausibility intervals extending 10, 15, or 20 years on both sides of c appears
to work reasonably well. When running the method (4), one should always make sure that the weights γ̂i
get very small near the edge of the plausibility interval; if not, the interval should be made larger.

14

Procedure 1. Optimized Regression Discontinuity Inference
This algorithm provides confidence intervals for the conditional average treatment effect
τ (c), given an a-priori bound B on the second derivative of the functions µw (x). We
assume that the conditional variance parameters σi2 are unknown; if they are known,
they should be used as in (4). This procedure is implemented in our R package optrdd.11
1. Pick a large window r, such that data with |Xi − c| > r can be safely ignored
without loss of efficiency. (Here, we can select r = ∞, but this may result in
unnecessary computational burden.)
2. Run ordinary least-squares regression of Yi on the interaction of Xi and Wi over
the window |Xi − c| ≤ r. Let σ̂ 2 be the residual error from this regression.
3. Obtain γ̂ via the quadratic program (14), with σi set to σ̂ and weights outside of
the range |Xi − c| ≤ r set to 0.
4. Confirm that the optimized weights γ̂i are small for |Xi − c| ≈ r. If not, start
again with a larger value of r.12
Pn
Pn
5. Estimate τ̂ = i=1 γ̂i Yi and ŝ2 = i=1 γ̂i2 (Yi − µ̂Wi (Xi ))2 , where the µ̂Wi (Xi )
are predictions from the least squares regression from step 1.
6. Build confidence intervals as in (12).

3.1

Application: The Effect of Compulsory Schooling

In our first application, we consider a dataset from Oreopoulos [2006], who studied the
effect of raising the minimum school-leaving age on earnings as an adult. The effect is
identified by the UK changing its minimum school-leaving age from 14 to 15 in 1947, and
the response is log-earnings among those with non-zero earnings (in 1998 pounds). This
dataset exhibits notable discreteness in its running variable, and was used by Kolesár and
Rothe [2018] to illustrate the value of their bias-adjusted confidence intervals for discrete
regression discontinuity designs. For our analysis, we pre-process our data exactly as in
Kolesár and Rothe [2018]; we refer the reader to their paper and to Oreopoulos [2006] for a
more in-depth discussion of the data.
As in Kolesár and Rothe [2018], we seek to identify the effect of the change in minimum
school-leaving age on average earnings via a local analysis around the regression discontinuity; our running variable is the year in which a person turned 14, with a treatment
threshold at 1947. Kolesár and Rothe [2018] consider analysis using local linear regression
with a rectangular kernel and a bandwidth chosen such as to make to make their honest confidence intervals as short as possible (recall that we can measure confidence interval length
without knowing the point estimate, and so tuning the interval length does not invalidate
inference). Here, we also consider local linear regression with a triangular kernel, as well as
our optimized design.13
13 Oreopoulos [2006] analyze the dataset using a global polynomial specification with clustered random
variables, following Lee and Card [2008]. However, as discussed in detail by Kolesár and Rothe [2018], this
approach does not yield valid confidence intervals.

15

5e−04
0e+00

gamma

−5e−04
−1e−03

Optimized
Triangular K.
Rectangular K.
40

42

44

46

48

50

52

54

Year turned 14

Figure 4: Weighting functions γ̂(Xi ) produced explicitly by our estimator (4), and implicitly via local linear regression with a rectangular or triangular kernel. Both local linear
regression methods have a finite bandwidth, and the effective weights of γ̂(Xi ) = 0 outside
this bandwidth are not shown. The weighting functions were generated with B = 0.012.
B
0.003
0.006
0.012
0.03

rect. kernel
0.0213 ± 0.0761
0.0578 ± 0.0894
0.0645 ± 0.1085
0.0645 ± 0.1460

tri. kernel
0.0321 ± 0.0737
0.0497 ± 0.0867
0.0633 ± 0.1037
0.0710 ± 0.1367

optimized
0.0302 ± 0.0716
0.0421 ± 0.0841
0.0557 ± 0.1003
0.0710 ± 0.1329

Table 1: Confidence interval (α = 95%) for the effect of raising the minimum school-leaving
age on average log-earnings, as given by local linear regression with a rectangular kernel, local
linear regression with a triangular kernel, and our optimized method (4). The confidence
intervals account for curvature effects, provided the second derivative is bounded by B.
In order to obtain confidence intervals, it remains to choose a bound B. Following the
discussion in Section 3, a 2nd-order polynomial fit with a “large” bandwidth of either 12 or
18 has a curvature of 0.006 (the estimate is insensitive to the choice of large bandwidth);
thus, we tried B = 0.006 and B = 0.012. We also consider the more extreme choices
B = 0.003 and B = 0.03. For σi2 , we proceed as discussed in Section 3. Figure 4 shows the
effective γ̂(Xi ) weighting functions for all 3 considered methods, with B = 0.012.
We present results in Table 1. Overall, these results are in line with those presented in
Figure 2. The optimized method yields materially shorter confidence intervals than local
linear regression with a rectangular kernel: for example, with B = 0.03, the rectangular kernel intervals are 11% longer. In comparison, the triangular kernel comes closer to matching
the performance of our method, although the optimized method still has shorter confidence
intervals. Moreover, when considering comparisons with the triangular kernel, we note that
the rectangular kernel is far more prevalent in practice, and that the motivation for using
the triangular kernel often builds on the optimality results of Cheng et al. [1997]. And, once

16

number of students
summer school attendance

fail reading
fail math pass math
3,586
1,488
69.6%
61%

pass reading
fail math pass math
10,331
15,336
52.9%
10.6%

Table 2: Summary statistics for a subset of the dataset of Matsudaira [2008].
one has set out on a quest for optimal weighting functions, there appears to be little reason
to not just use the actually optimal weighting function (4).
Finally, we note that a bound B on the second derivative also implies that the quadratic
approximation (8) holds with the same bound B. Thus, we could in principle also use the
method of Armstrong and Kolesár [2018] to obtain uniform asymptotic confidence intervals
here. However, the constraint (8) is weaker than the actual assumption we were willing to
make (i.e., that the functions µw (·) have a bounded second derivative), and so the resulting
confidence intervals are substantially larger. Using their approach on this dataset gives
confidence intervals of 0.0518 ± 0.0969 with B = 0.006 and 0.0682 ± 0.1760 with B = 0.03;
these intervals are not only noticeably longer than our intervals, but are also longer than the
best uniform confidence intervals we can get using local linear regression with a rectangular
kernel as in Kolesár and Rothe [2018]. Thus, the use of numerical convex optimization tools
that let us solve (4) instead of (8) can be of considerable value in practice.

4
4.1

Applications with Multivariate Discontinuities
A Discontinuity Design with Two Cutoffs

We next consider the behavior of our method in a specific variant of the multiple running
variable problem motivated by a common inference strategy in education. Some school districts mandate students to attend summer school if they fail a year-end standardized test
in either math or reading [Jacob and Lefgren, 2004, Matsudaira, 2008], and it is of course
important to understand the value of such summer schools. The fact that students are
mandated to summer school based on a sharp test score cutoff suggests a natural identification strategy via regression discontinuities; however, standard univariate techniques cannot
directly be applied as the regression discontinuity now no longer occurs along a point, but
rather along a surface in the bivariate space encoding both a student’s math and reading
scores.
We illustrate our approach using the dataset of Matsudaira [2008]. As discussed above,
the goal is to study the impact of summer school on future test scores, and the effect of
summer school is identified by a regression discontinuity: At the end of the school year,
students need to take year-end tests in math and reading; then, students failing either of
these tests are mandated to attend summer school. Here, we focus on the 2001 class of
graduating 5th graders, and filter the sample to only include the n = 30, 741 students whose
5th-grade math and reading scores both fall between 40 points of the passing threshold;
this represents 44.7% of the full sample. Matsudaira [2008] analyzed this dataset with
univariate methods, by using reading score as a running variable and only consider the
subset of students who passed the math exam, etc. This allows for a simple analysis, but
may also result in a loss of precision.14
14 In

order to make a formal power comparison, we need to compare two estimators that target the same

17

1.0

●

●

●

●

●

●

●

●

●

●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

● ●

●

●

●

●

●

●

●

●
●

●
●

●
●

●
●

●
●

●
●

●
●

●
●

●
●

●
●

●
●

●
●

●
●

● ●
● ●

●
●

●
●

●
●

●
●

●
●

●
●

●
●

●
●

●
●

●
●

●
●

●
●

●
●

●
●

●
●

●
●

●
●

●
●

●
●

●
●

●
●

● ●
● ●

●
●

●
●

●
●

●
●

●
●

●
●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

● ●

●

●

●

●

●

●

−1.0

−0.5

0.0

0.5

0.0
−0.5

reading score

0.5

●

●
●

−1.0

1.0
0.5
0.0

reading score

−0.5
−1.0

●

●

1.0

−1.0

math score

−0.5

0.0

0.5

●

1.0

math score

unweighted CATE (5)

weighted CATE (6)

Figure 5: Weights γ̂ underlying treatment effect estimates of the effect of summer school on
the following year’s reading scores, using both (5) which seeks to estimate the conditional
average treatment effect (CATE) at c = (0, 0), and the estimator (6) which allows weighted
CATE estimation. The size of γ̂i is depicted by the color, ranging from dark red (very
positive) to dark blue (very negative).
P In the right panel, the diamond marks the weighted
mean of the treated Xi -values, i.e.,
γ̂i Wi Xi . These plots were generated with a maximum
second derivative bound of B = 0.5 × 40−2 .
We present some summary statistics in Table 2. Clearly, not all students mandated to
attend summer school in fact attend, and some students who pass both tests still need to
attend for reasons discussed in Matsudaira [2008]. That being said, the effect of passing
tests on summer school attendance is quite strong and, furthermore, the treatment effect
of being mandated to summer school is interesting in its own right, so here we perform an
“intent to treat” analysis without considering non-compliance.
We consider both of our optimized estimators, (5) and (6), and compare weight functions
γ̂ learned by both methods in Figure 5. The estimator τ̂c is in fact quite conservative, and
only gives large weights to students who scored close to c. Our choice of estimating the
conditional average treatment effect at (0, 0) may have been particularly challenging, as it
is in a corner of control-space and so does not have particularly many control neighbors.
In contrast, the weighted method τ̂∗ appears to have effectively learned matching: It
constructs pairs of observations all along the treatment discontinuity, thus allowing it to use
more data while canceling out curvature effects due to µ0 (x). As seen in Table 2, in this
sample, it is much more common to fail math and pass reading than vice-versa; thus, the
mean of the samples used for “matching” lies closer to the math pass/fail-boundary than
the reading one.
In order to proceed with our approach, we again need to choose a value for B. Running a
2nd order polynomial regression on the next year’s math and reading scores for both treated
and control students separately, we find the largest curvature effect among the reading score
estimand. In the simplest case where τ (x) = τ is constant, our estimator (6) presents an unambiguous gain
in power over those considered in Matsudaira [2008].

18

subject
math
math
reading
reading

estimator:
B
0.5 × 40−2
1.0 × 40−2
0.5 × 40−2
1.0 × 40−2

unweighted CATE (5)
conf. int.
m. bias samp. err
0.037 ± 0.093 0.030
0.038
0.013 ± 0.126 0.041
0.052
0.014 ± 0.098 0.030
0.041
−0.015 ± 0.130 0.040
0.054

weighted CATE (6)
conf. int.
m. bias s. err
0.076 ± 0.037 0.009 0.017
0.068 ± 0.043 0.011 0.019
0.044 ± 0.037 0.009 0.017
0.047 ± 0.043 0.011 0.019

Table 3: Estimates for the effect of summer school on math and reading scores on the
following year’s test, using different estimators and choices of B. Reported are bias-adjusted
95% confidence intervals, a bound on the maximum bias given our choice of B, and an
estimate of the sampling error conditional on {Xi }.
of control students; roughly a curvature of 0.46 × 40−2 along the (1, 2) direction. Thus, we
run our algorithm with both an optimistic choice of B = 0.5 × 40−2 and a more conservative
choice B = 1.0 × 40−2 (we report curvatures on the “scale” of the plots in Figure 5, such
that a curvature of 1.0 × 40−2 results in a worst-case bias of 1 in the corners of the plot).
Results are given in Table 3. As expected, the confidence intervals using the weighted
method (6) are much shorter than those obtained using (5), allowing for a 0.95-level significant detection in the first case but not in the second. Since the weighting method allows
for shorter confidence intervals, and in practice seems to yield a matching-like estimator, we
expect it to be more often applicable than the unweighted estimator (5).
Figure 6 presents some further diagnostics for our result. The first two panels depict
a sensitivity analysis for our weighted CATE result: We vary the maximum bound B on
the second derivative, and examine how our confidence intervals change.15 The result on
the effect of summer school on math scores appears to be fairly robust, as we still get
significant bias-aware 95% confidence intervals at B = 2 × 40−2 , which is 4 times the largest
apparent curvature observed in the data. The third panel plots a measure of
Pthe effective
size of the treated and control samples used by the algorithm, ESSw = 1/ {i:Wi =w} γ̂i2 .
Although our analysis sample has almost exactly the same number of treated and control
units (W = 0.501), it appears that our algorithm is able to make use of more control than
treated samples, perhaps because the treated units are “wrapped around” the controls.
Finally, it is of course natural to ask whether the bivariate specification considered here
gave us anything in addition to the simpler approach used by Matsudaira [2008], i.e., of
estimating the treatment effect of summer school on the next year’s math exam by running
a univariate regression discontinuity analysis on only those students who passed the reading
exam, and vice-versa to the effect on the reading exam.16 We ran both of these analyses
using our method (14), again considering bounds B = 0.5 × 40−2 , 1 × 40−2 on the second
derivative. For math, we obtained 95% confidence intervals of 0.083±0.040 and 0.079±0.047
for the smaller and larger B-bounds respectively; for reading, we obtained 0.037 ± 0.075 and
15 We note that, if the CATE function τ (·) is not constant, then our weighted CATE estimand
P
τ̄∗ = i Wi γ̂i τ (Xi ) may vary with B. The result in Figure 6 should thus formally be interpreted as either a sensitivity analysis for the constant treatment effect parameter τ if we are willing to assume constant
treatment effects, or as a robustness check for our rejection of the global null hypothesis τ (x) = 0 for all x.
16 The estimator of Matsudaira [2008] is not exactly comparable to the two we consider. For example, when
only focusing on students who passed the reading exam, his estimator effectively averages treatment effects
over the math pass/fail boundary but not the reading pass/fail boundary. In contrast, we either estimate
the conditional average treatment effect at a point c (5), or allow for averaging over the full boundary (6).
It is unclear whether the restriction of Matsudaira [2008] to averaging over only one of the two boundary
segments targets a meaningfully more interpretable estimand than (6).

19

2

3

max second derivative

Math Conf. Interval

4

0

1

2

3

max second derivative

Reading Conf. Interval

4

1500
1000

effective sample size

2000

0.02 0.04 0.06 0.08 0.10

tau
1

−0.02

tau

0.02 0.04 0.06 0.08 0.10
−0.02

0

Treated
Controls

0.5

1.0

2.0

max second derivative

Effective Samp. Size

Figure 6: The first two panels depict a sensitivity analysis for our weighted CATE result,
for the math and reading outcomes respectively. We plot point estimates along with biasaware 95% confidence intervals for different choices of B; the choices of B used in Table 3
are indicated with dotted
P lines. The third panel depicts effective sample sizes used by the
procedure, ESSw = 1/ {i:Wi =w} γ̂i2 . For a given value of B, the γ̂i used for the math and
reading outcomes are the same. In all cases, B is multiplied by 402 for readability.
0.030 ± 0.090. In both cases, the corresponding bounds for the weighted estimator (6) in
Table 3 are shorter, despite accounting for the possibility of bivariate curvature effects. The
difference is particularly strong for the reading outcome, since our estimator τ̂∗ can also use
students near the math pass/fail-boundary for improved precision.17

4.2

A Geographic Discontinuity Design

Our final application expands on the example of Keele and Titiunik [2014], briefly discussed in Section 1.2, the goal of which is to estimate the effect of political advertising on
participation in presidential elections by comparing households straddling a media market
boundary. As discussed in Keele and Titiunik [2014], inference hinges on the fact that they
found a media market boundary that appears not to coincide with any other major administrative boundaries, thus making it more plausible that discontinuous responses across the
geographic boundary are in fact caused by varying media exposure.
The way Keele and Titiunik [2014] approach this problem is that, given a focal point c,
they first measure Euclidean distance of each sample to it, Di = kXi − ck2 , and then use
Di as a running variable in a univariate regression discontinuity analysis. They establish
consistency of their approach via a local regression argument following Hahn et al. [2001],
and also discuss advantages of their approach relative to one that considers shortest distance
to the treatment boundary (rather than c) as input to a univariate analysis. Beyond consistency arguments, however, the resulting estimator has no guarantees in terms of statistical
optimality. For example, the approach of Keele and Titiunik [2014] collapses all treated (and
17 The corresponding headline numbers from Matsudaira [2008] are a 95% confidence interval of 0.093 ±
0.029 for the effect on the math score, and 0.046 ± 0.045 for the reading score; see Tables 2 and 3, reduced
form estimates for 5th graders. These confidence intervals, however, do not formally account for bias.
They estimate the discontinuity parameter using a global cubic fit; such methods, however, do not reliably
eliminate bias [Gelman and Imbens, 2017].

20

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

−74.66

−74.62

latitude

40.30

●
●
●
●
●
●
●
●
●●
●
●
●
●
●●●
●
●
●●
●
●
●
●
●
●
●
● ●
●
●
●●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●●
●
●
●
●
●●
●
●● ●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●● ●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
● ●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
● ●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
● ●●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●●●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●●
●
●
●
●
●
●
● ●
●
●
●
●
●
●●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●●
●
●
●●
●●
●●
●
●
●
●
●
●
●
●●
●
●●
●
●
●●
●
●
●●
● ●
●
●
●
●
●
●
●
●●
●
●
●
●
● ●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
● ●
●●
●
●●
●
●
●
●
●●●
● ●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●●
●
●
●
● ● ●
●
●●●●●
●
●
●●
●●
●
●
●
●●
●
●●●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●● ●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●●
●
●
●
●●
●●
●●
●
●
●
● ●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
● ●
● ●
●
●●
●
●●
●
●
●
●
● ●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
● ●
●
●
●
●
●
●
●
●
●
●
● ●●
●
●
●
●
●●
●
●
●
●
●● ●
●
●●
● ●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●●
●●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
● ●
●
●●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
● ●
●● ●
●
●
●●
●
●
●
●
●
●
● ●● ● ● ●
●
● ●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
● ●
●●
●
●
●●
●●
● ●
●
●
●
●●
●
●
● ●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●●
●
●●
●
●
● ●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
● ●
●●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●●●
●
●
●
● ●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●●
●●●
●
●
●
●
●
●
●● ●
●●
●
●
●
●
●●●
●● ●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●●●
● ●
●
●
●
●
●
●●●
●
●
●●
●
●
●●●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●●
●
●
●●
●
●
●● ●
●
●
●
●
●
●●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●●
●
●●
●
●
●
● ●●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●● ●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●●
●
●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●●
●
● ●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
● ●●
●
●●
●
●
●
●
●
● ●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●●
●●
●●
●
●●●
●●
●
●
●
●
●
●
●
●●
●
●●
●
● ●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●● ●●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●●●
●
●●
●
●
●
●
●
●
●●
●●●
●
●
●
●
●
●
●
●
●
●
●
●●●●
●
●
●

40.34

●

●
●
●
●
●
●
●
●
●
●
●●
●
●

40.26

40.34
latitude

40.30
40.26

●
●●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●●
●

−74.58

●
●●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●●
●

●

●
●
●
●
●
●
●
●
●
●
●●
●
●

●
●
●
●
●
●
●
●
●●
●
●
●
●
●●●
●
●
●●
●
●
●
●
●
●
●
● ●
●
●
●●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●●
●
●
●
●
●●
●
●● ●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●● ●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
● ●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
● ●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
● ●●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●●●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●●
●
●
●
●
●
●
● ●
●
●
●
●
●
●●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●●
●
●
●●
●●
●●
●
●
●
●
●
●
●
●●
●
●●
●
●
●●
●
●
●●
● ●
●
●
●
●
●
●
●
●●
●
●
●
●
● ●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
● ●
●●
●
●●
●
●
●
●
●●●
● ●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●●
●
●
●
● ● ●
●
●●●●●
●
●
●●
●●
●
●
●
●●
●
●●●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●● ●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●●
●
●
●
●●
●●
●●
●
●
●
● ●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
● ●
● ●
●
●●
●
●●
●
●
●
●
● ●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
● ●
●
●
●
●
●
●
●
●
●
●
● ●●
●
●
●
●
●●
●
●
●
●
●● ●
●
●●
● ●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●●
●●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
● ●
●
●●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
● ●
●● ●
●
●
●●
●
●
●
●
●
●
● ●● ● ● ●
●
● ●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
● ●
●●
●
●
●●
●●
● ●
●
●
●
●●
●
●
● ●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●●
●
●●
●
●
● ●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
● ●
●●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●●●
●
●
●
● ●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●●
●●●
●
●
●
●
●
●
●● ●
●●
●
●
●
●
●●●
●● ●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●●●
● ●
●
●
●
●
●
●●●
●
●
●●
●
●
●●●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●●
●
●
●●
●
●
●● ●
●
●
●
●
●
●●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●●
●
●●
●
●
●
● ●●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●● ●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●●
●
●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●●
●
● ●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
● ●●
●
●●
●
●
●
●
●
● ●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●●
●●
●●
●
●●●
●●
●
●
●
●
●
●
●
●●
●
●●
●
● ●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●● ●●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●●●
●
●●
●
●
●
●
●
●
●●
●●●
●
●
●
●
●
●
●
●
●
●
●
●●●●
●
●
●

−74.66

longitude

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

−74.62

−74.58

longitude

Figure 7: Non-parametric estimates for turnout in the 2008 presidential elections (left panel)
and average age (right panel), as a function of geographic covariates. There is considerable
variation in the underlying signal: localized turnout rates range from 26% to 46%, while
localized age averages range from 43 years to 68 years. Darker regions correspond to larger
values of the response. These predictive surfaces were used to pick the curvature bound B.
respectively control) observations at distance Di from c to the same point in the univariate
representation, which may increase bias.
We apply our method to this problem and, following Keele and Titiunik [2014], seek
to measure the effect of media exposure on our main outcome of interest (participation
in the 2008 presidential elections), as well as placebo outcomes (age, black race, political
party affiliation, and gender). There were n = 24, 460 observations in this school district.
Relative to the other examples considered in our paper, the responses here have a much richer
dependence on the geographic running variables, thus making the problem of bounding B
much more delicate. To illustrate this issue, Figure 7 depicts non-parametric estimates of
expected turnout and age as a function of spatial location, and reveals strong local structure.
For example, we see two regions in the south-eastern and northern edges of the school district
with a notably higher average age than elsewhere.
We did not find simple quadratic methods for bounding B to be sufficiently sensitive to
this strong local structure, and thus chose instead to set B as the worst-case local curvature
of a non-parametric global fit. Specifically, we ran a cross-validated ridge regression with
interacted 7th-order natural splines as features on each side of the media market boundary,
and then set B at the 95-th percentile of the estimated operator norm curvature over all
the training points. Cross-validation was carried out via the 1.se rule in the R-package
glmnet [Friedman et al., 2010]. We caution that this type of local choice of B is only
possible on fairly large datasets with strong local effects. With smaller datasets or weaker
signals, cross-validation may over-smooth the estimated response function, thus resulting in
an over-optimistically small value for B.
Table 4 shows results from our analysis, both in terms of pointwise estimates following
(5) and weighted estimates (6). We set the curvature parameter B separately for each

21

τ at point
weighted τ

outcome pt. estimate
2008 turnout
0.130
age
8.605
black
-0.006
democrat
0.019
female
0.065
2008 turnout
0.045
age
3.105
black
0.016
democrat
-0.027
female
0.019

conf. interval max. bias std. err.
(-0.072, 0.331)
0.071
0.079
(-8.949, 26.158)
6.552
6.682
(-0.080, 0.069)
0.032
0.026
(-0.080, 0.118)
0.035
0.039
(-0.060, 0.191)
0.045
0.049
(-0.107, 0.197)
0.052
0.060
(-8.702, 14.913)
4.147
4.649
(-0.043, 0.075)
0.024
0.021
(-0.110, 0.055)
0.026
0.034
(-0.083, 0.120)
0.033
0.042

Table 4: Estimates for the effect of television advertising on voter turnout, as well as placebo
outcomes (age in years, black indicator, Democrat indicator, and female indicator). The
upper half of the table displays estimates for the conditional average treatment effect at the
point marked X in Figure 3, whereas the lower half allows for a weighted treatment effect
as in (6). We provide a point estimate for the treatment effect, a 95% confidence interval,
as well as the worst case bias and sampling error used to produce the confidence interval.
different outcome, following the procedure outlined above.18 The γ-weighting functions
underlying our analysis for the primary outcome are shown in Figure 3. Overall, we replicate
the finding of Keele and Titiunik [2014], in that the media market boundary does not
appear to have a substantial association with either the primary outcome of the placebo
outcomes. Relative to existing methods, the value of our approach is that it allows for
transparent inference of causal parameters using the original untransformed data (rather
than relying on a hand-constructed univariate running variable), and uniformly accounts
for spatial curvature effects.

5

Discussion

In this paper, we introduced an optimization-based approach to statistical inference in
regression discontinuity designs. By using numerical convex optimization tools, we explicitly
derive the minimax linear estimator for the regression discontinuity parameter under bounds
on the second derivative of the conditional response surface. Because any method based on
local linear regression is also a linear estimator of this type, our approach dominates local
linear regression in terms of minimax mean-squared error. We also show how our approach
can be used to build uniformly valid confidence intervals.
A key advantage of our procedure is that, given bounds on the second derivative, estimation of the regression discontinuity parameter is fully automatic. The proposed algorithm
is the same whether the running variable is continuous or discrete, and whether or not τ (c)
is point identified. Moreover, it does not depend on the shape of treatment assignment
boundary when X is multivariate. We end our discussion with some potential extensions of
our approach.
18 Keele and Titiunik [2014] also had a placebo outcome indicating Hispanic ethnicity. However, this
outcome does not appear to have much local structure, and cross-validated ridge regression learned a constant
function (resulting in B = 0). Thus, we omit this placebo check from our present analysis; another alternative
would have been to fall back on our previous approach and to select B via a global quadratic fit.

22

Fuzzy regression discontinuities In the present paper, we only considered sharp regression discontinuities, where the treatment assignment Wi is a deterministic function of
Xi . However,
 there is also considerable interest in fuzzy discontinuities, where Wi is random but P Wi = 1 Xi = x has a jump at the threshold c; see Imbens and Lemieux [2008]
for a review. In this case, it is common to interpret the indicator 1 ({Xi ≥ c}) as an instrument, and then to estimate a local average treatment effect via two-stage local linear
regression [Imbens and Angrist, 1994]. By analogy, we can estimate treatment effects with
fuzzy regression discontinuity via two-stage optimized designs as
n
n
. X
X
τ̂LAT E =
γ̂i Yi
γ̂i Wi ,
(19)
i=1

i=1

where the γ̂i are obtained as in (14) with an appropriate choice penalty on the maximal
squared imbalance t2 . This approach would clearly be consistent based on results established
in this paper; however, deriving the best way to trade off bias and variance in specifying
γ̂i and extending the approach of Donoho [1994] for uniform asymptotic inference is left for
future work.
Balancing auxiliary covariates In many applications, we have access to auxiliary covariates Zi ∈ Rp that are predictive of Yi but unrelated to the treatment assignment near
the boundary c. As discussed in, e.g., Imbens and Lemieux [2008], such covariates are not
necessary for identification; but controlling for them can increase robustness to hidden biases. One natural way to use such auxiliary covariates in our optimized designs is to require
the weights γ̂i to balance these covariates, i.e., to add a constraint
n
X

γ̂i Zij = 0 for all j = 1, ..., p

(20)

i=1

to the optimization problem (4).19 In principle, if the distribution of Zi is in fact independent
of Xi when Xi is near the threshold c, we would expect the balance conditions (20) to hold
approximately even if we do not enforce them; however, explicitly enforcing such balance
may improve robustness.20 If we have an additive, linear dependence of Yi on Zi , then
enforcing balance as in (20) would also result in variance
reduction,
as the conditional


variance of our estimator
τ̂
would
now
depend
on
Var
Y
X
,
Z
,
which
is always smaller
i
i
i


or equal to Var Yi Xi .
Working with generic regularity assumptions Following standard practice in the
regression discontinuity literature, we focused on minimax linear inference under bounds on
the second derivative of µw (·) [e.g., Kolesár and Rothe, 2018, Imbens and Kalyanaraman,
2012]. However, our conceptual framework can also be applied with higher order smoothness
assumptions via bounds on the k-th derivative of µw (·), and can easily be combined with
other forms of structural information about the conditional response functions (e.g., perhaps
we know from theory that the functions µw (·) must be concave). Thanks to the flexibility
of our optimization-based approach, acting on either of these ideas would simply involve
implementing the required software using standard convex optimization libraries.
19 The constraint (20) is a linear constraint, and so the optimization problem (14) remains a quadratic
program with this constraint.
20 A related idea would be to use the covariates Z for post-hoc specification testing as in Heckman and
i
Hotz [1989] or Imbens and Lemieux [2008]. Their strategy is to obtain weights γ̂i without looking at the
Zi , and then to reject the modeling strategy if (20) does not hold approximately.

23

References
Joshua D Angrist and Victor Lavy. Using Maimonides’ rule to estimate the effect of class
size on scholastic achievement. The Quarterly Journal of Economics, 114(2):533–575,
1999.
Timothy B Armstrong and Michal Kolesár. Simple and honest confidence intervals in nonparametric regression. 2016.
Timothy B Armstrong and Michal Kolesár. Optimal inference in a class of regression models.
Econometrica, 86(2):655–683, 2018.
Susan Athey, Guido W Imbens, and Stefan Wager. Approximate residual balancing: Debiased inference of average treatment effects in high dimensions. Journal of the Royal
Statistical Society: Series B (Statistical Methodology), (just-accepted), 2018.
Richard A Berk and David Rauma. Capitalizing on nonrandom assignment to treatments:
A regression-discontinuity evaluation of a crime-control program. Journal of the American
Statistical Association, 78(381):21–27, 1983.
Marinho Bertanha and Marcelo J Moreira. Impossible inference in econometrics: Theory and
applications to regression discontinuity, bunching, and exogeneity tests. arXiv preprint
arXiv:1612.02024, 2016.
Sandra E Black. Do better schools matter? Parental valuation of elementary education.
The Quarterly Journal of Economics, 114(2):577–599, 1999.
Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University
Press, 2004.
T Tony Cai and Mark G Low. A note on nonparametric estimation of linear functionals.
Annals of Statistics, 31(4):1140–1153, 2003.
Sebastian Calonico, Matias D Cattaneo, and Rocio Titiunik. Robust nonparametric confidence intervals for regression-discontinuity designs. Econometrica, 82(6):2295–2326, 2014.
Sebastian Calonico, Matias D Cattaneo, and Max H Farrell. On the effect of bias estimation
on coverage accuracy in nonparametric inference. Journal of the American Statistical
Association, pages 1–13, 2018.
Matias D Cattaneo, Brigham R Frandsen, and Rocio Titiunik. Randomization inference in
the regression discontinuity design: An application to party advantages in the us senate.
Journal of Causal Inference, 3(1):1–24, 2015.
Matias D Cattaneo, Michael Jansson, and Xinwei Ma. Simple local regression distribution
estimators with an application to manipulation testing. Unpublished Working Paper,
University of Michigan, and University of California Berkeley, 2016.
Matias D Cattaneo, Nicolás Idrobo, and Rocıo Titiunik. A practical introduction to regression discontinuity designs: Part II, 2017.
Devin Caughey and Jasjeet S Sekhon. Elections and the regression discontinuity design:
Lessons from close us house races, 1942-2008. Political Analysis, 19(4):385–408, 2011.
24

Ming-Yen Cheng, Jianqing Fan, and James S Marron. On automatic boundary corrections.
The Annals of Statistics, 25(4):1691–1708, 1997.
Richard K Crump, V Joseph Hotz, Guido W Imbens, and Oscar A Mitnik. Dealing with
limited overlap in estimation of average treatment effects. Biometrika, 96(1):187–199,
2009.
David L Donoho. Statistical estimation and optimal recovery. The Annals of Statistics, 22
(1):238–270, 1994.
David L Donoho and Richard C Liu. Geometrizing rates of convergence, III. The Annals
of Statistics, 19(2):668–701, 1991.
Brigham R Frandsen. Party bias in union representation elections: Testing for manipulation
in the regression discontinuity design when the running variable is discrete. In Regression Discontinuity Designs: Theory and Applications, pages 281–315. Emerald Publishing
Limited, 2017.
Jerome Friedman, Trevor Hastie, and Rob Tibshirani. Regularization paths for generalized
linear models via coordinate descent. Journal of Statistical Software, 33(1):1, 2010.
Wayne Yuan Gao. Minimax linear estimation at a boundary point. 2017.
Andrew Gelman and Guido Imbens. Why high-order polynomials should not be used in regression discontinuity designs. Journal of Business & Economic Statistics, (just-accepted),
2017.
Jinyong Hahn, Petra Todd, and Wilbert Van der Klaauw. Identification and estimation
of treatment effects with a regression-discontinuity design. Econometrica, 69(1):201–209,
2001.
James Heckman and Joseph Hotz. Alternative methods for evaluating the impact of training
programs. Journal of the American Statistical Association, 84(408):862–880, 1989.
Il’dar Abdullovich Ibragimov and Rafail Zalmanovich Khas’minskii. On nonparametric
estimation of the value of a linear functional in Gaussian white noise. Theory of Probability
& Its Applications, 29(1):18–32, 1985.
Guido W Imbens and Joshua D Angrist. Identification and estimation of local average
treatment effects. Econometrica, 62(2):467–475, 1994.
Guido W Imbens and Karthik Kalyanaraman. Optimal bandwidth choice for the regression
discontinuity estimator. The Review of Economic Studies, 79(3):933–959, 2012.
Guido W Imbens and Thomas Lemieux. Regression discontinuity designs: A guide to
practice. Journal of Econometrics, 142(2):615–635, 2008.
Guido W Imbens and Charles F Manski. Confidence intervals for partially identified parameters. Econometrica, 72(6):1845–1857, 2004.
Guido W Imbens and Donald B Rubin. Causal Inference in Statistics, Social, and Biomedical
Sciences. Cambridge University Press, 2015.

25

Brian A Jacob and Lars Lefgren. Remedial education and student achievement: A
regression-discontinuity analysis. Review of Economics and Statistics, 86(1):226–244,
2004.
Iain M Johnstone. Gaussian estimation: Sequence and wavelet models. Manuscript, 2011.
Anatoli B Juditsky and Arkadi S Nemirovski. Nonparametric estimation by convex programming. The Annals of Statistics, 37(5A):2278–2300, 2009.
Samuel Karlin. Some variational problems on certain Sobolev spaces and perfect splines.
Bulletin of the American Mathematical Society, 79(1):124–128, 1973.
Luke Keele, Rocı́o Titiunik, and José Zubizarreta. Enhancing a geographic regression discontinuity design through matching to estimate the effect of ballot initiatives on voter
turnout. Journal of the Royal Statistical Society, Series A, 178(1):223–239, 2015.
Luke J Keele and Rocio Titiunik. Geographic boundaries as regression discontinuities.
Political Analysis, 23(1):127–155, 2014.
Michal Kolesár and Christoph Rothe. Inference in regression discontinuity designs with a
discrete running variable. American Economic Review, (just-accepted), 2018.
Rafael Lalive. How do extended benefits affect unemployment duration? A regression
discontinuity approach. Journal of Econometrics, 142(2):785–806, 2008.
David S Lee. Randomized experiments from non-random selection in US House elections.
Journal of Econometrics, 142(2):675–697, 2008.
David S Lee and David Card. Regression discontinuity inference with specification error.
Journal of Econometrics, 142(2):655–674, 2008.
David S Lee and Thomas Lemieux. Regression discontinuity designs in economics. Journal
of Economic Literature, 48(2):281–355, 2010.
IL Legostaeva and AN Shiryaev. Minimax weights in a trend detection problem of a random
process. Theory of Probability & Its Applications, 16(2):344–349, 1971.
Fan Li, Alessandra Mattei, and Fabrizia Mealli. Evaluating the causal effect of university
grants on student dropout: evidence from a regression discontinuity design using principal
stratification. The Annals of Applied Statistics, 9(4):1906–1931, 2015.
Fan Li, Kari Lock Morgan, and Alan M Zaslavsky. Balancing covariates via propensity score
weighting. Journal of the American Statistical Association, pages 1–11, 2017.
Jens Ludwig and Douglas L Miller. Does head start improve children’s life chances? Evidence
from a regression discontinuity design. The Quarterly journal of economics, 122(1):159–
208, 2007.
Jordan D Matsudaira. Mandatory summer school and student achievement. Journal of
Econometrics, 142(2):829–850, 2008.
Justin McCrary. Manipulation of the running variable in the regression discontinuity design:
A density test. Journal of Econometrics, 142(2):698–714, 2008.

26

Jersey Neyman. Sur les applications de la théorie des probabilités aux experiences agricoles:
Essai des principes. Roczniki Nauk Rolniczych, 10:1–51, 1923.
Philip Oreopoulos. Estimating average and local average treatment effects of education
when compulsory schooling laws really matter. The American Economic Review, 96(1):
152–175, 2006.
John P Papay, John B Willett, and Richard J Murnane. Extending the regressiondiscontinuity approach to multiple assignment variables. Journal of Econometrics, 161
(2):203–207, 2011.
Jacob Ponstein. Approaches to the Theory of Optimization. Cambridge University Press,
2004.
Jack Porter. Estimation in the regression discontinuity model. 2003.
Sean F Reardon and Joseph P Robinson. Regression discontinuity designs with multiple
rating-score variables. Journal of Research on Educational Effectiveness, 5(1):83–104,
2012.
James Robins and Aad van der Vaart. Adaptive nonparametric confidence sets. The Annals
of Statistics, 34(1):229–253, 2006.
James Robins, Lingling Li, Eric Tchetgen, and Aad van der Vaart. Higher order influence
functions and minimax estimation of nonlinear functionals. In Probability and Statistics: Essays in Honor of David A. Freedman, pages 335–421. Institute of Mathematical
Statistics, 2008.
Joseph P Romano and Michael Wolf. Resurrecting weighted least squares. Journal of
Econometrics, 197(1):1–19, 2017.
Donald B Rubin. Estimating causal effects of treatments in randomized and nonrandomized
studies. Journal of Educational Psychology, 66(5):688, 1974.
Jerome Sacks and Donald Ylvisaker. Linear estimation for approximately linear models.
The Annals of Statistics, pages 1122–1137, 1978.
Donald L Thistlethwaite and Donald T Campbell. Regression-discontinuity analysis: An
alternative to the ex post facto experiment. Journal of Educational Psychology, 51(6):
309–317, 1960.
William MK Trochim. Research design for program evaluation: The regression-discontinuity
approach. Sage Publications, Inc, 1984.
Halbert White. A heteroskedasticity-consistent covariance matrix estimator and a direct
test for heteroskedasticity. Econometrica, 48(4):817–838, 1980.
Vivian C Wong, Peter M Steiner, and Thomas D Cook. Analyzing regression-discontinuity
designs with multiple assignment variables: A comparative study of four estimation methods. Journal of Educational and Behavioral Statistics, 38(2):107–141, 2013.
Tristan Zajonc. Regression discontinuity design with multiple forcing variables. In Essays
on Causal Inference for Public Policy, pages 45–81. 2012.

27

6
6.1

Appendix: Proofs
Proof of Theorem 1

By construction, we already know that




E τ̂ X1 , ..., Xn − τ (c) = b (γ̂) , Var τ̂ X1 , ..., Xn = s2 (γ̂) .
Thus, to establish our desired result, it suffices to establish asymptotic Gaussianity of
n

 X
τ̂ − E τ̂ X1 , ..., Xn =
γ̂i εi , εi = Yi − µWi (Xi ).
i=1

To do so, we use the Lyapunov central limit theorem. Thanks to our bound on the q-th
moment of the εi , we know that
n
X

n
.
X
. q

q
q
q
E (γ̂i εi ) γ̂i
s (γ̂) ≤
C γ̂iq (σmin
kγ̂k2 )

i=1

i=1

≤

C
q
σmin

sup
1≤i≤n





|γ̂i | kγ̂k2

q−2

→p 0

by assumption (10). Thus, in particular, there exists a sequence an such that an → 0 and
" n
#
X 
 q
q
lim sup P
E (γ̂i εi ) γ̂i / s (γ̂) > an = 0.
n→∞

i=1

.
Pn
Now, define weights γ̃i such that γ̃ = γ̂ whenever i=1 E[(γ̂i εi )q P
γ̂i ] sq (γ̂) ≤
Pann , and γ̃i =
n
1 for all i = 1, ..., n else. By Lyapunov’s central limit theorem, i=1 γ̃i εi / ( i=1 γi2 σi2 )1/2
is asymptotically standard normal. Moreover, we know that γ̃ = γ̂ with probability tending
to 1, and so our estimator τ̂ must also be conditionally standard normal as claimed.

6.2

Proof of Proposition 2

Our proposed approximation to (17) has two main components. First, we need to handle the
second derivative constraints k∇2 f˜w (x)k ≤ λ1 for all values of x ∈ X . One approach would
be to write this as a positive semidefinite matrix constraint, i.e., ∇2 f˜w (x)  λ1 Ik×k , and
then rely on semidefinite programming tools; here, however, we find that we can effectively
approximate k∇2 f˜w (x)k ≤ λ1 via a finite number of linear constraints. Second, we need
address the effect of finite differencing on our optimization.
For simplicity, throughout the following discussion, we take the curvature bound λ1 to be
fixed, and only consider minimization over λ2 , ..., λ5 , G, and the f˜w . Given this assumption,
as well as the fact that X is bounded, we can verify that our objective is Lipschitz, which is
helpful in bounding approximation errors. Then, after establishing that our approximation
scheme is valid for any fixed value of λ1 , it follows that the approximation is also valid when
we optimize over λ1 because our objective is strongly convex in λ1 .
We start by approximating the curvature constraint. Let A be any positive semidefinite
matrix, and let V = {v1 , ..., vl } be a set of vectors. Then, the spectral norm of A is bounded

28

by
−1
kAkV , where
kAkV ≤ kAk ≤ 1 − α2 (V)
o
n

2
kAkV := sup v > Av kvk2 : v ∈ V and
n n
o
o
2
α2 (V) = sup inf 1 − (u · v) / kvk : v ∈ V : kuk2 = 1 ;

(21)

note that α(V) measures the sine of the worst angular error we may make by approximating
a vector u with an element from V. The first inequality in (21) follows from the definition of
the spectral norm, while the second is immediate by expressing elements in V in the spectral
basis of A.
In our finite dimensional approximation to (17), we propose replacing the spectral norm
constraints k·k with constraints of the type k·kV described above. To motivate this strategy,
write L (k·k , λ1 ) for the objective value of (17) given a fixed value of λ1 , and let L (k·kV , λ1 )
denote the related quantity with the approximating constraint.
Because the value of the objective cannot get worse as we increase the size the size of
the constraint set, (21) implies that for any value of λ1 ,
 
L (k·kV , λ1 ) ≤ L (k·k , λ1 ) ≤ L k·kV , 1 − α2 (V) λ1 .
Moreover, any feasible solution to (17) with bound λ1 on the constraint can be turned into
˜
a feasible solution with bound
P −2cλ12 by simply multiplying f (·) and λ2 , ..., λ5 by c. When we
do so, the positive term
σi Gi term scales quadratically in c, while the negative term
λ2 − λ3 scales linearly in c (the second term must be negative, since otherwise we could
improve on the objective by setting everything to 0); in particular, this implies that
 

L k·kV , 1 − α2 (V) λ1 ≤ 1 − α2 (V) L (k·kV , λ1 ) .
Chaining these two inequalities together, we find that

L (k·kV , λ1 ) ≤ L (k·k , λ1 ) ≤ 1 − α2 (V) L (k·kV , λ1 ) ,
and so, given a small enough value of α2 (V), the values of the two objectives match. Finally,
because the objective of (17) is strongly convex in f˜(X1 ), ..., f˜(Xn ), λ2 , λ3 , λ4 , λ5 , we find
that if L (k·kV , λ1 ) − L (k·k , λ1 ) is goes to zero, then the solutions to the two optimization
problems must also converge to each other.
The remaining question is then to exhibit constructions of approximating sets V that
make α2 (V) small. With a two-dimensional running variable, there exist grid-aligned sets
V with 4 and 8 elements respectively, depicted in Figure 8, that attain values of α2 (V) of
0.15 and 0.05 respectively. This construction naturally extends to arbitrary k, resulting in
sets V whose size scales polynomially in the inverse of the tolerance α2 . In practice, though,
this other computational strategies may be needed when k is larger than 2 or 3.
Our second task is in showing that we can solve the k·kV -norm optimization problem
using a finite difference approximation. Let h be a bandwidth, and let Xh be the set of
grid points of a lattice with edge-length h that fall within h, and assume that: the set V
consists only of vectors whose end points lie on the unit lattice (as in, e.g., Figure 8), the
basis vectors ej belong to V for j = 1, ..., k, and that the center point c is in Xh . Then,
writing Nh (x) for a mapping that takes all points in X to one of the nearest points in Xh ,

29

Figure 8: Examples of grid-aligned approximating sets V in R2 , with 4 and 8 (nonredundant) elements respectively.

we consider the following finite-dimensional approximation to (17):
n

minimize

1 X −2 2
λ2
σi Gi + 12 + λ2 − λ3
4 i=1
4B

subject to Gi = f˜(h) (Nh (Xi )) + λ2 w(Xi ) + λ3 (1 − w(Xi ))
+ λ4 (Xi − c) + ψ λ5 (2w(Xi ) − 1)(Xi − c)


(h)
(h)
(h)
f˜(h) (x) = f˜0 (x) + ψ w(x) f˜1 (x) − f˜0 (x) ∀x ∈ Xh , λ1 ≥ 0, λ2 , λ3 ∈ R, λ4 , λ5 ∈ Rk ,
f˜w(h) (c) = 0, f˜w(h) (c + hej ) = 0 for all j = 1, ..., k, w ∈ {0, 1}
−2

khvk2

f˜w(h) (x + hv) + f˜w(h) (x − hv) − 2f˜w(h) (x) ≤ λ1 for all x ∈ Xh , v ∈ V, w ∈ {0, 1} .

By analogy to our earlier notation, write Lh (k·kV , λ1 ) for the objective value in the above
problem for a fixed value of λ1 .
We first bound Lh (k·kV , λ1 ) in terms of L (k·kV , λ1 ). To do so, simply note that any
feasible solution to the continuous problem can be turned into a solution to the discrete
problem by setting
f˜w(h) (x) ← f˜w (x) − h−1

k
X

(xj − cj ) f˜ (c + hej ) ,

j=1

and making appropriate corrections to λ2 , ..., λ5 . The goal of this affine correction is to
(h)
ensure that the discrete derivate constraints f˜w (c + hej ) = 0 are satisfied. Now, because
ej ∈ V and ∇f˜w (c) = 0, we know that f˜ (c + hej ) ≤ λ1 h2 /2; thus, because X is compact,
we find that Lh (k·kV , λ1 ) ≤ L (k·kV , λ1 ) + O (hλ1 ).
(h)
Now, to go the other direction, we start we a feasible solution f˜w (x) to the discrete
solution, and need to construct a feasible solution to the continuous problem without making
the objective much worse. To do so, let ϕk (·) for the standard Gaussian density in Rk , and
30

define

g̃w (z) =

h
βh

k X


ϕk

x∈Xh

x−z
βh



f˜w(h) (x) + aw · z,

with βh = h1/4 , where aw is a linear correction term used to ensure that ∇g̃w (c) = 0, and
we again adjust the parameters λ2 , ..., λ5 appropriately. Then we set f˜w (z) := g̃w (z) for all
points z for which a ball of radius h1/8 is contained inside X , and use Whitney’s extension
theorem to extend this function to the rest of X without increasing the supremum of the
second derivative. In order to verify that L (k·kV , λ1 ) ≤ Lh (k·kV , λ1 ) + o(1), it suffices to
show that the new f˜w (z) almost satisfy the desired curvature constraints with almost as
good an objective value as before; the rest of the argument then follows as usual.
As the a full discussion is rather technical, we here focus on simply showing how to
bound the second derivative of g̃w (z) along vectors v ∈ V at a point z in the interior of
(h)
X . We emphasize that the our construction of g̃w (z) and f˜w (z) from f˜w (x) is merely a
“proof of concept” used to establish asymptotic equivalence of two optimization problems,
and that this construction plays no role in the actual algorithm we use.
Given a point of interest z, it is helpful to partition the space Xh into chords indexed
by r ∈ R, such that any point x ∈ Xh can uniquely be written as x = ar + (δr + mh) v for
some r ∈ R and m ∈ Z. Here, ar is the intersection of the chord with the normal space of
v through z, and |δr | ≤ h/2 is the smallest correction term enabling such a representation.
Now, we see that

 

hk X hx − z, v/ kvk2 i2
x − z ˜(h)
d2
g̃w (z + tv/ kvk2 ) = k+2
− 1 ϕk
fw (x)
dt2
βh2
βh
βh x∈Xh
and, decomposing this quantity into a sum over chords, we get that
d2
g̃w (z + tv/ kvk2 )
dt2
! 

2
2
hk X X (δr + mh) kvk2
ar + (δr + mh) v
= k+2
− 1 ϕk
βh2
βh
βh r∈R m∈Sr
· f˜w(h) (z + ar + (δr + mh) v),
where Sr ⊂ Z denotes the set of valid m indices in the chord r.
Now, the reason we used a Gaussian kernel ϕ is that it has independent orthogonal components, and so we can split the k-dimensional Gaussian density ϕk into a (k−1)-dimensional
density over the space normal to v and a 1-dimensional density for the contribution along
v:
! 
  X

2
2
(δr + mh) kvk2
(δr + mh) kvk2
ar
hk X
ϕk−1
−
1
ϕ
· · · = k+2
1
βh
βh2
βh
βh
r∈R

m∈Sr

· f˜w(h) (z + ar + (δr + mh) v).
We are then ready to take a “Taylor expansion” where the first summand below captures
the contributions of a linear effect in each chord, and the second chord has the resulting

31

second-order terms:
! 

  X
2
2
(δr + mh) kvk2
(δr + mh) kvk2
ar
hk X
−
1
ϕ
· · · = k+2
ϕk−1
1
βh
βh2
βh
βh r∈R
m∈Sr



· f˜w(h) (z + ar + δr v) + m f˜w(h) (z + ar + (δr + h)v) − f˜w(h) (z + ar + δr v)
! 

  X
2
2
(δr + mh) kvk2
(δr + mh) kvk2
hk X
ar
−
1
ϕ
∆(r, m),
+ k+2
ϕk−1
1
βh
βh2
βh
βh
r∈R

m∈Sr

where the term ∆(r, m) simply denotes the residual
from the 1-st order approximation.
P
If we didn’t have any discretization, the m∈Sr sum inside the term due to first-order
effects would be exactly 0 for each r individually. Here, the discretization causes error on
the order of at most h at each sampling point while each chord has at most on the order of
h−1 points, meaning that the total sum is bounded on the order of
 
 


ar
h
hk X
ϕ
= O h1/4
=
O
k−1
3
k+2
βh
βh
βh
r∈R

given our chosen growth rate on βh . We also recall that z is in the interior of X , such that
1/2
a ball of radius βh centered at z is contained inside X ; thus, edge effects from Xh being
finite are negligible.
(h)
2
Meanwhile, by feasibility of f˜w (·), we know that |∆(r, m)| ≤ λ1 khvk2 m(m − 1)/2.
From here, we can verify that
! 
  X

2
2
(δr + mh) kvk2
(δr + mh) kvk2
hk X
ar
ϕ
−
1
ϕ
∆(r, m)
k−1
1
βh
βh2
βh
βhk+2 r∈R
m∈Sr
! 
  X

2
4
khvk2 m2
khvk2 m4
hk X
ar 1
(δr + mh) v
≤ λ1 k
ϕk−1
−
ϕ1
(1 + O (h))
βh 2
βh4
βh2
βh
βh r∈R
m∈Sr

 
βh
ar
hk X
+ o(1)
ϕk−1
= λ1 k
βh
h kvk2
βh
r∈R

= λ1 (1 + o(1))
in the limit where h → 0. To verify the last result, note that the density
ar in the
 of points
 
k−1 dimensional normal space to v is kvk2 h1−k ; we also recall that E Z 4 − E Z 2 /2 = 1
for a standard Gaussian random variable Z.

32

