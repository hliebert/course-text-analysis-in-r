Econometrica Supplementary Material

SUPPLEMENT TO â€œSAMPLING-BASED VERSUS DESIGN-BASED
UNCERTAINTY IN REGRESSION ANALYSISâ€
(Econometrica, Vol. 88, No. 1, January 2020, 265â€“296)
ALBERTO ABADIE
Massachusetts Institute of Technology and NBER
SUSAN ATHEY
Graduate School of Business, Stanford University and NBER
GUIDO W. IMBENS
Graduate School of Business and Department of Economics, Stanford University and NBER
JEFFREY M. WOOLDRIDGE
Department of Economics, Michigan State University

S.1. PROOFS OF THE RESULTS IN SECTION 2
S.1.1. Preliminary Calculations
NOTICE THAT FOR ANY
1 â‰¤ N â‰¤ n, we obtain

INTEGER

1 â‰¤ i â‰¤ n and conditional on sample size N, such that

N
E[Ri ] = 
n



N
N
var(Ri ) =
1âˆ’

n
n

Also, for any integers 1 â‰¤ j < k â‰¤ n,

 n

Ri = n var(Rj ) + n(n âˆ’ 1) cov(Rj  Rk ) = 0
var
i=1

This implies



var(Rj )
N
N
=âˆ’
1âˆ’

cov(Rj  Rk ) = âˆ’
nâˆ’1
n(n âˆ’ 1)
n

In turn, this implies
E[Ri Rj ] =

N(N âˆ’ 1)

n(n âˆ’ 1)

Let
n
1 
Ri Yi
YÌ„ =
N i=1

1
Î¼=
Yi 
n i=1
n

and

Alberto Abadie: abadie@mit.edu
Susan Athey: athey@stanford.edu
Guido W. Imbens: imbens@stanford.edu
Jeffrey M. Wooldridge: wooldri1@msu.edu
Â© 2020 The Econometric Society

https://doi.org/10.3982/ECTA12675

2

ABADIE, ATHEY, IMBENS, AND WOOLDRIDGE

Now,
E[YÌ„ ] =

n
1 
E[Ri ]Yi = Î¼
N i=1

Let
1 
S =
(Yi âˆ’ Î¼)2 
n âˆ’ 1 i=1
n

2
Y

Notice that
1
1 2
(Yi âˆ’ YÌ„ )2 =
Y âˆ’
n i=1
n i=1 i
n

n



1
Yi
n i=1
n

nâˆ’1  2
2 
Y
âˆ’
Yi Yj 
n2 i=1 i
n2 i=1 j=i+1
n

=

2

n

n

This implies
nSY2 =

n


2 
Yi Yj 
n âˆ’ 1 i=1 j=i+1
n

Yi2 âˆ’

i=1

n

Therefore,
n
n
n
2 
1 
2
var(R
)Y
+
cov(Ri  Rj )Yi Yj
i
i
N 2 i=1
N 2 i=1 j=i+1

 n
n
n

1
2 
2
= 2 var(R1 )
Yi âˆ’
Yi Yj
n âˆ’ 1 i=1 j=i+1
N
i=1

var(YÌ„ ) =

n
var(R1 )SY2
N2


SY2
N
=
1âˆ’

N
n

=

Let
n
1 
Ri Yi2 âˆ’

Ïƒ =
N i=1
2



n
1 
Ri Yi
N i=1

2


Then

Ïƒ2 =

n
n
n
n
1 
1 
2 
Ri Yi2 âˆ’ 2
Ri Yi2 âˆ’ 2
Ri Rj Yi Yj 
N i=1
N i=1
N i=1 j=i+1

Therefore,
n
n
n
 2
N âˆ’1  
2
N âˆ’1 2
1 N âˆ’1 2
SY 
Yi âˆ’
Yi Yj =
E 
Ïƒ =
n N i=1
n(n âˆ’ 1) N i=1 j=i+1
N

SAMPLING-BASED VERSUS DESIGN-BASED UNCERTAINTY

3

S.1.2. Causal versus Descriptive Estimands
Let
n
n
1 
1 

Î¸=
Ri Xi Yi âˆ’
Ri (1 âˆ’ Xi )Yi 
N1 i=1
N0 i=1

We will do all the analysis conditional on N1  N0 , for N1 > 0, N0 > 0, n1 > 0, and n0 > 0.
To economize notation, we will leave this conditioning implicit. Notice that
E[
Î¸|X] = Î¸desc 
where
Î¸desc =
=

n
n
1 
1 
Xi Yi âˆ’
(1 âˆ’ Xi )Yi
n1 i=1
n0 i=1
n
n
1 
1 
Xi Yiâˆ— (1) âˆ’
(1 âˆ’ Xi )Yiâˆ— (0)
n1 i=1
n0 i=1

By the law of total variance,

var(
Î¸) = E var(
Î¸|X) + var Î¸desc 
The expectation of Î¸desc over the randomization distribution is
n
n

1 
1 
E Î¸desc =
E[Xi ]Yi âˆ’
1 âˆ’ E[Xi ] Yi
n1 i=1
n0 i=1

1 
1 
(n1 /n)Yiâˆ— (1) âˆ’
(n0 /n)Yiâˆ— (0)
n1 i=1
n0 i=1
n

=

n

= Î¸causal 
For the variance of Î¸desc , we have to compute n square terms and n(n âˆ’ 1) cross-product
terms. Each square term is equal to
var(Xi ) âˆ—
var(Xi ) âˆ— 2 var(Xi ) âˆ— 2
Yi (1) +
Yi (0) + 2
Yi (1)Yiâˆ— (0)
n1 n0
n21
n20
 âˆ— 2

Yiâˆ— (0)2
Yiâˆ— (1)Yiâˆ— (0)
Yi (1)
+
+2
= var(Xi )

n1 n0
n21
n20
Recall from previous calculations that
cov(Xi  Xj ) = âˆ’

var(Xi )

nâˆ’1

Therefore, each of the cross-product terms is equal to
 âˆ—

âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
var(Xi ) Yi (1)Yj (1) Yi (1)Yj (0) Yi (0)Yj (1) Yi (0)Yj (0)
âˆ’
+
+
+

nâˆ’1
n1 n0
n1 n0
n21
n20

4

ABADIE, ATHEY, IMBENS, AND WOOLDRIDGE

Let Î¸i = Yiâˆ— (1) âˆ’ Yiâˆ— (0); then
nSÎ¸2 =

n

i=1

= nS + nS âˆ’ 2
2
1

2  âˆ—
Y (1) âˆ’ Yiâˆ— (0) Yjâˆ— (1) âˆ’ Yjâˆ— (0)
n âˆ’ 1 i=1 j=i+1 i
n

2

Yiâˆ— (1) âˆ’ Yiâˆ— (0) âˆ’

2
0

 n

i=1

n

1  âˆ—
Y (1)Y (0) âˆ’
Y (1)Yjâˆ— (0) + Yiâˆ— (0)Yjâˆ— (1)
n âˆ’ 1 i=1 j=i+1 i
n

âˆ—
i

n

âˆ—
i




As a result, we obtain


var Î¸

desc

nS12 nS02
nS12
nS02
nSÎ¸2
+
+
+
âˆ’
= var(Xi )
n1 n0 n1 n0 n1 n0
n21
n20
 2

2
n1 n0 nS1 nS0
nS12
nS02
nSÎ¸2
= 2
+
+
+
âˆ’
n1 n0 n1 n0 n1 n0
n
n21
n20
 2

n2 2
nSÎ¸2
n1 n0 n
2
S +
S âˆ’
= 2
n
n21 n0 1 n1 n20 0 n1 n0
=



S12 S02 SÎ¸2
+
âˆ’ 
n1 n0
n

Notice now that (because we condition on N1 and N0 )

 n
n
âˆ—
âˆ—


Y
(1)
(0)
Y
i
i
X
Ri Xi
âˆ’
Ri (1 âˆ’ Xi )
var(
Î¸|X) = var
N1
N0
i=1
i=1

 n

 n


Yiâˆ— (0)
Yiâˆ— (1)
X + var
X 
= var
Ri Xi
Ri (1 âˆ’ Xi )
N1
N0
i=1
i=1
Let us calculate the first term on the right-hand side of the last equation (the second term
will be analogous):
 n


Yiâˆ— (1)
var
X
Ri Xi
N1
i=1

n
n
n

Yiâˆ— (1)Yjâˆ— (1)
Yiâˆ— (1)2
2 
= var(Ri |Xi = 1)

Xi
âˆ’
Xi Xj
n1 âˆ’ 1 i=1 j=i+1
N12
N12
i=1
Taking expectations, the right-hand side becomes

n
n
âˆ—
âˆ—
2   n1 (n1 âˆ’ 1) Yi (1)Yj (1)
var(Ri |Xi = 1)
âˆ’
n N12
n1 âˆ’ 1 i=1 j=i+1 n(n âˆ’ 1)
N12
i=1


 n
n
n
n1 âˆ’ N1  âˆ— 2
2  âˆ—
n1 âˆ’ N1 2
1
âˆ—
Yi (1) âˆ’
Yi (1)Yj (1) =
S1 
=
nN1
n1
n
âˆ’
1
n
1 N1
i=1
i=1 j=i+1
n

n1 Y âˆ— (1)2
i

5

SAMPLING-BASED VERSUS DESIGN-BASED UNCERTAINTY

This implies
S2
S2
S2
var(
Î¸) = 1 + 0 âˆ’ Î¸ 
N1 N0
n
Now, notice that
E[
Î¸|R] = Î¸causalsample 
where
Î¸causalsample =

n
1 
Ri Yiâˆ— (1) âˆ’ Yiâˆ— (0) 
N i=1

Therefore, by the law of total variance,

var(
Î¸) = E var(
Î¸|R) + var Î¸causalsample 
The variance of Î¸causalsample is
n
N
n
2  âˆ—
var(Ri )  âˆ—
âˆ—
(1)
âˆ’
Y
(0)
âˆ’
Y
Yi (1) âˆ’ Yiâˆ— (0) Yjâˆ— (1) âˆ’ Yjâˆ— (0)
i
i
n
âˆ’
1
N2
i=1
i=1 j=i+1


N 2
1
1âˆ’
S
=
N
n Î¸

As a result,

E var(
Î¸|R) = var(
Î¸) âˆ’ var Î¸causalsample
=

S12
S2
S2
+ 0 âˆ’ Î¸
N1 N0 N

S.1.3. EHW Variance
The EHW variance estimator for 
Î¸ is
N1 âˆ’ 1 2 N0 âˆ’ 1 2
Vehw =
S +
S
N12 1
N02 0
where


2
n
n


1
1

S =
Ri Xi Yi âˆ’
Ri Xi Yi 
N1 âˆ’ 1 i=1
N1 i=1
2
1

and 
S02 is defined analogously. Using previous results, we obtain

1 
2

S12 =
Ri Xi Yi2 âˆ’
Ri Rj Xi Xj Yi Yj 
N1 i=1
N1 (N1 âˆ’ 1) i=1 j=i+1
n

n

n



6

ABADIE, ATHEY, IMBENS, AND WOOLDRIDGE

Therefore,
n
n
n


 2
1 
2
2

E S1 |X =
Xi Yi âˆ’
Xi Xj Yi Yj
n1 i=1
n1 (n1 âˆ’ 1) i=1 j=i+1

and
n
n
n


 2
2
1 âˆ— 2
E 
S1 =
Yi (1) âˆ’
Y âˆ— (1)Yjâˆ— (1)
n i=1
n(n âˆ’ 1) i=1 j=i+1 i

= S12 
Let


S2
S2
Vehw = 1 + 0 
N1 N0
Then

S2
S2
Î¸)
E Vehw = 1 + 0 â‰¥ var(
N1 N0
S.1.4. Bootstrap Variance
Consider the bootstrap variance estimator that draws N1 treated and N0 untreated observations separately,
VBboot =

1  (b)
2
Î¸ âˆ’ Î¸Ì„B 
B âˆ’ 1 b=1
B

where
n
n
1  (b)
1  (b)
(b)

Î¸ =
K Ri Xi Yi âˆ’
K Ri (1 âˆ’ Xi )Yi
N1 i=1 1i
N0 i=1 0i

and
1  (b)
Î¸ 
B b=1
B

Î¸Ì„B =

Conditional on R, and X, K1i(b) has a multinomial distribution with paramenter
{N1  1/N1      1/N1 } for units with Ri Xi = 1 and K0i(b) has a multinomial distribution with
parameters {N0  1/N0      1/N0 } for units with Ri (1 âˆ’ Xi ) = 1. The variables K1i(b) and
K0i(b) are independent of each other and independent across b = 1     B. As a result, for
Ri Xi = Rj Xj = 1 with i = j, we obtain

E K1i(b) |R X = 1

2
E K1i(b) |R X = (2N1 âˆ’ 1)/N1 
and


E K1i(b) K1i(b) |R X = (N1 âˆ’ 1)/N1 

SAMPLING-BASED VERSUS DESIGN-BASED UNCERTAINTY

7

Conditional on Y(1), Y(0), R, and X, the mean of the bootstrap variance is



 (b)
B
2
E VBboot |R X =
E 
Î¸ âˆ’ Î¸Ì„B |R X 
Bâˆ’1
Because of independence of the bootstrap weights between treatment samples, we obtain




 (b)
 (b)
 boot
B
B
2
2
E 
Î¸1 âˆ’ Î¸Ì„1B |R X +
E 
Î¸0 âˆ’ Î¸Ì„0B |R X 
E VB |R X =
Bâˆ’1
Bâˆ’1
where
n
1  (b)

K Ri Xi Yi
Î¸1(b) =
N1 i=1 1i

and
1  (b)
Î¸Ì„1B =
Î¸ 
B b=1 1
B

with analogous expressions for 
Î¸0(b) and Î¸Ì„0B . Now, let
â›

(1)
K11

âœ K (1)
âœ
K1 = âœ 12
â 
(1)
K1n

(2)
K11

Â·Â·Â·

(2)
K12



Â·Â·Â·

(2)
K1n

Â·Â·Â·
Â·Â·Â·

(B)
K11

â

(B) âŸ
K12
âŸ

 âŸ
 â 
(B)
K1n

and let K1Ï€ be a random permutation of the columns of K1 . Notice that Î¸Ì„1B is fixed conÎ¸1(b) is not. In addition,
ditional on R, X, and K1Ï€ , but 
 (b)
E 
Î¸1 |R X K1Ï€ = Î¸Ì„1B 
Therefore,
 (b) 2
 (b)
2
2

Î¸1 |R X K1Ï€ âˆ’ Î¸Ì„1B
E 
Î¸1 âˆ’ Î¸Ì„1B |R X K1Ï€ = E 
Then
 (b) 2
 2
 (b)
2
Î¸1 |R X âˆ’ E Î¸Ì„1B
|R X 
E 
Î¸1 âˆ’ Î¸Ì„1B |R X = E 
Let
N
1 

Ri Xi Yi 
Î¸1 =
N1 i=1

Notice that for any b and c, such that 1 â‰¤ b < c â‰¤ B, we have
 2
1  (b) 2
B âˆ’ 1 (b)(c)
Î¸1 |R X +
E Î¸1 Î¸1 |R X
|R X = E 
E Î¸Ì„1B
B
B
1  (b) 2
B âˆ’ 1 2
= E 
Î¸1 |R X +
Î¸1 
B
B

8

ABADIE, ATHEY, IMBENS, AND WOOLDRIDGE

Therefore,


 (b) 2
 (b)
Bâˆ’1
2
E 
Î¸1 |R X âˆ’ 
Î¸12 
E 
Î¸1 âˆ’ Î¸Ì„1B |R X =
B
In addition,


E 
Î¸

(b) 2
1

 n

n
n


N1 âˆ’ 1
1  2N1 âˆ’ 1
2
|R X = 2
Ri Xi Yi + 2
Ri Rj Xi Xj Yi Yj 
N1
N1
N1 i=1
i=1 j=i+1

Therefore,


E 
Î¸

(b) 2
1

 n

n
n
 N1 âˆ’ 1


1
1
|R X âˆ’ 
Î¸12 = 2
Ri Xi Yi2 âˆ’ 2
Ri Rj Xi Xj Yi Yj
N1
N1 i=1 N1
i=1 j=i+1


n
n
n


2
N1 âˆ’ 1 1 
2
Ri Xi Yi âˆ’
Ri Rj Xi Xj Yi Yj
=
N1 i=1
N1 (N1 âˆ’ 1) i=1 j=i+1
N12
=

and

N1 âˆ’ 1 
S1
N12



 (b)
B âˆ’ 1 N1 âˆ’ 1 2
2
E 
Î¸1 âˆ’ Î¸Ì„1B |R X =
S
B
N12 1

with the analogous result holding for E[(
Î¸0(b) âˆ’ Î¸Ì„0B )2 |R X]. It follows that

E VBboot |R X = Vehw 
PROOF OF LEMMA 1: Let
ââ›
â
Yni
Yni
Wni = â Xni â  â Xni â  
Zni
Zni
â›

(kl)
(kl)
(kl) , Î©(kl)
ni
 (kl) , Î©
be the (k l) element of Wni . Similarly, let W
, Wn(kl) , W
and let Wni
n
n
n
n , and Î©n , respectively. In order to have
n , Î©
be the (k l) elements of Rni Wni , Wn , W
(kl) well-defined, let W
 (kl) and Î©
 (kl) = Î©(kl) = 0 when N = 0 (this is without loss of
W
n
n
n
n
generality). Notice that, because nÏn â†’ âˆ, for any fixed 0 < Îµ < 1, there is n such that
for n > n , we have nÏn > âˆ’ log(Îµ). Therefore, for n > nÎµ , we obtain
n 
n

nÏn
log(Îµ)
Pr(N = 0) = 1 âˆ’
< 1+
< elog(Îµ) = Îµ
n
n

As a result, Pr(N = 0) â†’ 0 and
 (kl)
2
2

âˆ’ Î©(kl)
|N = 0 Pr(N = 0) = Î©(kl)
Pr(N = 0) â†’ 0
E W
n
n
n

SAMPLING-BASED VERSUS DESIGN-BASED UNCERTAINTY

9

by Assumption 5 and Holderâ€™s inequality. Notice that for any integer, m, such that 1 â‰¤
m â‰¤ n, we have

 
 (kl)
n
(kl)
N =m =0
Rni Wni âˆ’ E Wni
E
N
and




 (kl) âˆ’ Î©
E W
n

(kl) 2
n

|N = m = E


2

n 

1 n
(kl)
(kl)
N =m
âˆ’ E Wni
Rni Wni
n i=1 N

1 
E
n2 i=1
n

=






 (kl) 2
n
(kl)
Rni Wni
N =m
âˆ’ E Wni
N


 
2
n
1 
n
(kl)
Rni Wni
N =m
â‰¤ 2
E
N
n i=1
 n

1 1   (kl) 2
=
E Wni
m n i=1
â‰¤ C/m
for some positive constant, C, by Assumption 5. Let
(kl)

=
Î¾m



 (kl) âˆ’ Î©(kl)
W
n
n
0

2



if m > 0
if m = 0

and

Î¾m =

C/m
0

if m > 0
if m = 0

Now,
 (kl)
 (kl)
2

âˆ’ Î©(kl)
|N > 0 Pr(N > 0) = E 
Î¾N
E W
n
n
â‰¤ E[Î¾N ]
Applying Chernoffâ€™s bounds, for any Îµ > 0,
Pr(Î¾N > Îµ) â‰¤ Pr(0 < N < C/Îµ)
< Pr(N < C/Îµ)



nÏn âˆ’ C/Îµ
= Pr N < nÏn 1 âˆ’
nÏn
â‰¤ eâˆ’(

(nÏn âˆ’C/Îµ)2
)
2nÏn

â†’ 0

which implies that Î¾N converges in probability to zero. Because Î¾N is bounded, by the
portmanteau lemma we obtain E[Î¾N ] â†’ 0. As a result,
 (kl)

âˆ’ Î©(kl)
E W
n
n

2

â†’ 0

10

ABADIE, ATHEY, IMBENS, AND WOOLDRIDGE

For the second result, notice that




(kl) âˆ’ Î©
E Î©
n

(kl) 2
n

|N = m = E


2

 (kl)
1
âˆ’
E Wni
N =m
m
n

n 

Rni
i=1

2

 1 
 (kl)
m
N
=
m
E Wni
E
R
âˆ’
ni
2
n
m
i=1


n

 (kl) 2
m
1
=
1âˆ’
E Wni
mn
n
i=1
 n

1 1   (kl) 2

â‰¤
E Wni
m n i=1
n

=

2

Now, using the same argument as above, we obtain
 (kl)

E Î©
âˆ’ Î©(kl)
n
n

2

â†’ 0

The proof of the third result is analogous.

Q.E.D.

Co-editor Ulrich K. MÃ¼ller handled this manuscript.
Manuscript received 22 July, 2014; final version accepted 21 June, 2019; available online 31 July, 2019.

