Econometrica Supplementary Material

SUPPLEMENT TO “SAMPLING-BASED VERSUS DESIGN-BASED
UNCERTAINTY IN REGRESSION ANALYSIS”
(Econometrica, Vol. 88, No. 1, January 2020, 265–296)
ALBERTO ABADIE
Massachusetts Institute of Technology and NBER
SUSAN ATHEY
Graduate School of Business, Stanford University and NBER
GUIDO W. IMBENS
Graduate School of Business and Department of Economics, Stanford University and NBER
JEFFREY M. WOOLDRIDGE
Department of Economics, Michigan State University

S.1. PROOFS OF THE RESULTS IN SECTION 2
S.1.1. Preliminary Calculations
NOTICE THAT FOR ANY
1 ≤ N ≤ n, we obtain

INTEGER

1 ≤ i ≤ n and conditional on sample size N, such that

N
E[Ri ] = 
n



N
N
var(Ri ) =
1−

n
n

Also, for any integers 1 ≤ j < k ≤ n,

 n

Ri = n var(Rj ) + n(n − 1) cov(Rj  Rk ) = 0
var
i=1

This implies



var(Rj )
N
N
=−
1−

cov(Rj  Rk ) = −
n−1
n(n − 1)
n

In turn, this implies
E[Ri Rj ] =

N(N − 1)

n(n − 1)

Let
n
1 
Ri Yi
Ȳ =
N i=1

1
μ=
Yi 
n i=1
n

and

Alberto Abadie: abadie@mit.edu
Susan Athey: athey@stanford.edu
Guido W. Imbens: imbens@stanford.edu
Jeffrey M. Wooldridge: wooldri1@msu.edu
© 2020 The Econometric Society

https://doi.org/10.3982/ECTA12675

2

ABADIE, ATHEY, IMBENS, AND WOOLDRIDGE

Now,
E[Ȳ ] =

n
1 
E[Ri ]Yi = μ
N i=1

Let
1 
S =
(Yi − μ)2 
n − 1 i=1
n

2
Y

Notice that
1
1 2
(Yi − Ȳ )2 =
Y −
n i=1
n i=1 i
n

n



1
Yi
n i=1
n

n−1  2
2 
Y
−
Yi Yj 
n2 i=1 i
n2 i=1 j=i+1
n

=

2

n

n

This implies
nSY2 =

n


2 
Yi Yj 
n − 1 i=1 j=i+1
n

Yi2 −

i=1

n

Therefore,
n
n
n
2 
1 
2
var(R
)Y
+
cov(Ri  Rj )Yi Yj
i
i
N 2 i=1
N 2 i=1 j=i+1

 n
n
n

1
2 
2
= 2 var(R1 )
Yi −
Yi Yj
n − 1 i=1 j=i+1
N
i=1

var(Ȳ ) =

n
var(R1 )SY2
N2


SY2
N
=
1−

N
n

=

Let
n
1 
Ri Yi2 −

σ =
N i=1
2



n
1 
Ri Yi
N i=1

2


Then

σ2 =

n
n
n
n
1 
1 
2 
Ri Yi2 − 2
Ri Yi2 − 2
Ri Rj Yi Yj 
N i=1
N i=1
N i=1 j=i+1

Therefore,
n
n
n
 2
N −1  
2
N −1 2
1 N −1 2
SY 
Yi −
Yi Yj =
E 
σ =
n N i=1
n(n − 1) N i=1 j=i+1
N

SAMPLING-BASED VERSUS DESIGN-BASED UNCERTAINTY

3

S.1.2. Causal versus Descriptive Estimands
Let
n
n
1 
1 

θ=
Ri Xi Yi −
Ri (1 − Xi )Yi 
N1 i=1
N0 i=1

We will do all the analysis conditional on N1  N0 , for N1 > 0, N0 > 0, n1 > 0, and n0 > 0.
To economize notation, we will leave this conditioning implicit. Notice that
E[
θ|X] = θdesc 
where
θdesc =
=

n
n
1 
1 
Xi Yi −
(1 − Xi )Yi
n1 i=1
n0 i=1
n
n
1 
1 
Xi Yi∗ (1) −
(1 − Xi )Yi∗ (0)
n1 i=1
n0 i=1

By the law of total variance,

var(
θ) = E var(
θ|X) + var θdesc 
The expectation of θdesc over the randomization distribution is
n
n

1 
1 
E θdesc =
E[Xi ]Yi −
1 − E[Xi ] Yi
n1 i=1
n0 i=1

1 
1 
(n1 /n)Yi∗ (1) −
(n0 /n)Yi∗ (0)
n1 i=1
n0 i=1
n

=

n

= θcausal 
For the variance of θdesc , we have to compute n square terms and n(n − 1) cross-product
terms. Each square term is equal to
var(Xi ) ∗
var(Xi ) ∗ 2 var(Xi ) ∗ 2
Yi (1) +
Yi (0) + 2
Yi (1)Yi∗ (0)
n1 n0
n21
n20
 ∗ 2

Yi∗ (0)2
Yi∗ (1)Yi∗ (0)
Yi (1)
+
+2
= var(Xi )

n1 n0
n21
n20
Recall from previous calculations that
cov(Xi  Xj ) = −

var(Xi )

n−1

Therefore, each of the cross-product terms is equal to
 ∗

∗
∗
∗
∗
∗
∗
∗
var(Xi ) Yi (1)Yj (1) Yi (1)Yj (0) Yi (0)Yj (1) Yi (0)Yj (0)
−
+
+
+

n−1
n1 n0
n1 n0
n21
n20

4

ABADIE, ATHEY, IMBENS, AND WOOLDRIDGE

Let θi = Yi∗ (1) − Yi∗ (0); then
nSθ2 =

n

i=1

= nS + nS − 2
2
1

2  ∗
Y (1) − Yi∗ (0) Yj∗ (1) − Yj∗ (0)
n − 1 i=1 j=i+1 i
n

2

Yi∗ (1) − Yi∗ (0) −

2
0

 n

i=1

n

1  ∗
Y (1)Y (0) −
Y (1)Yj∗ (0) + Yi∗ (0)Yj∗ (1)
n − 1 i=1 j=i+1 i
n

∗
i

n

∗
i




As a result, we obtain


var θ

desc

nS12 nS02
nS12
nS02
nSθ2
+
+
+
−
= var(Xi )
n1 n0 n1 n0 n1 n0
n21
n20
 2

2
n1 n0 nS1 nS0
nS12
nS02
nSθ2
= 2
+
+
+
−
n1 n0 n1 n0 n1 n0
n
n21
n20
 2

n2 2
nSθ2
n1 n0 n
2
S +
S −
= 2
n
n21 n0 1 n1 n20 0 n1 n0
=



S12 S02 Sθ2
+
− 
n1 n0
n

Notice now that (because we condition on N1 and N0 )

 n
n
∗
∗


Y
(1)
(0)
Y
i
i
X
Ri Xi
−
Ri (1 − Xi )
var(
θ|X) = var
N1
N0
i=1
i=1

 n

 n


Yi∗ (0)
Yi∗ (1)
X + var
X 
= var
Ri Xi
Ri (1 − Xi )
N1
N0
i=1
i=1
Let us calculate the first term on the right-hand side of the last equation (the second term
will be analogous):
 n


Yi∗ (1)
var
X
Ri Xi
N1
i=1

n
n
n

Yi∗ (1)Yj∗ (1)
Yi∗ (1)2
2 
= var(Ri |Xi = 1)

Xi
−
Xi Xj
n1 − 1 i=1 j=i+1
N12
N12
i=1
Taking expectations, the right-hand side becomes

n
n
∗
∗
2   n1 (n1 − 1) Yi (1)Yj (1)
var(Ri |Xi = 1)
−
n N12
n1 − 1 i=1 j=i+1 n(n − 1)
N12
i=1


 n
n
n
n1 − N1  ∗ 2
2  ∗
n1 − N1 2
1
∗
Yi (1) −
Yi (1)Yj (1) =
S1 
=
nN1
n1
n
−
1
n
1 N1
i=1
i=1 j=i+1
n

n1 Y ∗ (1)2
i

5

SAMPLING-BASED VERSUS DESIGN-BASED UNCERTAINTY

This implies
S2
S2
S2
var(
θ) = 1 + 0 − θ 
N1 N0
n
Now, notice that
E[
θ|R] = θcausalsample 
where
θcausalsample =

n
1 
Ri Yi∗ (1) − Yi∗ (0) 
N i=1

Therefore, by the law of total variance,

var(
θ) = E var(
θ|R) + var θcausalsample 
The variance of θcausalsample is
n
N
n
2  ∗
var(Ri )  ∗
∗
(1)
−
Y
(0)
−
Y
Yi (1) − Yi∗ (0) Yj∗ (1) − Yj∗ (0)
i
i
n
−
1
N2
i=1
i=1 j=i+1


N 2
1
1−
S
=
N
n θ

As a result,

E var(
θ|R) = var(
θ) − var θcausalsample
=

S12
S2
S2
+ 0 − θ
N1 N0 N

S.1.3. EHW Variance
The EHW variance estimator for 
θ is
N1 − 1 2 N0 − 1 2
Vehw =
S +
S
N12 1
N02 0
where


2
n
n


1
1

S =
Ri Xi Yi −
Ri Xi Yi 
N1 − 1 i=1
N1 i=1
2
1

and 
S02 is defined analogously. Using previous results, we obtain

1 
2

S12 =
Ri Xi Yi2 −
Ri Rj Xi Xj Yi Yj 
N1 i=1
N1 (N1 − 1) i=1 j=i+1
n

n

n



6

ABADIE, ATHEY, IMBENS, AND WOOLDRIDGE

Therefore,
n
n
n


 2
1 
2
2

E S1 |X =
Xi Yi −
Xi Xj Yi Yj
n1 i=1
n1 (n1 − 1) i=1 j=i+1

and
n
n
n


 2
2
1 ∗ 2
E 
S1 =
Yi (1) −
Y ∗ (1)Yj∗ (1)
n i=1
n(n − 1) i=1 j=i+1 i

= S12 
Let


S2
S2
Vehw = 1 + 0 
N1 N0
Then

S2
S2
θ)
E Vehw = 1 + 0 ≥ var(
N1 N0
S.1.4. Bootstrap Variance
Consider the bootstrap variance estimator that draws N1 treated and N0 untreated observations separately,
VBboot =

1  (b)
2
θ − θ̄B 
B − 1 b=1
B

where
n
n
1  (b)
1  (b)
(b)

θ =
K Ri Xi Yi −
K Ri (1 − Xi )Yi
N1 i=1 1i
N0 i=1 0i

and
1  (b)
θ 
B b=1
B

θ̄B =

Conditional on R, and X, K1i(b) has a multinomial distribution with paramenter
{N1  1/N1      1/N1 } for units with Ri Xi = 1 and K0i(b) has a multinomial distribution with
parameters {N0  1/N0      1/N0 } for units with Ri (1 − Xi ) = 1. The variables K1i(b) and
K0i(b) are independent of each other and independent across b = 1     B. As a result, for
Ri Xi = Rj Xj = 1 with i = j, we obtain

E K1i(b) |R X = 1

2
E K1i(b) |R X = (2N1 − 1)/N1 
and


E K1i(b) K1i(b) |R X = (N1 − 1)/N1 

SAMPLING-BASED VERSUS DESIGN-BASED UNCERTAINTY

7

Conditional on Y(1), Y(0), R, and X, the mean of the bootstrap variance is



 (b)
B
2
E VBboot |R X =
E 
θ − θ̄B |R X 
B−1
Because of independence of the bootstrap weights between treatment samples, we obtain




 (b)
 (b)
 boot
B
B
2
2
E 
θ1 − θ̄1B |R X +
E 
θ0 − θ̄0B |R X 
E VB |R X =
B−1
B−1
where
n
1  (b)

K Ri Xi Yi
θ1(b) =
N1 i=1 1i

and
1  (b)
θ̄1B =
θ 
B b=1 1
B

with analogous expressions for 
θ0(b) and θ̄0B . Now, let
⎛

(1)
K11

⎜ K (1)
⎜
K1 = ⎜ 12
⎝ 
(1)
K1n

(2)
K11

···

(2)
K12



···

(2)
K1n

···
···

(B)
K11

⎞

(B) ⎟
K12
⎟

 ⎟
 ⎠
(B)
K1n

and let K1π be a random permutation of the columns of K1 . Notice that θ̄1B is fixed conθ1(b) is not. In addition,
ditional on R, X, and K1π , but 
 (b)
E 
θ1 |R X K1π = θ̄1B 
Therefore,
 (b) 2
 (b)
2
2

θ1 |R X K1π − θ̄1B
E 
θ1 − θ̄1B |R X K1π = E 
Then
 (b) 2
 2
 (b)
2
θ1 |R X − E θ̄1B
|R X 
E 
θ1 − θ̄1B |R X = E 
Let
N
1 

Ri Xi Yi 
θ1 =
N1 i=1

Notice that for any b and c, such that 1 ≤ b < c ≤ B, we have
 2
1  (b) 2
B − 1 (b)(c)
θ1 |R X +
E θ1 θ1 |R X
|R X = E 
E θ̄1B
B
B
1  (b) 2
B − 1 2
= E 
θ1 |R X +
θ1 
B
B

8

ABADIE, ATHEY, IMBENS, AND WOOLDRIDGE

Therefore,


 (b) 2
 (b)
B−1
2
E 
θ1 |R X − 
θ12 
E 
θ1 − θ̄1B |R X =
B
In addition,


E 
θ

(b) 2
1

 n

n
n


N1 − 1
1  2N1 − 1
2
|R X = 2
Ri Xi Yi + 2
Ri Rj Xi Xj Yi Yj 
N1
N1
N1 i=1
i=1 j=i+1

Therefore,


E 
θ

(b) 2
1

 n

n
n
 N1 − 1


1
1
|R X − 
θ12 = 2
Ri Xi Yi2 − 2
Ri Rj Xi Xj Yi Yj
N1
N1 i=1 N1
i=1 j=i+1


n
n
n


2
N1 − 1 1 
2
Ri Xi Yi −
Ri Rj Xi Xj Yi Yj
=
N1 i=1
N1 (N1 − 1) i=1 j=i+1
N12
=

and

N1 − 1 
S1
N12



 (b)
B − 1 N1 − 1 2
2
E 
θ1 − θ̄1B |R X =
S
B
N12 1

with the analogous result holding for E[(
θ0(b) − θ̄0B )2 |R X]. It follows that

E VBboot |R X = Vehw 
PROOF OF LEMMA 1: Let
⎞⎛
⎞
Yni
Yni
Wni = ⎝ Xni ⎠ ⎝ Xni ⎠ 
Zni
Zni
⎛

(kl)
(kl)
(kl) , Ω(kl)
ni
 (kl) , Ω
be the (k l) element of Wni . Similarly, let W
, Wn(kl) , W
and let Wni
n
n
n
n , and Ωn , respectively. In order to have
n , Ω
be the (k l) elements of Rni Wni , Wn , W
(kl) well-defined, let W
 (kl) and Ω
 (kl) = Ω(kl) = 0 when N = 0 (this is without loss of
W
n
n
n
n
generality). Notice that, because nρn → ∞, for any fixed 0 < ε < 1, there is n such that
for n > n , we have nρn > − log(ε). Therefore, for n > nε , we obtain
n 
n

nρn
log(ε)
Pr(N = 0) = 1 −
< 1+
< elog(ε) = ε
n
n

As a result, Pr(N = 0) → 0 and
 (kl)
2
2

− Ω(kl)
|N = 0 Pr(N = 0) = Ω(kl)
Pr(N = 0) → 0
E W
n
n
n

SAMPLING-BASED VERSUS DESIGN-BASED UNCERTAINTY

9

by Assumption 5 and Holder’s inequality. Notice that for any integer, m, such that 1 ≤
m ≤ n, we have

 
 (kl)
n
(kl)
N =m =0
Rni Wni − E Wni
E
N
and




 (kl) − Ω
E W
n

(kl) 2
n

|N = m = E


2

n 

1 n
(kl)
(kl)
N =m
− E Wni
Rni Wni
n i=1 N

1 
E
n2 i=1
n

=






 (kl) 2
n
(kl)
Rni Wni
N =m
− E Wni
N


 
2
n
1 
n
(kl)
Rni Wni
N =m
≤ 2
E
N
n i=1
 n

1 1   (kl) 2
=
E Wni
m n i=1
≤ C/m
for some positive constant, C, by Assumption 5. Let
(kl)

=
ξm



 (kl) − Ω(kl)
W
n
n
0

2



if m > 0
if m = 0

and

ξm =

C/m
0

if m > 0
if m = 0

Now,
 (kl)
 (kl)
2

− Ω(kl)
|N > 0 Pr(N > 0) = E 
ξN
E W
n
n
≤ E[ξN ]
Applying Chernoff’s bounds, for any ε > 0,
Pr(ξN > ε) ≤ Pr(0 < N < C/ε)
< Pr(N < C/ε)



nρn − C/ε
= Pr N < nρn 1 −
nρn
≤ e−(

(nρn −C/ε)2
)
2nρn

→ 0

which implies that ξN converges in probability to zero. Because ξN is bounded, by the
portmanteau lemma we obtain E[ξN ] → 0. As a result,
 (kl)

− Ω(kl)
E W
n
n

2

→ 0

10

ABADIE, ATHEY, IMBENS, AND WOOLDRIDGE

For the second result, notice that




(kl) − Ω
E Ω
n

(kl) 2
n

|N = m = E


2

 (kl)
1
−
E Wni
N =m
m
n

n 

Rni
i=1

2

 1 
 (kl)
m
N
=
m
E Wni
E
R
−
ni
2
n
m
i=1


n

 (kl) 2
m
1
=
1−
E Wni
mn
n
i=1
 n

1 1   (kl) 2

≤
E Wni
m n i=1
n

=

2

Now, using the same argument as above, we obtain
 (kl)

E Ω
− Ω(kl)
n
n

2

→ 0

The proof of the third result is analogous.

Q.E.D.

Co-editor Ulrich K. Müller handled this manuscript.
Manuscript received 22 July, 2014; final version accepted 21 June, 2019; available online 31 July, 2019.

