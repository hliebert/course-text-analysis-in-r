JOURNAL OF
Econometrics
ELSEVIER

Journal of Econometrics 73 (1996) 387-399

Bounding mean regressions when a binary regressor
is mismeasured
C h r i s t o p h e r R. B o l l i n g e r
Policy Research Center. GeorgiaState University, Atlanta, GA 30303-3083, USd
{ReceivedJuly 1992; final version received March 1995)

Abstract
In this paper I examine identification and estimation of mean regression models when
a binary regressor is mismeasured. I prove that bounds for the model parameters are
identified and provide simple estimators which are consistent and asymptotically normal.
When stronger prior information about the probability of misclassification is available,
the bounds can be made tighter. Again, a simple estimator for these cases is provided. All
results apply to parametric and nonparametric models. The paper concludes with a short
empirical example.
Key words: Measurement error; Binary variables; Identification
JEL classification: C!0; C20

1. Introduc~an
The topic of measurement error has a long history in econometrics. In
particular, it is well-known that when one or more regressors in a linear model
are mismeasured, least squares estimation is generally not consistent. However,
most of the literature has focused u p o n a continuous regressor. In this paper,
I examine identification and estimation of bounds for the model parameters
when the mismeasured regressor is a binary classification variable.
The model is formally represented by Model 1:
Y=-a+pZ+u,

E[ul Z] =0,

(I)

I thank Charles Manski, Arthur Goldberger, Mico Loretan, Chuck Michalopolous, Duncan
Chaplin, an assistant editor, and two anonymous referees for many helpful comments and suggestions. Any errors are, of course, my responsibility.
0304-4076/96/$15.00 © 1996 Elsevier Science S.A. All rights reserved
SSDI 0 3 0 4 - 4 0 7 6 ( 9 5 ) 0 1 7 3 0 - 2

388

C.R. Bollinger / Journal of Econometrics 73 (1996) 387-399

P r [ X = 0 I Z, Y-I =(1 - p ) (1 - Z ) + q Z ,

(2)

Pr[X=IIZ,

Z+p(I-Z),

(3)

0 < Pz < 1,

(4)

Y]=(I-q)

Z ~ Bernoulli(Pz)

p+q<l.

with

(5)

The term p is the probability of reporting X = 1 when Z = 0, and q is the
probability of reporting X = 0 when Z = 1. The researcher is interested in the
parameters (c¢,p), but is only able to observe Y, the dependent variable, and X,
the mismeasured version of Z. The focus in this paper will be on identification;
hence the discussion focuses on population relationships.
Aigner (1973) showed that in this model, the least squares regression of Y on
X does not yield consistent estimates of the parameters (~, p). The estimates will
be asymptotically biased toward zero. Knowledge of p and q can be used to
obtain consistent estimates (see Freeman, 1984; Aigner, 1973). Frisch (1934)
showed that in a classical errors-in-variables model with one regressor (where
the regressor is continuous and the error term is additive white noise), bounds
for the parameters are identified. Klepper and Learner (1984) generalized the
result to the case where k continuous regressors are mismeasured. Kiepper
{1988) derives bounds for Model I with the additional assumption that
p = q < 0.5. The assumption that p = q is rather strong. Recent empirical
evidence using validation data to estimate misclassification rates in survey data
suggests that the assumption fails in practice (Freeman, 1984; Poterba and
Summers, 1986; Mathiowetz and Duncan, 1988; Boilinger and David, 1993).
in Section 2, I derive bounds for/~ and the other parameters in Model
I. Klepper's (1988) results cannot be obtained by simply substituting p = q into
the bounds derived here. In fact, the assumption p = q is strong prior knowledge, and is fully utilized by Klepper, resulting in tighter bounds. In Section 3,
! derive bounds when additional information, in the form of bounds on p and q,
is available, in Section 4, I apply the results to bound the union wage differential. Proofs of all theorems and lemmas can be found in the Appendix.

2. Bounds for Model I
The discussion will focus on Model I, but the results can easily be extended to
include linear models with other regressors which may be mismeasured also.
The results can also be extended to nonparametric models with other correctly
measured regressors.
The implications of assumption (5) and conditioning on Y in Eqs. {2) and {3)
are worth examining. Assumption (5) insures that the misclassification is not so
bad that X is independent of Z (the case where p + q = 1), or that the effective
definition of the classification has been reversed (when p + q > 1) which would

CR Bollinger / Journal of Econometrics 73 (1996) 387-399

389

occur if more than half the data were mismeasured. This also results in the
covariance between X and Z being positive. An anonymous referee pointed out
that a somewhat stronger assumption requiring p < ½ and q < ½ is reasonable.
This assumption is not necessary for the result in Theorem 1, and can be
imposed utilizing Theorems 2, 3, or 4 for various cases. Conditioning on Y in
Eqs. (2) and (3) implies that the error process generating X is independent from
the residual error, u, in the structural equation. If this assumption fails, the
bounds derived here do not hold. Krasker and Pratt (1986) and Erikson (1993)
study the case where the measurement error is not independent of the regression
error in a classical errors-in-variables model.
It is helpful to establish some notation. Let b be the slope from the least
squares projection of Y on X; let d be the inverse of the slope from the least
squares projection of X on Y; let Px be the marginal probability X = 1; let
p2xvbe the squared correlation between X and Y. The variances of Y and X are
represented by tr~ and a~, respectively. The covariance of Y and X is represented
by axr- Throughout this paper b, d, Px, tr2, t72, and trxr are all observable.
From the assumptions of the model, restrictions on unobservable parameters
are known. From (4) and (5), respectively, 0 < Pz < 1 and p + q < 1. By
definition probabilities are nonnegative: p/> 0 and q >t 0. Variances are also
nonnegative: o,2/> 0 and ,r2 1> 0. These restrictions, combined with the first and
second moments of the model and information about the error process from
Eqs. (2) and (3) imply a set of constraints on the unknown structural parameters
which give rise to Theorem 1. For the remainder of the paper I assume, without
loss of generality, that fl >/0, since it can be shown that sign(fl)= sign(b)
= sign(d), and i f / / > 0, then 0 < b < d.
Theorem 1. Given Model L if fl > O, then

0 < b ~< fl ~< max {d" Px + b-(l - P x ) , d ' ( l - Px) + b ' P x } ,

(6)

ElY] -d'ex

(7)

~< • ~< E[Y] - b . e x ,

0 <~ p <<,Px'(1 - P~r),

(8)

0 .< q -<< (1 - P~c) (l - p~cy).

(9)

l f fl = O, then b = fl = O. These bounds utilize all information contained in the first
and second moments of the observable data and are tight relative to this informa-

tion.
Bounds are also available for Pz and ~ , but are not presented here. The lower
bound on fl was originally shown by Aigner (1973). I give an alternative proof
and show that it is tight. The main focus of the paper will be on the upper bound.
The identification failure in any errors in variables model is due to the
inability to differentiate between measurement error and the residual error term

390

C.R. Bollinger / Journal of Econometrics 73 (1996) 387--399

u. The proof of Theorem 1 has two main parts. First, establish the maximum
amount of measurement error which can feasibly be present in the system. Then,
find the allocation of that error to the two distinct types of measurement error
[errors of classifying O's as l's (represented by p) and errors of classifying l's as
O's (represented by q)] which gives the largest feasible//.
To establish the maximum feasible amount of measurement error, first note
that Model I can be rewritten as a classical errors-in-variables model. The
mismeasured regressor is now Z* = t~ + 7 Z (where 6 = - p and ? = 1 - p - q),
with X = Z* + e, where e is uncorrelated with Z. The regression slope is now
0 = ill?. It is well known that 0 is bounded by b and d. The term 0 is an index of
the amount of measurement error in the system: for a given/~, a larger 0 implies
larger p and/or q. The case where 0 --- d represents the maximal amount of
measurement error.
Given the amount of measurement error, as indexed by 0, the allocation of
this error to p and q is determined. From the classical errors in variables model,
it can be shown that V i e ] = F I X ] (1 - b/O). Here, the variance o f t can also be
written as a function o f p and q (see Lemma 3). These two equations can then be
used to describe the set of feasible values of p and q given the amount of
measurement error as indexed by 0. The largest feasible fl over the set of feasible
values for 0, p, and q can then be found.
If P x > ½, then the upper bound is associated with the case where p = 0 and
q = (1 - Px) (1 - P:~r). If P x < ½, then the upper bound is associated with
q = 0 and p = Px(l - p2xr). Thus the upper bound is associated with a lopsided
allocation of the total feasible measurement error. This lopsided error is misclassification from the largest of the two classes to the smallest. Further, the values
of p and q which are associated with the upper bound on/~ are both less than
½. Therefore, restricting p and q to be both less than ½would not alter the bounds

for/L
An anonymous referee has pointed out that a parsimonious representation of
the relationship between/L b, p, and q is
# =

P x ( l - Px){l - p - q)
(Px-

pXI - P ~ -

q)

(lO)

Further, the referee remarks that the upper bound on fl given that p and q are
restricted to some set is the maximum of Eq. (10) on that set. The restriction that
p + q be less than one, or even that p < ½ and q < ½ is not sufficient to arrive at
a bound. Even the result in Lemma 1 is not sufficient since p can be arbitrarily
close to Px and q can be arbitrarily close to 1 - Px. The result in Lemma
3 derives a more restrictive set for the feasible values of p and q utilizing the
information in the variance of Y. Note that this set is a function of//. While it is
possible to use the approach suggested by the referee, the approach here yields
a simpler expression for the feasible values of p and q. Additionally, the

C.R. Bollinger / Journal of Econoraetrics 73 (/996) 387-399

391

approach taken here highlights the fact that not only does the amount of error,
as measured by the term 0, impact the bounds, but the allocation of that error to
errors of omission or errors of commission is of critical importance also.
The bounds presented here are for a simple model with one regressor.
Extending the bounds to apply to a linear model with other regressors is
relatively straightforward for both the case where the other regressors are
correctly measured, and the ease where the other regressors may have classical
measurement error. The details of that extension can be found in Bollinger
(1993).

3. Imposing other information

In Section 2, I imposed the relatively weak assumption that p + q < 1.
However, in many cases stronger information may be available. This information may take many forms. The cases I will discuss here are cases where there
exist known M and K such that p ~< M and q ~< K or where there exist known
m and k such that p >I m and q >t k. Other cases are discussed in Bollinger (1993).
The restrictions that p ~< M and q ~< K insure a stronger relation, that is less
measurement error, between X and Z, and will only affect the upper bound on
since the lower bound on fl is achieved when no measurement error is present.
However, the restriction that p >/m and q >t k will affect both the upper and the
lower bound on ft.
Since the general case derived above implies that p <~ P x ( 1 - P~rr) and
q <~(1 - Px)(l - p2r) clearly any additional information about p and q must
improve on at least one of these bounds. In particular, since the upper bound
is associated with either the case where p = 0 and q ( 1 - Px) (1 -P~tr)
when Px > ½, or the case where p = Px(l - pZr) and q = 0 when Px < ½, the
restriction that p ~< M and q <~ K must improve on the case associated
with the general upper bound. Hence, if Px > ½, then K must be less than
(1 - Px)(1 - p 2 ) ; if Px < ½, then M must be less than Px(1 - P~v).
if p ~< M < Px(1 - P~tr) and q ~< K < (1 - Px) (1 - P~v), then two possible
cases arise. In the first case, the original maximum feasible amount of measurement error is still feasible. Then the new information only affects the feasible
allocations of the measurement error to p and q. In the second case, the values of
M and K are so low that the original maximal amount of measurement error is
no longer feasible. In this case, not only do the new bounds affect the feasible
allocation of the measurement error, but the maximal feasible amount of
measurement error is reduced.
Theorem 2 gives the upper bound for the case where Px <½ and
p <~ M < Px(1 - pZxr) and K > (1 - Px) (1 - P~tr). The case where Px > ½ is
symmetric. Theorem 3 gives the upper bound for the case where
p <~ M < e x ( l - P~tr) and q ~< K < (1 - Px) (1 - p2r).

392

CR. Bollinger / Journal of Econometrics 73 (1996) 387-399

Theorem 2. Given Model I with Px < ½ and the additional information that
p <~ M < Px(l - p~r) but K > ( l - P x ) ( l - p ~ r ) f o r s o m e k n o w n
M and K,
then

~<

fd-ex

+ b.(l -

).

ex),

max (d(P x - M) + b(l -- Px) (Px/(Px -- M))

(! 1)

Theorem3. Given Model I with the additional information that p <~ M <
Px(I - P~r) and q ~< K < (1 - Px) (1 - p2r),for some known M and K, then

fl <~

m a x f1d. ( - Px - K) + b. Px ((1 - Px)/(I - Px - K)))
( d ( P x -- M) + b(! - Px) ((Px/(Px - M))
~'

(12)

if
[
(l - P x ) P x
d~<b ( l - P x
K)(Px-Mi

]

;

(13)

otherwise

~<(I-M-K)b

[ ( I - P x (1-- Px)Px
]"
K)(Px-M)

(14)

It may seem possible to 'bootstrap' up to tighter bounds by using Eqs. (8) and
(9) from Theorem 1. Inspection of the results in Theorems 2 and 3 will
demonstrate this approach will simply return the original upper bounds from
Theorem 1.
The results for prior information bounding p and q from below are very
similar. In this case, the lower bounds clearly rule out the minimum feasible
amount of measurement error. Hence, the expression for the new minimum is
similar to case two of Theorem 3. Since the upper bound on fl occurs when either
p = 0 or q = 0, the restrictions on p and q have an impact on the upper bound
similar to case one of Theorem 3. The new bounds are given in Theorem 4.
Theorem 4. Given Model I with the additional information that m <~p and k <~ q,
for some known m and k and the condition that
d>~b[i I

(I-Px)Px

05)

],

then
fl>

(l-m-k)h [ (1

(I l- -' XP -x-)KPIx( I ' x l

, ,]

(16)

and
~'d(l - Px - k) + b" Px((l - Px)/(l - Px - k)) }
fl <~ max ( d ( P x - m) + b(! - Px) (Px/(Px - m))

(17)

CR. Bollinger / Journal of Econometrics 73 (1996) 387-399

393

The condition in the theorem insures that the lower bounds do not rule out all
feasible values for the measurement error. If p and q are bounded both from
above and below, the upper bound on fl is the least of the two upper bounds
from Theorems 3 and 4.

4. Empirical example: Bounding the union wage differential

The simplest extension of the results above is to a linear structural equation
where additional regressors are assumed to be measured without error. This also
requires that the measurement :~rror process for the mismeasured binary regressor must be independent of the other regressors. This case is represented by
Model 11:
Y=~+fllZt

+~'~Z_2+u,

E[ulZ~,_Z2]=O,

(18)

Pr[X1 = 0 I Z1, _Z2] = (1 - p) (1 - Zl) + qZt,

09)

P r [ X , = 1 I Z~, _Z2, Y] =(1 - q ) Z ~ + p ( l - Z~),

(20)

Zt "~ Bernoulli (Pz)

(21)

p + q < 1.

with

0 < Pz < 1,

{22)

Again, p and q are the misclassification rates. The researcher can observe Y, X 1,
the mismeasured version of the binary regressor Z~, and _Z2,the vector of other
correctly measured regressors, in the particular example here, Y is the natural
log of average hourly earnings, ZI is the true union status 11 if a member of
a union, 0 otherwise), while Xt is the reported union status, and the vector
_Zz contains the variables: Education in years, Potential Experience (Age
- Education - 6), Potential Experience squared, Race (1 if black), and Gender
(1 if female). This data set is a subsample from the May 1985 Current Population
Survey (CPS) of size 533 from Berndt (19911. One observation was dropped since
Age - Education - 6 was negative. I chose this data set for availability and
reproducibility. A more comprehensive analysis utilizing more recent data can
be found in Boilinger (1993). 1 assume that other variables are correctly measured and abstract from the problem of endogeneity of the Union variable in
order to focus on the bounds derived here.
Bounds, similar to those derived in Section 2, car, be shown for Model II. The
linear projection of Y on _Z2 yields the vector _b2 as a biased (due to the omitted
variable Z~) estimate for/~2 and an intercept term a. The linear projection of
X~ on _Z2 yields slope coefficients _H, an intercept term h0, and an r-squared of
R~z. The residuals Y* from the regression of Y on Z_z and the residuals X• from
the regression of X 1 on Z z can be shown to have a structure almost identical to
Model l. Hence, with only slight modification, the bounds from Model I can be
applied to the residual model to bound Pl. The bounds o n / ~ can then be used

394

CR. Bollinger/ Journal of Econometrics 73 (1996) 387-399

with the biased coefficient estimates from the short regression of Y on _Z2 to
obtain bounds on ~2. This result is summarized in Theorem 5. The term b is the
slope from the projection of Y* on X*. The term d is the inverse of the slope
from the projection of X* on Y*.
Theorem 5.

Given Model I! {ill >1 0 without loss of generality),

f d ( e x + (1 -- ex)R~cz) + b(i -- ex) (1 -- RZz)}
b <~/i~ ~ max (d((l - Px) + PxR2z) + bPx(l - R2xz)
_'

(23)

and the components of the slope vector ~2 are bounded by the terms b_z - _lib and
_b2 - _Hd. The lower and upper bound of each component are determined by the
sign of each component in Ho. The intercept term is bounded by
min {a - bho, a - dho}

(24)

max{a - bho, a - dho + Px(l - RZz) (d - b)}.

(25)

and

All of the terms in the bounds are easily estimable. In addition, estimable
asymptotic variances can be derived using standard delta method results. The
result can be modified when the other regressors in the model are potentially
mismeasured as well. The bounds derived by Kiepper and Learner (1984) can be
directly incorporated (see Bollinger, 1993).
Descriptive statistics for the sample are reported in Table 1. The estimated
upper and lower bounds for the slope coeflicients are reported in Table 2, with
standard errors of the estimates in parentheses. Since the term _b2 -_Hb is
associated with the lower bound on lit and the term _b2 - _Hd is associated with
the upper bound on/11, I have reported these as the vectors 'left' and 'right'
respectively. The estimated bounds for p and q are reported in Table 3.
The 'right' bounds can be very large relative to the 'left" bounds. It is
important to note that the 'left' bounds are also the estimates for the slope
coetfieients of the model if the measurement error is ignored. This implies that
measurement error has the potential to cause significant bias. However, in many
cases it is reasonable to bound p and q from above. I have chosen three sets of
values for M and K (the upper bounds on p and q, respectively). The first case
Table i
Descriptive statistics
Ln Wage

Education

Experience

Black

Gender

Union

Mean

2.06

! 3.01

17.86

0.13

0.45

0.18

Std. errol

0.53

2.61

12.37

0.33

0.50

0.38

C.R. Bollinger / Journal of Econometrics 73 (1996) 387-399

395

Table 2
Estimated bounds for linear model
Variable

Left bounds

Right bounds

Union

0.21
(0.05)

5.48
(I.23)

Constant

0.60
(0.12)

0.07
(0.65)

Education

0.09
(O.Ol)

0.07
(0.05)

Experience

0.04
(0.01 )

- 0.01
(0.03)

Experience squared

- 0.0005

-0.0001

(0.0001)

(0.0007)

Black

- 0.12
(0.05)

- 0.70
(0.35)

Gender

- 0.23
(0.04)

0.57
(0.21)

Table 3
Estimated bounds for p and q

p
q

Minimum

Maximum

0
0

0.1658
0.7546

sets M = 0.13 and K = 0.20. These values are chosen as representative of rather
weak assumptions (relative to what is known from Table 3) to illustrate the
sensitivity of the bounds to additional information. The second ease sets
~d = 0.1 and K = 0.1. This value can be thought of as a 'folk theorem" in which
measurement error is thought to be less than 10%. In the third case, I utilize
results from Freeman (1984) and set M - - 0 . 0 2 3 and K = 0.081. This case
represents Freeman's (1984) worst-case estimates of misreporting union status in
the CPS. The new 'right' bounds on all the parameters for each of these cases are
reported in Table 4. Since the information utilized has no impact on the 'left"
bounds, these remain the same as those reported in Table 2.
The most striking feature of the results presented in Table 4 is the sensitivity of
the upper bound to additional information. Focusing only on the values for the
union coefficient, one notes that even the first case with M = 0.13 and K = 0.20
results ie substantial improvement in precision (width) of the bounds. The

CR. Bollinger / Journal of Econometrics 73 (1996) 387-399

396

Table 4
E s t i m a t e d right b o u n d s for linear m o d e l under stronger information
Variable

M = 0.13, K = 0.20

M = K = 0.10

M = 0.023, K = 0.081

Union

0,83
(1.44)

0.47
(0.33)

0.24
(0.06)

Constant

0.52
(0.22)

0.57
(0.13)

0.60
(0.13)

Education

0.09

0.09

0.09

(O.Ol)

(O.Ol)

(O.Ol)

0.03
(0.02)

0.03
(0.01)

0.04
(0.01)

Experience s q u a r e d

- 0.0005
(0.0002)

- 0.0005
(0.0001)

- 0.0005
(0.0001)

Black

- 0.91

- 0.15

- 0.13

Experience

(0.2O)
Gender

- 0.10
(0.28)

(O.07)
- 0.19
(0.07)

(0.05)
- 0.23
(0.04)

estimated upper bound on the union coefficient falls from 5.48 to 0.83. In fact,
it can be shown that the bound on p is, in this case, driving the result. Hence,
the restriction on q could be relaxed even further with no degradation of
the precision. As the additional information is strengthened, by setting
M = K = 0.1, the bounds continue to tighten, but at a less dramatic rate: the
new upper bound on the union coefficient falls to 0.47. Finally, when M = 0.023
and K = 0.08, the bounds fall to 0.24.
Mismeasurement may have great impact on the estimates for parameters in
many models. The example here illustrates how serious this problem may be.
One approach to gaining insight on the impact of measurement error is to utilize
bounds, such as those presented here, to estimate the potential impact of
measurement error.

Appendix
P r o o f o f Theorem I

This proof will focus on the upper bound. The lower bound is derived
similarly.
Lemma 1.

Given Model I, Px > P and 1 - Px > q.

C.R. Bollinger / Journal of Econometrics 73 (1996) 387-399

397

By definition Px = Pz(1 - q) + (1 - Pz) P. The result follows from the
restrictions that 0 < Pz < 1 and p + q < 1. •
Proof.

Lemmo 2.

Given Model L sign(b) -- sign(r) = sign(0), and for fl > O, b <~ 0 <~d.

Model I can be rewritten as

Proof.

Y = (~ - 60) + OZ* + u,

(26)

X = Z* + e,

(27)

where = 1 - p - q, 6 = - p, 0 = [3/7, Z* = ~ + 7Z, and ~ is uncorrelated
with Z and u. Then it can be shown that

(28)

~ = ~ - 0#x~,
V [~] = ~2x(1 -

b/O),

(29)

#~ ~blfl~.

(30)

=

The first result in the lemma follows from the restrictions that ~z2 > 0 and
p + q < 1. The bound on 0 follows from the restrictions that a~ >/0 and V [~]
>/0. i
Lemma 3.

Given Model ! and any feasible value o f 0 from Lemma 2,

q=(l-Px)(I-( p - Px- ~ _p )
Proof.

b

(31)

(~)).

It can be shown that
\ 1 - p - q ] q ( l - q) +

p0

- p).

Setting this expression equal to Eq. (29) and solving for q yields the result.

(32)

•

Using the result of Lemma 3 and the definition of 7, the maximum feasible
value for y given 0 is

By definition of 0, fl = 7 * 0. Hence, the maximum value of fl is
flm,~ = max
0

7max0.

(34)

Solving {34) subject to the bounds on 0 from Lemma 2 yields the upper bound
on ft. The lower bound is found by finding 7rain for given 0 and similarly finding

C.R. Bollinger / Journal of Econometrics 73 U996) 387-399

398

/p~i,.. The bounds on ~ follow from
= E [ Y ] - O ( e x - p)

(35)

and the results from Lemma 2 and Lemma 3. The bounds on p and q follow from
Lemma 3 and the upper bound on 0. The bounds are tight since there exists
P, q, Pz, and V[u] to support any ff in the feasible region including the bounds
themselves. Q.E.D.

Proof of Theorem 2
Theorem 2 gives a new upper bound when only p is restricted and Px < ½. if
Px < ½, then the upper bound on fl is achieved when p = Px(1 - P~'r). The
restriction on p rules out this allocation. Specifically, the maximum value of
used in the proof of Theorem 1 is no longer feasible. The maximum value of
"/under the conditions in Theorem 2 occurs at either p = M or p = O. The proof
is completed as above by noting that 7maxogives maximum feasible fl for a given
0, and 0 = d gives the global maximum of p. Q.E.D.

Proof of Theorem 3
Lemma 4. Given Model I and the restrictions that p <~M < Px(l - P~r) and
q <<,K < (1 - Px) (1 - P~r), then
0~min

Id,b[

.,x.,.

(1-P-~-K-)(-P-~x-M)

]t

"

(36)

Proof. From Eq. (31), for a particular value of 0 to be feasible given the
restrictions on p and q, it must be that
K>~(I-Px)(I

(" .P.._x ~(b
-\Px- M ]\0))"

(37)

Rearranging this restriction and including the result from Lemma 2 gives the
result. III
Then the maximum value of ), occurs at either p = M or q = K or both.
Complete the proof by evaluating 7maxOat the maximum value of 0. Q.E.D.

Proof of Theorem 4
Lower Bound: As in Lemma 4, for a particular value of 0 to be feasible,

Rearranging gives the minimum value of 0 feasible for given k and m. This value
is only achieved when p = m and q = k. The lower bound follows.

Bollinger/ JournalofEconometrics73 (1996)387-399

c.R

399

Upper Bound: As in T h e o r e m 3, the m a x i m u m feasible value for ~, given
0 occurs at either p = m o r q = k, the feasible m i n i m u m s of p a n d q. C o m p l e t e
the p r o o f by evaluating 7re=x0 at the m a x i m u m value of 0 = d. Q.E.D.
Proof of Theorem 5
By definition Y* =
Z l on _Ze. Then,

fllZ* + u, where

VEX*] = v[x1]

Z* is the residual from the regression o f

(1 - RZz).

(39)

The results from L e m m a 2 a n d L e m m a 3 can be applied, yielding b o u n d s on
0 = fl~/~, a n d
q = tl -

vx)

(1

-

(\Px
P x - p ~((b_~(l_RZxz)+R~cz))"
) \\o)

(40)

Then~ the upper a n d lower b o u n d s for fit can be found as in T h e o r e m 1.
By d e f i n i t i o n b z = ~z + _Ffl~, where _F is the slope from the regression of Z~ on
_Z2. Also, _H = ~,_F. R e a r r a n g i n g a n d utilizing the b o u n d on 0 yield the b o u n d
on ~2. Q.E.D.

References
Aigner, Dennis J., 1973, Regression with a binary independent variable subject to errors of
observation, Journal of Econometrics 1, 49-60.
l~rndt, Ernst R., 1991,The practice of econometrics: Classical and contemporary (Addison-Wesley,
Reading, MA).
Bollinger, Christopher R., 1993, Measurement error in binary regressor with an application bounding the union wage differential, Ph.D. dissertation (University of Wisconsin, Madison, WI).
Bollinger, Christopher R. and Martin H. David, 1993, Modeling food stamp participation in the
presence of reporting errors, Social Science Research Institute working paper no. 9310 (University of Wisconsin, Madison, Wl).
Erikson, Timothy, 1993, Restricting regression slopes in the errors-in-variables model by bounding
the error correlation, Econometriea 61,959-970.
Frisch, R., 1934,Statistical confluence analysis by means of complete regression systems (University
Institute for Economics, Oslo).
Freeman, Richard B., 1984, Longitudinal analysis of the effects of trade unions, Journal of Labor
Economics 2, 1-26.
Klepper, Steven and Edward E. Learner, 1984, Consistent sets of estimates for regressions with
errors in all variables, Econometrica 52, 163-183.
Kleppcr, Steven, 1988, Bounding the effects of measurement error in regressions involving
dichotomous variables, Journal of Econometrics 37, 343-359.
Krasker, William S. and John W. Pratt, 1986, Bounding the effects of proxy variables on regression
coefficients. Econometrica 54, 641-655.
Mathiowetz, Nancy A. and Greg J. Duncan, 1988, Out of work, out of mind: Respon~ error in
retrospective reports of unemployment, Journal of Business and Economic Statistics 6, 221-229.
Poterba, James M. and Lawrence H. Summers, 1986, Reporting errors and labor market dynamics,
Econometrica 6, 221-229.

