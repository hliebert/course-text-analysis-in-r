arXiv:1608.00033v2 [math.ST] 31 May 2018

Locally Robust Semiparametric Estimation
Victor Chernozhukov

Juan Carlos Escanciano

Hidehiko Ichimura

MIT

Indiana University

University of Tokyo

Whitney K. Newey
MIT

James M. Robins
Harvard University

April 2018
Abstract
We give a general construction of debiased/locally robust/orthogonal (LR) moment
functions for GMM, where the derivative with respect to first step nonparametric estimation is zero and equivalently first step estimation has no effect on the influence function.
This construction consists of adding an estimator of the influence function adjustment term
for first step nonparametric estimation to identifying or original moment conditions. We
also give numerical methods for estimating LR moment functions that do not require an
explicit formula for the adjustment term.
LR moment conditions have reduced bias and so are important when the first step is
machine learning. We derive LR moment conditions for dynamic discrete choice based on
first step machine learning estimators of conditional choice probabilities.
We provide simple and general asymptotic theory for LR estimators based on sample
splitting. This theory uses the additive decomposition of LR moment conditions into an
identifying condition and a first step influence adjustment. Our conditions require only
mean square consistency and a few (generally either one or two) readily interpretable rate
conditions.
LR moment functions have the advantage of being less sensitive to first step estimation.
Some LR moment functions are also doubly robust meaning they hold if one first step
is incorrect. We give novel classes of doubly robust moment functions and characterize
double robustness. For doubly robust estimators our asymptotic theory only requires one
rate condition.
Keywords: Local robustness, orthogonal moments, double robustness, semiparametric
estimation, bias, GMM.
JEL classification: C13; C14; C21; D24

1

1

Introduction

There are many economic parameters that depend on nonparametric or large dimensional first
steps. Examples include dynamic discrete choice, games, average consumer surplus, and treatment effects. This paper shows how to construct moment functions for GMM estimators that are
debiased/locally robust/orthogonal (LR), where moment conditions have a zero derivative with
respect to the first step. We show that LR moment functions can be constructed by adding the
influence function adjustment for first step estimation to the original moment functions. This
construction can also be interpreted as a decomposition of LR moment functions into identifying
moment functions and a first step influence function term. We use this decomposition to give
simple and general conditions for root-n consistency and asymptotic normality, with different
properties being assumed for the identifying and influence function terms. The conditions are
easily interpretable mean square consistency and second order remainder conditions based on
estimated moments that use cross-fitting (sample splitting). We also give numerical estimators
of the influence function adjustment.
LR moment functions have several advantages. LR moment conditions bias correct in a way
that eliminates the large biases from plugging in first step machine learning estimators found
in Belloni, Chernozhukov, and Hansen (2014). LR moment functions can be used to construct
debiased/double machine learning (DML) estimators, as in Chernozhukov et al. (2017, 2018).
We illustrate by deriving LR moment functions for dynamic discrete choice estimation based
on conditional choice probabilities. We provide a DML estimator for dynamic discrete choice that
uses first step machine learning of conditional choice probabilities. We find that it performs well
in a Monte Carlo example. Such structural models provide a potentially important application
of DML, because of potentially high dimensional state spaces. Adding the first step influence
adjustment term provides a general way to construct LR moment conditions for structural
models so that machine learning can be used for first step estimation of conditional choice
probabilities, state transition distributions, and other unknown functions on which structural
estimators depend.
LR moment conditions also have the advantage of being relatively insensitive to small variation away from the first step true function. This robustness property is appealing in many
settings where it may be difficult to get the first step completely correct. Many interesting and
useful LR moment functions have the additional property that they are doubly robust (DR),
meaning moment conditions hold when one first step is not correct. We give novel classes of DR
moment conditions, including for average linear functionals of conditional expectations and probability densities. The construction of adding the first step influence function adjustment to an
identifying moment function is useful to obtain these moment conditions. We also give necessary
and sufficient conditions for a large class of moment functions to be DR. We find DR moments
have simpler and more general conditions for asymptotic normality, which helps motivate our
2

consideration of DR moment functions as special cases of LR ones. LR moment conditions also
help minimize sensitivity to misspecification as in Bonhomme and Weidner (2018).
LR moment conditions have smaller bias from first step estimation. We show that they have
the small bias property of Newey, Hsieh, and Robins (2004), that the bias of the moments is of
smaller order than the bias of the first step. This bias reduction leads to substantial improvements in finite sample properties in many cases relative to just using the original moment conditions. For dynamic discrete choice we find large bias reductions, moderate variance increases
and even reductions in some cases, and coverage probabilities substantially closer to nominal.
For machine learning estimators of the partially linear model, Chernozhukov et al. (2017, 2018)
found bias reductions so large that the LR estimator is root-n consistent but the estimator based
on the original moment condition is not. Substantial improvements were previously also found
for density weighted averages by Newey, Hsieh, and Robins (2004, NHR). The twicing kernel
estimators in NHR are numerically equal to LR estimators based on the original (before twicing)
kernel, as shown in Newey, Hsieh, Robins (1998), and the twicing kernel estimators were shown
to have smaller mean square error in large samples. Also, a Monte Carlo example in NHR finds
that the mean square error (MSE) of the LR estimator has a smaller minimum and is flatter as
a function of bandwidth than the MSE of Powell, Stock, and Stokerâ€™s (1989) density weighted
average derivative estimator. We expect similar finite sample improvements from LR moments
in other cases.
LR moment conditions have appeared in earlier work. They are semiparametric versions of
Neyman (1959) C-alpha test scores for parametric models. Hasminskii and Ibragimov (1978)
suggested LR estimation of functionals of a density and argued for their advantages over plug-in
estimators. Pfanzagl and Wefelmeyer (1981) considered using LR moment conditions for improving the asymptotic efficiency of functionals of distribution estimators. Bickel and Ritov
(1988) gave a LR estimator of the integrated squared density that attains root-n consistency
under minimal conditions. The Robinson (1988) semiparametric regression and Ichimura (1993)
index regression estimators are LR. Newey (1990) showed that LR moment conditions can be
obtained as residuals from projections on the tangent set in a semiparametric model. Newey
(1994a) showed that derivatives of an objective function where the first step has been â€concentrated outâ€ are LR, including the efficient score of a semiparametric model. NHR (1998, 2004)
gave estimators of averages that are linear in density derivative functionals with remainder rates
that are as fast as those in Bickel and Ritov (1988). Doubly robust moment functions have
been constructed by Robins, Rotnitzky, and Zhao (1994, 1995), Robins and Rotnitzky (1995),
Scharfstein, Rotnitzky, and Robins (1999), Robins, Rotnitzky, and van der Laan (2000), Robins
and Rotnitzky (2001), Graham (2011), and Firpo and Rothe (2017). They are widely used for
estimating treatment effects, e.g. Bang and Robins (2005). Van der Laan and Rubin (2006) developed targeted maximum likelihood to obtain a LR estimating equation based on the efficient

3

influence function of a semiparametric model. Robins et al. (2008, 2017) showed that efficient
influence functions are LR, characterized some doubly robust moment conditions, and developed
higher order influence functions that can reduce bias. Belloni, Chernozhukov, and Wei (2013),
Belloni, Chernozhukov, and Hansen (2014), Farrell (2015), Kandasamy et al. (2015), Belloni,
Chernozhukov, Fernandez-Val, and Hansen (2016), and Athey, Imbens, and Wager (2017) gave
LR estimators with machine learning first steps in several specific contexts.
A main contribution of this paper is the construction of LR moment conditions from any
moment condition and first step estimator that can result in a root-n consistent estimator of
the parameter of interest. This construction is based on the limit of the first step when a data
observation has a general distribution that allows for misspecification, similarly to Newey (1994).
LR moment functions are constructed by adding to identifying moment functions the influence
function of the true expectation of the identifying moment functions evaluated at the first step
limit, i.e. by adding the influence function term that accounts for first step estimation. The
addition of the influence adjustment â€partials outâ€ the first order effect of the first step on the
moments. This construction of LR moments extends those cited above for first step density and
distribution estimators to any first step, including instrumental variable estimators. Also, this
construction is estimator based rather than model based as in van der Laan and Rubin (2006)
and Robins et al. (2008, 2017). The construction depends only on the moment functions and
the first step rather than on a semiparametric model. Also, we use the fundamental Gateaux
derivative definition of the influence function to show LR rather than an embedding in a regular
semiparametric model.
The focus on the functional that is the true expected moments evaluated at the first step
limit is the key to this construction. This focus should prove useful for constructing LR moments
in many setting, including those where it has already been used to find the asymptotic variance
of semiparametric estimators, such as Newey (1994a), Pakes and Olley (1995), Hahn (1998), Ai
and Chen (2003), Hirano, Imbens, and Ridder (2003), Bajari, Hong, Krainer, and Nekipelov
(2010), Bajari, Chernozhukov, Hong, and Nekipelov (2009), Hahn and Ridder (2013, 2016), and
Ackerberg, Chen, Hahn, and Liao (2014), Hahn, Liao, and Ridder (2016). One can construct
LR moment functions in each of these settings by adding the first step influence function derived
for each case as an adjustment to the original, identifying moment functions.
Another contribution is the development of LR moment conditions for dynamic discrete
choice. We derive the influence adjustment for first step estimation of conditional choice probabilities as in Hotz and Miller (1993). We find encouraging Monte Carlo results when various
machine learning methods are used to construct the first step. We also give LR moment functions
for conditional moment restrictions based on orthogonal instruments.
An additional contribution is to provide general estimators of the influence adjustment term
that can be used to construct LR moments without knowing their form. These methods estimate

4

the adjustment term numerically, thus avoiding the need to know its form. It is beyond the scope
of this paper to develop machine learning versions of these numerical estimators. Such estimators
are developed by Chernozhukov, Newey, and Robins (2018) for average linear functionals of
conditional expectations.
Further contributions include novel classes of DR estimators, including linear functionals of
nonparametric instrumental variables and density estimators, and a characterization of (necessary and sufficient conditions for) double robustness. We also give related, novel partial
robustness results where original moment conditions are satisfied even when the first step is not
equal to the truth.
A main contribution is simple and general asymptotic theory for LR estimators that use
cross-fitting in the construction of the average moments. This theory is based on the structure
of LR moment conditions as an identifying moment condition depending on one first step plus
an influence adjustment that can depend on an additional first step. We give a remainder
decomposition that leads to mean square consistency conditions for first steps plus a few readily
interpretable rate conditions. For DR estimators there is only one rate condition, on a product
of sample remainders from two first step estimators, leading to particularly simple conditions.
This simplicity motivates our inclusion of results for DR estimators. This asymptotic theory
is also useful for existing moment conditions that are already known to be LR. Whenever the
moment condition can be decomposed into an identifying moment condition depending on one
first step and an influence function term that may depend on two first steps the simple and
general regularity conditions developed here will apply.
LR moments reduce that smoothing bias that results from first step nonparametric estimation
relative to original moment conditions. There are other sources of bias arising from nonlinearity
of moment conditions in the first step and the empirical distribution. Cattaneo and Jansson
(2017) and Cattaneo, Jansson, and Ma (2017) give useful bootstrap and jackknife methods that
reduce nonlinearity bias. Newey and Robins (2017) show that one can also remove this bias by
cross fitting in some settings. We allow for cross-fitting in this paper.
Section 2 describes the general construction of LR moment functions for semiparametric
GMM. Section 3 gives LR moment conditions for dynamic discrete choice. Section 4 shows how
to estimate the first step influence adjustment. Section 5 gives novel classes of DR moment
functions and characterizes double robustness. Section 6 gives an orthogonal instrument construction of LR moments based on conditional moment restrictions. Section 7 provides simple
and general asymptotic theory for LR estimators.

5

2

Locally Robust Moment Functions

The subject of this paper is GMM estimators of parameters where the sample moment functions
depend on a first step nonparametric or large dimensional estimator. We refer to these estimators
as semiparametric. We could also refer to them as GMM where first step estimators are â€œplugged
inâ€ the moments. This terminology seems awkward though, so we simply refer to them as
semiparametric GMM estimators. We denote such an estimator by Î²Ì‚, which is a function of the
data z1 , ..., zn where n is the number of observations. Throughout the paper we will assume that
the data observations zi are i.i.d. We denote the object that Î²Ì‚ estimates as Î²0 , the subscript
referring to the parameter value under the distribution F0 of zi .
To describe semiparametric GMM let m(z, Î², Î³) denote an r Ã— 1 vector of functions of the
data observation z, parameters of interest Î², and a function Î³ that may be vector valued. The
function Î³ can depend on Î² and z through those arguments of m. Here the function Î³ represents
some possible first step, such as an estimator, its limit, or a true function. A GMM estimator
can be based on a moment condition where Î²0 is the unique parameter vector satisfying
E[m(zi , Î²0 , Î³0 )] = 0,

(2.1)

and Î³0 is the true Î³. We assume that this moment condition identifies Î². Let Î³Ì‚ denote some
first step estimator of Î³0 . Plugging in Î³Ì‚ to obtain m(zi , Î², Î³Ì‚) and averaging over zi results in the
P
estimated sample moments mÌ‚(Î²) = ni=1 m(zi , Î², Î³Ì‚)/n. For WÌ‚ a positive semi-definite weighting
matrix a semiparametric GMM estimator is
Î²Ìƒ = arg min mÌ‚(Î²)T WÌ‚ mÌ‚(Î²),
Î²âˆˆB

where AT denotes the transpose of a matrix A and B is the parameter space for Î². Such
estimators have been considered by, e.g. Andrews (1994), Newey (1994a), Newey and McFadden
(1994), Pakes and Olley (1995), Chen and Liao (2015), and others.
Locally robust (LR) moment functions can be constructed by adding the influence function adjustment for the first step estimator Î³Ì‚ to the identifying or original moment functions
m(z, Î², Î³). To describe this influence adjustment let Î³(F ) denote the limit of Î³Ì‚ when zi has distribution F, where we restrict F only in that Î³(F ) exists and possibly other regularity conditions
are satisfied. That is, Î³(F ) is the limit of Î³Ì‚ under possible misspecification, similar to Newey
(1994). Let G be some other distribution and FÏ„ = (1 âˆ’ Ï„ )F0 + Ï„ G for 0 â‰¤ Ï„ â‰¤ 1, where F0
denotes the true distribution of zi . We assume that G is chosen so that Î³(FÏ„ ) is well defined for
Ï„ > 0 small enough and possibly other regularity conditions are satisfied, similarly to Ichimura
and Newey (2017). The influence function adjustment will be the function Ï†(z, Î², Î³, Î») such that
for all such G,
Z
d
E[m(zi , Î², Î³(FÏ„ ))] = Ï†(z, Î², Î³0, Î»0 )G(dz), E[Ï†(zi , Î², Î³0, Î»0 )] = 0,
(2.2)
dÏ„
6

where Î» is an additional nonparametric or large dimensional unknown object on which Ï†(z, Î², Î³, Î»)
depends and the derivative is from the right (i.e. for positive values of Ï„ ) and at Ï„ = 0.
This equation is the well known definition of the influence function Ï†(z, Î², Î³0 , Î»0 ) of Âµ(F ) =
E[m(zi , Î², Î³(F ))] as the Gateaux derivative of Âµ(F ), e.g. Huber (1981). The restriction of G so
that Î³(FÏ„ ) exists allows Ï†(z, Î², Î³0 , Î»0 ) to be the influence function when Î³(F ) is only well defined for certain types of distributions, such as when Î³(F ) is a conditional expectation or density.
The function Ï†(z, Î², Î³, Î») will generally exist when E[m(zi , Î², Î³(F ))] has a finite semiparametric
variance bound. Also Ï†(z, Î², Î³, Î») will generally be unique because we are not restricting G very
much. Also, note that Ï†(z, Î², Î³, Î») will be the influence adjustment term from Newey (1994a),
as discussed in Ichimura and Newey (2017).
LR moment functions can be constructed by adding Ï†(z, Î², Î³, Î») to m(z, Î², Î³) to obtain new
moment functions
Ïˆ(z, Î², Î³, Î») = m(z, Î², Î³) + Ï†(z, Î², Î³, Î»).
(2.3)
Let Î»Ì‚ be a nonparametric or large dimensional estimator having limit Î»(F ) when zi has distriP
bution F, with Î»(F0 ) = Î»0 . Also let ÏˆÌ‚(Î²) = ni=1 Ïˆ(zi , Î², Î³Ì‚, Î»Ì‚)/n. A LR GMM estimator can be
obtained as
(2.4)
Î²Ì‚ = arg min ÏˆÌ‚(Î²)T WÌ‚ ÏˆÌ‚(Î²).
Î²âˆˆB

âˆš
As usual a choice of WÌ‚ that minimizes the asymptotic variance of n(Î²Ì‚ âˆ’Î²0 ) will be a consistent
âˆš
estimator of the inverse of the asymptotic variance â„¦ of nÏˆÌ‚(Î²0 ). As we will further discuss,
Ïˆ(z, Î², Î³, Î») being LR will mean that the estimation of Î³ and Î» does not affect â„¦, so that â„¦ =
E[Ïˆ(zi , Î²0 , Î³0 , Î»0 )Ïˆ(zi , Î²0 , Î³0 , Î»0 )T ]. An optimal WÌ‚ also gives an efficient estimator in the wider
sense shown in Ackerberg, Chen, Hahn, and Liao (2014), making Î²Ì‚ efficient in a semiparametric
model where the only restrictions imposed are equation (2.1).
The LR property we consider is that the derivative of the true expectation of the moment
function with respect to the first step is zero, for a Gateaux derivative like that for the influence
function in equation (2.2). Define FÏ„ = (1 âˆ’ Ï„ )F0 + Ï„ G as before where G is such that both
Î³(FÏ„ ) and Î»(FÏ„ ) are well defined. The LR property is that for all G as specified,
d
E[Ïˆ(zi , Î², Î³(FÏ„ ), Î»(FÏ„ ))] = 0.
dÏ„

(2.5)

Note that this condition is the same as that of Newey (1994a) for the presence of Î³Ì‚ an Î»Ì‚ to
have no effect on the asymptotic distribution, when each FÏ„ is a regular parametric submodel.
âˆš
Consequently, the asymptotic variance of nÏˆÌ‚(Î²0 ) will be â„¦ as in the last paragraph.
To show LR of the moment functions Ïˆ(z, Î², Î³, Î») = m(z, Î², Î³) + Ï†(z, Î², Î³, Î») from equation
(2.3) we use the fact that the second, zero expectation condition in equation (2.2) must hold for
all possible true distributions. For any given Î² define Âµ(F ) = E[m(zi , Î², Î³(F ))] and Ï†(z, F ) =
Ï†(z, Î², Î³(F ), Î»(F )).
7

R
R
Theorem 1: If i) dÂµ(FÏ„ )/dÏ„ = Ï†(z, F0 )G(dz), ii) Ï†(z, FÏ„ )FÏ„ (dz) = 0 for all Ï„ âˆˆ [0, Ï„Ì„ ),
R
R
and iii) Ï†(z, FÏ„ )F0 (dz) and Ï†(z, FÏ„ )G(dz) are continuous at Ï„ = 0 then
d
dÂµ(FÏ„ )
E[Ï†(zi , FÏ„ )] = âˆ’
.
dÏ„
dÏ„

(2.6)

The proofs of this result and others are given in Appendix B. Assumptions i) and ii) of
Theorem 1 require that both parts of equation (2.2) hold with the second, zero mean condition
being satisfied when FÏ„ is the true distribution. Assumption iii) is a regularity condition. The
LR property follows from Theorem 1 by adding dÂµ(FÏ„ )/dÏ„ to both sides of equation (2.6) and
noting that the sum of derivatives is the derivative of the sum. Equation (2.6) shows that the
addition of Ï†(z, Î², Î³, Î») â€partials outâ€ the effect of the first step Î³ on the moment by â€cancellingâ€
the derivative of the identifying moment E[m(zi , Î², Î³(FÏ„ ))] with respect to Ï„ . This LR result for
Ïˆ(z, Î², Î³, Î») differs from the literature in its Gateaux derivative formulation and in the fact that
it is not a semiparametric influence function but is the hybrid sum of an identifying moment
function m(z, Î², Î³) and an influence function adjustment Ï†(z, Î², Î³, Î»).
Another zero derivative property of LR moment functions is useful. If the sets Î“ and Î› of
possible limits Î³(F ) and Î»(F ), respectively, are linear, Î³(F ) and Î»(F ) can vary separately from
one another, and certain functional differentiability conditions hold then LR moment functions
will have the property that for any Î³ âˆˆ Î“, Î» âˆˆ Î›, and ÏˆÌ„(Î³, Î») = E[Ïˆ(zi , Î²0 , Î³, Î»)],
âˆ‚
âˆ‚
ÏˆÌ„((1 âˆ’ Ï„ )Î³0 + Ï„ Î³, Î»0 ) = 0, ÏˆÌ„(Î³0 , (1 âˆ’ Ï„ )Î»0 + Ï„ Î») = 0.
âˆ‚Ï„
âˆ‚Ï„

(2.7)

That is, the expected value of the LR moment function will have a zero Gateaux derivative
with respect to each of the first steps Î³ and Î». This property will be useful for several results
to follow. Under still stronger smoothness conditions this zero derivative condition will result
in the existence of a constant C such that for a function norm kÂ·k,
ÏˆÌ„(Î³, Î»0 ) â‰¤ C kÎ³ âˆ’ Î³0 k2 , ÏˆÌ„(Î³0 , Î») â‰¤ C kÎ» âˆ’ Î»0 k2 ,

(2.8)

when kÎ³ âˆ’ Î³0 k and kÎ» âˆ’ Î»0 k are small enough. In Appendix B we give smoothness conditions
that are sufficient for LR to imply equations (2.7) and (2.8). When formulating regularity
conditions for particular moment functions and first step estimators it may be more convenient
to work directly with equation (2.7) and/or (2.8).
The approach of constructing LR moment functions by adding the influence adjustment
differs from the model based approach of using an efficient influence function or score for a
semiparametric model as moment functions . The approach here is estimator based rather than
model based. The influence adjustment Ï†(z, Î², Î³, Î») is determined by the limit Î³(F ) of the first
step estimator Î³Ì‚ and the moment functions m(z, Î², Î³) rather than by some underlying semiparametric model. This estimator based approach has proven useful for deriving the influence
8

function of a wide variety of semiparametric estimators, as mentioned in the Introduction. Here
this estimator based approach provides a general way to construct LR moment functions. For
any moment function m(z, Î², Î³) and first step estimator Î³Ì‚ a corresponding LR estimator can be
constructed as in equations (2.3) and (2.4).
The addition of Ï†(z, Î², Î³, Î») does not affect identification of Î² because Ï†(z, Î², Î³0, Î»0 ) has
expectation zero for any Î² and true F0 . Consequently, the LR GMM estimator will have the
âˆš
same asymptotic variance as the original GMM estimator Î²Ìƒ when n(Î²Ìƒ âˆ’ Î²0 ) is asymptotically
normal, under appropriate regularity conditions. The addition of Ï†(z, Î², Î³, Î») will change other
properties of the estimator. As discussed in Chernozhukov et al. (2017, 2018), it can even
remove enough bias so that the LR estimator is root-n consistent and the original estimator is
not.
If FÏ„ was modified so that Ï„ is a function of a smoothing parameter, e.g. a bandwidth,
and Ï„ gives the magnitude of the smoothing bias of Î³(FÏ„ ), then equation (2.5) is a small bias
condition, equivalent to
E[Ïˆ(zi , Î²0 , Î³(FÏ„ ), Î»(FÏ„ ))] = o(Ï„ ).
Here E[Ïˆ(zi , Î²0 , Î³(FÏ„ ), Î»(FÏ„ ))] is a bias in the moment condition resulting from smoothing that
shrinks faster than Ï„. In this sense LR GMM estimators have the small bias property considered
in NHR. This interpretation is also one sense in which LR GMM is â€debiased.â€
In some cases the original moment functions m(z, Î², Î³) are already LR and the influence
adjustment will be zero. An important class of moment functions that are LR are those where
m(z, Î², Î³) is the derivative with respect to Î² of an objective function where nonparametric
parts have been concentrated out. That is, suppose that there is a function q(z, Î², Î¶) such that
m(z, Î², Î³) = âˆ‚q(z, Î², Î¶(Î²))/âˆ‚Î² where Î¶(Î²) = arg maxÎ¶ E[q(zi , Î², Î¶)], where Î³ includes Î¶(Î²) and
possibly additional functions. Proposition 2 of Newey (1994a) and Lemma 2.5 of Chernozhukov
et al. (2018) then imply that m(z, Î², Î³) will be LR. This class of moment functions includes
various partially linear regression models where Î¶ represents a conditional expectation. It also
includes the efficient score for a semiparametric model, Newey (1994a, pp. 1358-1359).
Cross fitting, also known as sample splitting, has often been used to improve the properties
of semiparametric and machine learning estimators; e.g. see Bickel (1982), Schick (1986), and
Powell, Stock, and Stoker (1989). Cross fitting removes a source of bias and can be used to
construct estimators with remainder terms that converge to zero as fast as is known to be
possible, as in NHR and Newey and Robins (2017). Cross fitting is also useful for double
machine learning estimators, as outlined in Chernozhukov et al. (2017, 2018). For these reasons
we allow for cross-fitting, where sample moments have the form
n

1X
Ïˆ(zi , Î², Î³Ì‚i , Î»Ì‚i ),
ÏˆÌ‚(Î²) =
n i=1
9

with Î³Ì‚i and Î»Ì‚i being formed from observations other than the ith . This kind of cross fitting
removes an â€own observationâ€ bias term and is useful for showing root-n consistency when Î³Ì‚i
and Î»Ì‚i are machine learning estimators.
One version of cross-fitting with good properties in examples in Chernozhukov et al. (2018)
can be obtained by partitioning the observation indices into L groups Iâ„“ , (â„“ = 1, ..., L), forming
Î³Ì‚â„“ and Î»Ì‚â„“ from observations not in Iâ„“ , and constructing
L

1 XX
ÏˆÌ‚(Î²) =
Ïˆ(zi , Î², Î³Ì‚â„“ , Î»Ì‚â„“ ).
n â„“=1 iâˆˆI

(2.9)

â„“

Further bias reductions may be obtained in some cases by using different sets of observations for
computing Î³Ì‚â„“ and Î»Ì‚â„“ , leading to remainders that converge to zero as rapidly as known possible
in interesting cases; see Newey and Robins (2017). The asymptotic theory of Section 7 focuses
on this kind of cross fitting.
As an example we consider a bound on average equivalent variation. Let Î³0 (x) denote the
conditional expectation of quantity q conditional on x = (pT , y) where p = (p1 , pT2 )T is a vector
of prices and y is income. The object of interest is a bound on average equivalent variation for
a price change from pÌ„1 to pÌŒ1 given by
Z
Î²0 = E[ â„“(p1 , yi )Î³0 (p1 , p2i , yi )dp1 ], â„“(p1 , y) = w(y)1(pÌ„1 â‰¤ p1 â‰¤ pÌŒ1 ) exp{âˆ’B(p1 âˆ’ pÌ„1 )}],
where w(y) is a function of income and B a constant. It follows by Hausman and Newey
(2016) that if B is a lower (upper) bound on the income effect for all individuals then Î²0 is
an upper (lower) bound on the equivalent variation for a price change from pÌ„1 to pÌŒ1 , averaged
over heterogeneity, other prices p2i , and income yi . The function w(y) allows for averages over
income in specific ranges, as in Hausman and Newey (2017).
A moment function that could be used to estimate Î²0 is
Z
m(z, Î², Î³) = â„“(p1 , y)Î³(p1, p2 , y)dp1 âˆ’ Î².
Note that
Z
E[m(zi , Î²0 , Î³)] + Î²0 = E[ â„“(p1 , yi )Î³(p1 , p2i , yi )dp1 ] = E[Î»0 (xi )Î³(xi )], Î»0 (x) =

â„“(p1 , y)
,
f0 (p1 |p2 , y)

where f0 (p1 |p2 , y) is the conditional pdf of p1i given p2i and yi . Then by Proposition 4 of Newey
(1994) the influence function adjustment for any nonparametric estimator Î³Ì‚(x) of E[qi |xi = x]
is
Ï†(z, Î², Î³, Î») = Î»(x)[q âˆ’ Î³(x)].
Here Î»0 (x) is an example of an additional unknown function that is included in Ï†(z, Î², Î³, Î») but
not in the original moment functions m(z, Î², Î³). Let Î³Ì‚i (x) be an estimator of E[qi |xi = x] that
10

can depend on i and Î»Ì‚i (x) be an estimator of Î»0 (x), such as fË†i (p1 |p2 , y)âˆ’1â„“(p1 , y) for an estimator
fË†i (p1 |p2 , y). The LR estimator obtained by solving ÏˆÌ‚(Î²) = 0 for m(z, Î², Î³) and Ï†(z, Î², Î³, Î») as
above is

n Z
1X
â„“(p1 , yi )Î³Ì‚i (p1 , p2i , yi )dp1 + Î»Ì‚i (xi )[qi âˆ’ Î³Ì‚i (xi )] .
(2.10)
Î²Ì‚ =
n i=1

3

Machine Learning for Dynamic Discrete Choice

A challenging problem when estimating dynamic structural models is the dimensionality of
state spaces. Machine learning addresses this problem via model selection to estimate high
dimensional choice probabilities. These choice probabilities estimators can then be used in
conditional choice probability (CCP) estimators of structural parameters, following Hotz and
Miller (1993). In order for CCP estimators based on machine learning to be root-n consistent
they must be based on orthogonal (i.e. LR) moment conditions, see Chernozhukov et al. (2017,
2018). Adding the adjustment term provides the way to construct LR moment conditions from
known moment conditions for CCP estimators. In this Section we do so for the Rustâ€™s (1987)
model of dynamic discrete choice.
We consider an agent choosing among J discrete alternatives by maximizing the expected
present discounted value of utility. We assume that the per-period utility function for an agent
making choice j in period t is given by
Ujt = uj (xt , Î²0 ) + Ç«jt , (j = 1, ..., J; t = 1, 2, ...).
The vector xt is the observed state variables of the problem (e.g. work experience, number of
children, wealth) and the vector Î² is unknown parameters. The disturbances Ç«t = {Ç«1t , ..., Ç«Jt }
are not observed by the econometrician. As in much of the literature we assume that Ç«t is i.i.d.
over time with known CDF that has support RJ , is independent of xt , and xt is first-order
Markov.
To describe the agentâ€™s choice probabilities let Î´ denote a time discount parameter, vÌ„(x)
the expected value function, yjt âˆˆ {0, 1} the indicator that choice j is made and vÌ„j (xt ) =
uj (xt , Î²0 ) + Î´E[vÌ„(xt+1 )|xt , j] the expected value function conditional on choice j. As in Rust
(1987), we assume that in each period the agent makes the choice j that maximizes the expected
present discounted value of utility vÌ„j (xt ) + Ç«jt . The probability of choosing j in period t is then
Pj (vÌ„t ) = Pr(vÌ„j (xt ) + Ç«jt â‰¥ vÌ„k (xt ) + Ç«kt ; k = 1, ..., J), vÌ„t = (vÌ„1 (xt ), ..., vÌ„J (xt ))â€² .

(3.1)

These choice probabilities have a useful relationship to the structural parameters Î² when
there is a renewal choice, where the conditional distribution of xt+1 given the renewal choice and
xt does not depend on xt . Without loss of generality suppose that the renewal choice is j = 1.
11

Let vÌƒjt denote vÌƒj (xt ) = vÌ„j (xt ) âˆ’ vÌ„1 (xt ), so that vÌƒ1t â‰¡ 0. As usual, subtracting vÌ„1t from each vÌ„jt
in Pj (vÌ„t ) does not change the choice probabilities, so that they depend only on vÌƒt = (vÌƒ2t , ..., vÌƒJt ).
The renewal nature of j = 1 leads to a specific formula for vÌƒjt in terms of the per period
utilities ujt = uj (xt , Î²0 ) and the choice probabilities Pt = P (vÌƒt ) = (P1 (vÌ„t ), ...PJ (vÌ„t ))â€² . As in Hotz
and Miller (1993), there is a function P âˆ’1 (P ) such that vÌƒt = P âˆ’1 (Pt ). Let H(P ) denote the
function such that
H(Pt ) = E[ max {P âˆ’1 (Pt )j + Ç«jt }|xt ] = E[ max {vÌƒjt + Ç«jt }|xt ].
1â‰¤jâ‰¤J

1â‰¤jâ‰¤J

For example, for multinomial logit H(Pt) = .5772 âˆ’ ln(P1t ). Note that by j = 1 being a renewal
we have E[vÌ„t+1 |xt , 1] = C for a constant C, so that
vÌ„(xt ) = vÌ„1t + H(Pt ) = u1t + Î´C + H(Pt ).
It then follows that
vÌ„jt = ujt + Î´E[vÌ„(xt+1 )|xt , j] = ujt + Î´E[u1,t+1 + H(Pt+1 )|xt , j] + Î´ 2 C, (j = 1, ..., J).
Subtracting then gives
vÌƒjt = ujt âˆ’ u1t + Î´{E[u1,t+1 + H(Pt+1 )|xt , j] âˆ’ E[u1,t+1 + H(Pt+1 )|1]}.

(3.2)

This expression for the choice specific value function vÌƒjt depends only on uj (xt , Î²), H(Pt+1 ), and
conditional expectations given the state and choice, and so can be used to form semiparametric
moment functions.
To describe those moment functions let Î³1 (x) denote the vector of possible values of the
choice probabilities E[yt |xt = x], where yt = (y1t , ..., yJt )â€² . Also let Î³j (xt , Î², Î³1), (j = 2, ..., J)
denote a possible E[u1 (xt+1 , Î²) + H(Î³1(xt+1 ))|xt , j] as a function of Î², xt and Î³1 , and Î³J+1 (Î², Î³1)
a possible value of E[u1 (xt , Î²) + H(Î³1(xt+1 ))|1]. Then a possible value of vÌƒjt is given by
vÌƒj (xt , Î², Î³) = uj (xt , Î²) âˆ’ u1 (xt , Î²) + Î´[Î³j (xt , Î², Î³1) âˆ’ Î³J+1 (Î², Î³1)], (j = 2, ..., J).
These value function differences are semiparametric, depending on the function Î³1 of choice probabilities and the conditional expectations Î³j , (j = 2, ..., J). Let vÌƒ(xt , Î², Î³) = (vÌƒ2 (xt , Î², Î³), ..., vÌƒJ (xt , Î², Î³))â€²
and A(xt ) denote a matrix of functions of xt with J columns. Semiparametric moment functions
are given by
m(z, Î², Î³) = A(x)[y âˆ’ P (vÌƒ(x, Î², Î³))].
LR moment functions can be constructed by adding the adjustment term for the presence
of the first step Î³. This adjustment term is derived in Appendix A. It takes the form
Ï†(z, Î², Î³, Î») =

J+1
X
j=1

12

Ï†j (z, Î², Î³, Î»),

where Ï†j (z, Î², Î³, Î») is the adjustment term for Î³j holding all other components Î³ fixed at their
true values. To describe it define
PvÌƒj (vÌƒ) = âˆ‚P (vÌƒ)/âˆ‚vÌƒj , Ï€1 = Pr(yt1 = 1), Î»10 (x) = E[y1t |xt+1 = x],
ytj
Î»j0(x) = E[A(xt )PvÌƒj (vÌƒt )
|xt+1 = x], (j = 2, ..., J).
Pj (vÌƒt )

(3.3)

Then for wt = xt+1 and z = (y, x, w) let
Ï†1 (z, Î², Î³, Î») = âˆ’Î´

J
X
j=2

{Î»j (x) âˆ’ E[A(xt )PvÌƒj (vÌƒt )]Ï€1âˆ’1 Î»1 (x)} [âˆ‚H(Î³1 (x))/âˆ‚P ]â€² {y âˆ’ Î³1 (x)}

Ï†j (z, Î², Î³, Î») = âˆ’Î´A(x)PvÌƒj (vÌƒ(x, Î², Î³))
Ï†J+1 (z, Î², Î³, Î») = Î´

J
X
j=2

!

yj
{u1 (w, Î²) + H(Î³1(w)) âˆ’ Î³j (x, Î², Î³1)}, (j = 2, ..., J),
Pj (vÌƒ(x, Î², Î³))
!

E[A(xt )PvÌƒj (vÌƒ(xt , Î², Î³))] Ï€1âˆ’1 y1 {u1 (w, Î²) + H(Î³1 (w)) âˆ’ Î³J+1 (Î², Î³1)}.

Theorem 2: If the marginal distribution of xt does not vary with t then LR moment
functions for the dynamic discrete choice model are
Ïˆ(z, Î², Î³) = A(xt )[yt âˆ’ P (vÌƒ(xt , Î², Î³))] +

J+1
X

Ï†j (z, Î², Î»).

j=1

The form of Ïˆ(z, Î², Î³) is amenable to machine learning. A machine learning estimator of the
conditional choice probability vector Î³10 (x) is straightforward to compute and can then be used
throughout the construction of the orthogonal moment conditions everywhere Î³1 appears. If
u1 (x, Î²) is linear in x, say u1 (x, Î²) = xâ€²1 Î²1 for subvectors x1 and Î²1 of x and Î² respectively, then
machine learning estimators can be used to obtain EÌ‚[x1,t+1 |xt , j] and EÌ‚[HÌ‚t+1 |xj , j], (j = 2, ..., J),
and a sample average used to form Î³Ì‚J+1 (Î², Î³Ì‚1). The value function differences can then be
estimated as
vÌƒj (xt , Î², Î³Ì‚) = uj (xt , Î²) âˆ’ u1(xt , Î²) + EÌ‚[x1,t+1 |xt , j]â€² Î²1 âˆ’ EÌ‚[x1,t+1 |1]â€²Î²1 + EÌ‚[HÌ‚t+1 |xt , j] âˆ’ EÌ‚[HÌ‚t+1 |1].
Furthermore, denominator problems can be avoided by using structural probabilities (rather
than the machine learning estimators) in all denominator terms.
The challenging part of the machine learning for this estimator is the dependence on Î² of the
reverse conditional expectations in Î»1 (x). It may be computationally prohibitive and possibly
unstable to redo machine learning for each Î². One way to to deal with this complication is to
update Î² periodically, with more frequent updates near convergence. It is important that at
convergence the Î² in the reverse conditional expectations is the same as the Î² that appears
elsewhere.
13

With data zi that is i.i.d. over individuals these moment functions can be used for any t to
estimate the structural parameters Î². Also, for data for a single individual we could use a time
P âˆ’1
average Tt=1
Ïˆ(zt , Î², Î³)/(T âˆ’ 1) to estimate Î². It will be just as important to use LR moments
for estimation with a single individual as it is with a cross section of individuals, although our
asymptotic theory will not apply to that case.
Bajari, Chernozhukov, Hong, and Nekipelov (2009) derived the influence adjustment for
dynamic discrete games of imperfect information. Locally robust moment conditions for such
games could be formed using their results. We leave that formulation to future work.
As an example of the finite sample performance of the LR GMM we report a Monte Carlo
study of the LR estimator of this Section. The design of the experiment is loosely like the
bus replacement application of Rust (1987). Here xt is a state variable meant to represent the
lifetime of a bus engine. The transition density is
(
xt + N(.25, 1)2 , yt = 1,
.
xt+1 =
xt = 1 + N(.25, 1)2 , yt = 0.
where yt = 0 corresponds to replacement of the bus engine and yt = 1 to nonreplacement. We
assume that the agent chooses yt contingent on state to maximize
âˆ
X
t=1

âˆš
Î´ tâˆ’1 [yt (Î± xt + Îµt ) + (1 âˆ’ yt )RC], Î± = âˆ’.3, RC = âˆ’4.

The unconditional probability of replacement in this model is about 1/8, which is substantially
higher than that estimated in Rust (1987). The sample used for estimation was 1000 observations
for a single decision maker. We carried out 10, 000 replications.
We estimate the conditional choice probabilities by kernel and series nonparametric regression
and by logit lasso, random forest, and boosted tree machine learning methods. Logit conditional
choice probabilities and derivatives were used in the construction of Î»Ì‚j wherever they appear
in order to avoid denominator issues. The unknown conditional expectations in the Î»Ì‚j were
estimated by series regressions throughout. Kernel regression was also tried but did not work
particularly well and so results are not reported.
Table 1 reports the results of the experiment. Bias, standard deviation, and coverage probability for asymptotic 95 percent confidence intervals are given in Table 1.
Table 1

14

LR CCP Estimators, Dynamic Discrete Choice
Bias
Std Dev
95% Cov
Î± RC
Î± RC
Î± RC
Two step kernel -.24 .08 .08 .32 .01 .86
LR kernel
-.05 .02 .06 .32 .95 .92
Two step quad -.00 .14 .049 .33âˆ— .91 .89
LR quad
-.00 .01 .085 .39 .95 .92
Logit Lasso
-.12 .25 .06 .28 .74 .84
LR Logit Lasso -.09 .01 .08 .36 .93 .95
Random Forest -.15 -.44 .09 .50 .91 .98
LR Ran. For.
.00
.00 .06 .44 1.0 .98
Boosted Trees
-.10 -.28 .08 .50 .99 .99
LR Boost Tr.
.03
.09 .07 .47 .99 .97
Here we find bias reduction from the LR estimator in all cases. We also find variance
reduction from LR estimation when the first step is kernel estimation, random forests, and
boosted trees. The LR estimator also leads to actual coverage of confidence intervals being
closer to the nominal coverage. The results for random forests and boosted trees seem noisier
than the others, with higher standard deviations and confidence interval coverage probabilities
farther from nominal. Overall, we find substantial improvements from using LR moments rather
than only the identifying, original moments.

4

Estimating the Influence Adjustment

Construction of LR moment functions requires an estimator Ï†Ì‚(z, Î²) of the adjustment term.
The form of Ï†(z, Î², Î³, Î») is known for some cases from the semiparametric estimation literature.
Powell, Stock, and Stoker (1989) derived the adjustment term for density weighted average
derivatives. Newey (1994a) gave the adjustment term for mean square projections (including
conditional expectations), densities, and their derivatives. Hahn (1998) and Hirano, Imbens,
and Ridder (2003) used those results to obtain the adjustment term for treatment effect estimators, where the LR estimator will be the doubly robust estimator of Robins, Rotnitzky, and
Zhao (1994, 1995). Bajari, Hong, Krainer, and Nekipelov (2010) and Bajari, Chernozhukov,
Hong, and Nekipelov (2009) derived adjustment terms in some game models. Hahn and Ridder
(2013, 2016) derived adjustments in models with generated regressors including control functions. These prior results can be used to obtain LR estimators by adding the adjustment term
with nonparametric estimators plugged in.
For new cases it may be necessary to derive the form of the adjustment term. Also, it is
possible to numerically estimate the adjustment term based on series estimators and other non15

parametric estimators. In this Section we describe how to construct estimators of the adjustment
term in these ways.

4.1

Deriving the Formula for the Adjustment Term

One approach to estimating the adjustment term is to derive a formula for Ï†(z, Î², Î³, Î») and
then plug in Î³Ì‚ and Î»Ì‚ in that formula. A formula for Ï†(z, Î², Î³, Î») can be obtained as in Newey
(1994a). Let Î³(F ) be the limit of the nonparametric estimator Î³Ì‚ when zi has distribution F.
Also, let FÏ„ denote a regular parametric model of distributions with FÏ„ = F0 at Ï„ = 0 and
score (derivative of the log likelihood at Ï„ = 0) equal to S(z). Then under certain regularity
conditions Ï†(z, Î², Î³0, Î»0 ) will be the unique solution to
R
âˆ‚ m(z, Î², Î³(FÏ„ ))F0 (dz)
= E[Ï†(zi , Î², Î³0, Î»0 )S(zi )], E[Ï†(zi , Î², Î³0, Î»0 )] = 0,
(4.1)
âˆ‚Ï„
Ï„ =0
as {FÏ„ } and the corresponding score S(z) are allowed to vary over a family of parametric
models where the set of scores for the family has mean square closure that includes all mean
zero functions with finite variance. Equation (4.1) is a functional equation that can be solved to
find the adjustment term, as was done in many of the papers cited in the previous paragraph.
The influence adjustment can be calculated by taking a limit of the Gateaux derivative as
shown in Ichimura and Newey (2017). Let Î³(F ) be the limit of Î³Ì‚ when F is the true distribution
of zi , as before. Let Ghz be a family of distributions that approaches a point mass at z as h âˆ’â†’ 0.
If Ï†(zi , Î², Î³0, Î»0 ) is continuous in zi with probability one then


âˆ‚E[m(zi , Î², Î³(FÏ„h ))]
, FÏ„h = (1 âˆ’ Ï„ )F0 + Ï„ Ghz .
(4.2)
Ï†(z, Î², Î³0 , Î»0 ) = lim
hâˆ’â†’0
âˆ‚Ï„
Ï„ =0
This calculation is more constructive than equation (4.1) in the sense that the adjustment term
here is a limit of a derivative rather than the solution to a functional equation. In Sections 5
and 6 we use those results to construct LR estimators when the first step is a nonparametric
instrumental variables (NPIV) estimator.
With a formula for Ï†(z, Î², Î³, Î») in hand from either solving the functional equation in equation (4.1) or from calculating the limit of the derivative in equation (4.2), one can estimate the
adjustment term by plugging estimators Î³Ì‚ and Î»Ì‚ into Ï†(z, Î², Î³, Î»). This approach to estimating
LR moments can used to construct LR moments for the average surplus described near the end
of Section 2. There the adjustment term depends on the conditional density of p1i given p2i and
yi . Let fË†â„“ (p1 |p2 , y) be some estimator of the conditional pdf of p1i given p2i and yi . Plugging
1 ,y)
.This Î»Ì‚â„“ (x) can then be used in
that estimator into the formula for Î»0 (x) gives Î»Ì‚â„“ (x) = fË† â„“(p
(p
|p2 ,y)
1
â„“
equation (2.10).

16

4.2

Estimating the Influence Adjustment for First Step Series Estimators

Estimating the adjustment term is relatively straightforward when the first step is a series
estimator. The adjustment term can be estimated by treating the first step estimator as if it
were parametric and applying a standard formula for the adjustment term for parametric twostep estimators. Suppose that Î³Ì‚â„“ depends on the data through a K Ã— 1 vector Î¶Ì‚â„“ of parameter
estimators that has true value Î¶0 . Let m(z, Î², Î¶) denote m(z, Î², Î³) as a function of Î¶. Suppose
that there is a K Ã— 1 vector of functions h(z, Î¶) such that Î¶Ì‚â„“ satisfies
1 X
âˆš
h(zi , Î¶Ì‚â„“ ) = op (1),
nÌ„â„“ Â¯

(4.3)

iâˆˆIâ„“

where IÂ¯â„“ is a subset of observations, none which are included in Iâ„“ , and nÌ„â„“ is the number of
observations in IÂ¯â„“ . Then a standard calculation for parametric two-step estimators (e.g. Newey,
1984, and Murphy and Topel, 1985) gives the parametric adjustment term

Ï†(zi , Î², Î¶Ì‚â„“, Î¨Ì‚â„“ ) = Î¨Ì‚â„“ (Î²)h(zi , Î¶Ì‚â„“ ), Î¨Ì‚â„“ (Î²) = âˆ’

X âˆ‚m(zj , Î², Î¶Ì‚â„“)
jâˆˆIÂ¯â„“

âˆ‚Î¶

ï£«
ï£¶âˆ’1
X âˆ‚h(zj , Î¶Ì‚â„“ )
ï£­
ï£¸ , i âˆˆ Iâ„“ .
âˆ‚Î¶
Â¯
jâˆˆIâ„“

In many cases Ï†(zi , Î², Î¶Ì‚â„“ , Î¨Ì‚â„“ ) approximates the true adjustment term Ï†(z, Î², Î³0, Î»0 ), as shown
by Newey (1994a, 1997) and Ackerberg, Chen, and Hahn (2012) for estimating the asymptotic
variance of functions of series estimators. Here this approximation is used for estimation of Î²
instead of just for variance estimation. The estimated LR moment function will be
Ïˆ(zi , Î², Î¶Ì‚â„“ , Î¨Ì‚â„“ ) = m(zi , Î², Î¶Ì‚â„“ ) + Ï†(zi , Î², Î¶Ì‚â„“ , Î¨Ì‚â„“ ).

(4.4)

We note that if Î¶Ì‚â„“ were computed from the whole sample then Ï†Ì‚(Î²) = 0. This degeneracy does
not occur when cross-fitting is used, which removes â€own observationâ€ bias and is important for
first step machine learning estimators, as noted in Section 2.
We can apply this approach to construct LR moment functions for an estimator of the
average surplus bound example that is based on series regression. Here the first step estimator
of Î³0 (x) = E[qi |xi = x] will be that from an ordinary least regression of qi on a vector a(xi ) of
approximating functions. The corresponding m(z, Î², Î¶) and h(z, Î¶) are
Z
â€²
â€²
m(z, Î², Î¶) = A(x) Î¶ âˆ’ Î², h(z, Î¶) = a(x)[q âˆ’ a(x) Î¶], A(x) = â„“(p1 , y)a(p1, p2 , y)dp1.
Let Î¶Ì‚â„“ denote the least squares coefficients from regressing qi on a(xi ) for observations that are

17

not included in Iâ„“ . Then the estimator of the locally robust moments given in equation (4.4) is
Ïˆ(zi , Î², Î¶Ì‚â„“, Î¨Ì‚â„“ ) = A(xi )â€² Î¶Ì‚â„“ âˆ’ Î² + Î¨Ì‚â„“ a(xi )[qi âˆ’ a(xi )â€² Î¶Ì‚â„“ ],
ï£¶âˆ’1
ï£«
X
X
a(xj )a(xj )â€² ï£¸ .
A(xj )â€² ï£­
Î¨Ì‚â„“ =
jâˆˆIÂ¯â„“

jâˆˆIÂ¯â„“

It can be shown similarly to Newey (1994a, p. 1369) that Î¨Ì‚â„“ estimates the population least
squares coefficients from a regression of Î»0 (xi ) on a(xi ), so that Î»Ì‚â„“ (xi ) = Î¨Ì‚â„“ a(xi ) estimates
Î»0 (xi ). In comparison the LR estimator described in the previous subsection was based on an
explicit nonparametric estimator of f0 (p1 |p2 , y), while this Î»Ì‚â„“ (x) implicitly estimates the inverse
of that pdf via a mean-square approximation of Î»0 (xi ) by Î¨Ì‚â„“ a(xi ).
Chernozhukov, Newey, and Robins (2018) introduce machine learning methods for choosing
the functions to include in the vector A(x). This method can be combined with machine
learning methods for estimating E[qi |xi ] to construct a double machine learning estimator of
average surplus, as shown in Chernozhukov, Hausman, and Newey (2018).
In parametric models moment functions like those in equation (4.4) are used to â€partial
outâ€ nuisance parameters Î¶. For maximum likelihood these moment functions are the basis
of Neymanâ€™s (1959) C-alpha test. Wooldridge (1991) generalized such moment conditions to
nonlinear least squares and Lee (2005), Bera et al. (2010), and Chernozhukov et al. (2015) to
GMM. What is novel here is their use in the construction of semiparametric estimators and the
interpretation of the estimated LR moment functions Ïˆ(zi , Î², Î¶Ì‚â„“ , Î¨Ì‚â„“ ) as the sum of an original
moment function m(zi , Î², Î¶Ì‚â„“ ) and an influence adjustment Ï†(zi , Î², Î¶Ì‚â„“ , Î¨Ì‚â„“ ).

4.3

Estimating the Influence Adjustment with First Step Smoothing

The adjustment term can be estimated in a general way that allows for kernel density, locally
linear regression, and other kernel smoothing estimators for the first step. The idea is to differentiate with respect to the effect of the ith observation on sample moments. Newey (1994b) used
a special case of this approach to estimate the asymptotic variance of a functional of a kernel
based semiparametric or nonparametric estimator. Here we extend this method to a wider class
of first step estimators, such as locally linear regression, and apply it to estimate the adjustment
term for construction of LR moments.
We will describe this estimator for the case where Î³ is a vector of functions of a vector of
variables x. Let h(z, x, Î³) be a vector of functions of a data observation z, x, and a possible
P
realized value of Î³ (i.e. a vector of real numbers Î³). Also let hÌ‚â„“ (x, Î³) = jâˆˆIÂ¯â„“ h(zj , x, Î³)/nÌ„â„“
be a sample average over a set of observations IÂ¯â„“ not included in Iâ„“ , where nÌ„j is the number of
observations in IÂ¯j . We assume that the first step estimator Î³Ì‚â„“ (x) solves
0 = hÌ‚â„“ (x, Î³).
18

We suppress the dependence of h and Î³Ì‚ on a bandwidth. For example for a pdf Îº(u) a kernel
density estimator would correspond to h(zj , x, Î³) = Îº(x âˆ’ xj ) âˆ’ Î³ and a locally linear regression
would be Î³Ì‚1 (x) for
!
1
h(zj , x, Î³) = Îº(x âˆ’ xj )
[yj âˆ’ Î³1 âˆ’ (x âˆ’ xj )â€² Î³2 ].
x âˆ’ xj
To measure the effect of the ith observation on Î³Ì‚ let Î³Ì‚â„“iÎ¾ (x) be the solution to
0 = hÌ‚â„“ (x, Î³) + Î¾ Â· h(zi , x, Î³).
This Î³Ì‚â„“iÎ¾ (x) is the value of the function obtained from adding the contribution Î¾ Â· h(zi , x, Î³) of
the ith observation. An estimator of the adjustment term can be obtained by differentiating the
average of the original moment function with respect to Î¾ at Î¾ = 0. This procedure leads to an
estimated locally robust moment function given by
Ïˆ(zi , Î², Î³Ì‚â„“) = m(zi , Î², Î³Ì‚â„“ ) +

âˆ‚ 1 X
m(zj , Î², Î³Ì‚â„“iÎ¾ (Â·))
âˆ‚Î¾ nÌ„â„“ Â¯
jâˆˆIâ„“

.
Î¾=0

This estimator is a generalization of the influence function estimator for kernels in Newey
(1994b).

5

Double and Partial Robustness

The zero derivative condition in equation (2.5) is an appealing robustness property in and of
itself. A zero derivative means that the expected moment functions remain closer to zero than
Ï„ as Ï„ varies away from zero. This property can be interpreted as local insensitivity of the
moments to the value of Î³ being plugged in, with the moments remaining close to zero as Î³
varies away from its true value. Because it is difficult to get nonparametric functions exactly
right, especially in high dimensional settings, this property is an appealing one.
Such robustness considerations, well explained in Robins and Rotnitzky (2001), have motivated the development of doubly robust (DR) moment conditions. DR moment conditions have
expectation zero if one first stage component is incorrect. DR moment conditions allow two
chances for the moment conditions to hold, an appealing robustness feature. Also, DR moment
conditions have simpler conditions for asymptotic normality than general LR moment functions
as discussed in Section 7. Because many interesting LR moment conditions are also DR we
consider double robustness.
LR moments that are constructed by adding the adjustment term for first step estimation
provide candidates for DR moment functions. The derivative of the expected moments with
19

respect to each first step will be zero, a necessary condition for DR. The condition for moments
constructed in this way to be DR is the following:
Assumption 1: There are sets Î“ and Î› such that for all Î³ âˆˆ Î“ and Î» âˆˆ Î›
E[m(zi , Î²0 , Î³)] = âˆ’E[Ï†(zi , Î²0 , Î³, Î»0 )], E[Ï†(zi , Î²0 , Î³0 , Î»)] = 0.
This condition is just the definition of DR for the moment function Ïˆ(z, Î², Î³) = m(z, Î², Î³) +
Ï†(z, Î², Î³, Î»), pertaining to specific sets Î“ and Î›.
The construction of adding the adjustment term to an identifying or original moment function
leads to several novel classes of DR moment conditions. One such class has a first step that
satisfies a conditional moment restriction
E[yi âˆ’ Î³0 (wi )|xi ] = 0,

(5.1)

where wi is potentially endogenous and xi is a vector of instrumental variables. This condition
is the nonparametric instrumental variable (NPIV) restriction as in Newey and Powell (1989,
2003) and Newey (1991). A first step conditional expectation where Î³0 (xi ) = E[yi |xi ] is included
as special case with wi = xi . Ichimura and Newey (2017) showed that the adjustment term for
this step takes the form Ï†(z, Î³, Î») = Î»(x)[y âˆ’ Î³(w)] so m(z, Î², Î³) + Î»(x)[y âˆ’ Î³(x)] is a candidate
for a DR moment function. A sufficient condition for DR is:
Assumption 2: i) Equation (5.1) is satisfied; ii) Î› = {Î»(x) : E[Î»(xi )2 ] < âˆ} and Î“ =
{Î³(w) : E[Î³(wi )2 ] < âˆ}; iii) there is v(w) with E[v(wi )2 ] < âˆ such that E[m(zi , Î²0 , Î³)] =
E[v(wi ){Î³(wi ) âˆ’ Î³0 (wi )}] for all Î³ âˆˆ Î“; iv) there is Î»0 (x) such that v(wi ) = E[Î»0 (xi )|wi ]; and
v) E[yi2 ] < âˆ.
By the Riesz representation theorem condition iii) is necessary and sufficient for E[m(zi , Î²0 , Î³)]
to be a mean square continuous functional of Î³ with representer v(w). Condition iv) is an additional condition giving continuity in the reduced form difference E[Î³(wi ) âˆ’ Î³0 (wi )|xi ], as further
discussed in Ichimura and Newey (2017). Under this condition
E[m(zi , Î²0 , Î³)] = E[E[Î»0 (xi )|wi ]{Î³(wi ) âˆ’ Î³0 (wi )}] = E[Î»0 (xi ){Î³(wi ) âˆ’ Î³0 (wi )}]
= âˆ’E[Ï†(zi , Î³, Î»0 )], E[Ï†(zi , Î³0 , Î»)] = E[Î»(xi ){yi âˆ’ Î³0 (wi )}] = 0.

Thus Assumption 2 implies Assumption 1 so that we have
Theorem 3: If Assumption 2 is satisfied then m(z, Î², Î³) + Î»(x){y âˆ’ Î³(w)} is doubly robust.
There are many interesting, novel examples of DR moment conditions that are special cases
of Theorem 3. The average surplus bound is an example where yi = qi , wi = xi , xi is the observed
20

vector of prices and income, Î› = Î“ is the set of all measurable functions of xi with finite second
moment, and Î³0 (x) = E[yi |xi = x]. Let x1 denote p1 and x2 the vector of other prices and
income, so that x = (x1 , xâ€²2 )â€² . Also let f0 (x1 |x2 ) denote the conditional pdf of p1 given x2 and
R
â„“(x) = â„“(p1 , y) for income y. Let m(z, Î², Î³) = â„“(p1 , x2 )Î³(p1 , x2 )dp1 âˆ’ Î² as before. Multiplying
and dividing through by f0 (p1 |x2 ) gives, for all Î³, Î» âˆˆ Î“ and Î»0 (x) = f0 (x1 |x2 )âˆ’1 â„“(x),
Z
E[m(zi , Î²0 , Î³)] = E[ â„“(p1 , x2i )Î³(p1 , x2i )dp1 ]âˆ’Î²0 = E[E[Î»0 (xi )Î³(xi )|x2i ]]âˆ’Î²0 = E[Î»0 (xi ){Î³(xi )âˆ’Î³0 (xi )}].
Theorem 3 then implies that the LR moment function for average surplus m(z, Î², Î³) + Î»(x)[q âˆ’
Î³(x)] is DR. A corresponding DR estimator Î²Ì‚ is given in equation (2.10).
The surplus bound is an example of a parameter where Î²0 = E[g(zi , Î³0)] for some linear
functional g(z, Î³) of Î³ and for Î³0 satisfying the conditional moment restriction of equation (5.1).
R
For the surplus bound g(z, Î³) = â„“(p1 , x2 )Î³(p1 , x2 )dp1 . If Assumption 2 is satisfied then choosing
m(z, Î², Î³) = g(z, Î³) âˆ’ Î² a DR moment condition is g(z, Î³) âˆ’ Î² + Î»(x)[y âˆ’ Î³(w)]. A corresponding
DR estimator is
n
1X
{g(zi , Î³Ì‚i ) + Î»Ì‚i (xi )[yi âˆ’ Î³Ì‚i (wi )]},
(5.2)
Î²Ì‚ =
n i=1
where Î³Ì‚i (w) and Î»Ì‚i (x) are estimators of Î³0 (w) and Î»0 (x) respectively. An estimator Î³Ì‚i can be
constructed by nonparametric regression when wi = xi or NPIV in general. A series estimator
Î»Ì‚i (x) can be constructed similarly to the surplus bound example in Section 3.2. For wi = xi
Newey and Robins (2017) give such series estimators of Î»Ì‚i (x) and Chernozhukov, Newey, and
Robins (2018) show how to choose the approximating functions for Î»Ì‚i (xi ) by machine learning.
Simple and general conditions for root-n consistency and asymptotic normality of Î²Ì‚ that allow
for machine learning are given in Section 7.
Novel examples of the DR estimator in equation (5.2) wi = xi are given by Newey and
Robins (2017) and Chernozhukov, Newey, and Robins (2018). Also Appendix C provides a
generalization to Î³(w) and Î³(x) that satisfy orthogonality conditions more general than conditional moment restrictions and novel examples of those. A novel example with wi 6= xi is a
weighted average derivative of Î³0 (w) satisfying equation (5.1). Here g(z, Î³) = vÌ„(w)âˆ‚Î³(w)/âˆ‚w for
some weight function vÌ„(w). Let f0 (w) be the pdf of wi and v(w) = âˆ’f0 (w)âˆ’1 âˆ‚[vÌ„(w)f0(w)]/âˆ‚w,
assuming that derivatives exist. Assume that vÌ„(w)Î³(w)f0(w) is zero on the boundary of the
support of wi . Integration by parts then gives Assumption 2 iii). Assume also that there exists
Î»0 âˆˆ Î› with v(wi ) = E[Î»0 (xi )|wi ]. Then for estimators Î³Ì‚i and Î»Ì‚i a DR estimator of the weighted
average derivative is
n

âˆ‚Î³Ì‚i (wi )
1X
{vÌ„(wi )
+ Î»Ì‚i (xi )[yi âˆ’ Î³Ì‚i (wi )]}.
Î²Ì‚ =
n i=1
âˆ‚w
This is a DR version of the weighted average derivative estimator of Ai and Chen (2007). A
21

special case of this example is the DR moment condition for the weighted average derivative in
the exogenous case where wi = xi given in Firpo and Rothe (2017).
Theorem 3 includes existing DR moment functions as special cases where wi = xi , including
the mean with randomly missing data given by Robins and Rotnitzky (1995), the class of DR
estimators in Robins et al. (2008), and the DR estimators of Firpo and Rothe (2017). We
illustrate for the mean with missing data. Let w = x, x = (a, u) for an observed data indicator
a âˆˆ {0, 1} and covariates u, m(z, Î², Î³) = Î³(1, u) âˆ’ Î², and Î»0 (x) = a/ Pr(ai = 1|ui = u). Here it
is well known that
E[m(zi , Î²0 , Î³)] = E[Î³(1, ui )] âˆ’ Î²0 = E[Î»0 (xi ){Î³(xi ) âˆ’ Î³0 (xi )}] = âˆ’E[Î»0 (xi ){yi âˆ’ Î³(xi )}].
Then DR of the moment function Î³(1, w) âˆ’ Î² + Î»(x)[y âˆ’ Î³(x)] of Robins and Rotnitzky (1995)
follows by Proposition 5.
Another novel class of DR moment conditions are those where the first step Î³ is a pdf of a
function x of the data observation z. By Proposition 5 of Newey (1994a), the adjustment term
R
for such a first step is Ï†(z, Î², Î³, Î») = Î»(x) âˆ’ Î»(u)Î³(u)du for some possible Î». A sufficient
condition for the DR as in Assumption 1 is:
R
Assumption 3: xi has pdf Î³0 (x) and for Î“ = {Î³ : Î³(x) â‰¥ 0, Î³(x)dx = 1} there is Î»0 (x)
such that for all Î³ âˆˆ Î“,
Z
E[m(zi , Î²0 , Î³)] = Î»0 (x){Î³(x) âˆ’ Î³0 (x)}dx.
R
Note that for Ï†(z, Î³, Î») = Î»(x)âˆ’ Î»(xÌƒ)Î³(xÌƒ)dxÌƒ it follows from Assumption 3 that E[m(zi , Î²0 , Î³)] =
R
âˆ’E[Ï†(zi , Î³, Î»0 )] for all Î³ âˆˆ Î“. Also, E[Ï†(zi , Î³0 , Î»)] = E[Î»(xi )] âˆ’ Î»(xÌƒ)Î³0 (xÌƒ)dx = 0. Then Assumption 1 is satisfied so we have:
Theorem 4: If Assumption 3 is satisfied then m(z, Î², Î³) + Î»(x) âˆ’
The integrated squared density Î²0 =
Î»0 = Î³0 , and

R

R

Î»(xÌƒ)Î³(xÌƒ)dxÌƒ is DR.

Î³0 (x)2 dx is an example for m(z, Î², Î³) = Î³(x) âˆ’ Î²,

Ïˆ(z, Î², Î³, Î») = Î³(x) âˆ’ Î² + Î»(x) âˆ’

Z

Î»(xÌƒ)Î³(xÌƒ)dx.

This DR moment function seems to be novel. Another example is the density weighted average
derivative (DWAD) of Powell, Stock, and Stoker (1989), where m(z, Î², Î³) = âˆ’2y Â· âˆ‚Î³(x)/âˆ‚x âˆ’ Î².
Let Î´(xi ) = E[yi |xi ]Î³0 (xi ). Assuming that Î´(u)Î³(u) is zero on the boundary and differentiable,
integration by parts gives
Z
E[m(zi , Î²0 , Î³)] = âˆ’2E[yi âˆ‚Î³(xi )/âˆ‚x] âˆ’ Î²0 = [âˆ‚Î´(xÌƒ)/âˆ‚x]{Î³(xÌƒ) âˆ’ Î³0 (xÌƒ)}du,
22

so that Assumption 3 is satisfied with Î»0 (x) = âˆ‚Î´(x)/âˆ‚x. Then by Theorem 4
n

1X
âˆ‚Î³Ì‚i (xi ) âˆ‚ Î´Ì‚i (xi )
Î²Ì‚ =
{âˆ’2
+
âˆ’
n i=1
âˆ‚x
âˆ‚x

Z

âˆ‚ Î´Ì‚i (xÌƒ)
Î³Ì‚i (xÌƒ)dxÌƒ}
âˆ‚x

is a DR estimator. It was shown in NHR (1998) that the Powell, Stock, and Stoker (1989)
estimator with a twicing kernel is numerically equal to a leave one out version of this estimator
for the original (before twicing) kernel. Thus the DR result for Î²Ì‚ gives an interpretation of the
twicing kernel estimator as a DR estimator.
The expectation of the DR moment functions of both Theorem 3 and 4 are affine in Î³ and
Î» holding the other fixed at the truth. This property of DR moment functions is general, as we
show by the following characterization of DR moment functions:
Theorem 5: If Î“ and Î› are linear then Ïˆ(z, Î², Î³, Î») is DR if and only if
âˆ‚E[Ïˆ(zi , Î²0 , (1 âˆ’ Ï„ )Î³0 + Ï„ Î³, Î»0 )]|Ï„ =0 = 0, âˆ‚E[Ïˆ(zi , Î²0 , Î³0, (1 âˆ’ Ï„ )Î»0 + Ï„ Î»)]|Ï„ =0 = 0,
and E[Ïˆ(zi , Î²0 , Î³, Î»0 )] and E[Ïˆ(zi , Î²0 , Î³0 , Î»)] are affine in Î³ and Î» respectively.
The zero derivative condition of this result is a Gateaux derivative, componentwise version
of LR. Thus, we can focus a search for DR moment conditions on those that are LR. Also, a DR
moment function must have an expectation that is affine in each of Î³ and Î» while the other is
held fixed at the truth. It is sufficient for this condition that Ïˆ(zi , Î²0 , Î³, Î») be affine in each of
Î³ and Î» while the other is held fixed. This property can depend on how Î³ and Î» are specified.
For example the missing data DR moment function m(1, u) âˆ’ Î² + Ï€(u)âˆ’1 a[y âˆ’ Î³(x)] is not affine
in the propensity score Ï€(u) = Pr(ai = 1|ui = u) but is in Î»(x) = Ï€(u)âˆ’1 a.
In general Theorem 5 motivates the construction of DR moment functions by adding the
adjustment term to obtain a LR moment function that will then be DR if it is affine in Î³ and Î»
separately. It is interesting to note that in the NPIV setting of Theorem 3 and the density setting
of Theorem 4 that the adjustment term is always affine in Î³ and Î». It then follows from Theorem
5 that in those settings LR moment conditions are precisely those where E[m(zi , Î²0 , Î³)] is affine
in Î³. Robins and Rotnitzky (2001) gave conditions for the existence of DR moment conditions
in semiparametric models. Theorem 5 is complementary to those results in giving a complete
characterization of DR moments when Î“ and Î› are linear.
Assumptions 2 and 3 both specify that E[m(zi , Î²0 , Î³)] is continuous in an integrated squared
deviation norm. These continuity conditions are linked to finiteness of the semiparametric
variance bound for the functional E[m(zi , Î²0 , Î³)], as discussed in Newey and McFadden (1994)
for Assumption 2 with wi = xi and for Assumption 3. For Assumption 2 with wi 6= xi Severini
and Tripathi (2012) showed for m(z, Î², Î³) = v(w)Î³(w) âˆ’ Î² with known v(w) that the existence
of Î»0 (w) with v(wi ) = E[Î»0 (xi )|wi ] is necessary for the existence of a root-n consistent estimator
23

of Î². Thus the conditions of Assumption 2 are also linked to necessary conditions for root-n
consistent estimation when wi 6= xi .
Partial robustness refers to settings where E[m(zi , Î²0 , Î³Ì„)] = 0 for some Î³Ì„ 6= Î³0 . The novel DR
moment conditions given here lead to novel partial robustness results as we now demonstrate
in the conditional moment restriction setting of Assumption 2. When Î»0 (x) in Assumption 2 is
restricted in some way there may exist Î³Ìƒ 6= Î³0 with E[Î»0 (xi ){yi âˆ’ Î³Ìƒ(wi )}] = 0. Then
E[m(zi , Î²0 , Î³Ìƒ)] = âˆ’E[Î»0 (xi ){yi âˆ’ Î³Ìƒ(wi )}] = 0.
Consider the average derivative Î²0 = E[âˆ‚Î³0 (wi )/âˆ‚wr ] where m(z, Î², Î³) = âˆ‚Î³(w)/âˆ‚wr âˆ’ Î² for
some r. Let Î´ = (E[a(xi )p(wi )â€² ])âˆ’1 E[a(xi )yi ] be the limit of the linear IV estimator with right
hand side variables p(w) and the same number of instruments a(x). The following is a partial
robustness result that provides conditions for the average derivative of the linear IV estimator
to equal the true average derivative:
Theorem 6: If âˆ’âˆ‚ ln f0 (w)/âˆ‚wr = câ€² p(w) for a constant vector c, E[p(wi )p(wi )â€² ] is nonsingular, and E[a(xi )|wi = w] = Î p(w) for a square nonsingular Î  then for Î´ = (E[a(xi )p(wi )â€² ])âˆ’1 E[a(xi )yi ],
E[âˆ‚{p(wi )â€² Î´}/âˆ‚wr ] = E[âˆ‚Î³0 (wi )/âˆ‚wr ].
This result shows that if the density score is a linear combination of the right-hand side
variables p(w) used by linear IV, the conditional expectation of the instruments a(xi ) given wi
is a nonsingular linear combination of p(w), and p(w) has a nonsingular second moment matrix
then the average derivative of the linear IV estimator is the true average derivative. This is
a generalization to NPIV of Stokerâ€™s (1986) result that linear regression coefficients equal the
average derivatives when the regressors are multivariate Gaussian.
DR moment conditions can be used to identify parameters of interest. Under Assumption 1
Î²0 may be identified from
E[m(zi , Î²0 , Î³Ì„)] = âˆ’E[Ï†(zi , Î²0 , Î³Ì„, Î»0 )]
for any fixed Î³Ì„ when the solution Î²0 to this equation is unique.
Theorem 7: If Assumption 1 is satisfied, Î»0 is identified, and for some Î³Ì„ the equation
E[Ïˆ(zi , Î², Î³Ì„, Î»0 )] = 0 has a unique solution then Î²0 is identified as that solution.
Applying this result to the NPIV setting of Assumption 2 gives an explicit formula for certain
functionals of Î³0 (w) without requiring that the completeness identification condition of Newey
and Powell (1989, 2003) be satisfied, similarly to Santos (2011). Suppose that v(w) is identified,
e.g. as for the weighted average derivative. Since both w and x are observed it follows that
24

a solution Î»0 (x) to v(w) = E[Î»0 (x)|w] will be identified if such a solution exists. Plugging in
Î³Ì„ = 0 into the equation E[Ïˆ(zi , Î²0 , Î³Ì„, Î»0 )] = 0 gives
Corollary 8: If v(wi ) is identified and there exists Î»0 (xi ) such that v(wi ) = E[Î»0 (xi )|wi ]
then Î²0 = E[v(wi )Î³0 (wi )] is identified as Î²0 = E[Î»0 (xi )yi ].
Note that this result holds without the completeness condition. Identification of Î²0 =
E[v(wi )Î³0 (wi )] for known v(wi ) with v(wi ) = E[Î»0 (xi )|wi ] follows from Severini and Tripathi
(2006). Corollary 8 extends that analysis to the case where v(wi ) is only identified but not
necessarily known and links it to DR moment conditions. Santos (2011) gives a related formula
R
for a parameter Î²0 = vÌƒ(w)Î»0(w)dw. The formula here differs from Santos (2011) in being an
expectation rather than a Lebesgue integral. Santos (2011) constructed an estimator. That is
beyond the scope of this paper.

6

Conditional Moment Restrictions

Models of conditional moment restrictions that depend on unknown functions are important
in econometrics. In such models the nonparametric components may be determined simultaneously with the parametric components. In this setting it is useful to work directly with the
instrumental variables to obtain LR moment conditions rather than to make a first step influence adjustment. For that reason we focus in this Section on constructing LR moments by
orthogonalizing the instrumental variables.
Our orthogonal instruments framework is based on based on conditional moment restrictions
of the form
E[Ïj (zi , Î²0 , Î³0 )|xji ] = 0, (j = 1, ..., J),
(6.1)
where each Ïj (z, Î², Î³) is a scalar residual and xj are instruments that may differ across j. This
model is considered by Chamberlain (1992) and Ai and Chen (2003, 2007) when xj is the same
for each j and for Ai and Chen (2012) when the set of xj includes xjâˆ’1 . We allow the residual
vector Ï(z, Î², Î³) to depend on the entire function Î³ and not just on its value at some function
of the observed data zi .
In this framework we consider LR moment functions having the form
Ïˆ(z, Î², Î³, Î») = Î»(x)Ï(z, Î², Î³),

(6.2)

where Î»(x) = [Î»1 (x1 ), ..., Î»J (xJ )] is a matrix of instrumental variables with the j th column given
by Î»j (xj ). We will define orthogonal instruments to be those that make Ïˆ(z, Î², Î³, Î») locally
robust. To define orthogonal instrumental variables we assume that Î³ is allowed to vary over a
linear set Î“ as F varies. For each âˆ† âˆˆ Î“ let
âˆ‚E[ÏJ (zi , Î²0 , Î³0 + Ï„ âˆ†)|xJ ] â€²
âˆ‚E[Ï1 (zi , Î²0 , Î³0 + Ï„ âˆ†)|x1 ]
, ...,
).
ÏÌ„Î³ (x, âˆ†) = (
âˆ‚Ï„
âˆ‚Ï„
25

This ÏÌ„Î³ (x, âˆ†) is the Gateaux derivative with respect to Î³ of the conditional expectation of the
residuals in the direction âˆ†. We characterize Î»0 (x) as orthogonal if
E[Î»0 (xi )ÏÌ„Î³ (xi , âˆ†)] = 0 for all âˆ† âˆˆ Î“.
We assume that ÏÌ„Î³ (x, âˆ†) is linear in âˆ† and consider the Hilbert space of vectors of random
vectors a(x) = (a1 (x1 ), ..., aJ (xJ )) with inner product ha, bi = E[a(xi )â€² b(xi )]. Let Î›Ì„Î³ denote
the closure of the set {ÏÌ„Î³ (x, âˆ†) : âˆ† âˆˆ Î“} in that Hilbert space. Orthogonal instruments are
those where each row of Î»0 (x) is orthogonal to Î›Ì„Î³ . They can be interpreted as instrumental
variables where the effect of estimation of Î³ has been partialed out. When Î»0 (x) is orthogonal
then Ïˆ(z, Î², Î³, Î») = Î»(x)Ï(z, Î², Î³) is LR:
Theorem 9: If each row of Î»0 (x) is orthogonal to Î›Ì„Î³ then the moment functions in equation
(6.2) are LR.
We also have a DR result:
Theorem 10: If each row of Î»0 (x) is orthogonal to Î›Ì„Î³ and Ï(z, Î², Î³) is affine in Î³ âˆˆ Î“ then
the moment functions in equation (6.2) are DR for Î› = {Î»(x) : E[Î»(xi )â€² Ï(zi , Î²0 , Î³0 )â€² Ï(zi , Î²0 , Î³0 )Î»(xi )].
There are many ways to construct orthogonal instruments. For instance, given a r Ã— (J âˆ’ 1)
matrix of instrumental variables Î»(x) one could construct corresponding orthogonal ones Î»0 (xi )
as the matrix where each row of Î»(x) is replaced by the residual from the least squares projection
of the corresponding row of Î»(x) on Î›Ì„Î³ . For local identification of Î² we also require that
rank( âˆ‚E[Ïˆ(zi , Î², Î³0)]/âˆ‚Î²|Î²=Î²0 ) = dim(Î²).

(6.3)

A model where Î²0 is identified from semiparametric conditional moment restrictions with
common instrumental variables is a special case where xji is the same for each j. In this case
there is a way to construct orthogonal instruments that leads to an efficient estimator of Î²0 .
Let Î£(xi ) denote some positive definite matrix with its smallest eigenvalue bounded away from
zero, so that Î£(xi )âˆ’1 is bounded. Let ha, biÎ£ = E[a(xi )â€² Î£(xi )âˆ’1 b(xi )] denote an inner product
and note that Î›Ì„Î³ is closed in this inner product by Î£(xi )âˆ’1 bounded. Let Î»ÌƒÎ£
k (xi , Î») denote the
â€²
th
residual from the least squares projection of the k row Î» (x) ek of Î»(x) on Î›Ì„Î³ with the inner
product ha, biÎ£ . Then for all âˆ† âˆˆ Î“,
â€²
âˆ’1
E[Î»ÌƒÎ£
k (xi , Î») Î£(xi ) ÏÌ„Î³ (xi , âˆ†)] = 0,
Î£
Î£
âˆ’1
so that for Î»ÌƒÎ£ (xi , Î») = [Î»ÌƒÎ£
are
1 (xi , Î»), ..., Î»Ìƒr (xi , Î»)] the instrumental variables Î»Ìƒ (xi , Î»)Î£(xi )
Î£
orthogonal. Also, Î»Ìƒ (xi , Î») can be interpreted as the solution to

min

{D(x):D(x)â€² ek âˆˆÎ›Ì„Î³ ,k=1,...,r}

tr(E[{Î»(xi ) âˆ’ D(xi )}Î£(xi )âˆ’1 {Î»(xi ) âˆ’ D(xi )}â€² ])
26

where the minimization is in the positive semidefinite sense.
The orthogonal instruments that minimize the asymptotic variance of GMM in the class of
GMM estimators with orthogonal instruments are given by
âˆ—

Î»âˆ—0 (x) = Î»ÌƒÎ£ (x, Î»Î² )Î£âˆ— (x)âˆ’1 , Î»Î² (xi ) =

âˆ‚E[Ï(zi , Î², Î³0)|xi ]
âˆ‚Î²

â€²
Î²=Î²0

, Î£âˆ— (xi ) = V ar(Ïi |xi ), Ïi = Ï(zi , Î²0 , Î³0).

Theorem 11: The instruments Ï•âˆ— (xi ) give an efficient estimator in the class of IV estimators with orthogonal instruments.
The asymptotic variance of the GMM estimator with optimal orthogonal instruments is
âˆ’1
(E[mâˆ—i mâˆ—â€²
= E[Î»Ìƒ(xi , Î»âˆ— , Î£âˆ— )Î£âˆ— (xi )âˆ’1 Î»Ìƒ(xi , Î»âˆ— , Î£âˆ— )â€² ])âˆ’1 .
i ])

This matrix coincides with the semiparametric variance bound of Ai and Chen (2003). Estimation of the optimal orthogonal instruments is beyond the scope of this paper. The series
estimator of Ai and Chen (2003) could be used for this.
This framework includes moment restrictions with a NPIV first step Î³ satisfying E[Ï(zi , Î³0 )|xi ] =
0 where we can specify Ï1 (z, Î², Î³) = m(z, Î², Î³), x1i = 1, Ï2 (z, Î², Î³) = Ï(z, Î³), and x2i = xi . It
generalizes that setup by allowing for more residuals Ïj (z, Î², Î³), (j â‰¥ 3) and allowing all residuals
to depend on Î².

7

Asymptotic Theory

In this Section we give simple and general asymptotic theory for LR estimators that incorporates
the cross-fitting of equation (2.9). Throughout we use the structure of LR moment functions that
are the sum Ïˆ(z, Î², Î³, Î») = m(z, Î², Î³)+Ï†(z, Î², Î³, Î») of an identifying or original moment function
m(z, Î², Î³) depending on a first step function Î³ and an influence adjustment term Ï†(z, Î², Î³, Î»)
that can depend on an additional first step Î». The asymptotic theory will apply to any moment
function that can be decomposed into a function of a single nonparametric estimator and a
function of two nonparametric estimators. This structure and LR leads to particularly simple
and general conditions.
The conditions we give are composed of mean square consistency conditions for first steps
and one, two, or three rate conditions for quadratic remainders. We will only use one quadratic
âˆš
remainder rate for DR moment conditions, involving faster than 1/ n convergence of products
of estimation errors for Î³Ì‚ and Î»Ì‚. When E[m(zi , Î²0 , Î³) + Ï†(zi , Î²0 , Î³, Î»0 )] is not affine in Î³ we
will impose a second rate condition that involves faster than nâˆ’1/4 convergence of Î³Ì‚. When
E[Ï†(zi , Î³0 , Î»)] is also not affine in Î» we will impose a third rate condition that involves faster
than nâˆ’1/4 convergence of Î»Ì‚. Most adjustment terms Ï†(z, Î², Î³, Î») of which we are aware, including
27

for first step conditional moment restrictions and densities, have E[Ï†(zi , Î²0 , Î³0 , Î»)] affine in Î»,
so that faster nâˆ’1/4 convergence of Î»Ì‚ will not be required under our conditions. It will suffice
for most LR estimators which we know of to have faster than nâˆ’1/4 convergence of Î³Ì‚ and faster
âˆš
than 1/ n convergence of the product of estimation errors for Î³Ì‚ and Î»Ì‚, with only the latter
condition imposed for DR moment functions. We also impose some additional conditions for
convergence of the Jacobian of the moments and sample second moments that give asymptotic
normality and consistent asymptotic variance estimation for Î²Ì‚.
An important intermediate result for asymptotic normality is
n
âˆš
1 X
Ïˆ(zi , Î²0 , Î³0 , Î»0 ) + op (1),
(7.1)
nÏˆÌ‚(Î²0 ) = âˆš
n i=1
where ÏˆÌ‚(Î²) is the cross-fit, sample, LR moments of equation (2.9). This result will mean
that the presence of the first step estimators has no effect on the limiting distribution of the
moments at the true Î²0 . To formulate conditions for this result we decompose the difference
between the left and right-hand sides into several remainders. Let Ï†(z, Î³, Î») = Ï†(z, Î²0 , Î³, Î»),
Ï†Ì„(Î³, Î») = E[Ï†(zi , Î³, Î»)], and mÌ„(Î³) = E[m(zi , Î²0 , Î³)], so that ÏˆÌ„(Î³, Î») = mÌ„(Î³) + Ï†Ì„(Î³, Î») Then
adding and subtracting terms gives
n
X
âˆš
n[ÏˆÌ‚(Î²0 ) âˆ’
Ïˆ(zi , Î²0 , Î³0, Î»0 )/n] = RÌ‚1 + RÌ‚2 + RÌ‚3 + RÌ‚4 ,
(7.2)
i=1

where

n

1 X
RÌ‚1 = âˆš
[m(zi , Î²0 , Î³Ì‚i ) âˆ’ m(zi , Î²0 , Î³0 ) âˆ’ mÌ„(Î³Ì‚i )]
n i=1

(7.3)

n

1 X
+âˆš
[Ï†(zi , Î³Ì‚i , Î»0 ) âˆ’ Ï†(zi , Î³0 , Î»0 ) âˆ’ Ï†Ì„(Î³Ì‚i , Î»0 ) + Ï†(zi , Î³0 , Î»Ì‚i ) âˆ’ Ï†(zi , Î³0 , Î»0 ) âˆ’ Ï†Ì„(Î³0 , Î»Ì‚i )],
n i=1
n

1 X
RÌ‚2 = âˆš
[Ï†(zi , Î³Ì‚i , Î»Ì‚i ) âˆ’ Ï†(zi , Î³Ì‚i , Î»0 ) âˆ’ Ï†(zi , Î³0 , Î»Ì‚i ) + Ï†(zi , Î³0 , Î»0 )],
n i=1
n

1 X
RÌ‚3 = âˆš
ÏˆÌ„(Î³Ì‚i , Î»0 ),
n i=1

n

1 X
RÌ‚4 = âˆš
Ï†Ì„(Î³0 , Î»Ì‚i ),
n i=1

We specify regularity conditions sufficient for each of RÌ‚1 , RÌ‚2 , RÌ‚3 , and RÌ‚4 to converge in
probability to zero so that equation (7.1) will hold. The remainder term RÌ‚1 is a stochastic
equicontinuity term as in Andrews (1994). We give mean square consistency conditions for
p
RÌ‚1 âˆ’â†’ 0 in Assumption 3.
The remainder term RÌ‚2 is a second order remainder that involves both Î³Ì‚ and Î»Ì‚. When the
influence adjustment is Ï†(z, Î³, Î») = Î»(x)[y âˆ’ Î³(w)], as for conditional moment restrictions, then
n
âˆ’1 X
RÌ‚2 = âˆš
[Î»Ì‚i (xi ) âˆ’ Î»0 (xi )][Î³Ì‚i (wi ) âˆ’ Î³0 (wi )].
n i=1
28

RÌ‚2 will converge to zero when the product of convergence rates for Î»Ì‚i (xi ) and Î³Ì‚i (wi ) is faster
âˆš
than 1/ n. However, that is not the weakest possible condition. Weaker conditions for locally
linear regression first steps are given by Firpo and Rothe (2017) and for series regression first
steps by Newey and Robins (2017). These weaker conditions still require that the product of
âˆš
biases of Î»Ì‚i (xi ) and Î³Ì‚i (wi ) converge to zero faster than 1/ n but have weaker conditions for
p
variance terms. We allow for these weaker conditions by allowing RÌ‚2 âˆ’â†’ 0 as a regularity
condition. Assumption 5 gives these conditions.
p
p
We will have RÌ‚3 = RÌ‚4 = 0 in the DR case of Assumption 1, where RÌ‚1 âˆ’â†’ 0 and RÌ‚2 âˆ’â†’ 0
will suffice for equation (7.1). In non DR cases LR leads to ÏˆÌ„(Î³, Î»0 ) = mÌ„(Î³) + Ï†Ì„(Î³, Î»0 ) having a
p
zero functional derivative with respect to Î³ at Î³0 so that RÌ‚3 âˆ’â†’ 0 when Î³Ì‚i converges to Î³0 at a
rapid enough, feasible rate. For example if ÏˆÌ„(Î³, Î»0 ) is twice continuously Frechet differentiable
in a neighborhood of Î³0 for a norm kÂ·k , with zero Frechet derivative at Î³0 . Then
RÌ‚3 â‰¤ C
p

L
X
âˆš
â„“=1

p

n kÎ³Ì‚â„“ âˆ’ Î³0 k2 âˆ’â†’ 0

when kÎ³Ì‚ âˆ’ Î³0 k = op (nâˆ’1/4 ). Here RÌ‚3 âˆ’â†’ 0 when each Î³Ì‚â„“ converges to Î³0 more quickly than nâˆ’1/4 .
It may be possible to weaken this condition by bias correcting m(z, Î², Î³Ì‚), as by the bootstrap
in Cattaneo and Jansson (2017), by the jackknife in Cattaneo Ma and Jansson (2017), and by
cross-fitting in Newey and Robins (2017). Consideration of such bias corrections for m(z, Î², Î³Ì‚)
is beyond the scope of this paper.
In many cases RÌ‚4 = 0 even though the moment conditions are not DR. For example that is
true when Î³Ì‚ is a pdf or when Î³0 estimates the solution to a conditional moment restriction. In
p
such cases mean square consistency, RÌ‚2 âˆ’â†’ 0, and faster than nâˆ’1/4 consistency of Î³Ì‚ suffices
for equation (7.1); no convergence rate for Î»Ì‚ is needed. The simplification that RÌ‚4 = 0 seems
to be the result of Î» being a Riesz representer for the linear functional that is the derivative of
mÌ„(Î³) with respect to Î³. Such a Riesz representer will enter Ï†Ì„(Î», Î³0) linearly, leading to RÌ‚4 = 0.
p
When RÌ‚4 6= 0 then RÌ‚4 âˆ’â†’ 0 will follow from twice Frechet differentiability of Ï†Ì„(Î», Î³0 ) in Î» and
faster than nâˆ’1/4 convergence of Î»Ì‚.
All of the conditions can be easily checked for a wide variety of machine learning and conventional nonparametric estimators. There are well known conditions for mean square consistency
for many conventional and machine learning methods. Rates for products of estimation errors
are also know for many first step estimators as are conditions for nâˆ’1/4 consistency. Thus,
the simple conditions we give here are general enough to apply to a wide variety of first step
estimators.
p
The first formal assumption of this section is sufficient for RÌ‚1 âˆ’â†’ 0.
R

Assumption 4: For each â„“ = 1, ..., L, i) Either m(z, Î²0 , Î³) does not depend on z or
R
p
p
{m(z, Î²0 , Î³Ì‚â„“ ) âˆ’ m(z, Î²0 , Î³0 )}2 F0 (dz) âˆ’â†’ 0, ii) {Ï†(z, Î³Ì‚â„“ , Î»0 ) âˆ’ Ï†(z, Î³0 , Î»0 )}2 F0 (dz) âˆ’â†’ 0, and
29

R

p

{Ï†(z, Î³0 , Î»Ì‚â„“ ) âˆ’ Ï†(z, Î³0 , Î»0 )}2 F0 (dz) âˆ’â†’ 0;

The cross-fitting used in the construction of ÏˆÌ‚(Î²0 ) is what makes the mean-square consistency
p
p
conditions of Assumption 4 sufficient for RÌ‚1 âˆ’â†’ 0. The next condition is sufficient for RÌ‚2 âˆ’â†’ 0.
Assumption 5: For each â„“ = 1, ..., L, either i)
Z
âˆš
p
n max |Ï†j (z, Î³Ì‚â„“ , Î»Ì‚â„“ ) âˆ’ Ï†j (z, Î³0 , Î»Ì‚â„“ ) âˆ’ Ï†j (z, Î³Ì‚â„“ , Î»0 ) + Ï†j (z, Î³0 , Î»0 )|F0 (dz) âˆ’â†’ 0
j

p

or ii) RÌ‚2 âˆ’â†’ 0.
p

As previously discussed, this condition allows for just RÌ‚2 âˆ’â†’ 0 in order to allow the weak
regularity conditions of Firpo and Rothe (2017) and Newey and Robins (2017). The first result
of this Section shows that Assumptions 4 and 5 are sufficient for equation (7.1) when the moment
functions are DR.
Lemma 12: If Assumption 1 is satisfied, with probability approaching one Î³Ì‚ âˆˆ Î“, Î»Ì‚ âˆˆ Î›,
and Assumptions 4 and 5 are satisfied then equation (7.1) is satisfied.
An important class of DR estimators are those from equation (5.2). The following result
gives conditions for asymptotic linearity of these estimators:
Theorem 13: If a) Assumptions 2 and 4 i) are satisfied with Î³Ì‚ âˆˆ Î“ and Î»Ì‚ âˆˆ Î› with
probability approaching one; b) Î»0 (xi ) and E[{yi âˆ’ Î³0 (wi )}2 |xi ] are bounded; c) for each â„“ =
R
R
p
p
1, ..., L, [Î³Ì‚â„“ (w) âˆ’ Î³0 (w)]2 F0 (dz) âˆ’â†’ 0, [Î»Ì‚â„“ (x) âˆ’ Î»0 (x)]2 F0 (dz) âˆ’â†’ 0, and either
âˆš

n

Z

or

2

[Î³Ì‚â„“ (w) âˆ’ Î³0 (w)] F0 (dw)

1/2 Z

2

[Î»Ì‚â„“ (x) âˆ’ Î»0 (x)] F0 (dx)

1/2

p

âˆ’â†’ 0

1 X
p
âˆš
{Î³Ì‚â„“ (wi ) âˆ’ Î³0 (wi )}{Î»Ì‚â„“ (xi ) âˆ’ Î»0 (xi )} âˆ’â†’ 0;
n iâˆˆI
â„“

then

âˆš

n

1 X
[g(zi , Î³0) âˆ’ Î²0 + Î»0 (xi ){yi âˆ’ Î³0 (wi )}] + op (1).
n(Î²Ì‚ âˆ’ Î²0 ) = âˆš
n i=1

The conditions of this result are simple, general, and allow for machine learning first steps.
Conditions a) and b) simply require mean square consistency of the first step estimators Î³Ì‚ and
Î»Ì‚. The only convergence rate condition is c), which requires a product of estimation errors for
âˆš
the two first steps to go to zero faster than 1/ n. This condition allows for a trade-off in
convergence rates between the two first steps, and can be satisfied even when one of the two
30

rates is not very fast. This trade-off can be important when Î»0 (x) is not continuous in one of
the components of x, as in the surplus bound example. Discontinuity in x can limit that rate at
which Î»0 (x) can be estimated. This result extends the results of Chernozhukov et al. (2018) and
Farrell (2015) for DR estimators of treatment effects to the whole novel class of DR estimators
from equation (5.2) with machine learning first steps. In interesting related work, Athey et al.
(2016) show root-n consistent estimation of an average treatment effect is possible under very
weak conditions on the propensity score, under strong sparsity of the regression function. Thus,
for machine learning the conditions here and in Athey et al. (2016) are complementary and
one may prefer either depending on whether or not the regression function can be estimated
extremely well based on a sparse method. The results here apply to many more DR moment
conditions.
DR moment conditions have the special feature that RÌ‚3 and RÌ‚4 in Proposition 4 are equal
to zero. For estimators that are not DR we impose that RÌ‚3 and RÌ‚4 converge to zero.
Assumption 6: For each â„“ = 1, ..., L, i)

âˆš

p

nÏˆÌ„(Î³Ì‚â„“ , Î»0 ) âˆ’â†’ 0 and ii)

âˆš

p

nÏ†Ì„(Î³0 , Î»Ì‚â„“ ) âˆ’â†’ 0.

Assumption 6 requires that Î³Ì‚ converge to Î³0 rapidly enough but places no restrictions on
the convergence rate of Î»Ì‚ when Ï†Ì„(Î³0 , Î»Ì‚â„“ ) = 0.
Lemma 14: If Assumptions 4-6 are satisfied then equation (7.1) is satisfied.
Assumptions 4-6 are based on the decomposition of LR moment functions into an identifying
part and an influence function adjustment. These conditions differ from other previous work in
semiparametric estimation, as in Andrews (1994), Newey (1994), Newey and McFadden (1994),
Chen, Linton, and van Keilegom (2003), Ichimura and Lee (2010), Escanciano et al. (2016), and
Chernozhukov et al. (2018), that are not based on this decomposition. The conditions extend
Chernozhukov et. al. (2018) to many more DR estimators and to estimators that are nonlinear
in Î³Ì‚ but only require a convergence rate for Î³Ì‚ and not for Î»Ì‚.
This framework helps explain the potential problems with â€plugging inâ€ a first step machine
learning estimator into a moment function that is not LR. Lemma 14 implies that if Assumptions
P
âˆš
âˆš p
4-6 are satisfied for some Î»Ì‚ then nmÌ‚(Î²0 ) âˆ’ ni=1 Ïˆ(zi , Î²0 , Î³0)/ n âˆ’â†’ 0 if and only if
n

1 X
p
Ï†(zi , Î³Ì‚, Î»Ì‚) âˆ’â†’ 0.
RÌ‚5 = âˆš
n i=1

(7.4)

The plug-in method will fail when this equation does not hold. For example, suppose Î³0 = E[y|x]
so that by Proposition 4 of Newey (1994),
n

n

âˆ’1 X
1 X
âˆš
Ï†(zi , Î³Ì‚, Î»Ì‚) = âˆš
Î»Ì‚i (xi )[yi âˆ’ Î³Ì‚i (xi )].
n i=1
n i=1
31

p

Here RÌ‚5 âˆ’â†’ 0 is an approximate orthogonality condition between the approximation Î»Ì‚i (xi )
to Î»0 (xi ) and the nonparametric first stage residuals yi âˆ’ Î³Ì‚i (xi ). Machine learning uses model
selection in the construction of Î³Ì‚i (xi ). If the model selected by Î³Ì‚i (xi ) to approximate Î³0 (xi ) is
not rich (or dense) enough to also approximate Î»0 (xi ) then Î»Ì‚i (xi ) need not be approximately
orthogonal to yi âˆ’ Î³Ì‚i (xi ) and RÌ‚5 need not converge to zero. In particular, if the variables selected
to be used to approximate Î³0 (xi ) cannot be used to also approximate Î»0 (xi ) then the approximate
orthogonality condition can fail. This phenomenon helps explain the poor performance of the
plug-in estimator shown in Belloni, Chernozhukov, and Hansen (2014) and Chernozhukov et al.
(2017, 2018). The plug-in estimator can be root-n consistent if the only thing being selected is
an overall order of approximation, as in the series estimation results of Newey (1994). General
conditions for root-n consistency of the plug-in estimator can be formulated using Assumptions
p
4-6 and RÌ‚2 âˆ’â†’ 0, which we do in Appendix D.
Another component of an asymptotic normality result is convergence of the Jacobian term
âˆ‚ ÏˆÌ‚(Î²)/âˆ‚Î² to M = E[âˆ‚Ïˆ(zi , Î², Î³0, Î»0 )/âˆ‚Î²|Î²=Î²0 ]. We impose the following condition for this
purpose.
Assumption 7: M exists and there is a neighborhood N of Î²0 and kÂ·k such that i) for each
p
p
â„“, kÎ³Ì‚â„“ âˆ’ Î³0 k âˆ’â†’ 0, Î»Ì‚â„“ âˆ’ Î»0 âˆ’â†’ 0; ii) for all kÎ³ âˆ’ Î³0 k and kÎ» âˆ’ Î»0 k small enough Ïˆ(zi , Î², Î³, Î»)
is differentiable in Î² on N with probability approaching 1 iii) there is Î¶ â€² > 0 and d(zi ) with
E[d(zi )] < âˆ such that for Î² âˆˆ N and kÎ³ âˆ’ Î³0 k small enough
â€²
âˆ‚Ïˆ(zi , Î², Î³, Î») âˆ‚Ïˆ(zi , Î²0 , Î³, Î»)
â‰¤ d(zi ) kÎ² âˆ’ Î²0 kÎ¶ ;
âˆ’
âˆ‚Î²
âˆ‚Î²

iii) For each â„“ = 1, ..., L, j, and k,
0,

R

p

âˆ‚Ïˆj (z, Î²0 , Î³Ì‚â„“ , Î»Ì‚â„“ )/âˆ‚Î²k âˆ’ âˆ‚Ïˆj (z, Î²0 , Î³0 , Î»0 )/âˆ‚Î²k F0 (dz) âˆ’â†’

The following intermediate result gives Jacobian convergence.
p

Lemma 15: If Assumption 7 is satisfied then for any Î²Ì„ âˆ’â†’ Î²0 , ÏˆÌ‚(Î²) is differentiable at Î²Ì„
p
with probability approaching one and âˆ‚ ÏˆÌ‚(Î²Ì„)/âˆ‚Î² âˆ’â†’ M.
With these results in place the asymptotic normality of semiparametric GMM follows in a
standard way.
p

p

Theorem 16: If Assumptions 4-7 are satisfied, Î²Ì‚ âˆ’â†’ Î²0 , WÌ‚ âˆ’â†’ W , M â€² W M is nonsingular, and E[kÏˆ(zi , Î²0 , Î³0 , Î»0 )k2 ] < âˆ then for â„¦ = E[Ïˆ(zi , Î²0 , Î³0 , Î»0 )Ïˆ(zi , Î²0 , Î³0 , Î»0 )â€² ],
âˆš

d

n(Î²Ì‚ âˆ’ Î²0 ) âˆ’â†’ N(0, V ), V = (M â€² W M)âˆ’1 M â€² W â„¦W M(M â€² W M)âˆ’1 .

32

It is also useful to have a consistent estimator of the asymptotic variance of Î²Ì‚. As usual such
an estimator can be constructed as
VÌ‚ = (MÌ‚ â€² WÌ‚ MÌ‚ )âˆ’1 MÌ‚ â€² WÌ‚ â„¦Ì‚WÌ‚ MÌ‚ (MÌ‚ â€² WÌ‚ MÌ‚)âˆ’1 ,
L

MÌ‚ =

1 XX
âˆ‚ ÏˆÌ‚(Î²Ì‚)
Ïˆ(zi , Î²Ì‚, Î³Ì‚â„“ , Î»Ì‚â„“ )Ïˆ(zi , Î²Ì‚, Î³Ì‚â„“ , Î»Ì‚â„“ )â€² .
, â„¦Ì‚ =
âˆ‚Î²
n â„“=1 iâˆˆI
â„“

Note that this variance estimator ignores the estimation of Î³ and Î» which works here because
the moment conditions are LR. The following result gives conditions for consistency of VÌ‚ .
Theorem 17: If Assumptions 4 and 7 are satisfied with E[b(zi )2 ] < âˆ, M â€² W M is nonsingular, and
Z
2
p
Ï†(z, Î³Ì‚â„“ , Î»Ì‚â„“ ) âˆ’ Ï†(z, Î³0 , Î»Ì‚â„“ ) âˆ’ Ï†(z, Î³Ì‚â„“ , Î»0 ) + Ï†(z, Î³0 , Î»0 ) F0 (dz) âˆ’â†’ 0
p

p

then â„¦Ì‚ âˆ’â†’ â„¦ and VÌ‚ âˆ’â†’ V.
In this section we have used cross-fitting and a decomposition of moment conditions into
identifying and influence adjustment components to formulate simple and general conditions for
asymptotic normality of LR GMM estimators. For reducing higher order bias and variance it
may be desirable to let the number of groups grow with the sample size. That case is beyond
the scope of this paper.

8

Appendix A: Proofs of Theorems

Proof of Theorem 1: By ii) and iii),
Z
Z
0 = (1 âˆ’ Ï„ ) Ï†(z, FÏ„ )F0 (dz) + Ï„ Ï†(z, FÏ„ )G(dz).

Dividing by Ï„ and solving gives
Z
Z
Z
1
Ï†(z, FÏ„ )F0 (dz) = âˆ’ Ï†(z, FÏ„ )G(dz) + Ï†(z, FÏ„ )F0 (z).
Ï„

Taking limits as Ï„ âˆ’â†’ 0, Ï„ > 0 and using i) gives
Z
Z
d
dÂµ(FÏ„ )
Ï†(z, FÏ„ )F0 (dz) = âˆ’ Ï†(z, F0 )G(dz) + 0 = âˆ’
. Q.E.D.
dÏ„
dÏ„
Proof of Theorem 2: We begin by deriving Ï†1 , the adjustment term for the first step CCP
estimation. We use the definitions given in the body of the paper. We also let
PvÌƒj (vÌƒ) = âˆ‚P (vÌƒ)/âˆ‚vÌƒj , Ï€1 = Pr(yt1 = 1), Î»10 (x) = E[y1t |xt+1 = x],
ytj
Î»j0(x) = E[A(xt )PvÌƒj (vÌƒt )
|xt+1 = x], (j = 2, ..., J).
Pj (vÌƒt )
33

Consider a parametric submodel as described in Section 4 and let Î³1 (x, Ï„ ) denote the conditional
expectation of yt given xt under the parametric submodel. Note that for vÌƒt = vÌƒ(xt ),
âˆ‚E[H(Î³1 (xt+1 , Ï„ ))|xt , ytj = 1]
]
âˆ‚Ï„
ytj
âˆ‚
E[A(xt )Pvj (vÌƒt )
H(Î³1(xt+1 , Ï„ ))]
=
âˆ‚Ï„
Pj (vÌƒt )
âˆ‚
ytj
=
E[E[A(xt )Pvj (vÌƒt )
|xt+1 ]H(Î³1 (xt+1 , Ï„ ))]
âˆ‚Ï„
Pj (vÌƒt )
âˆ‚
âˆ‚
=
E[Î»j0 (xt+1 )H(Î³1(xt+1 , Ï„ ))] =
E[Î»j0 (xt )H(Î³1 (xt , Ï„ ))]
âˆ‚Ï„
âˆ‚Ï„
âˆ‚H(Î³10 (xt )) â€²
âˆ‚H(Î³10 (xt )) â€² âˆ‚Î³1 (xt , Ï„ )
{yt âˆ’ Î³10 (xt )}S(zt )].
] = E[Î»j0 (xt )
= E[Î»j0(xt )
âˆ‚P
âˆ‚Ï„
âˆ‚P

E[A(xt )PvÌƒj (vÌƒt )

where the last (sixth) equality follows as in Proposition 4 of Newey (1994a), and the fourth
equality follows by equality of the marginal distributions of xt and xt+1 . Similarly, for Ï€1 =
Pr(yt1 = 1) and Î»10 (x) = E[y1t |xt+1 = x] we have
âˆ‚E[Ï€1âˆ’1 y1t H(Î³1 (xt+1 , Ï„ ))]
âˆ‚E[Ï€1âˆ’1 Î»10 (xt+1 )H(Î³1 (xt+1 , Ï„ ))]
âˆ‚E[H(Î³1 (xt+1 , Ï„ ))|yt1 = 1]
=
=
âˆ‚Ï„
âˆ‚Ï„
âˆ‚Ï„
âˆ’1
âˆ‚E[Ï€1 Î»10 (xt )H(Î³1 (xt , Ï„ ))]
=
âˆ‚Ï„
âˆ‚H(Î³10 (xt )) â€²
âˆ’1
{yt âˆ’ Î³10 (xt )}S(zt )]
= E[Ï€1 Î»10 (xt )
âˆ‚P
Then combining terms gives
âˆ‚E[m(zt , Î²0 , Î³1 (Ï„ ), Î³âˆ’10 )]
âˆ‚Ï„
J
X
âˆ‚E[H(Î³1 (xt+1 , Ï„ ))|xt , ytj = 1]
= âˆ’Î´
{E[A(xt )Pvj (vÌƒt )
]
âˆ‚Ï„
j=2
âˆ’ E[A(xt )Pvj (vÌƒt )]
= âˆ’Î´

J
X
j=2

âˆ‚E[H(Î³1 (xt+1 , Ï„ ))|yt1 = 1]
}
âˆ‚Ï„

E[{Î»j0 (xt ) âˆ’ E[A(xt )PvÌƒj (vÌƒt )]Ï€1âˆ’1 Î»10 (xt )}

âˆ‚H(Î³10 (xt )) â€²
{yt âˆ’ Î³10 (xt )}S(zt )]
âˆ‚P

= E[Ï†1 (zt , Î²0 , Î³0 , Î»0 )S(zt )].

Next, we show the result for Ï†j (z, Î², Î³, Î») for 2 â‰¤ j â‰¤ J. As in the proof of Proposition 4 of
Newey (1994a), for any wt we have
ytj
âˆ‚
E[wt |xt , ytj = 1, Ï„ ] = E[
{wt âˆ’ E[wt |xt , ytj = 1]}S(zt )|xt ].
âˆ‚Ï„
Pj (vÌƒt )
34

It follows that
âˆ‚E[m(zt , Î²0 , Î³j (Ï„ ), Î³âˆ’j,0)]
âˆ‚E[u1,t+1 + Ht+1 |xt , ytj = 1, Ï„ ]
= âˆ’Î´E[A(xt )Pvj (vÌƒt )
]
âˆ‚Ï„
âˆ‚Ï„
âˆ‚
= âˆ’Î´ E[E[A(xt )Pvj (vÌƒt ){u1,t+1 + Ht+1 }|xt , ytj = 1, Ï„ ]].
âˆ‚Ï„
ytj
= âˆ’Î´E[A(xt )Pvj (vÌƒt )
{u1,t+1 + Ht+1 âˆ’ Î³j0(xt , Î²0 , Î³1 )}S(zt )]
Pj (vÌƒt )
= E[Ï†j (zt , Î²0 , Î³0 , Î»0 )S(zt )],
showing that the formula for Ï†j is correct. The proof for Ï†J+1 follows similarly. Q.E.D.
Proof of Theorem 3: Given in text.
Proof of Theorem 4: Given in text.
Proof of Theorem 5: Let ÏˆÌ„(Î³, Î») = E[Ïˆ(zi , Î²0 , Î³, Î»)]. Suppose that Ïˆ(z, Î², Î³, Î») is DR.
Then for any Î³ 6= Î³0 , Î³ âˆˆ Î“ we have
0 = ÏˆÌ„(Î³, Î»0 ) = ÏˆÌ„(Î³0 , Î»0 ) = ÏˆÌ„((1 âˆ’ Ï„ )Î³0 + Ï„ Î³, Î»0 ),
for any Ï„. Therefore for any Ï„ ,
ÏˆÌ„((1 âˆ’ Ï„ )Î³0 + Ï„ Î³, Î»0 ) = 0 = (1 âˆ’ Ï„ )ÏˆÌ„(Î³0, Î»0 ) + Ï„ ÏˆÌ„(Î³, Î»0 ),
so that ÏˆÌ„(Î³, Î»0 ) is affine in Î³. Also by the previous equation ÏˆÌ„((1 âˆ’ Ï„ )Î³0 + Ï„ Î³, Î»0 ) = 0 identically
in Ï„ so that
âˆ‚
ÏˆÌ„((1 âˆ’ Ï„ )Î³0 + Ï„ Î³, Î»0 ) = 0,
âˆ‚Ï„
where the derivative with respect to Ï„ is evaluated at Ï„ = 0. Applying the same argument
switching of Î» and Î³ we find that ÏˆÌ„(Î³0 , Î») is affine in Î» and âˆ‚ ÏˆÌ„(Î³0 , (1 âˆ’ Ï„ )Î»0 + Ï„ Î»)/âˆ‚Ï„ = 0.
Next suppose that ÏˆÌ„(Î³, Î»0 ) is affine Î³ and âˆ‚ ÏˆÌ„((1âˆ’Ï„ )Î³0 +Ï„ Î³, Î»0 )/âˆ‚Ï„ = 0. Then by ÏˆÌ„(Î³0, Î»0 ) =
0, for any Î³ âˆˆ Î“,
ÏˆÌ„(Î³, Î»0 ) = âˆ‚[Ï„ ÏˆÌ„(Î³, Î»0 )]/âˆ‚Ï„ = âˆ‚[(1 âˆ’ Ï„ )ÏˆÌ„(Î³0 , Î»0 ) + Ï„ ÏˆÌ„(Î³, Î»0 )]/âˆ‚Ï„
= âˆ‚ ÏˆÌ„((1 âˆ’ Ï„ )Î³0 + Ï„ Î³, Î»0 )/âˆ‚Ï„ = 0.

Switching the roles of Î³ and Î» it follows analogously that ÏˆÌ„(Î³0 , Î») = 0 for all Î» âˆˆ Î›, so ÏˆÌ„(Î³, Î»)
is doubly robust. Q.E.D.
Proof of Theorem 6: Let Î»0 (x) = âˆ’câ€² Î âˆ’1 a(x) so that E[Î»0 (xi )|wi ] = âˆ’câ€² Î âˆ’1 Î p(wi ) =
âˆ’câ€² p(wi ).Then integration by parts gives
E[m(zi , Î²0 , Î³Ìƒ)] = E[câ€² p(wi ){Î³Ìƒ(wi ) âˆ’ Î³0 (wi )}] = âˆ’E[Î³0 (xi ){Î³Ìƒ(wi ) âˆ’ Î³0 (wi )}]

= E[Î³0 (xi ){yi âˆ’ Î³Ìƒ(wi )}] = âˆ’câ€² Î âˆ’1 E[a(xi ){yi âˆ’ Î³Ìƒ(wi )}] = 0. Q.E.D.
35

Proof of Theorem 7: If Î»0 is identified then m(z, Î², Î³Ì„, Î»0 ) is identified for every Î². By DR
E[m(zi , Î², Î³Ì„, Î»0 )] = 0
at Î² = Î²0 and by assumption this is the only Î² where this equation is satisfied. Q.E.D.
Proof of Corollary 8: Given in text.
Proof of Theorem 9: Note that for Ïi = Ï(zi , Î²0 , Î³0 ),
ÏˆÌ„(Î³0 , (1 âˆ’ Ï„ )Î»0 + Ï„ Î»)] = (1 âˆ’ Ï„ )E[Î»0 (xi )Ïi ] + Ï„ E[Î»(xi )Ïi ] = 0.

(8.1)

Differentiating gives the second equality in eq. (2.7). Also, for âˆ† = Î³ âˆ’ Î³0 ,
âˆ‚ ÏˆÌ„((1 âˆ’ Ï„ )Î³0 + Ï„ Î³, Î»0 )
= E[Î»0 (xi )ÏÌ„(xi , âˆ†)] = 0,
âˆ‚Ï„
giving the first equality in eq. (2.7). Q.E.D.
Proof of Theorem 10: The first equality in eq. (8.1) of the proof of Theorem 9 shows that
ÏˆÌ„(Î³0 , Î») is affine in Î». Also,
ÏˆÌ„((1âˆ’Ï„ )Î³0 +Ï„ Î³, Î»0 ) = E[Î»0 (xi ){(1âˆ’Ï„ )Ï(zi , Î²0 , Î³0 )+Ï„ Ï(zi , Î²0 , Î³)}] = (1âˆ’Ï„ )ÏˆÌ„(Î³0 , Î»0 )+Ï„ ÏˆÌ„(Î³, Î»0 ),
so that ÏˆÌ„(Î³, Î»0 ) is affine in Î³. The conclusion then follows by Theorem 5. Q.E.D.
âˆ—

Proof of Theorem 11: To see that Î»ÌƒÎ£ (xi , Î»âˆ— )Î£âˆ— (xi )âˆ’1 minimizes the asymptotic variance note that for any orthogonal instrumental variable matrix Î»0 (x), by the rows of Î»Î² (xi ) âˆ’
âˆ—
Î»ÌƒÎ£ (xi , Î»Î² ) being in Î›Ì„Î³ ,
âˆ—

âˆ—

M = E[Î»0 (xi )Î»Î² (xi )â€² ] = E[Î»0 (xi )Î»ÌƒÎ£ (xi , Î»Î² )â€² ] = E[Î»0 (xi )Ïi Ïâ€²i Î£âˆ— (xi )âˆ’1 Î»ÌƒÎ£ (xi , Î»Î² )â€² ].
Since the instruments are orthogonal the asymptotic variance matrix of the GMM estimator with
p
âˆ—
WÌ‚ âˆ’â†’ W is the same as if Î³Ì‚ = Î³0 . Define mi = M â€² W Î»0 (xi )Ïi and mâˆ—i = Î»ÌƒÎ£ (xi , Î»Î² )Î£âˆ— (xi )âˆ’1 Ïi .
The asymptotic variance of the GMM estimator for orthogonal instruments Î»0 (x) is
âˆ’1
â€²
âˆ— âˆ’1â€²
(M â€² W M)âˆ’1 M â€² W E[Î»0 (xi )Ïi Ïâ€²i Î»0 (xi )â€² ]W M(M â€² W M)âˆ’1 = (E[mi mâˆ—â€²
.
i ]) E[mi mi ](E[mi mi ])

The fact that this matrix is minimized in the positive semidefinite sense for mi = mâˆ—i is well
known, e.g. see Newey and McFadden (1994). Q.E.D.
The following result is useful for the results of Section 7:
p

Lemma A1: If Assumption 4 is satisfied then RÌ‚1 âˆ’â†’ 0. If Assumption 5 is satisfied then
p
RÌ‚2 âˆ’â†’ 0.
36

Ë† iâ„“ = m(zi , Î³Ì‚â„“ )âˆ’m(zi , Î³0 )âˆ’ mÌ„(Î³Ì‚â„“ ) for i âˆˆ Iâ„“ and let Z c denote the observations
Proof: Define âˆ†
â„“
c
zi for i âˆˆ
/ Iâ„“ . Note that Î³Ì‚â„“ depends only on Zâ„“ . By construction and independence of Zâ„“c and
Ë† iâ„“ |Z c ] = 0. Also by independence of the observations, E[âˆ†
Ë† iâ„“ âˆ†
Ë† jâ„“ |Z c ] = 0
zi , i âˆˆ Iâ„“ we have E[âˆ†
â„“
â„“
R
Ë† 2 |Z c ] â‰¤ [m(z, Î³Ì‚â„“ ) âˆ’ m(z, Î³0 )]2 F0 (dz). Then we have
for i, j âˆˆ Iâ„“ . Furthermore, for i âˆˆ Iâ„“ E[âˆ†
iâ„“ â„“
1 XË†
E[ âˆš
âˆ†iâ„“
n iâˆˆI
â„“

!2

!2
X
X
1
Ë† 2 |Z c ]
Ë† iâ„“ |Z c ] = 1
E[âˆ†
âˆ†
|Zâ„“c ] = E[
iâ„“ â„“
â„“
n
n
iâˆˆIâ„“
iâˆˆIâ„“
Z
p
â‰¤ [m(z, Î³Ì‚â„“ ) âˆ’ m(z, Î³0 )]2 F0 (dz) âˆ’â†’ 0.

P
p
Ë† iâ„“ /âˆšn âˆ’â†’
The conditional Markov inequality then implies that iâˆˆIâ„“ âˆ†
0. The analogous results
Ë† iâ„“ = Ï†(zi , Î³Ì‚â„“ , Î»0 ) âˆ’ Ï†(zi , Î³0 , Î»0 ) âˆ’ Ï†Ì„(Î³Ì‚â„“ , Î»0 ) and âˆ†
Ë† iâ„“ = Ï†(zi , Î³0 , Î»Ì‚â„“ ) âˆ’ Ï†(zi , Î³0 , Î»0 ) âˆ’
also hold for âˆ†
Ï†Ì„(Î³0 , Î»Ì‚â„“ ). Summing across these three terms and across â„“ = 1, ..., L gives the first conclusion.
For the second conclusion, note that under the first hypothesis of Assumption 5,
1 X
E[ âˆš
[Ï†j (zi , Î³Ì‚â„“ , Î»Ì‚â„“ ) âˆ’ Ï†j (zi , Î³0, Î»Ì‚â„“ ) âˆ’ Ï†j (zi , Î³Ì‚â„“ , Î»0 ) + Ï†j (zi , Î³0 , Î»0 )] |Zâ„“c ]
n iâˆˆI
â„“
1 X
E[ Ï†j (zi , Î³Ì‚â„“ , Î»Ì‚â„“ ) âˆ’ Ï†j (zi , Î³0 , Î»Ì‚â„“ ) âˆ’ Ï†j (zi , Î³Ì‚â„“ , Î»0 ) + Ï†j (zi , Î³0 , Î»0 ) |Zâ„“c ]
â‰¤âˆš
n iâˆˆI
Z â„“
âˆš
p
â‰¤ n
Ï†j (z, Î³Ì‚â„“ , Î»Ì‚â„“ ) âˆ’ Ï†j (z, Î³0 , Î»Ì‚â„“ ) âˆ’ Ï†j (z, Î³Ì‚â„“ , Î»0 ) + Ï†j (zi , Î³0 , Î»0 ) F0 (dz) âˆ’â†’ 0,
p

so RÌ‚2 âˆ’â†’ 0 follows by the conditional Markov and triangle inequalities. The second hypothesis
p
of Assumption 5 is just RÌ‚2 âˆ’â†’ 0. Q.E.D.
Proof of Lemma 12: By Assumption 1 and the hypotheses that Î³Ì‚i âˆˆ Î“ and Î»Ì‚i âˆˆ Î› we
p
p
have RÌ‚3 = RÌ‚4 = 0. By Lemma A1 we have RÌ‚1 âˆ’â†’ 0 and RÌ‚2 âˆ’â†’ 0. The conclusion then follows
by the triangle inequality. Q.E.D.
Proof of Theorem 13: Note that for Îµ = y âˆ’ Î³0 (w)
Ï†(z, Î³Ì‚, Î»0 ) âˆ’ Ï†(z, Î³0 , Î»0 ) = Î»0 (x)[Î³Ì‚(w) âˆ’ Î³0 (w)],
Ï†(z, Î³0 , Î»Ì‚) âˆ’ Ï†(z, Î³0 , Î»0 ) = [Î»Ì‚(x) âˆ’ Î»0 (x)]Îµ,

Ï†(z, Î³Ì‚â„“ , Î»Ì‚â„“ ) âˆ’ Ï†(z, Î³0 , Î»Ì‚â„“ ) âˆ’ Ï†(z, Î³Ì‚â„“ , Î»0 ) + Ï†j (z, Î³0 , Î»0 ) = âˆ’[Î»Ì‚(x) âˆ’ Î»0 (x)][Î³Ì‚(x) âˆ’ Î³0 (x)].
The first part of Assumption 4 ii) then follows by
Z
Z
2
[Ï†(z, Î³Ì‚â„“ , Î»0 ) âˆ’ Ï†(z, Î³0 , Î»0 )] F0 (dz) = Î»0 (x)2 [Î³Ì‚(w) âˆ’ Î³0 (w)]2 F0 (dz)
Z
p
â‰¤ C [Î³Ì‚(w) âˆ’ Î³0 (w)]2 F0 (dz) âˆ’â†’ 0.
37

The second part of Assumption 4 ii) follows by
Z
Z
2
[Ï†(z, Î³0 , Î»Ì‚â„“ ) âˆ’ Ï†(z, Î³0 , Î»0 )] F0 (dz) = [Î»Ì‚â„“ (x) âˆ’ Î»0 (x)]2 Îµ2 F0 (dz)
Z h
i2
=
Î»Ì‚â„“ (x) âˆ’ Î»0 (x) E[Îµ2 |x]F0 (dz)
Z h
i2
p
â‰¤C
Î»Ì‚â„“ (x) âˆ’ Î»0 (x) F0 (dz) âˆ’â†’ 0.
Next, note that by the Cauchy-Schwartz inequality,
Z
âˆš
n |Ï†(z, Î³Ì‚â„“ , Î»Ì‚â„“ ) âˆ’ Ï†(z, Î³0 , Î»Ì‚â„“ ) âˆ’ Ï†(z, Î³Ì‚â„“ , Î»0 ) + Ï†(z, Î³0 , Î»0 )|F0 (dz)
Z
âˆš
[Î»Ì‚â„“ (x) âˆ’ Î»0 (x)][Î³Ì‚â„“ (w) âˆ’ Î³0 (w)] F0 (dx)
= n
Z
Z
âˆš
2
1/2
â‰¤ n{ [Î»Ì‚â„“ (x) âˆ’ Î»0 (x)] F0 (dx)} { [Î³Ì‚â„“ (w) âˆ’ Î³0 (w)]2F0 (dw)}1/2 .

Then the first rate condition of Assumption 5 holds under the first rate condition of Theorem
13 while the second condition of Assumption 5 holds under the last hypothesis of Theorem 13.
Then eq. (7.1) holds by Lemma 12, and the conclusion by rearranging the terms in eq. (7.1).
Q.E.D.
Proof of Lemma 14: Follows by Lemma A1 and the triangle inequality. Q.E.D.
P
Proof of Lemma 15: Let MÌ‚(Î²) = âˆ‚ ÏˆÌ‚(Î²)/âˆ‚Î² when it exists, MÌƒâ„“ = nâˆ’1 iâˆˆIâ„“ âˆ‚Ïˆ(zi , Î²0 , Î³Ì‚â„“ , Î»Ì‚â„“ )/âˆ‚Î²,
P
and MÌ„â„“ = nâˆ’1 iâˆˆIâ„“ âˆ‚Ïˆ(zi , Î²0 , Î³0 , Î»0 )/âˆ‚Î². By the law of large numbers, and Assumption 5 iii),
PL
p
â„“=1 MÌ„â„“ âˆ’â†’ M. Also, by condition iii) for each j and k,
Z
p
â„“
E[|MÌƒâ„“jk âˆ’ MÌ„â„“jk ||Z ] â‰¤
âˆ‚Ïˆj (z, Î²0 , Î³Ì‚â„“ , Î»Ì‚â„“ )/âˆ‚Î²k âˆ’ âˆ‚Ïˆj (z, Î²0 , Î³0 , Î»0 )/âˆ‚Î²k F0 (dz) âˆ’â†’ 0.
Then by the conditional Markov inequality, for each â„“,
p

MÌƒâ„“ âˆ’ MÌ„â„“ âˆ’â†’ 0.
It follows by the triangle inequality that
p
one we have for any Î²Ì„ âˆ’â†’ Î²0
MÌ‚ (Î²Ì„) âˆ’

L
X
â„“=1

n

MÌƒâ„“ â‰¤

PL

p

â„“=1

MÌƒâ„“ âˆ’â†’ M. Also, with probability approaching

1X
d(zi )
n i=1

!

Î²Ì„ âˆ’ Î²0

Î¶â€²

p

= Op (1)op (1) âˆ’â†’ 0.

The conclusion then follows by the triangle inequality. Q.E.D.
Proof of Theorem 16: The conclusion follows in a standard manner from the conclusions
of Lemmas 14 and 15. Q.E.D.
38

Proof of Theorem 17: Let ÏˆÌ‚i = Ïˆ(zi , Î²Ì‚, Î³Ì‚â„“ , Î»Ì‚â„“ ) and Ïˆi = Ïˆ(zi , Î²0 , Î³0 , Î»0 ). By standard
2
P
p
arguments (e.g. Newey, 1994), it suffices to show that ni=1 ÏˆÌ‚i âˆ’ Ïˆi /n âˆ’â†’ 0. Note that
ÏˆÌ‚i âˆ’ Ïˆi =

5
X
j=1

Ë† ji , âˆ†
Ë† 1i = Ïˆ(zi , Î²Ì‚, Î³Ì‚â„“ , Î»Ì‚â„“ ) âˆ’ Ïˆ(zi , Î²0 , Î³Ì‚â„“ , Î»Ì‚â„“ ), âˆ†
Ë† 2i = m(zi , Î²0 , Î³Ì‚â„“ ) âˆ’ m(zi , Î²0 , Î³0 ),
âˆ†

Ë† 3i = Ï†(zi , Î³Ì‚â„“ , Î»0 ) âˆ’ Ï†(zi , Î³0 , Î»0 ), âˆ†
Ë† 4i = Ï†(zi , Î³0 , Î»Ì‚â„“ ) âˆ’ Ï†(zi , Î³0 , Î»0 ),
âˆ†
Ë† 5i = Ï†(zi , Î³Ì‚â„“ , Î»Ì‚â„“ ) âˆ’ Ï†(zi , Î³Ì‚â„“ , Î»0 ) âˆ’ Ï†(zi , Î³0 , Î»Ì‚â„“ ) + Ï†(zi , Î³0, Î»0 ).
âˆ†
By standard arguments it suffices to show that for each j and â„“,
1X Ë†
âˆ†ji
n iâˆˆI
â„“

2

p

âˆ’â†’ 0.

(8.2)

For j = 1 it follows by a mean value expansion and Assumption 7 with E[b(zi )2 ] < âˆ that
!
2
2 p
1X âˆ‚
1 X
1X Ë† 2
âˆ†1i =
Î²Ì‚ âˆ’ Î² âˆ’â†’ 0,
b(zi )2
Ïˆ(zi , Î²Ì„, Î³Ì‚â„“ , Î»Ì‚â„“ )(Î²Ì‚ âˆ’ Î²) â‰¤
n iâˆˆI
n iâˆˆI âˆ‚Î²
n iâˆˆI
â„“

â„“

â„“

where Î²Ì„ is a mean value that actually differs from row to row of âˆ‚Ïˆ(zi , Î²Ì„, Î³Ì‚â„“ , Î»Ì‚â„“ )/âˆ‚Î². For j = 2
note that by Assumption 4,
Z
1X Ë† 2 â„“
p
âˆ†2i |Z ] â‰¤ km(z, Î²0 , Î³Ì‚â„“ ) âˆ’ m(z, Î²0 , Î³0 )k2 F0 (dz) âˆ’â†’ 0,
E[
n iâˆˆI
â„“

so eq. (8.2) holds by the conditional Markov inequality. For j = 3 and j = 4 eq. (8.2) follows
similarly. For j = 5, it follows from the hypotheses of Theorem 17 that
Z
2
1X Ë† 2 â„“
p
âˆ†5i |Z ] â‰¤
Ï†(z, Î³Ì‚â„“ , Î»Ì‚â„“ ) âˆ’ Ï†(z, Î³0 , Î»Ì‚â„“ ) âˆ’ Ï†(z, Î³Ì‚â„“ , Î»0 ) + Ï†(z, Î³0 , Î»0 ) F0 (dz) âˆ’â†’ 0.
E[
n iâˆˆI
â„“

Then eq. (8.2) holds for j = 5 by the conditional Markov inequality. Q.E.D.

9

Appendix B: Local Robustness and Derivatives of Expected Moments.

In this Appendix we give conditions sufficient for the LR property of equation (2.5) to imply
the properties in equations (2.7) and (2.8). As discussed following equation (2.8), it may be
convenient when specifying regularity conditions for specific moment functions to work directly
with (2.7) and/or (2.8).

39

Assumption B1: There are linear sets Î“ and Î› and a set G such that i) ÏˆÌ„(Î³, Î») is Frechet
differentiable at (Î³0 , Î»0 ); ii) for all G âˆˆ G the vector (Î³(FÏ„ ), Î»(FÏ„ )) is Frechet differentiable at
Ï„ = 0; iii) the closure of {âˆ‚(Î³(FÏ„ ), Î»(FÏ„ ))/âˆ‚Ï„ : G âˆˆ G} is Î“ Ã— Î›.
Theorem B1: If Assumption B1 is satisfied and equation (2.5) is satisfied for all G âˆˆ G
then equation (2.7) is satisfied.
Proof: Let ÏˆÌ„ â€² (Î³, Î») denote the Frechet derivative of ÏˆÌ„(Î³, Î») at (Î³0 , Î»0 ) in the direction (Î³, Î»),
which exists by i). By ii), the chain rule for Frechet derivatives (e.g. Proposition 7.3.1 of
G
Luenberger, 1969), and by eq. (2.5) it follows that for (âˆ†G
Î³ , âˆ†Î» ) = âˆ‚(Î³(FÏ„ ), Î»(FÏ„ ))/âˆ‚Ï„,
G
ÏˆÌ„ â€² (âˆ†G
Î³ , âˆ†Î» ) =

âˆ‚ ÏˆÌ„(Î³(FÏ„ ), Î»(FÏ„ ))
= 0.
âˆ‚Ï„

By ÏˆÌ„ â€² (Î³, Î») being a continuous linear function and iii) it follows that ÏˆÌ„ â€² (Î³, Î») = 0 for all (Î³, Î») âˆˆ
Î“ Ã— Î›. Therefore, for any Î³ âˆˆ Î“ and Î» âˆˆ Î›,
ÏˆÌ„ â€² (Î³ âˆ’ Î³0 , 0) = 0, ÏˆÌ„ â€² (0, Î» âˆ’ Î»0 ) = 0.
Equation (2.7) then follows by i). Q.E.D.
Theorem B2: If equation (2.7) is satisfied and in addition ÏˆÌ„(Î³, Î»0 ) and ÏˆÌ„(Î³0, Î») are twice
Frechet differentiable in open sets containing Î³0 and Î»0 respectively with bounded second derivative then equation (2.8) is satisfied.
Proof: Follows by Proposition 7.3.3 of Luenberger (1969). Q.E.D.

10

Appendix C: Doubly Robust Moment Functions for
Orthogonality Conditions

In this Appendix we generalize the DR estimators for conditional moment restrictions to orthogonality conditions for a general residual Ï(z, Î³) that is affine in Î³ but need not have the
form y âˆ’ Î³(w).
Assumption C1: There are linear sets Î“ and Î› of functions Î»(x) and Î³(w) that are closed in
mean square such that i) For any Î³, Î³Ìƒ âˆˆ Î“ and scalar Ï„, E[Ï(zi , Î³)2 ] < âˆ and Ï(z, (1âˆ’Ï„ )Î³+Ï„ Î³Ìƒ) =
(1 âˆ’ Ï„ )Ï(z, Î³) + Ï„ Ï(z, Î³Ìƒ) ; ii) E[Î»(xi )Ï(zi , Î³0)] = 0 for all Î» âˆˆ Î›; iii) there exists Î»0 âˆˆ Î› such
that E[m(zi , Î²0 , Î³)] = âˆ’E[Î»0 (xi )Ï(zi , Î³)] for all Î³ âˆˆ Î“.
Assumption C1 ii) could be thought of as an identification condition for Î³0 . For example, if
Î› is all functions of xi with finite mean square then ii) is E[Ï(zi , Î³0 )|xi ] = 0, the nonparametric
40

conditional moment restriction of Newey and Powell (2003) and Newey (1991). Assumption
C1 iii) also has an interesting interpretation. Let Î (a)(xi ) denote the orthogonal mean-square
projection of a random variable a(zi ) with finite second moment on Î“. Then by ii) and iii) we
have
E[m(zi , Î²0 , Î³)] = âˆ’E[Î»0 (xi )Ï(zi , Î³)] = E[Î»0 (xi )Î (Ï(Î³))(xi )]
= E[Î»0 (xi ){Î (Ï(Î³))(xi ) âˆ’ Î (Ï(Î³0 ))(xi )}]
= E[Î»0 (xi ){Î (Ï(Î³) âˆ’ Ï(Î³0 ))(xi )}].

Here we see that E[m(zi , Î²0 , Î³)] is a linear, mean-square continuous function of Î (Ï(Î³) âˆ’
Ï(Î³0 ))(xi ). The Riesz representation theorem will also imply that if E[m(zi , Î²0 , Î³)] is a linear,
mean-square continuous function of Î (Ï(Î³) âˆ’ Ï(Î³0 ))(xi ) then Î»0 (x) exists satisfying Assumption
C1 ii). For the case where wi = xi this mean-square continuity condition is necessary for existence of a root-n consistent estimator, as in Newey (1994) and Newey and McFadden (1994).
We conjecture that when wi need not equal xi this condition generalizes Severini and Tripathiâ€™s
(2012) necessary condition for existence of a root-n consistent estimator of Î²0 .
Noting that Assumptions 1 ii) and iii) are the conditions for double robustness we have
Theorem C1: If Assumption C1 is satisfied then Ïˆ(z, Î², Î³, Î») = m(z, Î², Î³) + Î»(x)Ï(z, Î³) is
doubly robust.
It is interesting to note that Î»0 (x) satisfying Assumption C1 iii) need not be unique. When
the closure of {Î (Ï(Î³))(xi ) : Î³ âˆˆ Î“} is not all of Î› then there will exist Î»Ìƒ âˆˆ Î› such that Î»Ìƒ 6= 0
and
E[Î»Ìƒ(xi )Ï(zi , Î³)] = E[Î»Ìƒ(xi )Î (Ï(Î³))(xi )] = 0 for all Î³ âˆˆ Î“.
In that case Assumption C1 iii) will also be satisfied for Î»0 (xi ) + Î»Ìƒ(xi ). We can think of this case
as one where Î³0 is overidentified, similarly to Chen and Santos (2015). As discussed in Ichimura
and Newey (2017), the different Î»0 (xi ) would correspond to different first step estimators.
The partial robustness results of the last Section can be extended to the orthogonality condition setting of Assumption C1. Let Î›âˆ— be a closed linear subset of Î›, such as finite dimensional
linear set and let Î³ âˆ— be such that E[Î»(xi )Ï(zi , Î³ âˆ— )] = 0 for all Î» âˆˆ Î›âˆ— . Note that if Î»0 âˆˆ Î›âˆ— it
follows by Theorem C1 that
E[m(zi , Î²0 , Î³ âˆ— )] = âˆ’E[Î»0 (xi )Ï(zi , Î³ âˆ— )] = 0.
Theorem C2: If Î›âˆ— is a closed linear subset of Î›, E[Î»(xi )Ï(zi , Î³ âˆ— )] = 0 for all Î» âˆˆ Î›âˆ— ,
and Assumption C2 iii) is satisfied with Î»0 âˆˆ Î›âˆ— then
E[m(zi , Î²0 , Î³ âˆ— )] = 0.
.
41

11

Appendix D: Regularity Conditions for Plug-in Estimators

In this Appendix we formulate regularity conditions for root-n consistency and asymptotic normality of the plug-in estimator Î²Ìƒ as described in Section 2, where m(z, Î², Î³) need not be LR.
These conditions are based on Assumptions 4-6 applied to the influence adjustment Ï†(z, Î³, Î»)
corresponding to m(z, Î², Î³) and Î³Ì‚. For this purpose we treat Î»Ì‚ as any object that can approximate Î»0 (x), not just as an estimator of Î»0 .
Theorem D1: If Assumptions 4-6 are satisfied, Assumption 7 is satisfied with m(z, Î², Î³)
p
p
replacing Ïˆ(z, Î², Î³, Î»), Î²Ìƒ âˆ’â†’ Î²0 , WÌ‚ âˆ’â†’ W , M â€² W M is nonsingular, E[kÏˆ(zi , Î²0 , Î³0 , Î»0 )k2 ] < âˆ,
and
n
1 X
p
RÌ‚5 = âˆš
Ï†(zi , Î³Ì‚i , Î»Ì‚i ) âˆ’â†’ 0,
n i=1
then for â„¦ = E[Ïˆ(zi , Î²0 , Î³0, Î»0 )Ïˆ(zi , Î²0 , Î³0 , Î»0 )â€² ],
âˆš

d

n(Î²Ì‚ âˆ’ Î²0 ) âˆ’â†’ N(0, V ), V = (M â€² W M)âˆ’1 M â€² W â„¦W M(M â€² W M)âˆ’1 .
p

p

The condition RÌ‚5 âˆ’â†’ 0 was discussed in Section 7. It is interesting to note that RÌ‚5 âˆ’â†’ 0
appears to be a complicated condition that seems to depend on details of the estimator Î³Ì‚i in a
way that Assumptions 4-7 do not. In this way the regularity conditions for the LR estimator
seem to be more simple and general than those for the plug-in estimator.
Acknowledgements
Whitney Newey gratefully acknowledges support by the NSF. Helpful comments were provided by M. Cattaneo, B. Deaner, J. Hahn, M. Jansson, Z. Liao, A. Pakes, R. Moon, A. de
Paula, V. Semenova, and participants in seminars at Cambridge, Columbia, Cornell, HarvardMIT, UCL, USC, Yale, and Xiamen. B. Deaner provided capable research assistance.

REFERENCES
Ackerberg, D., X. Chen, and J. Hahn (2012): â€A Practical Asymptotic Variance Estimator
for Two-step Semiparametric Estimators,â€ The Review of Economics and Statistics 94: 481â€“498.
Ackerberg, D., X. Chen, J. Hahn, and Z. Liao (2014): â€Asymptotic Efficiency of Semiparametric Two-Step GMM,â€ The Review of Economic Studies 81: 919â€“943.
Ai, C. and X. Chen (2003): â€œEfficient Estimation of Models with Conditional Moment Restrictions Containing Unknown Functions,â€ Econometrica 71, 1795-1843.
42

Ai, C. and X. Chen (2007): â€Estimation of Possibly Misspecified Semiparametric Conditional
Moment Restriction Models with Different Conditioning Variables,â€ Journal of Econometrics
141, 5â€“43.
Ai, C. and X. Chen (2012): â€The Semiparametric Efficiency Bound for Models of Sequential
Moment Restrictions Containing Unknown Functions,â€ Journal of Econometrics 170, 442â€“457.
Andrews, D.W.K. (1994): â€œAsymptotics for Semiparametric Models via Stochastic Equicontinuity,â€ Econometrica 62, 43-72.
Athey, S., G. Imbens, and S. Wager (2017): â€Efficient Inference of Average Treatment Effects in High Dimensions via Approximate Residual Balancing,â€ Journal of the Royal Statistical
Society, Series B, forthcoming.
Bajari, P., V. Chernozhukov, H. Hong, and D. Nekipelov (2009): â€Nonparametric and
Semiparametric Analysis of a Dynamic Discrete Game,â€ working paper, Stanford.
Bajari, P., H. Hong, J. Krainer, and D. Nekipelov (2010): â€Estimating Static Models of
Strategic Interactions,â€ Journal of Business and Economic Statistics 28, 469-482.
Bang, and J.M. Robins (2005): â€Doubly Robust Estimation in Missing Data and Causal Inference Models,â€ Biometrics 61, 962â€“972.
Belloni, A., D. Chen, V. Chernozhukov, and C. Hansen (2012): â€œSparse Models and
Methods for Optimal Instruments with an Application to Eminent Domain,â€ Econometrica 80,
2369â€“2429.
Belloni, A., V. Chernozhukov, and Y. Wei (2013): â€œHonest Confidence Regions for Logistic
Regression with a Large Number of Controls,â€ arXiv preprint arXiv:1304.3969.
Belloni, A., V. Chernozhukov, and C. Hansen (2014): â€Inference on Treatment Effects
after Selection among High-Dimensional Controls,â€ The Review of Economic Studies 81, 608â€“
650.
Belloni, A., V. Chernozhukov, I. Fernandez-Val, and C. Hansen (2016): â€Program
Evaluation and Causal Inference with High-Dimensional Data,â€ Econometrica 85, 233-298.
Bera, A.K., G. Montes-Rojas, and W. Sosa-Escudero (2010): â€General Specification
Testing with Locally Misspecified Models,â€ Econometric Theory 26, 1838â€“1845.
Bickel, P.J. (1982): â€On Adaptive Estimation,â€ Annals of Statistics 10, 647-671.
Bickel, P.J. and Y. Ritov (1988): â€Estimating Integrated Squared Density Derivatives: Sharp
Best Order of Convergence Estimates,â€ SankhyaÌ„: The Indian Journal of Statistics, Series A
238, 381-393.
Bickel, P.J., C.A.J. Klaassen, Y. Ritov, and J.A. Wellner (1993): Efficient and Adaptive
Estimation for Semiparametric Models, Springer-Verlag, New York.
Bickel, P.J. and Y. Ritov (2003): â€Nonparametric Estimators Which Can Be â€Plugged-in,â€
43

Annals of Statistics 31, 1033-1053.
Bonhomme, S., and M. Weidner (2018): â€Minimizing Sensitivity to Misspecification,â€ working
paper.
Cattaneo, M.D., and M. Jansson (2017): â€Kernel-Based Semiparametric Estimators: Small
Bandwidth Asymptotics and Bootstrap Consistency,â€ Econometrica, forthcoming.
Cattaneo, M.D., M. Jansson, and X. Ma (2017): â€Two-step Estimation and Inference with
Possibly Many Included Covariates,â€ working paper.
Chamberlain, G. (1987): â€œAsymptotic Efficiency in Estimation with Conditional Moment Restrictions,â€ Journal of Econometrics 34, 1987, 305â€“334.
Chamberlain, G. (1992): â€œEfficiency Bounds for Semiparametric Regression,â€ Econometrica 60,
567â€“596.
Chen, X. and X. Shen (1997): â€œSieve Extremum Estimates for Weakly Dependent Data,â€
Econometrica 66, 289-314.
Chen, X., O.B. Linton, and I. van Keilegom (2003): â€œEstimation of Semiparametric Models
when the Criterion Function Is Not Smooth,â€ Econometrica 71, 1591-1608.
Chen, X., and Z. Liao (2015): â€Sieve Semiparametric Two-Step GMM Under Weak Dependenceâ€, Journal of Econometrics 189, 163â€“186.
Chen, X., and A. Santos (2015): â€œOveridentification in Regular Models,â€ working paper.
Chernozhukov, V., C. Hansen, and M. Spindler (2015): â€Valid Post-Selection and PostRegularization Inference: An Elementary, General Approach,â€ Annual Review of Economics 7:
649â€“688.
Chernozhukov, V., G.W. Imbens and W.K. Newey (2007): â€Instrumental Variable Identification and Estimation of Nonseparable Models,â€ Journal of Econometrics 139, 4-14.
Chernozhukov, V., D. Chetverikov, M. Demirer, E. Duflo, C. Hansen, W. Newey
(2017): â€Double/Debiased/Neyman Machine Learning of Treatment Effects,â€ American Economic Review Papers and Proceedings 107, 261-65.
Chernozhukov, V., D. Chetverikov, M. Demirer, E. Duflo, C. Hansen, W. Newey,
J. Robins (2018): â€Debiased/Double Machine Learning for Treatment and Structural Parameters,Econometrics Journal 21, C1-C68.
Chernozhukov, V., J.A. Hausman, and W.K. Newey (2018): â€Demand Analysis with Many
Prices,â€ working paper, MIT.
Chernozhukov, V., W.K. Newey, J. Robins (2018): â€Double/De-Biased Machine Learning
Using Regularized Riesz Representers,â€ arxiv.
Escanciano, J-C., D. Jacho-ChaÂ´vez, and A. Lewbel (2016): â€œIdentification and Estimation of Semiparametric Two Step Modelsâ€, Quantitative Economics 7, 561-589.
44

Farrell, M. (2015): â€Robust Inference on Average Treatment Effects with Possibly More Covariates than Observations,â€ Journal of Econometrics 189, 1â€“23.
Firpo, S. and C. Rothe (2017): â€Semiparametric Two-Step Estimation Using Doubly Robust
Moment Conditions,â€ working paper.
Graham, B.W. (2011): â€Efficiency Bounds for Missing Data Models with Semiparametric Restrictions,â€ Econometrica 79, 437â€“452.
Hahn, J. (1998): â€On the Role of the Propensity Score in Efficient Semiparametric Estimation
of Average Treatment Effects,â€ Econometrica 66, 315-331.
Hahn, J. and G. Ridder (2013): â€Asymptotic Variance of Semiparametric Estimators With
Generated Regressors,â€ Econometrica 81, 315-340.
Hahn, J. and G. Ridder (2016): â€œThree-stage Semi-Parametric Inference: Control Variables
and Differentiability,â€ working paper.â€
Hahn, J., Z. Liao, and G. Ridder (2016): â€Nonparametric Two-Step Sieve M Estimation and
Inference,â€ working paper, UCLA.
Hasminskii, R.Z. and I.A. Ibragimov (1978): â€On the Nonparametric Estimation of Functionals,â€ Proceedings of the 2nd Prague Symposium on Asymptotic Statistics, 41-51.
Hausman, J.A., and W.K. Newey (2016): â€Individual Heterogeneity and Average Welfare,â€
Econometrica 84, 1225-1248.
Hausman, J.A., and W.K. Newey (2017): â€Nonparametric Welfare Analysis,â€ Annual Review
of Economics 9, 521â€“546.
Hirano, K., G. Imbens, and G. Ridder (2003): â€Efficient Estimation of Average Treatment
Effects Using the Estimated Propensity Score,â€ Econometrica 71: 1161â€“1189.
Hotz, V.J. and R.A. Miller (1993): â€Conditional Choice Probabilities and the Estimation of
Dynamic Models,â€ Review of Economic Studies 60, 497-529.
Huber, P. (1981): Robust Statistics, New York: Wiley.
Ichimura, H. (1993): â€Estimation of Single Index Models,â€ Journal of Econometrics 58, 71-120.
Ichimura, H., and S. Lee (2010): â€œCharacterization of the Asymptotic Distribution of Semiparametric M-Estimators,â€ Journal of Econometrics 159, 252â€“266.
Ichimura, H. and W.K. Newey (2017): â€The Influence Function of Semiparametric Estimators,â€ CEMMAP Working Paper, CWP06/17.
Kandasamy, K., A. Krishnamurthy, B. PÂ´oczos, L. Wasserman, J.M. Robins (2015):
â€Influence Functions for Machine Learning: Nonparametric Estimators for Entropies, Divergences and Mutual Informations,â€ arxiv.
Lee, Lung-fei (2005): â€œA C(Î±)-type Gradient Test in the GMM Approach,â€ working paper.

45

Luenberger, D.G. (1969): Optimization by Vector Space Methods, New York: Wiley.
Murphy, K.M. and R.H. Topel (1985): â€Estimation and Inference in Two-Step Econometric
Models,â€ Journal of Business and Economic Statistics 3, 370-379.
Newey, W.K. (1984): â€A Method of Moments Interpretation of Sequential Estimators,â€ Economics Letters 14, 201-206.
Newey, W.K. (1990): â€Semiparametric Efficiency Bounds,â€ Journal of Applied Econometrics 5,
99-135.
Newey, W.K. (1991): â€Uniform Convergence in Probability and Stochastic Equicontinuity,â€
Econometrica 59, 1161-1167.
Newey, W.K. (1994a): â€The Asymptotic Variance of Semiparametric Estimators,â€ Econometrica
62, 1349-1382.
Newey, W.K. (1994b): â€Kernel Estimation of Partial Means and a General Variance Estimator,â€
Econometric Theory 10, 233-253.
Newey, W.K. (1997): â€Convergence Rates and Asymptotic Normality for Series Estimators,â€
Journal of Econometrics 79, 147-168.
Newey, W.K. (1999): â€Consistency of Two-Step Sample Selection Estimators Despite Misspecification of Distribution,â€ Economics Letters 63, 129-132.
Newey, W.K., and D. McFadden (1994): â€œLarge Sample Estimation and Hypothesis Testing,â€
in Handbook of Econometrics, Vol. 4, ed. by R. Engle, and D. McFadden, pp. 2113-2241. North
Holland.
Newey, W.K., and J.L. Powell (1989): â€Instrumental Variable Estimation of Nonparametric
Models,â€ presented at Econometric Society winter meetings, 1988.
Newey, W.K., and J.L. Powell (2003): â€Instrumental Variable Estimation of Nonparametric
Models,â€ Econometrica 71, 1565-1578.
Newey, W.K., F. Hsieh, and J.M. Robins (1998): â€œUndersmoothing and Bias Corrected
Functional Estimation,â€ MIT Dept. of Economics working paper 72, 947-962.
Newey, W.K., F. Hsieh, and J.M. Robins (2004): â€œTwicing Kernels and a Small Bias Property
of Semiparametric Estimators,â€ Econometrica 72, 947-962.
Newey, W.K., and J. Robins (2017): â€Cross Fitting and Fast Remainder Rates for Semiparametric Estimation,â€ arxiv.
Neyman, J. (1959): â€œOptimal Asymptotic Tests of Composite Statistical Hypotheses,â€ Probability
and Statistics, the Harald Cramer Volume, ed., U. Grenander, New York, Wiley.
Pfanzagl, J., and W. Wefelmeyer (1982): â€Contributions to a General Asymptotic Statistical Theory. Springer Lecture Notes in Statistics.

46

Pakes, A. and G.S. Olley (1995): â€A Limit Theorem for a Smooth Class of Semiparametric
Estimators,â€ Journal of Econometrics 65, 295-332.
Powell, J.L., J.H. Stock, and T.M. Stoker (1989): â€Semiparametric Estimation of Index
Coefficients,â€ Econometrica 57, 1403-1430.
Robins, J.M., A. Rotnitzky, and L.P. Zhao (1994): â€Estimation of Regression Coefficients
When Some Regressors Are Not Always Observed,â€ Journal of the American Statistical Association 89: 846â€“866.
Robins, J.M. and A. Rotnitzky (1995): â€Semiparametric Efficiency in Multivariate Regression
Models with Missing Data,â€ Journal of the American Statistical Association 90:122â€“129.
Robins, J.M., A. Rotnitzky, and L.P. Zhao (1995): â€Analysis of Semiparametric Regression
Models for Repeated Outcomes in the Presence of Missing Data,â€ Journal of the American
Statistical Association 90,106â€“121.
Robins, J.M.,and A. Rotnitzky (2001): Comment on â€œSemiparametric Inference: Question
and an Answer Likelihoodâ€ by P.A. Bickel and J. Kwon, Statistica Sinica 11, 863-960.
Robins, J.M., A. Rotnitzky, and M. van der Laan (2000): â€Comment on â€™On Profile
Likelihoodâ€™ by S. A. Murphy and A. W. van der Vaart, Journal of the American Statistical
Association 95, 431-435.
Robins, J., M. Sued, Q. Lei-Gomez, and A. Rotnitzky (2007): â€Comment: Performance of
Double-Robust Estimators When Inverse Probabilityâ€™ Weights Are Highly Variable,â€ Statistical
Science 22, 544â€“559.
Robins, J.M., L. Li, E. Tchetgen, and A. van der Vaart (2008): â€Higher Order Influence
Functions and Minimax Estimation of Nonlinear Functionals,â€ IMS Collections Probability and
Statistics: Essays in Honor of David A. Freedman, Vol 2, 335-421.
Robins, J.M., L. Li, R. Mukherjee, E. Tchetgen, and A. van der Vaart (2017): â€Higher
Order Estimating Equations for High-Dimensional Models,â€ Annals of Statistics, forthcoming.
Robinson, P.M. (1988): â€â€˜Root-N-consistent Semiparametric Regression,â€ Econometrica 56, 931954.
Rust, J. (1987): â€Optimal Replacement of GMC Bus Engines: An Empirical Model of Harold
Zurcher,â€ Econometrica 55, 999-1033.
Santos, A. (2011): â€Instrumental Variable Methods for Recovering Continuous Linear Functionals,â€ Journal of Econometrics, 161, 129-146.
Scharfstein D.O., A. Rotnitzky, and J.M. Robins (1999): Rejoinder to â€œAdjusting For
Nonignorable Drop-out Using Semiparametric Non-response Models,â€ Journal of the American
Statistical Association 94, 1135-1146.
Severini, T. and G. Tripathi (2006): â€Some Identification Issues in Nonparametric Linear
47

Models with Endogenous Regressors,â€ Econometric Theory 22, 258-278.
Severini, T. and G. Tripathi (2012): â€Efficiency Bounds for Estimating Linear Functionals
of Nonparametric Regression Models with Endogenous Regressors,â€ Journal of Econometrics
170, 491-498.
Schick, A. (1986): â€On Asymptotically Efficient Estimation in Semiparametric Models,â€ Annals
of Statistics 14, 1139-1151.
Stoker, T. (1986): â€Consistent Estimation of Scaled Coefficients,â€ Econometrica 54, 1461-1482.
Tamer, E. (2003): â€Incomplete Simultaneous Discrete Response Model with Multiple Equilibria,â€
Review of Economic Studies 70, 147-165.
van der Laan, M. and Rubin (2006): â€Targeted Maximum Likelihood Learning,â€ U.C. Berkeley
Division of Biostatistics Working Paper Series. Working Paper 213.
van der Vaart, A.W. (1991): â€œOn Differentiable Functionals,â€ The Annals of Statistics, 19,
178-204.
van der Vaart, A.W. (1998): Asymptotic Statistics, Cambride University Press, Cambridge,
England.
van der Vaart, A.W. (2014): â€Higher Order Tangent Spaces and Influence Functions,â€ Statistical Science 29, 679â€“686.
Wooldridge, J.M. (1991): â€œOn the Application of Robust, Regression-Based Diagnostics to
Models of Conditional Means and Conditional Variances,â€ Journal of Econometrics 47, 5-46.

48

