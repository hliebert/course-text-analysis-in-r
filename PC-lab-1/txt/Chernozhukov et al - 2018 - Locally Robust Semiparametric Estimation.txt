arXiv:1608.00033v2 [math.ST] 31 May 2018

Locally Robust Semiparametric Estimation
Victor Chernozhukov

Juan Carlos Escanciano

Hidehiko Ichimura

MIT

Indiana University

University of Tokyo

Whitney K. Newey
MIT

James M. Robins
Harvard University

April 2018
Abstract
We give a general construction of debiased/locally robust/orthogonal (LR) moment
functions for GMM, where the derivative with respect to first step nonparametric estimation is zero and equivalently first step estimation has no effect on the influence function.
This construction consists of adding an estimator of the influence function adjustment term
for first step nonparametric estimation to identifying or original moment conditions. We
also give numerical methods for estimating LR moment functions that do not require an
explicit formula for the adjustment term.
LR moment conditions have reduced bias and so are important when the first step is
machine learning. We derive LR moment conditions for dynamic discrete choice based on
first step machine learning estimators of conditional choice probabilities.
We provide simple and general asymptotic theory for LR estimators based on sample
splitting. This theory uses the additive decomposition of LR moment conditions into an
identifying condition and a first step influence adjustment. Our conditions require only
mean square consistency and a few (generally either one or two) readily interpretable rate
conditions.
LR moment functions have the advantage of being less sensitive to first step estimation.
Some LR moment functions are also doubly robust meaning they hold if one first step
is incorrect. We give novel classes of doubly robust moment functions and characterize
double robustness. For doubly robust estimators our asymptotic theory only requires one
rate condition.
Keywords: Local robustness, orthogonal moments, double robustness, semiparametric
estimation, bias, GMM.
JEL classification: C13; C14; C21; D24

1

1

Introduction

There are many economic parameters that depend on nonparametric or large dimensional first
steps. Examples include dynamic discrete choice, games, average consumer surplus, and treatment effects. This paper shows how to construct moment functions for GMM estimators that are
debiased/locally robust/orthogonal (LR), where moment conditions have a zero derivative with
respect to the first step. We show that LR moment functions can be constructed by adding the
influence function adjustment for first step estimation to the original moment functions. This
construction can also be interpreted as a decomposition of LR moment functions into identifying
moment functions and a first step influence function term. We use this decomposition to give
simple and general conditions for root-n consistency and asymptotic normality, with different
properties being assumed for the identifying and influence function terms. The conditions are
easily interpretable mean square consistency and second order remainder conditions based on
estimated moments that use cross-fitting (sample splitting). We also give numerical estimators
of the influence function adjustment.
LR moment functions have several advantages. LR moment conditions bias correct in a way
that eliminates the large biases from plugging in first step machine learning estimators found
in Belloni, Chernozhukov, and Hansen (2014). LR moment functions can be used to construct
debiased/double machine learning (DML) estimators, as in Chernozhukov et al. (2017, 2018).
We illustrate by deriving LR moment functions for dynamic discrete choice estimation based
on conditional choice probabilities. We provide a DML estimator for dynamic discrete choice that
uses first step machine learning of conditional choice probabilities. We find that it performs well
in a Monte Carlo example. Such structural models provide a potentially important application
of DML, because of potentially high dimensional state spaces. Adding the first step influence
adjustment term provides a general way to construct LR moment conditions for structural
models so that machine learning can be used for first step estimation of conditional choice
probabilities, state transition distributions, and other unknown functions on which structural
estimators depend.
LR moment conditions also have the advantage of being relatively insensitive to small variation away from the first step true function. This robustness property is appealing in many
settings where it may be difficult to get the first step completely correct. Many interesting and
useful LR moment functions have the additional property that they are doubly robust (DR),
meaning moment conditions hold when one first step is not correct. We give novel classes of DR
moment conditions, including for average linear functionals of conditional expectations and probability densities. The construction of adding the first step influence function adjustment to an
identifying moment function is useful to obtain these moment conditions. We also give necessary
and sufficient conditions for a large class of moment functions to be DR. We find DR moments
have simpler and more general conditions for asymptotic normality, which helps motivate our
2

consideration of DR moment functions as special cases of LR ones. LR moment conditions also
help minimize sensitivity to misspecification as in Bonhomme and Weidner (2018).
LR moment conditions have smaller bias from first step estimation. We show that they have
the small bias property of Newey, Hsieh, and Robins (2004), that the bias of the moments is of
smaller order than the bias of the first step. This bias reduction leads to substantial improvements in finite sample properties in many cases relative to just using the original moment conditions. For dynamic discrete choice we find large bias reductions, moderate variance increases
and even reductions in some cases, and coverage probabilities substantially closer to nominal.
For machine learning estimators of the partially linear model, Chernozhukov et al. (2017, 2018)
found bias reductions so large that the LR estimator is root-n consistent but the estimator based
on the original moment condition is not. Substantial improvements were previously also found
for density weighted averages by Newey, Hsieh, and Robins (2004, NHR). The twicing kernel
estimators in NHR are numerically equal to LR estimators based on the original (before twicing)
kernel, as shown in Newey, Hsieh, Robins (1998), and the twicing kernel estimators were shown
to have smaller mean square error in large samples. Also, a Monte Carlo example in NHR finds
that the mean square error (MSE) of the LR estimator has a smaller minimum and is flatter as
a function of bandwidth than the MSE of Powell, Stock, and Stoker’s (1989) density weighted
average derivative estimator. We expect similar finite sample improvements from LR moments
in other cases.
LR moment conditions have appeared in earlier work. They are semiparametric versions of
Neyman (1959) C-alpha test scores for parametric models. Hasminskii and Ibragimov (1978)
suggested LR estimation of functionals of a density and argued for their advantages over plug-in
estimators. Pfanzagl and Wefelmeyer (1981) considered using LR moment conditions for improving the asymptotic efficiency of functionals of distribution estimators. Bickel and Ritov
(1988) gave a LR estimator of the integrated squared density that attains root-n consistency
under minimal conditions. The Robinson (1988) semiparametric regression and Ichimura (1993)
index regression estimators are LR. Newey (1990) showed that LR moment conditions can be
obtained as residuals from projections on the tangent set in a semiparametric model. Newey
(1994a) showed that derivatives of an objective function where the first step has been ”concentrated out” are LR, including the efficient score of a semiparametric model. NHR (1998, 2004)
gave estimators of averages that are linear in density derivative functionals with remainder rates
that are as fast as those in Bickel and Ritov (1988). Doubly robust moment functions have
been constructed by Robins, Rotnitzky, and Zhao (1994, 1995), Robins and Rotnitzky (1995),
Scharfstein, Rotnitzky, and Robins (1999), Robins, Rotnitzky, and van der Laan (2000), Robins
and Rotnitzky (2001), Graham (2011), and Firpo and Rothe (2017). They are widely used for
estimating treatment effects, e.g. Bang and Robins (2005). Van der Laan and Rubin (2006) developed targeted maximum likelihood to obtain a LR estimating equation based on the efficient

3

influence function of a semiparametric model. Robins et al. (2008, 2017) showed that efficient
influence functions are LR, characterized some doubly robust moment conditions, and developed
higher order influence functions that can reduce bias. Belloni, Chernozhukov, and Wei (2013),
Belloni, Chernozhukov, and Hansen (2014), Farrell (2015), Kandasamy et al. (2015), Belloni,
Chernozhukov, Fernandez-Val, and Hansen (2016), and Athey, Imbens, and Wager (2017) gave
LR estimators with machine learning first steps in several specific contexts.
A main contribution of this paper is the construction of LR moment conditions from any
moment condition and first step estimator that can result in a root-n consistent estimator of
the parameter of interest. This construction is based on the limit of the first step when a data
observation has a general distribution that allows for misspecification, similarly to Newey (1994).
LR moment functions are constructed by adding to identifying moment functions the influence
function of the true expectation of the identifying moment functions evaluated at the first step
limit, i.e. by adding the influence function term that accounts for first step estimation. The
addition of the influence adjustment ”partials out” the first order effect of the first step on the
moments. This construction of LR moments extends those cited above for first step density and
distribution estimators to any first step, including instrumental variable estimators. Also, this
construction is estimator based rather than model based as in van der Laan and Rubin (2006)
and Robins et al. (2008, 2017). The construction depends only on the moment functions and
the first step rather than on a semiparametric model. Also, we use the fundamental Gateaux
derivative definition of the influence function to show LR rather than an embedding in a regular
semiparametric model.
The focus on the functional that is the true expected moments evaluated at the first step
limit is the key to this construction. This focus should prove useful for constructing LR moments
in many setting, including those where it has already been used to find the asymptotic variance
of semiparametric estimators, such as Newey (1994a), Pakes and Olley (1995), Hahn (1998), Ai
and Chen (2003), Hirano, Imbens, and Ridder (2003), Bajari, Hong, Krainer, and Nekipelov
(2010), Bajari, Chernozhukov, Hong, and Nekipelov (2009), Hahn and Ridder (2013, 2016), and
Ackerberg, Chen, Hahn, and Liao (2014), Hahn, Liao, and Ridder (2016). One can construct
LR moment functions in each of these settings by adding the first step influence function derived
for each case as an adjustment to the original, identifying moment functions.
Another contribution is the development of LR moment conditions for dynamic discrete
choice. We derive the influence adjustment for first step estimation of conditional choice probabilities as in Hotz and Miller (1993). We find encouraging Monte Carlo results when various
machine learning methods are used to construct the first step. We also give LR moment functions
for conditional moment restrictions based on orthogonal instruments.
An additional contribution is to provide general estimators of the influence adjustment term
that can be used to construct LR moments without knowing their form. These methods estimate

4

the adjustment term numerically, thus avoiding the need to know its form. It is beyond the scope
of this paper to develop machine learning versions of these numerical estimators. Such estimators
are developed by Chernozhukov, Newey, and Robins (2018) for average linear functionals of
conditional expectations.
Further contributions include novel classes of DR estimators, including linear functionals of
nonparametric instrumental variables and density estimators, and a characterization of (necessary and sufficient conditions for) double robustness. We also give related, novel partial
robustness results where original moment conditions are satisfied even when the first step is not
equal to the truth.
A main contribution is simple and general asymptotic theory for LR estimators that use
cross-fitting in the construction of the average moments. This theory is based on the structure
of LR moment conditions as an identifying moment condition depending on one first step plus
an influence adjustment that can depend on an additional first step. We give a remainder
decomposition that leads to mean square consistency conditions for first steps plus a few readily
interpretable rate conditions. For DR estimators there is only one rate condition, on a product
of sample remainders from two first step estimators, leading to particularly simple conditions.
This simplicity motivates our inclusion of results for DR estimators. This asymptotic theory
is also useful for existing moment conditions that are already known to be LR. Whenever the
moment condition can be decomposed into an identifying moment condition depending on one
first step and an influence function term that may depend on two first steps the simple and
general regularity conditions developed here will apply.
LR moments reduce that smoothing bias that results from first step nonparametric estimation
relative to original moment conditions. There are other sources of bias arising from nonlinearity
of moment conditions in the first step and the empirical distribution. Cattaneo and Jansson
(2017) and Cattaneo, Jansson, and Ma (2017) give useful bootstrap and jackknife methods that
reduce nonlinearity bias. Newey and Robins (2017) show that one can also remove this bias by
cross fitting in some settings. We allow for cross-fitting in this paper.
Section 2 describes the general construction of LR moment functions for semiparametric
GMM. Section 3 gives LR moment conditions for dynamic discrete choice. Section 4 shows how
to estimate the first step influence adjustment. Section 5 gives novel classes of DR moment
functions and characterizes double robustness. Section 6 gives an orthogonal instrument construction of LR moments based on conditional moment restrictions. Section 7 provides simple
and general asymptotic theory for LR estimators.

5

2

Locally Robust Moment Functions

The subject of this paper is GMM estimators of parameters where the sample moment functions
depend on a first step nonparametric or large dimensional estimator. We refer to these estimators
as semiparametric. We could also refer to them as GMM where first step estimators are “plugged
in” the moments. This terminology seems awkward though, so we simply refer to them as
semiparametric GMM estimators. We denote such an estimator by β̂, which is a function of the
data z1 , ..., zn where n is the number of observations. Throughout the paper we will assume that
the data observations zi are i.i.d. We denote the object that β̂ estimates as β0 , the subscript
referring to the parameter value under the distribution F0 of zi .
To describe semiparametric GMM let m(z, β, γ) denote an r × 1 vector of functions of the
data observation z, parameters of interest β, and a function γ that may be vector valued. The
function γ can depend on β and z through those arguments of m. Here the function γ represents
some possible first step, such as an estimator, its limit, or a true function. A GMM estimator
can be based on a moment condition where β0 is the unique parameter vector satisfying
E[m(zi , β0 , γ0 )] = 0,

(2.1)

and γ0 is the true γ. We assume that this moment condition identifies β. Let γ̂ denote some
first step estimator of γ0 . Plugging in γ̂ to obtain m(zi , β, γ̂) and averaging over zi results in the
P
estimated sample moments m̂(β) = ni=1 m(zi , β, γ̂)/n. For Ŵ a positive semi-definite weighting
matrix a semiparametric GMM estimator is
β̃ = arg min m̂(β)T Ŵ m̂(β),
β∈B

where AT denotes the transpose of a matrix A and B is the parameter space for β. Such
estimators have been considered by, e.g. Andrews (1994), Newey (1994a), Newey and McFadden
(1994), Pakes and Olley (1995), Chen and Liao (2015), and others.
Locally robust (LR) moment functions can be constructed by adding the influence function adjustment for the first step estimator γ̂ to the identifying or original moment functions
m(z, β, γ). To describe this influence adjustment let γ(F ) denote the limit of γ̂ when zi has distribution F, where we restrict F only in that γ(F ) exists and possibly other regularity conditions
are satisfied. That is, γ(F ) is the limit of γ̂ under possible misspecification, similar to Newey
(1994). Let G be some other distribution and Fτ = (1 − τ )F0 + τ G for 0 ≤ τ ≤ 1, where F0
denotes the true distribution of zi . We assume that G is chosen so that γ(Fτ ) is well defined for
τ > 0 small enough and possibly other regularity conditions are satisfied, similarly to Ichimura
and Newey (2017). The influence function adjustment will be the function φ(z, β, γ, λ) such that
for all such G,
Z
d
E[m(zi , β, γ(Fτ ))] = φ(z, β, γ0, λ0 )G(dz), E[φ(zi , β, γ0, λ0 )] = 0,
(2.2)
dτ
6

where λ is an additional nonparametric or large dimensional unknown object on which φ(z, β, γ, λ)
depends and the derivative is from the right (i.e. for positive values of τ ) and at τ = 0.
This equation is the well known definition of the influence function φ(z, β, γ0 , λ0 ) of µ(F ) =
E[m(zi , β, γ(F ))] as the Gateaux derivative of µ(F ), e.g. Huber (1981). The restriction of G so
that γ(Fτ ) exists allows φ(z, β, γ0 , λ0 ) to be the influence function when γ(F ) is only well defined for certain types of distributions, such as when γ(F ) is a conditional expectation or density.
The function φ(z, β, γ, λ) will generally exist when E[m(zi , β, γ(F ))] has a finite semiparametric
variance bound. Also φ(z, β, γ, λ) will generally be unique because we are not restricting G very
much. Also, note that φ(z, β, γ, λ) will be the influence adjustment term from Newey (1994a),
as discussed in Ichimura and Newey (2017).
LR moment functions can be constructed by adding φ(z, β, γ, λ) to m(z, β, γ) to obtain new
moment functions
ψ(z, β, γ, λ) = m(z, β, γ) + φ(z, β, γ, λ).
(2.3)
Let λ̂ be a nonparametric or large dimensional estimator having limit λ(F ) when zi has distriP
bution F, with λ(F0 ) = λ0 . Also let ψ̂(β) = ni=1 ψ(zi , β, γ̂, λ̂)/n. A LR GMM estimator can be
obtained as
(2.4)
β̂ = arg min ψ̂(β)T Ŵ ψ̂(β).
β∈B

√
As usual a choice of Ŵ that minimizes the asymptotic variance of n(β̂ −β0 ) will be a consistent
√
estimator of the inverse of the asymptotic variance Ω of nψ̂(β0 ). As we will further discuss,
ψ(z, β, γ, λ) being LR will mean that the estimation of γ and λ does not affect Ω, so that Ω =
E[ψ(zi , β0 , γ0 , λ0 )ψ(zi , β0 , γ0 , λ0 )T ]. An optimal Ŵ also gives an efficient estimator in the wider
sense shown in Ackerberg, Chen, Hahn, and Liao (2014), making β̂ efficient in a semiparametric
model where the only restrictions imposed are equation (2.1).
The LR property we consider is that the derivative of the true expectation of the moment
function with respect to the first step is zero, for a Gateaux derivative like that for the influence
function in equation (2.2). Define Fτ = (1 − τ )F0 + τ G as before where G is such that both
γ(Fτ ) and λ(Fτ ) are well defined. The LR property is that for all G as specified,
d
E[ψ(zi , β, γ(Fτ ), λ(Fτ ))] = 0.
dτ

(2.5)

Note that this condition is the same as that of Newey (1994a) for the presence of γ̂ an λ̂ to
have no effect on the asymptotic distribution, when each Fτ is a regular parametric submodel.
√
Consequently, the asymptotic variance of nψ̂(β0 ) will be Ω as in the last paragraph.
To show LR of the moment functions ψ(z, β, γ, λ) = m(z, β, γ) + φ(z, β, γ, λ) from equation
(2.3) we use the fact that the second, zero expectation condition in equation (2.2) must hold for
all possible true distributions. For any given β define µ(F ) = E[m(zi , β, γ(F ))] and φ(z, F ) =
φ(z, β, γ(F ), λ(F )).
7

R
R
Theorem 1: If i) dµ(Fτ )/dτ = φ(z, F0 )G(dz), ii) φ(z, Fτ )Fτ (dz) = 0 for all τ ∈ [0, τ̄ ),
R
R
and iii) φ(z, Fτ )F0 (dz) and φ(z, Fτ )G(dz) are continuous at τ = 0 then
d
dµ(Fτ )
E[φ(zi , Fτ )] = −
.
dτ
dτ

(2.6)

The proofs of this result and others are given in Appendix B. Assumptions i) and ii) of
Theorem 1 require that both parts of equation (2.2) hold with the second, zero mean condition
being satisfied when Fτ is the true distribution. Assumption iii) is a regularity condition. The
LR property follows from Theorem 1 by adding dµ(Fτ )/dτ to both sides of equation (2.6) and
noting that the sum of derivatives is the derivative of the sum. Equation (2.6) shows that the
addition of φ(z, β, γ, λ) ”partials out” the effect of the first step γ on the moment by ”cancelling”
the derivative of the identifying moment E[m(zi , β, γ(Fτ ))] with respect to τ . This LR result for
ψ(z, β, γ, λ) differs from the literature in its Gateaux derivative formulation and in the fact that
it is not a semiparametric influence function but is the hybrid sum of an identifying moment
function m(z, β, γ) and an influence function adjustment φ(z, β, γ, λ).
Another zero derivative property of LR moment functions is useful. If the sets Γ and Λ of
possible limits γ(F ) and λ(F ), respectively, are linear, γ(F ) and λ(F ) can vary separately from
one another, and certain functional differentiability conditions hold then LR moment functions
will have the property that for any γ ∈ Γ, λ ∈ Λ, and ψ̄(γ, λ) = E[ψ(zi , β0 , γ, λ)],
∂
∂
ψ̄((1 − τ )γ0 + τ γ, λ0 ) = 0, ψ̄(γ0 , (1 − τ )λ0 + τ λ) = 0.
∂τ
∂τ

(2.7)

That is, the expected value of the LR moment function will have a zero Gateaux derivative
with respect to each of the first steps γ and λ. This property will be useful for several results
to follow. Under still stronger smoothness conditions this zero derivative condition will result
in the existence of a constant C such that for a function norm k·k,
ψ̄(γ, λ0 ) ≤ C kγ − γ0 k2 , ψ̄(γ0 , λ) ≤ C kλ − λ0 k2 ,

(2.8)

when kγ − γ0 k and kλ − λ0 k are small enough. In Appendix B we give smoothness conditions
that are sufficient for LR to imply equations (2.7) and (2.8). When formulating regularity
conditions for particular moment functions and first step estimators it may be more convenient
to work directly with equation (2.7) and/or (2.8).
The approach of constructing LR moment functions by adding the influence adjustment
differs from the model based approach of using an efficient influence function or score for a
semiparametric model as moment functions . The approach here is estimator based rather than
model based. The influence adjustment φ(z, β, γ, λ) is determined by the limit γ(F ) of the first
step estimator γ̂ and the moment functions m(z, β, γ) rather than by some underlying semiparametric model. This estimator based approach has proven useful for deriving the influence
8

function of a wide variety of semiparametric estimators, as mentioned in the Introduction. Here
this estimator based approach provides a general way to construct LR moment functions. For
any moment function m(z, β, γ) and first step estimator γ̂ a corresponding LR estimator can be
constructed as in equations (2.3) and (2.4).
The addition of φ(z, β, γ, λ) does not affect identification of β because φ(z, β, γ0, λ0 ) has
expectation zero for any β and true F0 . Consequently, the LR GMM estimator will have the
√
same asymptotic variance as the original GMM estimator β̃ when n(β̃ − β0 ) is asymptotically
normal, under appropriate regularity conditions. The addition of φ(z, β, γ, λ) will change other
properties of the estimator. As discussed in Chernozhukov et al. (2017, 2018), it can even
remove enough bias so that the LR estimator is root-n consistent and the original estimator is
not.
If Fτ was modified so that τ is a function of a smoothing parameter, e.g. a bandwidth,
and τ gives the magnitude of the smoothing bias of γ(Fτ ), then equation (2.5) is a small bias
condition, equivalent to
E[ψ(zi , β0 , γ(Fτ ), λ(Fτ ))] = o(τ ).
Here E[ψ(zi , β0 , γ(Fτ ), λ(Fτ ))] is a bias in the moment condition resulting from smoothing that
shrinks faster than τ. In this sense LR GMM estimators have the small bias property considered
in NHR. This interpretation is also one sense in which LR GMM is ”debiased.”
In some cases the original moment functions m(z, β, γ) are already LR and the influence
adjustment will be zero. An important class of moment functions that are LR are those where
m(z, β, γ) is the derivative with respect to β of an objective function where nonparametric
parts have been concentrated out. That is, suppose that there is a function q(z, β, ζ) such that
m(z, β, γ) = ∂q(z, β, ζ(β))/∂β where ζ(β) = arg maxζ E[q(zi , β, ζ)], where γ includes ζ(β) and
possibly additional functions. Proposition 2 of Newey (1994a) and Lemma 2.5 of Chernozhukov
et al. (2018) then imply that m(z, β, γ) will be LR. This class of moment functions includes
various partially linear regression models where ζ represents a conditional expectation. It also
includes the efficient score for a semiparametric model, Newey (1994a, pp. 1358-1359).
Cross fitting, also known as sample splitting, has often been used to improve the properties
of semiparametric and machine learning estimators; e.g. see Bickel (1982), Schick (1986), and
Powell, Stock, and Stoker (1989). Cross fitting removes a source of bias and can be used to
construct estimators with remainder terms that converge to zero as fast as is known to be
possible, as in NHR and Newey and Robins (2017). Cross fitting is also useful for double
machine learning estimators, as outlined in Chernozhukov et al. (2017, 2018). For these reasons
we allow for cross-fitting, where sample moments have the form
n

1X
ψ(zi , β, γ̂i , λ̂i ),
ψ̂(β) =
n i=1
9

with γ̂i and λ̂i being formed from observations other than the ith . This kind of cross fitting
removes an ”own observation” bias term and is useful for showing root-n consistency when γ̂i
and λ̂i are machine learning estimators.
One version of cross-fitting with good properties in examples in Chernozhukov et al. (2018)
can be obtained by partitioning the observation indices into L groups Iℓ , (ℓ = 1, ..., L), forming
γ̂ℓ and λ̂ℓ from observations not in Iℓ , and constructing
L

1 XX
ψ̂(β) =
ψ(zi , β, γ̂ℓ , λ̂ℓ ).
n ℓ=1 i∈I

(2.9)

ℓ

Further bias reductions may be obtained in some cases by using different sets of observations for
computing γ̂ℓ and λ̂ℓ , leading to remainders that converge to zero as rapidly as known possible
in interesting cases; see Newey and Robins (2017). The asymptotic theory of Section 7 focuses
on this kind of cross fitting.
As an example we consider a bound on average equivalent variation. Let γ0 (x) denote the
conditional expectation of quantity q conditional on x = (pT , y) where p = (p1 , pT2 )T is a vector
of prices and y is income. The object of interest is a bound on average equivalent variation for
a price change from p̄1 to p̌1 given by
Z
β0 = E[ ℓ(p1 , yi )γ0 (p1 , p2i , yi )dp1 ], ℓ(p1 , y) = w(y)1(p̄1 ≤ p1 ≤ p̌1 ) exp{−B(p1 − p̄1 )}],
where w(y) is a function of income and B a constant. It follows by Hausman and Newey
(2016) that if B is a lower (upper) bound on the income effect for all individuals then β0 is
an upper (lower) bound on the equivalent variation for a price change from p̄1 to p̌1 , averaged
over heterogeneity, other prices p2i , and income yi . The function w(y) allows for averages over
income in specific ranges, as in Hausman and Newey (2017).
A moment function that could be used to estimate β0 is
Z
m(z, β, γ) = ℓ(p1 , y)γ(p1, p2 , y)dp1 − β.
Note that
Z
E[m(zi , β0 , γ)] + β0 = E[ ℓ(p1 , yi )γ(p1 , p2i , yi )dp1 ] = E[λ0 (xi )γ(xi )], λ0 (x) =

ℓ(p1 , y)
,
f0 (p1 |p2 , y)

where f0 (p1 |p2 , y) is the conditional pdf of p1i given p2i and yi . Then by Proposition 4 of Newey
(1994) the influence function adjustment for any nonparametric estimator γ̂(x) of E[qi |xi = x]
is
φ(z, β, γ, λ) = λ(x)[q − γ(x)].
Here λ0 (x) is an example of an additional unknown function that is included in φ(z, β, γ, λ) but
not in the original moment functions m(z, β, γ). Let γ̂i (x) be an estimator of E[qi |xi = x] that
10

can depend on i and λ̂i (x) be an estimator of λ0 (x), such as fˆi (p1 |p2 , y)−1ℓ(p1 , y) for an estimator
fˆi (p1 |p2 , y). The LR estimator obtained by solving ψ̂(β) = 0 for m(z, β, γ) and φ(z, β, γ, λ) as
above is

n Z
1X
ℓ(p1 , yi )γ̂i (p1 , p2i , yi )dp1 + λ̂i (xi )[qi − γ̂i (xi )] .
(2.10)
β̂ =
n i=1

3

Machine Learning for Dynamic Discrete Choice

A challenging problem when estimating dynamic structural models is the dimensionality of
state spaces. Machine learning addresses this problem via model selection to estimate high
dimensional choice probabilities. These choice probabilities estimators can then be used in
conditional choice probability (CCP) estimators of structural parameters, following Hotz and
Miller (1993). In order for CCP estimators based on machine learning to be root-n consistent
they must be based on orthogonal (i.e. LR) moment conditions, see Chernozhukov et al. (2017,
2018). Adding the adjustment term provides the way to construct LR moment conditions from
known moment conditions for CCP estimators. In this Section we do so for the Rust’s (1987)
model of dynamic discrete choice.
We consider an agent choosing among J discrete alternatives by maximizing the expected
present discounted value of utility. We assume that the per-period utility function for an agent
making choice j in period t is given by
Ujt = uj (xt , β0 ) + ǫjt , (j = 1, ..., J; t = 1, 2, ...).
The vector xt is the observed state variables of the problem (e.g. work experience, number of
children, wealth) and the vector β is unknown parameters. The disturbances ǫt = {ǫ1t , ..., ǫJt }
are not observed by the econometrician. As in much of the literature we assume that ǫt is i.i.d.
over time with known CDF that has support RJ , is independent of xt , and xt is first-order
Markov.
To describe the agent’s choice probabilities let δ denote a time discount parameter, v̄(x)
the expected value function, yjt ∈ {0, 1} the indicator that choice j is made and v̄j (xt ) =
uj (xt , β0 ) + δE[v̄(xt+1 )|xt , j] the expected value function conditional on choice j. As in Rust
(1987), we assume that in each period the agent makes the choice j that maximizes the expected
present discounted value of utility v̄j (xt ) + ǫjt . The probability of choosing j in period t is then
Pj (v̄t ) = Pr(v̄j (xt ) + ǫjt ≥ v̄k (xt ) + ǫkt ; k = 1, ..., J), v̄t = (v̄1 (xt ), ..., v̄J (xt ))′ .

(3.1)

These choice probabilities have a useful relationship to the structural parameters β when
there is a renewal choice, where the conditional distribution of xt+1 given the renewal choice and
xt does not depend on xt . Without loss of generality suppose that the renewal choice is j = 1.
11

Let ṽjt denote ṽj (xt ) = v̄j (xt ) − v̄1 (xt ), so that ṽ1t ≡ 0. As usual, subtracting v̄1t from each v̄jt
in Pj (v̄t ) does not change the choice probabilities, so that they depend only on ṽt = (ṽ2t , ..., ṽJt ).
The renewal nature of j = 1 leads to a specific formula for ṽjt in terms of the per period
utilities ujt = uj (xt , β0 ) and the choice probabilities Pt = P (ṽt ) = (P1 (v̄t ), ...PJ (v̄t ))′ . As in Hotz
and Miller (1993), there is a function P −1 (P ) such that ṽt = P −1 (Pt ). Let H(P ) denote the
function such that
H(Pt ) = E[ max {P −1 (Pt )j + ǫjt }|xt ] = E[ max {ṽjt + ǫjt }|xt ].
1≤j≤J

1≤j≤J

For example, for multinomial logit H(Pt) = .5772 − ln(P1t ). Note that by j = 1 being a renewal
we have E[v̄t+1 |xt , 1] = C for a constant C, so that
v̄(xt ) = v̄1t + H(Pt ) = u1t + δC + H(Pt ).
It then follows that
v̄jt = ujt + δE[v̄(xt+1 )|xt , j] = ujt + δE[u1,t+1 + H(Pt+1 )|xt , j] + δ 2 C, (j = 1, ..., J).
Subtracting then gives
ṽjt = ujt − u1t + δ{E[u1,t+1 + H(Pt+1 )|xt , j] − E[u1,t+1 + H(Pt+1 )|1]}.

(3.2)

This expression for the choice specific value function ṽjt depends only on uj (xt , β), H(Pt+1 ), and
conditional expectations given the state and choice, and so can be used to form semiparametric
moment functions.
To describe those moment functions let γ1 (x) denote the vector of possible values of the
choice probabilities E[yt |xt = x], where yt = (y1t , ..., yJt )′ . Also let γj (xt , β, γ1), (j = 2, ..., J)
denote a possible E[u1 (xt+1 , β) + H(γ1(xt+1 ))|xt , j] as a function of β, xt and γ1 , and γJ+1 (β, γ1)
a possible value of E[u1 (xt , β) + H(γ1(xt+1 ))|1]. Then a possible value of ṽjt is given by
ṽj (xt , β, γ) = uj (xt , β) − u1 (xt , β) + δ[γj (xt , β, γ1) − γJ+1 (β, γ1)], (j = 2, ..., J).
These value function differences are semiparametric, depending on the function γ1 of choice probabilities and the conditional expectations γj , (j = 2, ..., J). Let ṽ(xt , β, γ) = (ṽ2 (xt , β, γ), ..., ṽJ (xt , β, γ))′
and A(xt ) denote a matrix of functions of xt with J columns. Semiparametric moment functions
are given by
m(z, β, γ) = A(x)[y − P (ṽ(x, β, γ))].
LR moment functions can be constructed by adding the adjustment term for the presence
of the first step γ. This adjustment term is derived in Appendix A. It takes the form
φ(z, β, γ, λ) =

J+1
X
j=1

12

φj (z, β, γ, λ),

where φj (z, β, γ, λ) is the adjustment term for γj holding all other components γ fixed at their
true values. To describe it define
Pṽj (ṽ) = ∂P (ṽ)/∂ṽj , π1 = Pr(yt1 = 1), λ10 (x) = E[y1t |xt+1 = x],
ytj
λj0(x) = E[A(xt )Pṽj (ṽt )
|xt+1 = x], (j = 2, ..., J).
Pj (ṽt )

(3.3)

Then for wt = xt+1 and z = (y, x, w) let
φ1 (z, β, γ, λ) = −δ

J
X
j=2

{λj (x) − E[A(xt )Pṽj (ṽt )]π1−1 λ1 (x)} [∂H(γ1 (x))/∂P ]′ {y − γ1 (x)}

φj (z, β, γ, λ) = −δA(x)Pṽj (ṽ(x, β, γ))
φJ+1 (z, β, γ, λ) = δ

J
X
j=2

!

yj
{u1 (w, β) + H(γ1(w)) − γj (x, β, γ1)}, (j = 2, ..., J),
Pj (ṽ(x, β, γ))
!

E[A(xt )Pṽj (ṽ(xt , β, γ))] π1−1 y1 {u1 (w, β) + H(γ1 (w)) − γJ+1 (β, γ1)}.

Theorem 2: If the marginal distribution of xt does not vary with t then LR moment
functions for the dynamic discrete choice model are
ψ(z, β, γ) = A(xt )[yt − P (ṽ(xt , β, γ))] +

J+1
X

φj (z, β, λ).

j=1

The form of ψ(z, β, γ) is amenable to machine learning. A machine learning estimator of the
conditional choice probability vector γ10 (x) is straightforward to compute and can then be used
throughout the construction of the orthogonal moment conditions everywhere γ1 appears. If
u1 (x, β) is linear in x, say u1 (x, β) = x′1 β1 for subvectors x1 and β1 of x and β respectively, then
machine learning estimators can be used to obtain Ê[x1,t+1 |xt , j] and Ê[Ĥt+1 |xj , j], (j = 2, ..., J),
and a sample average used to form γ̂J+1 (β, γ̂1). The value function differences can then be
estimated as
ṽj (xt , β, γ̂) = uj (xt , β) − u1(xt , β) + Ê[x1,t+1 |xt , j]′ β1 − Ê[x1,t+1 |1]′β1 + Ê[Ĥt+1 |xt , j] − Ê[Ĥt+1 |1].
Furthermore, denominator problems can be avoided by using structural probabilities (rather
than the machine learning estimators) in all denominator terms.
The challenging part of the machine learning for this estimator is the dependence on β of the
reverse conditional expectations in λ1 (x). It may be computationally prohibitive and possibly
unstable to redo machine learning for each β. One way to to deal with this complication is to
update β periodically, with more frequent updates near convergence. It is important that at
convergence the β in the reverse conditional expectations is the same as the β that appears
elsewhere.
13

With data zi that is i.i.d. over individuals these moment functions can be used for any t to
estimate the structural parameters β. Also, for data for a single individual we could use a time
P −1
average Tt=1
ψ(zt , β, γ)/(T − 1) to estimate β. It will be just as important to use LR moments
for estimation with a single individual as it is with a cross section of individuals, although our
asymptotic theory will not apply to that case.
Bajari, Chernozhukov, Hong, and Nekipelov (2009) derived the influence adjustment for
dynamic discrete games of imperfect information. Locally robust moment conditions for such
games could be formed using their results. We leave that formulation to future work.
As an example of the finite sample performance of the LR GMM we report a Monte Carlo
study of the LR estimator of this Section. The design of the experiment is loosely like the
bus replacement application of Rust (1987). Here xt is a state variable meant to represent the
lifetime of a bus engine. The transition density is
(
xt + N(.25, 1)2 , yt = 1,
.
xt+1 =
xt = 1 + N(.25, 1)2 , yt = 0.
where yt = 0 corresponds to replacement of the bus engine and yt = 1 to nonreplacement. We
assume that the agent chooses yt contingent on state to maximize
∞
X
t=1

√
δ t−1 [yt (α xt + εt ) + (1 − yt )RC], α = −.3, RC = −4.

The unconditional probability of replacement in this model is about 1/8, which is substantially
higher than that estimated in Rust (1987). The sample used for estimation was 1000 observations
for a single decision maker. We carried out 10, 000 replications.
We estimate the conditional choice probabilities by kernel and series nonparametric regression
and by logit lasso, random forest, and boosted tree machine learning methods. Logit conditional
choice probabilities and derivatives were used in the construction of λ̂j wherever they appear
in order to avoid denominator issues. The unknown conditional expectations in the λ̂j were
estimated by series regressions throughout. Kernel regression was also tried but did not work
particularly well and so results are not reported.
Table 1 reports the results of the experiment. Bias, standard deviation, and coverage probability for asymptotic 95 percent confidence intervals are given in Table 1.
Table 1

14

LR CCP Estimators, Dynamic Discrete Choice
Bias
Std Dev
95% Cov
α RC
α RC
α RC
Two step kernel -.24 .08 .08 .32 .01 .86
LR kernel
-.05 .02 .06 .32 .95 .92
Two step quad -.00 .14 .049 .33∗ .91 .89
LR quad
-.00 .01 .085 .39 .95 .92
Logit Lasso
-.12 .25 .06 .28 .74 .84
LR Logit Lasso -.09 .01 .08 .36 .93 .95
Random Forest -.15 -.44 .09 .50 .91 .98
LR Ran. For.
.00
.00 .06 .44 1.0 .98
Boosted Trees
-.10 -.28 .08 .50 .99 .99
LR Boost Tr.
.03
.09 .07 .47 .99 .97
Here we find bias reduction from the LR estimator in all cases. We also find variance
reduction from LR estimation when the first step is kernel estimation, random forests, and
boosted trees. The LR estimator also leads to actual coverage of confidence intervals being
closer to the nominal coverage. The results for random forests and boosted trees seem noisier
than the others, with higher standard deviations and confidence interval coverage probabilities
farther from nominal. Overall, we find substantial improvements from using LR moments rather
than only the identifying, original moments.

4

Estimating the Influence Adjustment

Construction of LR moment functions requires an estimator φ̂(z, β) of the adjustment term.
The form of φ(z, β, γ, λ) is known for some cases from the semiparametric estimation literature.
Powell, Stock, and Stoker (1989) derived the adjustment term for density weighted average
derivatives. Newey (1994a) gave the adjustment term for mean square projections (including
conditional expectations), densities, and their derivatives. Hahn (1998) and Hirano, Imbens,
and Ridder (2003) used those results to obtain the adjustment term for treatment effect estimators, where the LR estimator will be the doubly robust estimator of Robins, Rotnitzky, and
Zhao (1994, 1995). Bajari, Hong, Krainer, and Nekipelov (2010) and Bajari, Chernozhukov,
Hong, and Nekipelov (2009) derived adjustment terms in some game models. Hahn and Ridder
(2013, 2016) derived adjustments in models with generated regressors including control functions. These prior results can be used to obtain LR estimators by adding the adjustment term
with nonparametric estimators plugged in.
For new cases it may be necessary to derive the form of the adjustment term. Also, it is
possible to numerically estimate the adjustment term based on series estimators and other non15

parametric estimators. In this Section we describe how to construct estimators of the adjustment
term in these ways.

4.1

Deriving the Formula for the Adjustment Term

One approach to estimating the adjustment term is to derive a formula for φ(z, β, γ, λ) and
then plug in γ̂ and λ̂ in that formula. A formula for φ(z, β, γ, λ) can be obtained as in Newey
(1994a). Let γ(F ) be the limit of the nonparametric estimator γ̂ when zi has distribution F.
Also, let Fτ denote a regular parametric model of distributions with Fτ = F0 at τ = 0 and
score (derivative of the log likelihood at τ = 0) equal to S(z). Then under certain regularity
conditions φ(z, β, γ0, λ0 ) will be the unique solution to
R
∂ m(z, β, γ(Fτ ))F0 (dz)
= E[φ(zi , β, γ0, λ0 )S(zi )], E[φ(zi , β, γ0, λ0 )] = 0,
(4.1)
∂τ
τ =0
as {Fτ } and the corresponding score S(z) are allowed to vary over a family of parametric
models where the set of scores for the family has mean square closure that includes all mean
zero functions with finite variance. Equation (4.1) is a functional equation that can be solved to
find the adjustment term, as was done in many of the papers cited in the previous paragraph.
The influence adjustment can be calculated by taking a limit of the Gateaux derivative as
shown in Ichimura and Newey (2017). Let γ(F ) be the limit of γ̂ when F is the true distribution
of zi , as before. Let Ghz be a family of distributions that approaches a point mass at z as h −→ 0.
If φ(zi , β, γ0, λ0 ) is continuous in zi with probability one then


∂E[m(zi , β, γ(Fτh ))]
, Fτh = (1 − τ )F0 + τ Ghz .
(4.2)
φ(z, β, γ0 , λ0 ) = lim
h−→0
∂τ
τ =0
This calculation is more constructive than equation (4.1) in the sense that the adjustment term
here is a limit of a derivative rather than the solution to a functional equation. In Sections 5
and 6 we use those results to construct LR estimators when the first step is a nonparametric
instrumental variables (NPIV) estimator.
With a formula for φ(z, β, γ, λ) in hand from either solving the functional equation in equation (4.1) or from calculating the limit of the derivative in equation (4.2), one can estimate the
adjustment term by plugging estimators γ̂ and λ̂ into φ(z, β, γ, λ). This approach to estimating
LR moments can used to construct LR moments for the average surplus described near the end
of Section 2. There the adjustment term depends on the conditional density of p1i given p2i and
yi . Let fˆℓ (p1 |p2 , y) be some estimator of the conditional pdf of p1i given p2i and yi . Plugging
1 ,y)
.This λ̂ℓ (x) can then be used in
that estimator into the formula for λ0 (x) gives λ̂ℓ (x) = fˆ ℓ(p
(p
|p2 ,y)
1
ℓ
equation (2.10).

16

4.2

Estimating the Influence Adjustment for First Step Series Estimators

Estimating the adjustment term is relatively straightforward when the first step is a series
estimator. The adjustment term can be estimated by treating the first step estimator as if it
were parametric and applying a standard formula for the adjustment term for parametric twostep estimators. Suppose that γ̂ℓ depends on the data through a K × 1 vector ζ̂ℓ of parameter
estimators that has true value ζ0 . Let m(z, β, ζ) denote m(z, β, γ) as a function of ζ. Suppose
that there is a K × 1 vector of functions h(z, ζ) such that ζ̂ℓ satisfies
1 X
√
h(zi , ζ̂ℓ ) = op (1),
n̄ℓ ¯

(4.3)

i∈Iℓ

where I¯ℓ is a subset of observations, none which are included in Iℓ , and n̄ℓ is the number of
observations in I¯ℓ . Then a standard calculation for parametric two-step estimators (e.g. Newey,
1984, and Murphy and Topel, 1985) gives the parametric adjustment term

φ(zi , β, ζ̂ℓ, Ψ̂ℓ ) = Ψ̂ℓ (β)h(zi , ζ̂ℓ ), Ψ̂ℓ (β) = −

X ∂m(zj , β, ζ̂ℓ)
j∈I¯ℓ

∂ζ


−1
X ∂h(zj , ζ̂ℓ )

 , i ∈ Iℓ .
∂ζ
¯
j∈Iℓ

In many cases φ(zi , β, ζ̂ℓ , Ψ̂ℓ ) approximates the true adjustment term φ(z, β, γ0, λ0 ), as shown
by Newey (1994a, 1997) and Ackerberg, Chen, and Hahn (2012) for estimating the asymptotic
variance of functions of series estimators. Here this approximation is used for estimation of β
instead of just for variance estimation. The estimated LR moment function will be
ψ(zi , β, ζ̂ℓ , Ψ̂ℓ ) = m(zi , β, ζ̂ℓ ) + φ(zi , β, ζ̂ℓ , Ψ̂ℓ ).

(4.4)

We note that if ζ̂ℓ were computed from the whole sample then φ̂(β) = 0. This degeneracy does
not occur when cross-fitting is used, which removes ”own observation” bias and is important for
first step machine learning estimators, as noted in Section 2.
We can apply this approach to construct LR moment functions for an estimator of the
average surplus bound example that is based on series regression. Here the first step estimator
of γ0 (x) = E[qi |xi = x] will be that from an ordinary least regression of qi on a vector a(xi ) of
approximating functions. The corresponding m(z, β, ζ) and h(z, ζ) are
Z
′
′
m(z, β, ζ) = A(x) ζ − β, h(z, ζ) = a(x)[q − a(x) ζ], A(x) = ℓ(p1 , y)a(p1, p2 , y)dp1.
Let ζ̂ℓ denote the least squares coefficients from regressing qi on a(xi ) for observations that are

17

not included in Iℓ . Then the estimator of the locally robust moments given in equation (4.4) is
ψ(zi , β, ζ̂ℓ, Ψ̂ℓ ) = A(xi )′ ζ̂ℓ − β + Ψ̂ℓ a(xi )[qi − a(xi )′ ζ̂ℓ ],
−1

X
X
a(xj )a(xj )′  .
A(xj )′ 
Ψ̂ℓ =
j∈I¯ℓ

j∈I¯ℓ

It can be shown similarly to Newey (1994a, p. 1369) that Ψ̂ℓ estimates the population least
squares coefficients from a regression of λ0 (xi ) on a(xi ), so that λ̂ℓ (xi ) = Ψ̂ℓ a(xi ) estimates
λ0 (xi ). In comparison the LR estimator described in the previous subsection was based on an
explicit nonparametric estimator of f0 (p1 |p2 , y), while this λ̂ℓ (x) implicitly estimates the inverse
of that pdf via a mean-square approximation of λ0 (xi ) by Ψ̂ℓ a(xi ).
Chernozhukov, Newey, and Robins (2018) introduce machine learning methods for choosing
the functions to include in the vector A(x). This method can be combined with machine
learning methods for estimating E[qi |xi ] to construct a double machine learning estimator of
average surplus, as shown in Chernozhukov, Hausman, and Newey (2018).
In parametric models moment functions like those in equation (4.4) are used to ”partial
out” nuisance parameters ζ. For maximum likelihood these moment functions are the basis
of Neyman’s (1959) C-alpha test. Wooldridge (1991) generalized such moment conditions to
nonlinear least squares and Lee (2005), Bera et al. (2010), and Chernozhukov et al. (2015) to
GMM. What is novel here is their use in the construction of semiparametric estimators and the
interpretation of the estimated LR moment functions ψ(zi , β, ζ̂ℓ , Ψ̂ℓ ) as the sum of an original
moment function m(zi , β, ζ̂ℓ ) and an influence adjustment φ(zi , β, ζ̂ℓ , Ψ̂ℓ ).

4.3

Estimating the Influence Adjustment with First Step Smoothing

The adjustment term can be estimated in a general way that allows for kernel density, locally
linear regression, and other kernel smoothing estimators for the first step. The idea is to differentiate with respect to the effect of the ith observation on sample moments. Newey (1994b) used
a special case of this approach to estimate the asymptotic variance of a functional of a kernel
based semiparametric or nonparametric estimator. Here we extend this method to a wider class
of first step estimators, such as locally linear regression, and apply it to estimate the adjustment
term for construction of LR moments.
We will describe this estimator for the case where γ is a vector of functions of a vector of
variables x. Let h(z, x, γ) be a vector of functions of a data observation z, x, and a possible
P
realized value of γ (i.e. a vector of real numbers γ). Also let ĥℓ (x, γ) = j∈I¯ℓ h(zj , x, γ)/n̄ℓ
be a sample average over a set of observations I¯ℓ not included in Iℓ , where n̄j is the number of
observations in I¯j . We assume that the first step estimator γ̂ℓ (x) solves
0 = ĥℓ (x, γ).
18

We suppress the dependence of h and γ̂ on a bandwidth. For example for a pdf κ(u) a kernel
density estimator would correspond to h(zj , x, γ) = κ(x − xj ) − γ and a locally linear regression
would be γ̂1 (x) for
!
1
h(zj , x, γ) = κ(x − xj )
[yj − γ1 − (x − xj )′ γ2 ].
x − xj
To measure the effect of the ith observation on γ̂ let γ̂ℓiξ (x) be the solution to
0 = ĥℓ (x, γ) + ξ · h(zi , x, γ).
This γ̂ℓiξ (x) is the value of the function obtained from adding the contribution ξ · h(zi , x, γ) of
the ith observation. An estimator of the adjustment term can be obtained by differentiating the
average of the original moment function with respect to ξ at ξ = 0. This procedure leads to an
estimated locally robust moment function given by
ψ(zi , β, γ̂ℓ) = m(zi , β, γ̂ℓ ) +

∂ 1 X
m(zj , β, γ̂ℓiξ (·))
∂ξ n̄ℓ ¯
j∈Iℓ

.
ξ=0

This estimator is a generalization of the influence function estimator for kernels in Newey
(1994b).

5

Double and Partial Robustness

The zero derivative condition in equation (2.5) is an appealing robustness property in and of
itself. A zero derivative means that the expected moment functions remain closer to zero than
τ as τ varies away from zero. This property can be interpreted as local insensitivity of the
moments to the value of γ being plugged in, with the moments remaining close to zero as γ
varies away from its true value. Because it is difficult to get nonparametric functions exactly
right, especially in high dimensional settings, this property is an appealing one.
Such robustness considerations, well explained in Robins and Rotnitzky (2001), have motivated the development of doubly robust (DR) moment conditions. DR moment conditions have
expectation zero if one first stage component is incorrect. DR moment conditions allow two
chances for the moment conditions to hold, an appealing robustness feature. Also, DR moment
conditions have simpler conditions for asymptotic normality than general LR moment functions
as discussed in Section 7. Because many interesting LR moment conditions are also DR we
consider double robustness.
LR moments that are constructed by adding the adjustment term for first step estimation
provide candidates for DR moment functions. The derivative of the expected moments with
19

respect to each first step will be zero, a necessary condition for DR. The condition for moments
constructed in this way to be DR is the following:
Assumption 1: There are sets Γ and Λ such that for all γ ∈ Γ and λ ∈ Λ
E[m(zi , β0 , γ)] = −E[φ(zi , β0 , γ, λ0 )], E[φ(zi , β0 , γ0 , λ)] = 0.
This condition is just the definition of DR for the moment function ψ(z, β, γ) = m(z, β, γ) +
φ(z, β, γ, λ), pertaining to specific sets Γ and Λ.
The construction of adding the adjustment term to an identifying or original moment function
leads to several novel classes of DR moment conditions. One such class has a first step that
satisfies a conditional moment restriction
E[yi − γ0 (wi )|xi ] = 0,

(5.1)

where wi is potentially endogenous and xi is a vector of instrumental variables. This condition
is the nonparametric instrumental variable (NPIV) restriction as in Newey and Powell (1989,
2003) and Newey (1991). A first step conditional expectation where γ0 (xi ) = E[yi |xi ] is included
as special case with wi = xi . Ichimura and Newey (2017) showed that the adjustment term for
this step takes the form φ(z, γ, λ) = λ(x)[y − γ(w)] so m(z, β, γ) + λ(x)[y − γ(x)] is a candidate
for a DR moment function. A sufficient condition for DR is:
Assumption 2: i) Equation (5.1) is satisfied; ii) Λ = {λ(x) : E[λ(xi )2 ] < ∞} and Γ =
{γ(w) : E[γ(wi )2 ] < ∞}; iii) there is v(w) with E[v(wi )2 ] < ∞ such that E[m(zi , β0 , γ)] =
E[v(wi ){γ(wi ) − γ0 (wi )}] for all γ ∈ Γ; iv) there is λ0 (x) such that v(wi ) = E[λ0 (xi )|wi ]; and
v) E[yi2 ] < ∞.
By the Riesz representation theorem condition iii) is necessary and sufficient for E[m(zi , β0 , γ)]
to be a mean square continuous functional of γ with representer v(w). Condition iv) is an additional condition giving continuity in the reduced form difference E[γ(wi ) − γ0 (wi )|xi ], as further
discussed in Ichimura and Newey (2017). Under this condition
E[m(zi , β0 , γ)] = E[E[λ0 (xi )|wi ]{γ(wi ) − γ0 (wi )}] = E[λ0 (xi ){γ(wi ) − γ0 (wi )}]
= −E[φ(zi , γ, λ0 )], E[φ(zi , γ0 , λ)] = E[λ(xi ){yi − γ0 (wi )}] = 0.

Thus Assumption 2 implies Assumption 1 so that we have
Theorem 3: If Assumption 2 is satisfied then m(z, β, γ) + λ(x){y − γ(w)} is doubly robust.
There are many interesting, novel examples of DR moment conditions that are special cases
of Theorem 3. The average surplus bound is an example where yi = qi , wi = xi , xi is the observed
20

vector of prices and income, Λ = Γ is the set of all measurable functions of xi with finite second
moment, and γ0 (x) = E[yi |xi = x]. Let x1 denote p1 and x2 the vector of other prices and
income, so that x = (x1 , x′2 )′ . Also let f0 (x1 |x2 ) denote the conditional pdf of p1 given x2 and
R
ℓ(x) = ℓ(p1 , y) for income y. Let m(z, β, γ) = ℓ(p1 , x2 )γ(p1 , x2 )dp1 − β as before. Multiplying
and dividing through by f0 (p1 |x2 ) gives, for all γ, λ ∈ Γ and λ0 (x) = f0 (x1 |x2 )−1 ℓ(x),
Z
E[m(zi , β0 , γ)] = E[ ℓ(p1 , x2i )γ(p1 , x2i )dp1 ]−β0 = E[E[λ0 (xi )γ(xi )|x2i ]]−β0 = E[λ0 (xi ){γ(xi )−γ0 (xi )}].
Theorem 3 then implies that the LR moment function for average surplus m(z, β, γ) + λ(x)[q −
γ(x)] is DR. A corresponding DR estimator β̂ is given in equation (2.10).
The surplus bound is an example of a parameter where β0 = E[g(zi , γ0)] for some linear
functional g(z, γ) of γ and for γ0 satisfying the conditional moment restriction of equation (5.1).
R
For the surplus bound g(z, γ) = ℓ(p1 , x2 )γ(p1 , x2 )dp1 . If Assumption 2 is satisfied then choosing
m(z, β, γ) = g(z, γ) − β a DR moment condition is g(z, γ) − β + λ(x)[y − γ(w)]. A corresponding
DR estimator is
n
1X
{g(zi , γ̂i ) + λ̂i (xi )[yi − γ̂i (wi )]},
(5.2)
β̂ =
n i=1
where γ̂i (w) and λ̂i (x) are estimators of γ0 (w) and λ0 (x) respectively. An estimator γ̂i can be
constructed by nonparametric regression when wi = xi or NPIV in general. A series estimator
λ̂i (x) can be constructed similarly to the surplus bound example in Section 3.2. For wi = xi
Newey and Robins (2017) give such series estimators of λ̂i (x) and Chernozhukov, Newey, and
Robins (2018) show how to choose the approximating functions for λ̂i (xi ) by machine learning.
Simple and general conditions for root-n consistency and asymptotic normality of β̂ that allow
for machine learning are given in Section 7.
Novel examples of the DR estimator in equation (5.2) wi = xi are given by Newey and
Robins (2017) and Chernozhukov, Newey, and Robins (2018). Also Appendix C provides a
generalization to γ(w) and γ(x) that satisfy orthogonality conditions more general than conditional moment restrictions and novel examples of those. A novel example with wi 6= xi is a
weighted average derivative of γ0 (w) satisfying equation (5.1). Here g(z, γ) = v̄(w)∂γ(w)/∂w for
some weight function v̄(w). Let f0 (w) be the pdf of wi and v(w) = −f0 (w)−1 ∂[v̄(w)f0(w)]/∂w,
assuming that derivatives exist. Assume that v̄(w)γ(w)f0(w) is zero on the boundary of the
support of wi . Integration by parts then gives Assumption 2 iii). Assume also that there exists
λ0 ∈ Λ with v(wi ) = E[λ0 (xi )|wi ]. Then for estimators γ̂i and λ̂i a DR estimator of the weighted
average derivative is
n

∂γ̂i (wi )
1X
{v̄(wi )
+ λ̂i (xi )[yi − γ̂i (wi )]}.
β̂ =
n i=1
∂w
This is a DR version of the weighted average derivative estimator of Ai and Chen (2007). A
21

special case of this example is the DR moment condition for the weighted average derivative in
the exogenous case where wi = xi given in Firpo and Rothe (2017).
Theorem 3 includes existing DR moment functions as special cases where wi = xi , including
the mean with randomly missing data given by Robins and Rotnitzky (1995), the class of DR
estimators in Robins et al. (2008), and the DR estimators of Firpo and Rothe (2017). We
illustrate for the mean with missing data. Let w = x, x = (a, u) for an observed data indicator
a ∈ {0, 1} and covariates u, m(z, β, γ) = γ(1, u) − β, and λ0 (x) = a/ Pr(ai = 1|ui = u). Here it
is well known that
E[m(zi , β0 , γ)] = E[γ(1, ui )] − β0 = E[λ0 (xi ){γ(xi ) − γ0 (xi )}] = −E[λ0 (xi ){yi − γ(xi )}].
Then DR of the moment function γ(1, w) − β + λ(x)[y − γ(x)] of Robins and Rotnitzky (1995)
follows by Proposition 5.
Another novel class of DR moment conditions are those where the first step γ is a pdf of a
function x of the data observation z. By Proposition 5 of Newey (1994a), the adjustment term
R
for such a first step is φ(z, β, γ, λ) = λ(x) − λ(u)γ(u)du for some possible λ. A sufficient
condition for the DR as in Assumption 1 is:
R
Assumption 3: xi has pdf γ0 (x) and for Γ = {γ : γ(x) ≥ 0, γ(x)dx = 1} there is λ0 (x)
such that for all γ ∈ Γ,
Z
E[m(zi , β0 , γ)] = λ0 (x){γ(x) − γ0 (x)}dx.
R
Note that for φ(z, γ, λ) = λ(x)− λ(x̃)γ(x̃)dx̃ it follows from Assumption 3 that E[m(zi , β0 , γ)] =
R
−E[φ(zi , γ, λ0 )] for all γ ∈ Γ. Also, E[φ(zi , γ0 , λ)] = E[λ(xi )] − λ(x̃)γ0 (x̃)dx = 0. Then Assumption 1 is satisfied so we have:
Theorem 4: If Assumption 3 is satisfied then m(z, β, γ) + λ(x) −
The integrated squared density β0 =
λ0 = γ0 , and

R

R

λ(x̃)γ(x̃)dx̃ is DR.

γ0 (x)2 dx is an example for m(z, β, γ) = γ(x) − β,

ψ(z, β, γ, λ) = γ(x) − β + λ(x) −

Z

λ(x̃)γ(x̃)dx.

This DR moment function seems to be novel. Another example is the density weighted average
derivative (DWAD) of Powell, Stock, and Stoker (1989), where m(z, β, γ) = −2y · ∂γ(x)/∂x − β.
Let δ(xi ) = E[yi |xi ]γ0 (xi ). Assuming that δ(u)γ(u) is zero on the boundary and differentiable,
integration by parts gives
Z
E[m(zi , β0 , γ)] = −2E[yi ∂γ(xi )/∂x] − β0 = [∂δ(x̃)/∂x]{γ(x̃) − γ0 (x̃)}du,
22

so that Assumption 3 is satisfied with λ0 (x) = ∂δ(x)/∂x. Then by Theorem 4
n

1X
∂γ̂i (xi ) ∂ δ̂i (xi )
β̂ =
{−2
+
−
n i=1
∂x
∂x

Z

∂ δ̂i (x̃)
γ̂i (x̃)dx̃}
∂x

is a DR estimator. It was shown in NHR (1998) that the Powell, Stock, and Stoker (1989)
estimator with a twicing kernel is numerically equal to a leave one out version of this estimator
for the original (before twicing) kernel. Thus the DR result for β̂ gives an interpretation of the
twicing kernel estimator as a DR estimator.
The expectation of the DR moment functions of both Theorem 3 and 4 are affine in γ and
λ holding the other fixed at the truth. This property of DR moment functions is general, as we
show by the following characterization of DR moment functions:
Theorem 5: If Γ and Λ are linear then ψ(z, β, γ, λ) is DR if and only if
∂E[ψ(zi , β0 , (1 − τ )γ0 + τ γ, λ0 )]|τ =0 = 0, ∂E[ψ(zi , β0 , γ0, (1 − τ )λ0 + τ λ)]|τ =0 = 0,
and E[ψ(zi , β0 , γ, λ0 )] and E[ψ(zi , β0 , γ0 , λ)] are affine in γ and λ respectively.
The zero derivative condition of this result is a Gateaux derivative, componentwise version
of LR. Thus, we can focus a search for DR moment conditions on those that are LR. Also, a DR
moment function must have an expectation that is affine in each of γ and λ while the other is
held fixed at the truth. It is sufficient for this condition that ψ(zi , β0 , γ, λ) be affine in each of
γ and λ while the other is held fixed. This property can depend on how γ and λ are specified.
For example the missing data DR moment function m(1, u) − β + π(u)−1 a[y − γ(x)] is not affine
in the propensity score π(u) = Pr(ai = 1|ui = u) but is in λ(x) = π(u)−1 a.
In general Theorem 5 motivates the construction of DR moment functions by adding the
adjustment term to obtain a LR moment function that will then be DR if it is affine in γ and λ
separately. It is interesting to note that in the NPIV setting of Theorem 3 and the density setting
of Theorem 4 that the adjustment term is always affine in γ and λ. It then follows from Theorem
5 that in those settings LR moment conditions are precisely those where E[m(zi , β0 , γ)] is affine
in γ. Robins and Rotnitzky (2001) gave conditions for the existence of DR moment conditions
in semiparametric models. Theorem 5 is complementary to those results in giving a complete
characterization of DR moments when Γ and Λ are linear.
Assumptions 2 and 3 both specify that E[m(zi , β0 , γ)] is continuous in an integrated squared
deviation norm. These continuity conditions are linked to finiteness of the semiparametric
variance bound for the functional E[m(zi , β0 , γ)], as discussed in Newey and McFadden (1994)
for Assumption 2 with wi = xi and for Assumption 3. For Assumption 2 with wi 6= xi Severini
and Tripathi (2012) showed for m(z, β, γ) = v(w)γ(w) − β with known v(w) that the existence
of λ0 (w) with v(wi ) = E[λ0 (xi )|wi ] is necessary for the existence of a root-n consistent estimator
23

of β. Thus the conditions of Assumption 2 are also linked to necessary conditions for root-n
consistent estimation when wi 6= xi .
Partial robustness refers to settings where E[m(zi , β0 , γ̄)] = 0 for some γ̄ 6= γ0 . The novel DR
moment conditions given here lead to novel partial robustness results as we now demonstrate
in the conditional moment restriction setting of Assumption 2. When λ0 (x) in Assumption 2 is
restricted in some way there may exist γ̃ 6= γ0 with E[λ0 (xi ){yi − γ̃(wi )}] = 0. Then
E[m(zi , β0 , γ̃)] = −E[λ0 (xi ){yi − γ̃(wi )}] = 0.
Consider the average derivative β0 = E[∂γ0 (wi )/∂wr ] where m(z, β, γ) = ∂γ(w)/∂wr − β for
some r. Let δ = (E[a(xi )p(wi )′ ])−1 E[a(xi )yi ] be the limit of the linear IV estimator with right
hand side variables p(w) and the same number of instruments a(x). The following is a partial
robustness result that provides conditions for the average derivative of the linear IV estimator
to equal the true average derivative:
Theorem 6: If −∂ ln f0 (w)/∂wr = c′ p(w) for a constant vector c, E[p(wi )p(wi )′ ] is nonsingular, and E[a(xi )|wi = w] = Πp(w) for a square nonsingular Π then for δ = (E[a(xi )p(wi )′ ])−1 E[a(xi )yi ],
E[∂{p(wi )′ δ}/∂wr ] = E[∂γ0 (wi )/∂wr ].
This result shows that if the density score is a linear combination of the right-hand side
variables p(w) used by linear IV, the conditional expectation of the instruments a(xi ) given wi
is a nonsingular linear combination of p(w), and p(w) has a nonsingular second moment matrix
then the average derivative of the linear IV estimator is the true average derivative. This is
a generalization to NPIV of Stoker’s (1986) result that linear regression coefficients equal the
average derivatives when the regressors are multivariate Gaussian.
DR moment conditions can be used to identify parameters of interest. Under Assumption 1
β0 may be identified from
E[m(zi , β0 , γ̄)] = −E[φ(zi , β0 , γ̄, λ0 )]
for any fixed γ̄ when the solution β0 to this equation is unique.
Theorem 7: If Assumption 1 is satisfied, λ0 is identified, and for some γ̄ the equation
E[ψ(zi , β, γ̄, λ0 )] = 0 has a unique solution then β0 is identified as that solution.
Applying this result to the NPIV setting of Assumption 2 gives an explicit formula for certain
functionals of γ0 (w) without requiring that the completeness identification condition of Newey
and Powell (1989, 2003) be satisfied, similarly to Santos (2011). Suppose that v(w) is identified,
e.g. as for the weighted average derivative. Since both w and x are observed it follows that
24

a solution λ0 (x) to v(w) = E[λ0 (x)|w] will be identified if such a solution exists. Plugging in
γ̄ = 0 into the equation E[ψ(zi , β0 , γ̄, λ0 )] = 0 gives
Corollary 8: If v(wi ) is identified and there exists λ0 (xi ) such that v(wi ) = E[λ0 (xi )|wi ]
then β0 = E[v(wi )γ0 (wi )] is identified as β0 = E[λ0 (xi )yi ].
Note that this result holds without the completeness condition. Identification of β0 =
E[v(wi )γ0 (wi )] for known v(wi ) with v(wi ) = E[λ0 (xi )|wi ] follows from Severini and Tripathi
(2006). Corollary 8 extends that analysis to the case where v(wi ) is only identified but not
necessarily known and links it to DR moment conditions. Santos (2011) gives a related formula
R
for a parameter β0 = ṽ(w)λ0(w)dw. The formula here differs from Santos (2011) in being an
expectation rather than a Lebesgue integral. Santos (2011) constructed an estimator. That is
beyond the scope of this paper.

6

Conditional Moment Restrictions

Models of conditional moment restrictions that depend on unknown functions are important
in econometrics. In such models the nonparametric components may be determined simultaneously with the parametric components. In this setting it is useful to work directly with the
instrumental variables to obtain LR moment conditions rather than to make a first step influence adjustment. For that reason we focus in this Section on constructing LR moments by
orthogonalizing the instrumental variables.
Our orthogonal instruments framework is based on based on conditional moment restrictions
of the form
E[ρj (zi , β0 , γ0 )|xji ] = 0, (j = 1, ..., J),
(6.1)
where each ρj (z, β, γ) is a scalar residual and xj are instruments that may differ across j. This
model is considered by Chamberlain (1992) and Ai and Chen (2003, 2007) when xj is the same
for each j and for Ai and Chen (2012) when the set of xj includes xj−1 . We allow the residual
vector ρ(z, β, γ) to depend on the entire function γ and not just on its value at some function
of the observed data zi .
In this framework we consider LR moment functions having the form
ψ(z, β, γ, λ) = λ(x)ρ(z, β, γ),

(6.2)

where λ(x) = [λ1 (x1 ), ..., λJ (xJ )] is a matrix of instrumental variables with the j th column given
by λj (xj ). We will define orthogonal instruments to be those that make ψ(z, β, γ, λ) locally
robust. To define orthogonal instrumental variables we assume that γ is allowed to vary over a
linear set Γ as F varies. For each ∆ ∈ Γ let
∂E[ρJ (zi , β0 , γ0 + τ ∆)|xJ ] ′
∂E[ρ1 (zi , β0 , γ0 + τ ∆)|x1 ]
, ...,
).
ρ̄γ (x, ∆) = (
∂τ
∂τ
25

This ρ̄γ (x, ∆) is the Gateaux derivative with respect to γ of the conditional expectation of the
residuals in the direction ∆. We characterize λ0 (x) as orthogonal if
E[λ0 (xi )ρ̄γ (xi , ∆)] = 0 for all ∆ ∈ Γ.
We assume that ρ̄γ (x, ∆) is linear in ∆ and consider the Hilbert space of vectors of random
vectors a(x) = (a1 (x1 ), ..., aJ (xJ )) with inner product ha, bi = E[a(xi )′ b(xi )]. Let Λ̄γ denote
the closure of the set {ρ̄γ (x, ∆) : ∆ ∈ Γ} in that Hilbert space. Orthogonal instruments are
those where each row of λ0 (x) is orthogonal to Λ̄γ . They can be interpreted as instrumental
variables where the effect of estimation of γ has been partialed out. When λ0 (x) is orthogonal
then ψ(z, β, γ, λ) = λ(x)ρ(z, β, γ) is LR:
Theorem 9: If each row of λ0 (x) is orthogonal to Λ̄γ then the moment functions in equation
(6.2) are LR.
We also have a DR result:
Theorem 10: If each row of λ0 (x) is orthogonal to Λ̄γ and ρ(z, β, γ) is affine in γ ∈ Γ then
the moment functions in equation (6.2) are DR for Λ = {λ(x) : E[λ(xi )′ ρ(zi , β0 , γ0 )′ ρ(zi , β0 , γ0 )λ(xi )].
There are many ways to construct orthogonal instruments. For instance, given a r × (J − 1)
matrix of instrumental variables λ(x) one could construct corresponding orthogonal ones λ0 (xi )
as the matrix where each row of λ(x) is replaced by the residual from the least squares projection
of the corresponding row of λ(x) on Λ̄γ . For local identification of β we also require that
rank( ∂E[ψ(zi , β, γ0)]/∂β|β=β0 ) = dim(β).

(6.3)

A model where β0 is identified from semiparametric conditional moment restrictions with
common instrumental variables is a special case where xji is the same for each j. In this case
there is a way to construct orthogonal instruments that leads to an efficient estimator of β0 .
Let Σ(xi ) denote some positive definite matrix with its smallest eigenvalue bounded away from
zero, so that Σ(xi )−1 is bounded. Let ha, biΣ = E[a(xi )′ Σ(xi )−1 b(xi )] denote an inner product
and note that Λ̄γ is closed in this inner product by Σ(xi )−1 bounded. Let λ̃Σ
k (xi , λ) denote the
′
th
residual from the least squares projection of the k row λ (x) ek of λ(x) on Λ̄γ with the inner
product ha, biΣ . Then for all ∆ ∈ Γ,
′
−1
E[λ̃Σ
k (xi , λ) Σ(xi ) ρ̄γ (xi , ∆)] = 0,
Σ
Σ
−1
so that for λ̃Σ (xi , λ) = [λ̃Σ
are
1 (xi , λ), ..., λ̃r (xi , λ)] the instrumental variables λ̃ (xi , λ)Σ(xi )
Σ
orthogonal. Also, λ̃ (xi , λ) can be interpreted as the solution to

min

{D(x):D(x)′ ek ∈Λ̄γ ,k=1,...,r}

tr(E[{λ(xi ) − D(xi )}Σ(xi )−1 {λ(xi ) − D(xi )}′ ])
26

where the minimization is in the positive semidefinite sense.
The orthogonal instruments that minimize the asymptotic variance of GMM in the class of
GMM estimators with orthogonal instruments are given by
∗

λ∗0 (x) = λ̃Σ (x, λβ )Σ∗ (x)−1 , λβ (xi ) =

∂E[ρ(zi , β, γ0)|xi ]
∂β

′
β=β0

, Σ∗ (xi ) = V ar(ρi |xi ), ρi = ρ(zi , β0 , γ0).

Theorem 11: The instruments ϕ∗ (xi ) give an efficient estimator in the class of IV estimators with orthogonal instruments.
The asymptotic variance of the GMM estimator with optimal orthogonal instruments is
−1
(E[m∗i m∗′
= E[λ̃(xi , λ∗ , Σ∗ )Σ∗ (xi )−1 λ̃(xi , λ∗ , Σ∗ )′ ])−1 .
i ])

This matrix coincides with the semiparametric variance bound of Ai and Chen (2003). Estimation of the optimal orthogonal instruments is beyond the scope of this paper. The series
estimator of Ai and Chen (2003) could be used for this.
This framework includes moment restrictions with a NPIV first step γ satisfying E[ρ(zi , γ0 )|xi ] =
0 where we can specify ρ1 (z, β, γ) = m(z, β, γ), x1i = 1, ρ2 (z, β, γ) = ρ(z, γ), and x2i = xi . It
generalizes that setup by allowing for more residuals ρj (z, β, γ), (j ≥ 3) and allowing all residuals
to depend on β.

7

Asymptotic Theory

In this Section we give simple and general asymptotic theory for LR estimators that incorporates
the cross-fitting of equation (2.9). Throughout we use the structure of LR moment functions that
are the sum ψ(z, β, γ, λ) = m(z, β, γ)+φ(z, β, γ, λ) of an identifying or original moment function
m(z, β, γ) depending on a first step function γ and an influence adjustment term φ(z, β, γ, λ)
that can depend on an additional first step λ. The asymptotic theory will apply to any moment
function that can be decomposed into a function of a single nonparametric estimator and a
function of two nonparametric estimators. This structure and LR leads to particularly simple
and general conditions.
The conditions we give are composed of mean square consistency conditions for first steps
and one, two, or three rate conditions for quadratic remainders. We will only use one quadratic
√
remainder rate for DR moment conditions, involving faster than 1/ n convergence of products
of estimation errors for γ̂ and λ̂. When E[m(zi , β0 , γ) + φ(zi , β0 , γ, λ0 )] is not affine in γ we
will impose a second rate condition that involves faster than n−1/4 convergence of γ̂. When
E[φ(zi , γ0 , λ)] is also not affine in λ we will impose a third rate condition that involves faster
than n−1/4 convergence of λ̂. Most adjustment terms φ(z, β, γ, λ) of which we are aware, including
27

for first step conditional moment restrictions and densities, have E[φ(zi , β0 , γ0 , λ)] affine in λ,
so that faster n−1/4 convergence of λ̂ will not be required under our conditions. It will suffice
for most LR estimators which we know of to have faster than n−1/4 convergence of γ̂ and faster
√
than 1/ n convergence of the product of estimation errors for γ̂ and λ̂, with only the latter
condition imposed for DR moment functions. We also impose some additional conditions for
convergence of the Jacobian of the moments and sample second moments that give asymptotic
normality and consistent asymptotic variance estimation for β̂.
An important intermediate result for asymptotic normality is
n
√
1 X
ψ(zi , β0 , γ0 , λ0 ) + op (1),
(7.1)
nψ̂(β0 ) = √
n i=1
where ψ̂(β) is the cross-fit, sample, LR moments of equation (2.9). This result will mean
that the presence of the first step estimators has no effect on the limiting distribution of the
moments at the true β0 . To formulate conditions for this result we decompose the difference
between the left and right-hand sides into several remainders. Let φ(z, γ, λ) = φ(z, β0 , γ, λ),
φ̄(γ, λ) = E[φ(zi , γ, λ)], and m̄(γ) = E[m(zi , β0 , γ)], so that ψ̄(γ, λ) = m̄(γ) + φ̄(γ, λ) Then
adding and subtracting terms gives
n
X
√
n[ψ̂(β0 ) −
ψ(zi , β0 , γ0, λ0 )/n] = R̂1 + R̂2 + R̂3 + R̂4 ,
(7.2)
i=1

where

n

1 X
R̂1 = √
[m(zi , β0 , γ̂i ) − m(zi , β0 , γ0 ) − m̄(γ̂i )]
n i=1

(7.3)

n

1 X
+√
[φ(zi , γ̂i , λ0 ) − φ(zi , γ0 , λ0 ) − φ̄(γ̂i , λ0 ) + φ(zi , γ0 , λ̂i ) − φ(zi , γ0 , λ0 ) − φ̄(γ0 , λ̂i )],
n i=1
n

1 X
R̂2 = √
[φ(zi , γ̂i , λ̂i ) − φ(zi , γ̂i , λ0 ) − φ(zi , γ0 , λ̂i ) + φ(zi , γ0 , λ0 )],
n i=1
n

1 X
R̂3 = √
ψ̄(γ̂i , λ0 ),
n i=1

n

1 X
R̂4 = √
φ̄(γ0 , λ̂i ),
n i=1

We specify regularity conditions sufficient for each of R̂1 , R̂2 , R̂3 , and R̂4 to converge in
probability to zero so that equation (7.1) will hold. The remainder term R̂1 is a stochastic
equicontinuity term as in Andrews (1994). We give mean square consistency conditions for
p
R̂1 −→ 0 in Assumption 3.
The remainder term R̂2 is a second order remainder that involves both γ̂ and λ̂. When the
influence adjustment is φ(z, γ, λ) = λ(x)[y − γ(w)], as for conditional moment restrictions, then
n
−1 X
R̂2 = √
[λ̂i (xi ) − λ0 (xi )][γ̂i (wi ) − γ0 (wi )].
n i=1
28

R̂2 will converge to zero when the product of convergence rates for λ̂i (xi ) and γ̂i (wi ) is faster
√
than 1/ n. However, that is not the weakest possible condition. Weaker conditions for locally
linear regression first steps are given by Firpo and Rothe (2017) and for series regression first
steps by Newey and Robins (2017). These weaker conditions still require that the product of
√
biases of λ̂i (xi ) and γ̂i (wi ) converge to zero faster than 1/ n but have weaker conditions for
p
variance terms. We allow for these weaker conditions by allowing R̂2 −→ 0 as a regularity
condition. Assumption 5 gives these conditions.
p
p
We will have R̂3 = R̂4 = 0 in the DR case of Assumption 1, where R̂1 −→ 0 and R̂2 −→ 0
will suffice for equation (7.1). In non DR cases LR leads to ψ̄(γ, λ0 ) = m̄(γ) + φ̄(γ, λ0 ) having a
p
zero functional derivative with respect to γ at γ0 so that R̂3 −→ 0 when γ̂i converges to γ0 at a
rapid enough, feasible rate. For example if ψ̄(γ, λ0 ) is twice continuously Frechet differentiable
in a neighborhood of γ0 for a norm k·k , with zero Frechet derivative at γ0 . Then
R̂3 ≤ C
p

L
X
√
ℓ=1

p

n kγ̂ℓ − γ0 k2 −→ 0

when kγ̂ − γ0 k = op (n−1/4 ). Here R̂3 −→ 0 when each γ̂ℓ converges to γ0 more quickly than n−1/4 .
It may be possible to weaken this condition by bias correcting m(z, β, γ̂), as by the bootstrap
in Cattaneo and Jansson (2017), by the jackknife in Cattaneo Ma and Jansson (2017), and by
cross-fitting in Newey and Robins (2017). Consideration of such bias corrections for m(z, β, γ̂)
is beyond the scope of this paper.
In many cases R̂4 = 0 even though the moment conditions are not DR. For example that is
true when γ̂ is a pdf or when γ0 estimates the solution to a conditional moment restriction. In
p
such cases mean square consistency, R̂2 −→ 0, and faster than n−1/4 consistency of γ̂ suffices
for equation (7.1); no convergence rate for λ̂ is needed. The simplification that R̂4 = 0 seems
to be the result of λ being a Riesz representer for the linear functional that is the derivative of
m̄(γ) with respect to γ. Such a Riesz representer will enter φ̄(λ, γ0) linearly, leading to R̂4 = 0.
p
When R̂4 6= 0 then R̂4 −→ 0 will follow from twice Frechet differentiability of φ̄(λ, γ0 ) in λ and
faster than n−1/4 convergence of λ̂.
All of the conditions can be easily checked for a wide variety of machine learning and conventional nonparametric estimators. There are well known conditions for mean square consistency
for many conventional and machine learning methods. Rates for products of estimation errors
are also know for many first step estimators as are conditions for n−1/4 consistency. Thus,
the simple conditions we give here are general enough to apply to a wide variety of first step
estimators.
p
The first formal assumption of this section is sufficient for R̂1 −→ 0.
R

Assumption 4: For each ℓ = 1, ..., L, i) Either m(z, β0 , γ) does not depend on z or
R
p
p
{m(z, β0 , γ̂ℓ ) − m(z, β0 , γ0 )}2 F0 (dz) −→ 0, ii) {φ(z, γ̂ℓ , λ0 ) − φ(z, γ0 , λ0 )}2 F0 (dz) −→ 0, and
29

R

p

{φ(z, γ0 , λ̂ℓ ) − φ(z, γ0 , λ0 )}2 F0 (dz) −→ 0;

The cross-fitting used in the construction of ψ̂(β0 ) is what makes the mean-square consistency
p
p
conditions of Assumption 4 sufficient for R̂1 −→ 0. The next condition is sufficient for R̂2 −→ 0.
Assumption 5: For each ℓ = 1, ..., L, either i)
Z
√
p
n max |φj (z, γ̂ℓ , λ̂ℓ ) − φj (z, γ0 , λ̂ℓ ) − φj (z, γ̂ℓ , λ0 ) + φj (z, γ0 , λ0 )|F0 (dz) −→ 0
j

p

or ii) R̂2 −→ 0.
p

As previously discussed, this condition allows for just R̂2 −→ 0 in order to allow the weak
regularity conditions of Firpo and Rothe (2017) and Newey and Robins (2017). The first result
of this Section shows that Assumptions 4 and 5 are sufficient for equation (7.1) when the moment
functions are DR.
Lemma 12: If Assumption 1 is satisfied, with probability approaching one γ̂ ∈ Γ, λ̂ ∈ Λ,
and Assumptions 4 and 5 are satisfied then equation (7.1) is satisfied.
An important class of DR estimators are those from equation (5.2). The following result
gives conditions for asymptotic linearity of these estimators:
Theorem 13: If a) Assumptions 2 and 4 i) are satisfied with γ̂ ∈ Γ and λ̂ ∈ Λ with
probability approaching one; b) λ0 (xi ) and E[{yi − γ0 (wi )}2 |xi ] are bounded; c) for each ℓ =
R
R
p
p
1, ..., L, [γ̂ℓ (w) − γ0 (w)]2 F0 (dz) −→ 0, [λ̂ℓ (x) − λ0 (x)]2 F0 (dz) −→ 0, and either
√

n

Z

or

2

[γ̂ℓ (w) − γ0 (w)] F0 (dw)

1/2 Z

2

[λ̂ℓ (x) − λ0 (x)] F0 (dx)

1/2

p

−→ 0

1 X
p
√
{γ̂ℓ (wi ) − γ0 (wi )}{λ̂ℓ (xi ) − λ0 (xi )} −→ 0;
n i∈I
ℓ

then

√

n

1 X
[g(zi , γ0) − β0 + λ0 (xi ){yi − γ0 (wi )}] + op (1).
n(β̂ − β0 ) = √
n i=1

The conditions of this result are simple, general, and allow for machine learning first steps.
Conditions a) and b) simply require mean square consistency of the first step estimators γ̂ and
λ̂. The only convergence rate condition is c), which requires a product of estimation errors for
√
the two first steps to go to zero faster than 1/ n. This condition allows for a trade-off in
convergence rates between the two first steps, and can be satisfied even when one of the two
30

rates is not very fast. This trade-off can be important when λ0 (x) is not continuous in one of
the components of x, as in the surplus bound example. Discontinuity in x can limit that rate at
which λ0 (x) can be estimated. This result extends the results of Chernozhukov et al. (2018) and
Farrell (2015) for DR estimators of treatment effects to the whole novel class of DR estimators
from equation (5.2) with machine learning first steps. In interesting related work, Athey et al.
(2016) show root-n consistent estimation of an average treatment effect is possible under very
weak conditions on the propensity score, under strong sparsity of the regression function. Thus,
for machine learning the conditions here and in Athey et al. (2016) are complementary and
one may prefer either depending on whether or not the regression function can be estimated
extremely well based on a sparse method. The results here apply to many more DR moment
conditions.
DR moment conditions have the special feature that R̂3 and R̂4 in Proposition 4 are equal
to zero. For estimators that are not DR we impose that R̂3 and R̂4 converge to zero.
Assumption 6: For each ℓ = 1, ..., L, i)

√

p

nψ̄(γ̂ℓ , λ0 ) −→ 0 and ii)

√

p

nφ̄(γ0 , λ̂ℓ ) −→ 0.

Assumption 6 requires that γ̂ converge to γ0 rapidly enough but places no restrictions on
the convergence rate of λ̂ when φ̄(γ0 , λ̂ℓ ) = 0.
Lemma 14: If Assumptions 4-6 are satisfied then equation (7.1) is satisfied.
Assumptions 4-6 are based on the decomposition of LR moment functions into an identifying
part and an influence function adjustment. These conditions differ from other previous work in
semiparametric estimation, as in Andrews (1994), Newey (1994), Newey and McFadden (1994),
Chen, Linton, and van Keilegom (2003), Ichimura and Lee (2010), Escanciano et al. (2016), and
Chernozhukov et al. (2018), that are not based on this decomposition. The conditions extend
Chernozhukov et. al. (2018) to many more DR estimators and to estimators that are nonlinear
in γ̂ but only require a convergence rate for γ̂ and not for λ̂.
This framework helps explain the potential problems with ”plugging in” a first step machine
learning estimator into a moment function that is not LR. Lemma 14 implies that if Assumptions
P
√
√ p
4-6 are satisfied for some λ̂ then nm̂(β0 ) − ni=1 ψ(zi , β0 , γ0)/ n −→ 0 if and only if
n

1 X
p
φ(zi , γ̂, λ̂) −→ 0.
R̂5 = √
n i=1

(7.4)

The plug-in method will fail when this equation does not hold. For example, suppose γ0 = E[y|x]
so that by Proposition 4 of Newey (1994),
n

n

−1 X
1 X
√
φ(zi , γ̂, λ̂) = √
λ̂i (xi )[yi − γ̂i (xi )].
n i=1
n i=1
31

p

Here R̂5 −→ 0 is an approximate orthogonality condition between the approximation λ̂i (xi )
to λ0 (xi ) and the nonparametric first stage residuals yi − γ̂i (xi ). Machine learning uses model
selection in the construction of γ̂i (xi ). If the model selected by γ̂i (xi ) to approximate γ0 (xi ) is
not rich (or dense) enough to also approximate λ0 (xi ) then λ̂i (xi ) need not be approximately
orthogonal to yi − γ̂i (xi ) and R̂5 need not converge to zero. In particular, if the variables selected
to be used to approximate γ0 (xi ) cannot be used to also approximate λ0 (xi ) then the approximate
orthogonality condition can fail. This phenomenon helps explain the poor performance of the
plug-in estimator shown in Belloni, Chernozhukov, and Hansen (2014) and Chernozhukov et al.
(2017, 2018). The plug-in estimator can be root-n consistent if the only thing being selected is
an overall order of approximation, as in the series estimation results of Newey (1994). General
conditions for root-n consistency of the plug-in estimator can be formulated using Assumptions
p
4-6 and R̂2 −→ 0, which we do in Appendix D.
Another component of an asymptotic normality result is convergence of the Jacobian term
∂ ψ̂(β)/∂β to M = E[∂ψ(zi , β, γ0, λ0 )/∂β|β=β0 ]. We impose the following condition for this
purpose.
Assumption 7: M exists and there is a neighborhood N of β0 and k·k such that i) for each
p
p
ℓ, kγ̂ℓ − γ0 k −→ 0, λ̂ℓ − λ0 −→ 0; ii) for all kγ − γ0 k and kλ − λ0 k small enough ψ(zi , β, γ, λ)
is differentiable in β on N with probability approaching 1 iii) there is ζ ′ > 0 and d(zi ) with
E[d(zi )] < ∞ such that for β ∈ N and kγ − γ0 k small enough
′
∂ψ(zi , β, γ, λ) ∂ψ(zi , β0 , γ, λ)
≤ d(zi ) kβ − β0 kζ ;
−
∂β
∂β

iii) For each ℓ = 1, ..., L, j, and k,
0,

R

p

∂ψj (z, β0 , γ̂ℓ , λ̂ℓ )/∂βk − ∂ψj (z, β0 , γ0 , λ0 )/∂βk F0 (dz) −→

The following intermediate result gives Jacobian convergence.
p

Lemma 15: If Assumption 7 is satisfied then for any β̄ −→ β0 , ψ̂(β) is differentiable at β̄
p
with probability approaching one and ∂ ψ̂(β̄)/∂β −→ M.
With these results in place the asymptotic normality of semiparametric GMM follows in a
standard way.
p

p

Theorem 16: If Assumptions 4-7 are satisfied, β̂ −→ β0 , Ŵ −→ W , M ′ W M is nonsingular, and E[kψ(zi , β0 , γ0 , λ0 )k2 ] < ∞ then for Ω = E[ψ(zi , β0 , γ0 , λ0 )ψ(zi , β0 , γ0 , λ0 )′ ],
√

d

n(β̂ − β0 ) −→ N(0, V ), V = (M ′ W M)−1 M ′ W ΩW M(M ′ W M)−1 .

32

It is also useful to have a consistent estimator of the asymptotic variance of β̂. As usual such
an estimator can be constructed as
V̂ = (M̂ ′ Ŵ M̂ )−1 M̂ ′ Ŵ Ω̂Ŵ M̂ (M̂ ′ Ŵ M̂)−1 ,
L

M̂ =

1 XX
∂ ψ̂(β̂)
ψ(zi , β̂, γ̂ℓ , λ̂ℓ )ψ(zi , β̂, γ̂ℓ , λ̂ℓ )′ .
, Ω̂ =
∂β
n ℓ=1 i∈I
ℓ

Note that this variance estimator ignores the estimation of γ and λ which works here because
the moment conditions are LR. The following result gives conditions for consistency of V̂ .
Theorem 17: If Assumptions 4 and 7 are satisfied with E[b(zi )2 ] < ∞, M ′ W M is nonsingular, and
Z
2
p
φ(z, γ̂ℓ , λ̂ℓ ) − φ(z, γ0 , λ̂ℓ ) − φ(z, γ̂ℓ , λ0 ) + φ(z, γ0 , λ0 ) F0 (dz) −→ 0
p

p

then Ω̂ −→ Ω and V̂ −→ V.
In this section we have used cross-fitting and a decomposition of moment conditions into
identifying and influence adjustment components to formulate simple and general conditions for
asymptotic normality of LR GMM estimators. For reducing higher order bias and variance it
may be desirable to let the number of groups grow with the sample size. That case is beyond
the scope of this paper.

8

Appendix A: Proofs of Theorems

Proof of Theorem 1: By ii) and iii),
Z
Z
0 = (1 − τ ) φ(z, Fτ )F0 (dz) + τ φ(z, Fτ )G(dz).

Dividing by τ and solving gives
Z
Z
Z
1
φ(z, Fτ )F0 (dz) = − φ(z, Fτ )G(dz) + φ(z, Fτ )F0 (z).
τ

Taking limits as τ −→ 0, τ > 0 and using i) gives
Z
Z
d
dµ(Fτ )
φ(z, Fτ )F0 (dz) = − φ(z, F0 )G(dz) + 0 = −
. Q.E.D.
dτ
dτ
Proof of Theorem 2: We begin by deriving φ1 , the adjustment term for the first step CCP
estimation. We use the definitions given in the body of the paper. We also let
Pṽj (ṽ) = ∂P (ṽ)/∂ṽj , π1 = Pr(yt1 = 1), λ10 (x) = E[y1t |xt+1 = x],
ytj
λj0(x) = E[A(xt )Pṽj (ṽt )
|xt+1 = x], (j = 2, ..., J).
Pj (ṽt )
33

Consider a parametric submodel as described in Section 4 and let γ1 (x, τ ) denote the conditional
expectation of yt given xt under the parametric submodel. Note that for ṽt = ṽ(xt ),
∂E[H(γ1 (xt+1 , τ ))|xt , ytj = 1]
]
∂τ
ytj
∂
E[A(xt )Pvj (ṽt )
H(γ1(xt+1 , τ ))]
=
∂τ
Pj (ṽt )
∂
ytj
=
E[E[A(xt )Pvj (ṽt )
|xt+1 ]H(γ1 (xt+1 , τ ))]
∂τ
Pj (ṽt )
∂
∂
=
E[λj0 (xt+1 )H(γ1(xt+1 , τ ))] =
E[λj0 (xt )H(γ1 (xt , τ ))]
∂τ
∂τ
∂H(γ10 (xt )) ′
∂H(γ10 (xt )) ′ ∂γ1 (xt , τ )
{yt − γ10 (xt )}S(zt )].
] = E[λj0 (xt )
= E[λj0(xt )
∂P
∂τ
∂P

E[A(xt )Pṽj (ṽt )

where the last (sixth) equality follows as in Proposition 4 of Newey (1994a), and the fourth
equality follows by equality of the marginal distributions of xt and xt+1 . Similarly, for π1 =
Pr(yt1 = 1) and λ10 (x) = E[y1t |xt+1 = x] we have
∂E[π1−1 y1t H(γ1 (xt+1 , τ ))]
∂E[π1−1 λ10 (xt+1 )H(γ1 (xt+1 , τ ))]
∂E[H(γ1 (xt+1 , τ ))|yt1 = 1]
=
=
∂τ
∂τ
∂τ
−1
∂E[π1 λ10 (xt )H(γ1 (xt , τ ))]
=
∂τ
∂H(γ10 (xt )) ′
−1
{yt − γ10 (xt )}S(zt )]
= E[π1 λ10 (xt )
∂P
Then combining terms gives
∂E[m(zt , β0 , γ1 (τ ), γ−10 )]
∂τ
J
X
∂E[H(γ1 (xt+1 , τ ))|xt , ytj = 1]
= −δ
{E[A(xt )Pvj (ṽt )
]
∂τ
j=2
− E[A(xt )Pvj (ṽt )]
= −δ

J
X
j=2

∂E[H(γ1 (xt+1 , τ ))|yt1 = 1]
}
∂τ

E[{λj0 (xt ) − E[A(xt )Pṽj (ṽt )]π1−1 λ10 (xt )}

∂H(γ10 (xt )) ′
{yt − γ10 (xt )}S(zt )]
∂P

= E[φ1 (zt , β0 , γ0 , λ0 )S(zt )].

Next, we show the result for φj (z, β, γ, λ) for 2 ≤ j ≤ J. As in the proof of Proposition 4 of
Newey (1994a), for any wt we have
ytj
∂
E[wt |xt , ytj = 1, τ ] = E[
{wt − E[wt |xt , ytj = 1]}S(zt )|xt ].
∂τ
Pj (ṽt )
34

It follows that
∂E[m(zt , β0 , γj (τ ), γ−j,0)]
∂E[u1,t+1 + Ht+1 |xt , ytj = 1, τ ]
= −δE[A(xt )Pvj (ṽt )
]
∂τ
∂τ
∂
= −δ E[E[A(xt )Pvj (ṽt ){u1,t+1 + Ht+1 }|xt , ytj = 1, τ ]].
∂τ
ytj
= −δE[A(xt )Pvj (ṽt )
{u1,t+1 + Ht+1 − γj0(xt , β0 , γ1 )}S(zt )]
Pj (ṽt )
= E[φj (zt , β0 , γ0 , λ0 )S(zt )],
showing that the formula for φj is correct. The proof for φJ+1 follows similarly. Q.E.D.
Proof of Theorem 3: Given in text.
Proof of Theorem 4: Given in text.
Proof of Theorem 5: Let ψ̄(γ, λ) = E[ψ(zi , β0 , γ, λ)]. Suppose that ψ(z, β, γ, λ) is DR.
Then for any γ 6= γ0 , γ ∈ Γ we have
0 = ψ̄(γ, λ0 ) = ψ̄(γ0 , λ0 ) = ψ̄((1 − τ )γ0 + τ γ, λ0 ),
for any τ. Therefore for any τ ,
ψ̄((1 − τ )γ0 + τ γ, λ0 ) = 0 = (1 − τ )ψ̄(γ0, λ0 ) + τ ψ̄(γ, λ0 ),
so that ψ̄(γ, λ0 ) is affine in γ. Also by the previous equation ψ̄((1 − τ )γ0 + τ γ, λ0 ) = 0 identically
in τ so that
∂
ψ̄((1 − τ )γ0 + τ γ, λ0 ) = 0,
∂τ
where the derivative with respect to τ is evaluated at τ = 0. Applying the same argument
switching of λ and γ we find that ψ̄(γ0 , λ) is affine in λ and ∂ ψ̄(γ0 , (1 − τ )λ0 + τ λ)/∂τ = 0.
Next suppose that ψ̄(γ, λ0 ) is affine γ and ∂ ψ̄((1−τ )γ0 +τ γ, λ0 )/∂τ = 0. Then by ψ̄(γ0, λ0 ) =
0, for any γ ∈ Γ,
ψ̄(γ, λ0 ) = ∂[τ ψ̄(γ, λ0 )]/∂τ = ∂[(1 − τ )ψ̄(γ0 , λ0 ) + τ ψ̄(γ, λ0 )]/∂τ
= ∂ ψ̄((1 − τ )γ0 + τ γ, λ0 )/∂τ = 0.

Switching the roles of γ and λ it follows analogously that ψ̄(γ0 , λ) = 0 for all λ ∈ Λ, so ψ̄(γ, λ)
is doubly robust. Q.E.D.
Proof of Theorem 6: Let λ0 (x) = −c′ Π−1 a(x) so that E[λ0 (xi )|wi ] = −c′ Π−1 Πp(wi ) =
−c′ p(wi ).Then integration by parts gives
E[m(zi , β0 , γ̃)] = E[c′ p(wi ){γ̃(wi ) − γ0 (wi )}] = −E[γ0 (xi ){γ̃(wi ) − γ0 (wi )}]

= E[γ0 (xi ){yi − γ̃(wi )}] = −c′ Π−1 E[a(xi ){yi − γ̃(wi )}] = 0. Q.E.D.
35

Proof of Theorem 7: If λ0 is identified then m(z, β, γ̄, λ0 ) is identified for every β. By DR
E[m(zi , β, γ̄, λ0 )] = 0
at β = β0 and by assumption this is the only β where this equation is satisfied. Q.E.D.
Proof of Corollary 8: Given in text.
Proof of Theorem 9: Note that for ρi = ρ(zi , β0 , γ0 ),
ψ̄(γ0 , (1 − τ )λ0 + τ λ)] = (1 − τ )E[λ0 (xi )ρi ] + τ E[λ(xi )ρi ] = 0.

(8.1)

Differentiating gives the second equality in eq. (2.7). Also, for ∆ = γ − γ0 ,
∂ ψ̄((1 − τ )γ0 + τ γ, λ0 )
= E[λ0 (xi )ρ̄(xi , ∆)] = 0,
∂τ
giving the first equality in eq. (2.7). Q.E.D.
Proof of Theorem 10: The first equality in eq. (8.1) of the proof of Theorem 9 shows that
ψ̄(γ0 , λ) is affine in λ. Also,
ψ̄((1−τ )γ0 +τ γ, λ0 ) = E[λ0 (xi ){(1−τ )ρ(zi , β0 , γ0 )+τ ρ(zi , β0 , γ)}] = (1−τ )ψ̄(γ0 , λ0 )+τ ψ̄(γ, λ0 ),
so that ψ̄(γ, λ0 ) is affine in γ. The conclusion then follows by Theorem 5. Q.E.D.
∗

Proof of Theorem 11: To see that λ̃Σ (xi , λ∗ )Σ∗ (xi )−1 minimizes the asymptotic variance note that for any orthogonal instrumental variable matrix λ0 (x), by the rows of λβ (xi ) −
∗
λ̃Σ (xi , λβ ) being in Λ̄γ ,
∗

∗

M = E[λ0 (xi )λβ (xi )′ ] = E[λ0 (xi )λ̃Σ (xi , λβ )′ ] = E[λ0 (xi )ρi ρ′i Σ∗ (xi )−1 λ̃Σ (xi , λβ )′ ].
Since the instruments are orthogonal the asymptotic variance matrix of the GMM estimator with
p
∗
Ŵ −→ W is the same as if γ̂ = γ0 . Define mi = M ′ W λ0 (xi )ρi and m∗i = λ̃Σ (xi , λβ )Σ∗ (xi )−1 ρi .
The asymptotic variance of the GMM estimator for orthogonal instruments λ0 (x) is
−1
′
∗ −1′
(M ′ W M)−1 M ′ W E[λ0 (xi )ρi ρ′i λ0 (xi )′ ]W M(M ′ W M)−1 = (E[mi m∗′
.
i ]) E[mi mi ](E[mi mi ])

The fact that this matrix is minimized in the positive semidefinite sense for mi = m∗i is well
known, e.g. see Newey and McFadden (1994). Q.E.D.
The following result is useful for the results of Section 7:
p

Lemma A1: If Assumption 4 is satisfied then R̂1 −→ 0. If Assumption 5 is satisfied then
p
R̂2 −→ 0.
36

ˆ iℓ = m(zi , γ̂ℓ )−m(zi , γ0 )− m̄(γ̂ℓ ) for i ∈ Iℓ and let Z c denote the observations
Proof: Define ∆
ℓ
c
zi for i ∈
/ Iℓ . Note that γ̂ℓ depends only on Zℓ . By construction and independence of Zℓc and
ˆ iℓ |Z c ] = 0. Also by independence of the observations, E[∆
ˆ iℓ ∆
ˆ jℓ |Z c ] = 0
zi , i ∈ Iℓ we have E[∆
ℓ
ℓ
R
ˆ 2 |Z c ] ≤ [m(z, γ̂ℓ ) − m(z, γ0 )]2 F0 (dz). Then we have
for i, j ∈ Iℓ . Furthermore, for i ∈ Iℓ E[∆
iℓ ℓ
1 Xˆ
E[ √
∆iℓ
n i∈I
ℓ

!2

!2
X
X
1
ˆ 2 |Z c ]
ˆ iℓ |Z c ] = 1
E[∆
∆
|Zℓc ] = E[
iℓ ℓ
ℓ
n
n
i∈Iℓ
i∈Iℓ
Z
p
≤ [m(z, γ̂ℓ ) − m(z, γ0 )]2 F0 (dz) −→ 0.

P
p
ˆ iℓ /√n −→
The conditional Markov inequality then implies that i∈Iℓ ∆
0. The analogous results
ˆ iℓ = φ(zi , γ̂ℓ , λ0 ) − φ(zi , γ0 , λ0 ) − φ̄(γ̂ℓ , λ0 ) and ∆
ˆ iℓ = φ(zi , γ0 , λ̂ℓ ) − φ(zi , γ0 , λ0 ) −
also hold for ∆
φ̄(γ0 , λ̂ℓ ). Summing across these three terms and across ℓ = 1, ..., L gives the first conclusion.
For the second conclusion, note that under the first hypothesis of Assumption 5,
1 X
E[ √
[φj (zi , γ̂ℓ , λ̂ℓ ) − φj (zi , γ0, λ̂ℓ ) − φj (zi , γ̂ℓ , λ0 ) + φj (zi , γ0 , λ0 )] |Zℓc ]
n i∈I
ℓ
1 X
E[ φj (zi , γ̂ℓ , λ̂ℓ ) − φj (zi , γ0 , λ̂ℓ ) − φj (zi , γ̂ℓ , λ0 ) + φj (zi , γ0 , λ0 ) |Zℓc ]
≤√
n i∈I
Z ℓ
√
p
≤ n
φj (z, γ̂ℓ , λ̂ℓ ) − φj (z, γ0 , λ̂ℓ ) − φj (z, γ̂ℓ , λ0 ) + φj (zi , γ0 , λ0 ) F0 (dz) −→ 0,
p

so R̂2 −→ 0 follows by the conditional Markov and triangle inequalities. The second hypothesis
p
of Assumption 5 is just R̂2 −→ 0. Q.E.D.
Proof of Lemma 12: By Assumption 1 and the hypotheses that γ̂i ∈ Γ and λ̂i ∈ Λ we
p
p
have R̂3 = R̂4 = 0. By Lemma A1 we have R̂1 −→ 0 and R̂2 −→ 0. The conclusion then follows
by the triangle inequality. Q.E.D.
Proof of Theorem 13: Note that for ε = y − γ0 (w)
φ(z, γ̂, λ0 ) − φ(z, γ0 , λ0 ) = λ0 (x)[γ̂(w) − γ0 (w)],
φ(z, γ0 , λ̂) − φ(z, γ0 , λ0 ) = [λ̂(x) − λ0 (x)]ε,

φ(z, γ̂ℓ , λ̂ℓ ) − φ(z, γ0 , λ̂ℓ ) − φ(z, γ̂ℓ , λ0 ) + φj (z, γ0 , λ0 ) = −[λ̂(x) − λ0 (x)][γ̂(x) − γ0 (x)].
The first part of Assumption 4 ii) then follows by
Z
Z
2
[φ(z, γ̂ℓ , λ0 ) − φ(z, γ0 , λ0 )] F0 (dz) = λ0 (x)2 [γ̂(w) − γ0 (w)]2 F0 (dz)
Z
p
≤ C [γ̂(w) − γ0 (w)]2 F0 (dz) −→ 0.
37

The second part of Assumption 4 ii) follows by
Z
Z
2
[φ(z, γ0 , λ̂ℓ ) − φ(z, γ0 , λ0 )] F0 (dz) = [λ̂ℓ (x) − λ0 (x)]2 ε2 F0 (dz)
Z h
i2
=
λ̂ℓ (x) − λ0 (x) E[ε2 |x]F0 (dz)
Z h
i2
p
≤C
λ̂ℓ (x) − λ0 (x) F0 (dz) −→ 0.
Next, note that by the Cauchy-Schwartz inequality,
Z
√
n |φ(z, γ̂ℓ , λ̂ℓ ) − φ(z, γ0 , λ̂ℓ ) − φ(z, γ̂ℓ , λ0 ) + φ(z, γ0 , λ0 )|F0 (dz)
Z
√
[λ̂ℓ (x) − λ0 (x)][γ̂ℓ (w) − γ0 (w)] F0 (dx)
= n
Z
Z
√
2
1/2
≤ n{ [λ̂ℓ (x) − λ0 (x)] F0 (dx)} { [γ̂ℓ (w) − γ0 (w)]2F0 (dw)}1/2 .

Then the first rate condition of Assumption 5 holds under the first rate condition of Theorem
13 while the second condition of Assumption 5 holds under the last hypothesis of Theorem 13.
Then eq. (7.1) holds by Lemma 12, and the conclusion by rearranging the terms in eq. (7.1).
Q.E.D.
Proof of Lemma 14: Follows by Lemma A1 and the triangle inequality. Q.E.D.
P
Proof of Lemma 15: Let M̂(β) = ∂ ψ̂(β)/∂β when it exists, M̃ℓ = n−1 i∈Iℓ ∂ψ(zi , β0 , γ̂ℓ , λ̂ℓ )/∂β,
P
and M̄ℓ = n−1 i∈Iℓ ∂ψ(zi , β0 , γ0 , λ0 )/∂β. By the law of large numbers, and Assumption 5 iii),
PL
p
ℓ=1 M̄ℓ −→ M. Also, by condition iii) for each j and k,
Z
p
ℓ
E[|M̃ℓjk − M̄ℓjk ||Z ] ≤
∂ψj (z, β0 , γ̂ℓ , λ̂ℓ )/∂βk − ∂ψj (z, β0 , γ0 , λ0 )/∂βk F0 (dz) −→ 0.
Then by the conditional Markov inequality, for each ℓ,
p

M̃ℓ − M̄ℓ −→ 0.
It follows by the triangle inequality that
p
one we have for any β̄ −→ β0
M̂ (β̄) −

L
X
ℓ=1

n

M̃ℓ ≤

PL

p

ℓ=1

M̃ℓ −→ M. Also, with probability approaching

1X
d(zi )
n i=1

!

β̄ − β0

ζ′

p

= Op (1)op (1) −→ 0.

The conclusion then follows by the triangle inequality. Q.E.D.
Proof of Theorem 16: The conclusion follows in a standard manner from the conclusions
of Lemmas 14 and 15. Q.E.D.
38

Proof of Theorem 17: Let ψ̂i = ψ(zi , β̂, γ̂ℓ , λ̂ℓ ) and ψi = ψ(zi , β0 , γ0 , λ0 ). By standard
2
P
p
arguments (e.g. Newey, 1994), it suffices to show that ni=1 ψ̂i − ψi /n −→ 0. Note that
ψ̂i − ψi =

5
X
j=1

ˆ ji , ∆
ˆ 1i = ψ(zi , β̂, γ̂ℓ , λ̂ℓ ) − ψ(zi , β0 , γ̂ℓ , λ̂ℓ ), ∆
ˆ 2i = m(zi , β0 , γ̂ℓ ) − m(zi , β0 , γ0 ),
∆

ˆ 3i = φ(zi , γ̂ℓ , λ0 ) − φ(zi , γ0 , λ0 ), ∆
ˆ 4i = φ(zi , γ0 , λ̂ℓ ) − φ(zi , γ0 , λ0 ),
∆
ˆ 5i = φ(zi , γ̂ℓ , λ̂ℓ ) − φ(zi , γ̂ℓ , λ0 ) − φ(zi , γ0 , λ̂ℓ ) + φ(zi , γ0, λ0 ).
∆
By standard arguments it suffices to show that for each j and ℓ,
1X ˆ
∆ji
n i∈I
ℓ

2

p

−→ 0.

(8.2)

For j = 1 it follows by a mean value expansion and Assumption 7 with E[b(zi )2 ] < ∞ that
!
2
2 p
1X ∂
1 X
1X ˆ 2
∆1i =
β̂ − β −→ 0,
b(zi )2
ψ(zi , β̄, γ̂ℓ , λ̂ℓ )(β̂ − β) ≤
n i∈I
n i∈I ∂β
n i∈I
ℓ

ℓ

ℓ

where β̄ is a mean value that actually differs from row to row of ∂ψ(zi , β̄, γ̂ℓ , λ̂ℓ )/∂β. For j = 2
note that by Assumption 4,
Z
1X ˆ 2 ℓ
p
∆2i |Z ] ≤ km(z, β0 , γ̂ℓ ) − m(z, β0 , γ0 )k2 F0 (dz) −→ 0,
E[
n i∈I
ℓ

so eq. (8.2) holds by the conditional Markov inequality. For j = 3 and j = 4 eq. (8.2) follows
similarly. For j = 5, it follows from the hypotheses of Theorem 17 that
Z
2
1X ˆ 2 ℓ
p
∆5i |Z ] ≤
φ(z, γ̂ℓ , λ̂ℓ ) − φ(z, γ0 , λ̂ℓ ) − φ(z, γ̂ℓ , λ0 ) + φ(z, γ0 , λ0 ) F0 (dz) −→ 0.
E[
n i∈I
ℓ

Then eq. (8.2) holds for j = 5 by the conditional Markov inequality. Q.E.D.

9

Appendix B: Local Robustness and Derivatives of Expected Moments.

In this Appendix we give conditions sufficient for the LR property of equation (2.5) to imply
the properties in equations (2.7) and (2.8). As discussed following equation (2.8), it may be
convenient when specifying regularity conditions for specific moment functions to work directly
with (2.7) and/or (2.8).

39

Assumption B1: There are linear sets Γ and Λ and a set G such that i) ψ̄(γ, λ) is Frechet
differentiable at (γ0 , λ0 ); ii) for all G ∈ G the vector (γ(Fτ ), λ(Fτ )) is Frechet differentiable at
τ = 0; iii) the closure of {∂(γ(Fτ ), λ(Fτ ))/∂τ : G ∈ G} is Γ × Λ.
Theorem B1: If Assumption B1 is satisfied and equation (2.5) is satisfied for all G ∈ G
then equation (2.7) is satisfied.
Proof: Let ψ̄ ′ (γ, λ) denote the Frechet derivative of ψ̄(γ, λ) at (γ0 , λ0 ) in the direction (γ, λ),
which exists by i). By ii), the chain rule for Frechet derivatives (e.g. Proposition 7.3.1 of
G
Luenberger, 1969), and by eq. (2.5) it follows that for (∆G
γ , ∆λ ) = ∂(γ(Fτ ), λ(Fτ ))/∂τ,
G
ψ̄ ′ (∆G
γ , ∆λ ) =

∂ ψ̄(γ(Fτ ), λ(Fτ ))
= 0.
∂τ

By ψ̄ ′ (γ, λ) being a continuous linear function and iii) it follows that ψ̄ ′ (γ, λ) = 0 for all (γ, λ) ∈
Γ × Λ. Therefore, for any γ ∈ Γ and λ ∈ Λ,
ψ̄ ′ (γ − γ0 , 0) = 0, ψ̄ ′ (0, λ − λ0 ) = 0.
Equation (2.7) then follows by i). Q.E.D.
Theorem B2: If equation (2.7) is satisfied and in addition ψ̄(γ, λ0 ) and ψ̄(γ0, λ) are twice
Frechet differentiable in open sets containing γ0 and λ0 respectively with bounded second derivative then equation (2.8) is satisfied.
Proof: Follows by Proposition 7.3.3 of Luenberger (1969). Q.E.D.

10

Appendix C: Doubly Robust Moment Functions for
Orthogonality Conditions

In this Appendix we generalize the DR estimators for conditional moment restrictions to orthogonality conditions for a general residual ρ(z, γ) that is affine in γ but need not have the
form y − γ(w).
Assumption C1: There are linear sets Γ and Λ of functions λ(x) and γ(w) that are closed in
mean square such that i) For any γ, γ̃ ∈ Γ and scalar τ, E[ρ(zi , γ)2 ] < ∞ and ρ(z, (1−τ )γ+τ γ̃) =
(1 − τ )ρ(z, γ) + τ ρ(z, γ̃) ; ii) E[λ(xi )ρ(zi , γ0)] = 0 for all λ ∈ Λ; iii) there exists λ0 ∈ Λ such
that E[m(zi , β0 , γ)] = −E[λ0 (xi )ρ(zi , γ)] for all γ ∈ Γ.
Assumption C1 ii) could be thought of as an identification condition for γ0 . For example, if
Λ is all functions of xi with finite mean square then ii) is E[ρ(zi , γ0 )|xi ] = 0, the nonparametric
40

conditional moment restriction of Newey and Powell (2003) and Newey (1991). Assumption
C1 iii) also has an interesting interpretation. Let Π(a)(xi ) denote the orthogonal mean-square
projection of a random variable a(zi ) with finite second moment on Γ. Then by ii) and iii) we
have
E[m(zi , β0 , γ)] = −E[λ0 (xi )ρ(zi , γ)] = E[λ0 (xi )Π(ρ(γ))(xi )]
= E[λ0 (xi ){Π(ρ(γ))(xi ) − Π(ρ(γ0 ))(xi )}]
= E[λ0 (xi ){Π(ρ(γ) − ρ(γ0 ))(xi )}].

Here we see that E[m(zi , β0 , γ)] is a linear, mean-square continuous function of Π(ρ(γ) −
ρ(γ0 ))(xi ). The Riesz representation theorem will also imply that if E[m(zi , β0 , γ)] is a linear,
mean-square continuous function of Π(ρ(γ) − ρ(γ0 ))(xi ) then λ0 (x) exists satisfying Assumption
C1 ii). For the case where wi = xi this mean-square continuity condition is necessary for existence of a root-n consistent estimator, as in Newey (1994) and Newey and McFadden (1994).
We conjecture that when wi need not equal xi this condition generalizes Severini and Tripathi’s
(2012) necessary condition for existence of a root-n consistent estimator of β0 .
Noting that Assumptions 1 ii) and iii) are the conditions for double robustness we have
Theorem C1: If Assumption C1 is satisfied then ψ(z, β, γ, λ) = m(z, β, γ) + λ(x)ρ(z, γ) is
doubly robust.
It is interesting to note that λ0 (x) satisfying Assumption C1 iii) need not be unique. When
the closure of {Π(ρ(γ))(xi ) : γ ∈ Γ} is not all of Λ then there will exist λ̃ ∈ Λ such that λ̃ 6= 0
and
E[λ̃(xi )ρ(zi , γ)] = E[λ̃(xi )Π(ρ(γ))(xi )] = 0 for all γ ∈ Γ.
In that case Assumption C1 iii) will also be satisfied for λ0 (xi ) + λ̃(xi ). We can think of this case
as one where γ0 is overidentified, similarly to Chen and Santos (2015). As discussed in Ichimura
and Newey (2017), the different λ0 (xi ) would correspond to different first step estimators.
The partial robustness results of the last Section can be extended to the orthogonality condition setting of Assumption C1. Let Λ∗ be a closed linear subset of Λ, such as finite dimensional
linear set and let γ ∗ be such that E[λ(xi )ρ(zi , γ ∗ )] = 0 for all λ ∈ Λ∗ . Note that if λ0 ∈ Λ∗ it
follows by Theorem C1 that
E[m(zi , β0 , γ ∗ )] = −E[λ0 (xi )ρ(zi , γ ∗ )] = 0.
Theorem C2: If Λ∗ is a closed linear subset of Λ, E[λ(xi )ρ(zi , γ ∗ )] = 0 for all λ ∈ Λ∗ ,
and Assumption C2 iii) is satisfied with λ0 ∈ Λ∗ then
E[m(zi , β0 , γ ∗ )] = 0.
.
41

11

Appendix D: Regularity Conditions for Plug-in Estimators

In this Appendix we formulate regularity conditions for root-n consistency and asymptotic normality of the plug-in estimator β̃ as described in Section 2, where m(z, β, γ) need not be LR.
These conditions are based on Assumptions 4-6 applied to the influence adjustment φ(z, γ, λ)
corresponding to m(z, β, γ) and γ̂. For this purpose we treat λ̂ as any object that can approximate λ0 (x), not just as an estimator of λ0 .
Theorem D1: If Assumptions 4-6 are satisfied, Assumption 7 is satisfied with m(z, β, γ)
p
p
replacing ψ(z, β, γ, λ), β̃ −→ β0 , Ŵ −→ W , M ′ W M is nonsingular, E[kψ(zi , β0 , γ0 , λ0 )k2 ] < ∞,
and
n
1 X
p
R̂5 = √
φ(zi , γ̂i , λ̂i ) −→ 0,
n i=1
then for Ω = E[ψ(zi , β0 , γ0, λ0 )ψ(zi , β0 , γ0 , λ0 )′ ],
√

d

n(β̂ − β0 ) −→ N(0, V ), V = (M ′ W M)−1 M ′ W ΩW M(M ′ W M)−1 .
p

p

The condition R̂5 −→ 0 was discussed in Section 7. It is interesting to note that R̂5 −→ 0
appears to be a complicated condition that seems to depend on details of the estimator γ̂i in a
way that Assumptions 4-7 do not. In this way the regularity conditions for the LR estimator
seem to be more simple and general than those for the plug-in estimator.
Acknowledgements
Whitney Newey gratefully acknowledges support by the NSF. Helpful comments were provided by M. Cattaneo, B. Deaner, J. Hahn, M. Jansson, Z. Liao, A. Pakes, R. Moon, A. de
Paula, V. Semenova, and participants in seminars at Cambridge, Columbia, Cornell, HarvardMIT, UCL, USC, Yale, and Xiamen. B. Deaner provided capable research assistance.

REFERENCES
Ackerberg, D., X. Chen, and J. Hahn (2012): ”A Practical Asymptotic Variance Estimator
for Two-step Semiparametric Estimators,” The Review of Economics and Statistics 94: 481–498.
Ackerberg, D., X. Chen, J. Hahn, and Z. Liao (2014): ”Asymptotic Efficiency of Semiparametric Two-Step GMM,” The Review of Economic Studies 81: 919–943.
Ai, C. and X. Chen (2003): “Efficient Estimation of Models with Conditional Moment Restrictions Containing Unknown Functions,” Econometrica 71, 1795-1843.
42

Ai, C. and X. Chen (2007): ”Estimation of Possibly Misspecified Semiparametric Conditional
Moment Restriction Models with Different Conditioning Variables,” Journal of Econometrics
141, 5–43.
Ai, C. and X. Chen (2012): ”The Semiparametric Efficiency Bound for Models of Sequential
Moment Restrictions Containing Unknown Functions,” Journal of Econometrics 170, 442–457.
Andrews, D.W.K. (1994): “Asymptotics for Semiparametric Models via Stochastic Equicontinuity,” Econometrica 62, 43-72.
Athey, S., G. Imbens, and S. Wager (2017): ”Efficient Inference of Average Treatment Effects in High Dimensions via Approximate Residual Balancing,” Journal of the Royal Statistical
Society, Series B, forthcoming.
Bajari, P., V. Chernozhukov, H. Hong, and D. Nekipelov (2009): ”Nonparametric and
Semiparametric Analysis of a Dynamic Discrete Game,” working paper, Stanford.
Bajari, P., H. Hong, J. Krainer, and D. Nekipelov (2010): ”Estimating Static Models of
Strategic Interactions,” Journal of Business and Economic Statistics 28, 469-482.
Bang, and J.M. Robins (2005): ”Doubly Robust Estimation in Missing Data and Causal Inference Models,” Biometrics 61, 962–972.
Belloni, A., D. Chen, V. Chernozhukov, and C. Hansen (2012): “Sparse Models and
Methods for Optimal Instruments with an Application to Eminent Domain,” Econometrica 80,
2369–2429.
Belloni, A., V. Chernozhukov, and Y. Wei (2013): “Honest Confidence Regions for Logistic
Regression with a Large Number of Controls,” arXiv preprint arXiv:1304.3969.
Belloni, A., V. Chernozhukov, and C. Hansen (2014): ”Inference on Treatment Effects
after Selection among High-Dimensional Controls,” The Review of Economic Studies 81, 608–
650.
Belloni, A., V. Chernozhukov, I. Fernandez-Val, and C. Hansen (2016): ”Program
Evaluation and Causal Inference with High-Dimensional Data,” Econometrica 85, 233-298.
Bera, A.K., G. Montes-Rojas, and W. Sosa-Escudero (2010): ”General Specification
Testing with Locally Misspecified Models,” Econometric Theory 26, 1838–1845.
Bickel, P.J. (1982): ”On Adaptive Estimation,” Annals of Statistics 10, 647-671.
Bickel, P.J. and Y. Ritov (1988): ”Estimating Integrated Squared Density Derivatives: Sharp
Best Order of Convergence Estimates,” Sankhyā: The Indian Journal of Statistics, Series A
238, 381-393.
Bickel, P.J., C.A.J. Klaassen, Y. Ritov, and J.A. Wellner (1993): Efficient and Adaptive
Estimation for Semiparametric Models, Springer-Verlag, New York.
Bickel, P.J. and Y. Ritov (2003): ”Nonparametric Estimators Which Can Be ”Plugged-in,”
43

Annals of Statistics 31, 1033-1053.
Bonhomme, S., and M. Weidner (2018): ”Minimizing Sensitivity to Misspecification,” working
paper.
Cattaneo, M.D., and M. Jansson (2017): ”Kernel-Based Semiparametric Estimators: Small
Bandwidth Asymptotics and Bootstrap Consistency,” Econometrica, forthcoming.
Cattaneo, M.D., M. Jansson, and X. Ma (2017): ”Two-step Estimation and Inference with
Possibly Many Included Covariates,” working paper.
Chamberlain, G. (1987): “Asymptotic Efficiency in Estimation with Conditional Moment Restrictions,” Journal of Econometrics 34, 1987, 305–334.
Chamberlain, G. (1992): “Efficiency Bounds for Semiparametric Regression,” Econometrica 60,
567–596.
Chen, X. and X. Shen (1997): “Sieve Extremum Estimates for Weakly Dependent Data,”
Econometrica 66, 289-314.
Chen, X., O.B. Linton, and I. van Keilegom (2003): “Estimation of Semiparametric Models
when the Criterion Function Is Not Smooth,” Econometrica 71, 1591-1608.
Chen, X., and Z. Liao (2015): ”Sieve Semiparametric Two-Step GMM Under Weak Dependence”, Journal of Econometrics 189, 163–186.
Chen, X., and A. Santos (2015): “Overidentification in Regular Models,” working paper.
Chernozhukov, V., C. Hansen, and M. Spindler (2015): ”Valid Post-Selection and PostRegularization Inference: An Elementary, General Approach,” Annual Review of Economics 7:
649–688.
Chernozhukov, V., G.W. Imbens and W.K. Newey (2007): ”Instrumental Variable Identification and Estimation of Nonseparable Models,” Journal of Econometrics 139, 4-14.
Chernozhukov, V., D. Chetverikov, M. Demirer, E. Duflo, C. Hansen, W. Newey
(2017): ”Double/Debiased/Neyman Machine Learning of Treatment Effects,” American Economic Review Papers and Proceedings 107, 261-65.
Chernozhukov, V., D. Chetverikov, M. Demirer, E. Duflo, C. Hansen, W. Newey,
J. Robins (2018): ”Debiased/Double Machine Learning for Treatment and Structural Parameters,Econometrics Journal 21, C1-C68.
Chernozhukov, V., J.A. Hausman, and W.K. Newey (2018): ”Demand Analysis with Many
Prices,” working paper, MIT.
Chernozhukov, V., W.K. Newey, J. Robins (2018): ”Double/De-Biased Machine Learning
Using Regularized Riesz Representers,” arxiv.
Escanciano, J-C., D. Jacho-Cha´vez, and A. Lewbel (2016): “Identification and Estimation of Semiparametric Two Step Models”, Quantitative Economics 7, 561-589.
44

Farrell, M. (2015): ”Robust Inference on Average Treatment Effects with Possibly More Covariates than Observations,” Journal of Econometrics 189, 1–23.
Firpo, S. and C. Rothe (2017): ”Semiparametric Two-Step Estimation Using Doubly Robust
Moment Conditions,” working paper.
Graham, B.W. (2011): ”Efficiency Bounds for Missing Data Models with Semiparametric Restrictions,” Econometrica 79, 437–452.
Hahn, J. (1998): ”On the Role of the Propensity Score in Efficient Semiparametric Estimation
of Average Treatment Effects,” Econometrica 66, 315-331.
Hahn, J. and G. Ridder (2013): ”Asymptotic Variance of Semiparametric Estimators With
Generated Regressors,” Econometrica 81, 315-340.
Hahn, J. and G. Ridder (2016): “Three-stage Semi-Parametric Inference: Control Variables
and Differentiability,” working paper.”
Hahn, J., Z. Liao, and G. Ridder (2016): ”Nonparametric Two-Step Sieve M Estimation and
Inference,” working paper, UCLA.
Hasminskii, R.Z. and I.A. Ibragimov (1978): ”On the Nonparametric Estimation of Functionals,” Proceedings of the 2nd Prague Symposium on Asymptotic Statistics, 41-51.
Hausman, J.A., and W.K. Newey (2016): ”Individual Heterogeneity and Average Welfare,”
Econometrica 84, 1225-1248.
Hausman, J.A., and W.K. Newey (2017): ”Nonparametric Welfare Analysis,” Annual Review
of Economics 9, 521–546.
Hirano, K., G. Imbens, and G. Ridder (2003): ”Efficient Estimation of Average Treatment
Effects Using the Estimated Propensity Score,” Econometrica 71: 1161–1189.
Hotz, V.J. and R.A. Miller (1993): ”Conditional Choice Probabilities and the Estimation of
Dynamic Models,” Review of Economic Studies 60, 497-529.
Huber, P. (1981): Robust Statistics, New York: Wiley.
Ichimura, H. (1993): ”Estimation of Single Index Models,” Journal of Econometrics 58, 71-120.
Ichimura, H., and S. Lee (2010): “Characterization of the Asymptotic Distribution of Semiparametric M-Estimators,” Journal of Econometrics 159, 252–266.
Ichimura, H. and W.K. Newey (2017): ”The Influence Function of Semiparametric Estimators,” CEMMAP Working Paper, CWP06/17.
Kandasamy, K., A. Krishnamurthy, B. P´oczos, L. Wasserman, J.M. Robins (2015):
”Influence Functions for Machine Learning: Nonparametric Estimators for Entropies, Divergences and Mutual Informations,” arxiv.
Lee, Lung-fei (2005): “A C(α)-type Gradient Test in the GMM Approach,” working paper.

45

Luenberger, D.G. (1969): Optimization by Vector Space Methods, New York: Wiley.
Murphy, K.M. and R.H. Topel (1985): ”Estimation and Inference in Two-Step Econometric
Models,” Journal of Business and Economic Statistics 3, 370-379.
Newey, W.K. (1984): ”A Method of Moments Interpretation of Sequential Estimators,” Economics Letters 14, 201-206.
Newey, W.K. (1990): ”Semiparametric Efficiency Bounds,” Journal of Applied Econometrics 5,
99-135.
Newey, W.K. (1991): ”Uniform Convergence in Probability and Stochastic Equicontinuity,”
Econometrica 59, 1161-1167.
Newey, W.K. (1994a): ”The Asymptotic Variance of Semiparametric Estimators,” Econometrica
62, 1349-1382.
Newey, W.K. (1994b): ”Kernel Estimation of Partial Means and a General Variance Estimator,”
Econometric Theory 10, 233-253.
Newey, W.K. (1997): ”Convergence Rates and Asymptotic Normality for Series Estimators,”
Journal of Econometrics 79, 147-168.
Newey, W.K. (1999): ”Consistency of Two-Step Sample Selection Estimators Despite Misspecification of Distribution,” Economics Letters 63, 129-132.
Newey, W.K., and D. McFadden (1994): “Large Sample Estimation and Hypothesis Testing,”
in Handbook of Econometrics, Vol. 4, ed. by R. Engle, and D. McFadden, pp. 2113-2241. North
Holland.
Newey, W.K., and J.L. Powell (1989): ”Instrumental Variable Estimation of Nonparametric
Models,” presented at Econometric Society winter meetings, 1988.
Newey, W.K., and J.L. Powell (2003): ”Instrumental Variable Estimation of Nonparametric
Models,” Econometrica 71, 1565-1578.
Newey, W.K., F. Hsieh, and J.M. Robins (1998): “Undersmoothing and Bias Corrected
Functional Estimation,” MIT Dept. of Economics working paper 72, 947-962.
Newey, W.K., F. Hsieh, and J.M. Robins (2004): “Twicing Kernels and a Small Bias Property
of Semiparametric Estimators,” Econometrica 72, 947-962.
Newey, W.K., and J. Robins (2017): ”Cross Fitting and Fast Remainder Rates for Semiparametric Estimation,” arxiv.
Neyman, J. (1959): “Optimal Asymptotic Tests of Composite Statistical Hypotheses,” Probability
and Statistics, the Harald Cramer Volume, ed., U. Grenander, New York, Wiley.
Pfanzagl, J., and W. Wefelmeyer (1982): ”Contributions to a General Asymptotic Statistical Theory. Springer Lecture Notes in Statistics.

46

Pakes, A. and G.S. Olley (1995): ”A Limit Theorem for a Smooth Class of Semiparametric
Estimators,” Journal of Econometrics 65, 295-332.
Powell, J.L., J.H. Stock, and T.M. Stoker (1989): ”Semiparametric Estimation of Index
Coefficients,” Econometrica 57, 1403-1430.
Robins, J.M., A. Rotnitzky, and L.P. Zhao (1994): ”Estimation of Regression Coefficients
When Some Regressors Are Not Always Observed,” Journal of the American Statistical Association 89: 846–866.
Robins, J.M. and A. Rotnitzky (1995): ”Semiparametric Efficiency in Multivariate Regression
Models with Missing Data,” Journal of the American Statistical Association 90:122–129.
Robins, J.M., A. Rotnitzky, and L.P. Zhao (1995): ”Analysis of Semiparametric Regression
Models for Repeated Outcomes in the Presence of Missing Data,” Journal of the American
Statistical Association 90,106–121.
Robins, J.M.,and A. Rotnitzky (2001): Comment on “Semiparametric Inference: Question
and an Answer Likelihood” by P.A. Bickel and J. Kwon, Statistica Sinica 11, 863-960.
Robins, J.M., A. Rotnitzky, and M. van der Laan (2000): ”Comment on ’On Profile
Likelihood’ by S. A. Murphy and A. W. van der Vaart, Journal of the American Statistical
Association 95, 431-435.
Robins, J., M. Sued, Q. Lei-Gomez, and A. Rotnitzky (2007): ”Comment: Performance of
Double-Robust Estimators When Inverse Probability’ Weights Are Highly Variable,” Statistical
Science 22, 544–559.
Robins, J.M., L. Li, E. Tchetgen, and A. van der Vaart (2008): ”Higher Order Influence
Functions and Minimax Estimation of Nonlinear Functionals,” IMS Collections Probability and
Statistics: Essays in Honor of David A. Freedman, Vol 2, 335-421.
Robins, J.M., L. Li, R. Mukherjee, E. Tchetgen, and A. van der Vaart (2017): ”Higher
Order Estimating Equations for High-Dimensional Models,” Annals of Statistics, forthcoming.
Robinson, P.M. (1988): ”‘Root-N-consistent Semiparametric Regression,” Econometrica 56, 931954.
Rust, J. (1987): ”Optimal Replacement of GMC Bus Engines: An Empirical Model of Harold
Zurcher,” Econometrica 55, 999-1033.
Santos, A. (2011): ”Instrumental Variable Methods for Recovering Continuous Linear Functionals,” Journal of Econometrics, 161, 129-146.
Scharfstein D.O., A. Rotnitzky, and J.M. Robins (1999): Rejoinder to “Adjusting For
Nonignorable Drop-out Using Semiparametric Non-response Models,” Journal of the American
Statistical Association 94, 1135-1146.
Severini, T. and G. Tripathi (2006): ”Some Identification Issues in Nonparametric Linear
47

Models with Endogenous Regressors,” Econometric Theory 22, 258-278.
Severini, T. and G. Tripathi (2012): ”Efficiency Bounds for Estimating Linear Functionals
of Nonparametric Regression Models with Endogenous Regressors,” Journal of Econometrics
170, 491-498.
Schick, A. (1986): ”On Asymptotically Efficient Estimation in Semiparametric Models,” Annals
of Statistics 14, 1139-1151.
Stoker, T. (1986): ”Consistent Estimation of Scaled Coefficients,” Econometrica 54, 1461-1482.
Tamer, E. (2003): ”Incomplete Simultaneous Discrete Response Model with Multiple Equilibria,”
Review of Economic Studies 70, 147-165.
van der Laan, M. and Rubin (2006): ”Targeted Maximum Likelihood Learning,” U.C. Berkeley
Division of Biostatistics Working Paper Series. Working Paper 213.
van der Vaart, A.W. (1991): “On Differentiable Functionals,” The Annals of Statistics, 19,
178-204.
van der Vaart, A.W. (1998): Asymptotic Statistics, Cambride University Press, Cambridge,
England.
van der Vaart, A.W. (2014): ”Higher Order Tangent Spaces and Influence Functions,” Statistical Science 29, 679–686.
Wooldridge, J.M. (1991): “On the Application of Robust, Regression-Based Diagnostics to
Models of Conditional Means and Conditional Variances,” Journal of Econometrics 47, 5-46.

48

