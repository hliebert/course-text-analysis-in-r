Journal

of Econometrics

1 (1973)

49-60.

0 North-Holland

Publishing

Company

REGRESSION WITH A BINARY INDEPENDENT
VARIABLE
SUBJECT TO ERRORS OF OBSERVATION
Dennis J. AIGNER
University of Wisconsin

1. Introduction
In a recent study of the socio-economic
effects of the disease bilharzia on the population of the Caribbean island of St. Lucia, Weisbrod
et al. (1973) make abundant use of regression methods to analyze the
various relationships
of interest. A common independent
variable in
their work is a dummy variable that indicates the presence or absence
of the disease in the person sampled (the observation unit). The nature
of diagnosis for this particular disease is such that if a person is diagnosed as having the disease, he does indeed have it. However, if the diagnosis is negative there is a non-zero probability,
q, that he has been
diagnosed incorrectly and has the disease.
It is the intent of this note to consider the effects of such an independent
variable - a binary variable subject td ‘errors of classification’ - in least squares regression. The results of both an analysis of bias
in least squares parameter estimates and of the availability of alternative
estimators is parallel to the classical case where both the variable and its
measurement
error are continuous
random variables. Many details will
not be repeated here in deference to the reader’s familiarity with that
subject. The important
practical difference between the two cases is
that the information
needed to obtain consistent (or nearly so) parameter estimates may be more readily available in the discrete case. In the
study cited, for instance, extraneous
information
about q is available
from patient medical histories and examination
data.
The following section of the present article contains a brief exposition of the nature of a discrete ‘classification error’. Previous authors
who have treated this material include Neyman (1950), Bross (1954)
and Lord and Novick (1968). Sect. 3 then takes up an analysis of the
effects of including an independent
variable subject to such measure-

D.J. Aigner, Classification errors in binary variables

50

ment error in a multiple regression model, for which the paper by
Cochran (1968) is a useful reference. This section also contains the
presentation
of a consistent
technique
for obtaining parameter estimates. A proof of consistency and details on precision measures for the
method are contained in an Appendix.

2. Classification

error in a binary variable

Let P be the true proportion
of persons having the disease. If the ith
element of the population is diagnosed as having the disease set xi = 1;
otherwise xi = 0. The proportion of persons (in a population of N) being
diagnosed as having the disease is ig = (1 IN) C& Xi, and relates to P as
follows. The proportion
of persons diagnosed negative is Q = (1 -P). Of
these, a proportion,
q, have the disease. If, in addition, for persons diagnosed as having the disease (of which there are Np), some proportion,
say V, do not have it in fact, then in the total population, Np- v@+
+ n( 1 --is) N actually have the disease. Therefore,

(I-v)P+qG .

P=

(1)

In an ‘errors-in-variables’
framework, we let x, as defined above, be
the observable random variable on a typical draw into the sample, and
further define the variables X and u by the relation x = X + U. X is the
‘true’ state of the sampled person with respect to the disease, and u is a
random measurement
error. Obviously, the marginal distribution
of X
is Bernoulli with parameter P.Likewise, a Bernoulli distribution
(with
parameter 4 characterizes
the distribution
of x. The joint distribution
of (x, u) is obtained through simple manipulation,
and is of particular
interest in what follows. It appears in the form of a joint frequency
table, below:
X

2.4

0

1 Tgj

-.l

1

f(u)

0

rl&

0

U--77)i2 (1-v)pu

1

0

f(x)

I e”

VP

(l-n@+
VP

(1-v)pu

(2)

D.J. Aigner, Classification errors in binary variables

51

Unlike the classical case, the measurement
error in x does not have
zero mean, nor is it uncorrelated
with the true part. From (2) we see
that ’
E(u) =

VP”-&! ,

V(u) =

VP+ qp - (VP-5432)

(34

whereas,
C(x, u) =

(v+Tj>F(j

(3c)

)

a useful result for the next section.

3. Incorporation

into the multiple regression model

Johnston (1963) considers the multiple regression model where every
independent
variable may be subject to random measurement error. We
shall pursue one of his suggested techniques for obtaining consistent estimates of the regression coefficients,
but we will suppose that of the
several independent
variables only x is subject to observation error. Extension to the more general case follows directly. Measurement error in
x alone is, of course, sufficient to bias all least squares estimates of regression coefficients,
not merely the coefficient
on x. The exception is
when x is orthogonal to all other independent variables.
For purposes of exposition
we partition the population
regression
function as follows,

y=zy+xp+e,

(4)

where y is a [(k-1)X 1 ] vector of unknown
parameters,
Z is an
[nX (k-l)]
matrix of observations on the ‘other’ independent
variables,
and E is the (nX 1) vector of residuals with E(E)= 0 and E(d) = 0~1.
All
variables are assumed to be measured as deviations from their respective
mean values.
Least squares applied to the operational version of (4), namely
y=Zy+xfl+(e-UP))
’ Our notation

is V(.) for the variance

(5)
operator

and C(.,.) for covariance.

D.J. Aigner, Classification errors in binary variables

52

will result in the estimators

(6)
where

The

accompanying

sampling

e= t;,,,- (I,=(2X)-

error

in (r’ fl)Ls, e, will be of the form

1 _$f’E-

<px,- 1 pup .

(7)

Assuming plim (:.?E) = 0, plim (ix’&
= Xx, and that u is asymptotically uncorrelated
with the members of Z,
plim e = -$plim
=-

(3

(’nXX
-‘“)-’

f

\

plim (-!$2.4)
\

ffpi

x cc-cu)

,XX

x ccca) /

(8)

where Hilx, .... BiXx, are elements of the kth column of Xj l. (8) is just a
special case of Cochran’s equation (8.8) (see Cochran, 1968, p. 65 1).
If x is the only independent
variable in the regression (with a constant term included), (8) reduces to
plime=-@[$$-$-I,

(9)

D.J. Aigner, Classification errors in binary variables

53

or, in terms of plim flLs i 2
plim flLs = p
Substituting

V(X) +
[

ax

V(x)

relevant

plim &s = @(1 ---v---77),

u)

1*

quantities,

(10)

( 10) is simply
(11)

or,
dim &_s = P(l--77)>

(12)

for the bilharzia example. To correct for least squares bias in general
requires knowledge of both v and n.
The direction of the proportionate
bias is consistent with the usual
A
result, that flLs is biased downward.
In the bilharzia case the proportionate bias is equal to the probability
that a person diagnosed as not
having the disease does not have it, i.e., the probability of correct diagnosis.3 Clearly, were q known, the estimator & = &s/( 1-n) would be
consistent for 0.
In the multiple regression model, though information
about C(X, U)
may be available, and the remaining elements of (8) can - except for
fl - be estimated from A_%?_?,there is apparently no straightforward
way
of incorporating
these quantities into the least squares procedure in order to correct for bias as there is in the two-variable case.
Given knowledge of C(x, u), consistent estimators for-y and /3 can be
obtained, however, either from maximum likelihood or from a procedure Johnston
(1963, pp. 168-175)
suggests. The method of instrumental variables may also be of practical value in this application.4
Below we concentrate
on Johnston’s technique, ‘modified least squares’,
so named here for the resemblance of the equations determining these
estimators to the normal equations of least squares.
For the model (4), letting c = C(x, u), the modified least squares es‘Again, cf. Cochran (1968) p. 651, eq. (9.1).
3 For the case in point, Weisbrod et al. (1973) feel that n may be as large as 0.5!
4 The instrument for x must be chosen to be correlated with the true part, X, and uncorrelated with both E and u. There should be no difficulty finding an instrument for this problem,
since the diagnosis is but one of many functions of a patient’s symptoms.

D.J. Aigner, Classification

54

timators

errors in binary variables

are the 9 and fl given by:

+=
(I[

Mzz Mzx

1

MXuy

bxx

Mix

p^

-1

- 0

(13)

’

M2y is the k-element

where

column vector of sample covariances
with like interpretations
for M,,,M,,,and mxx.
Generally (13) is not equivalent to the equation set determining either
the maximum likelihood estimators
or the ordinary least squares estimators.5
The Appendix contains a consistency
proof for the estimators and
formulas for elements of their asymptotic
covariance matrix. Due to
their linearity they are asymptotically
normally distributed. With regard
to the matter of efficiency,
if one is willing to adopt a mean-squareerror (MSE) criterion for judging the virtues of competing estimators of
/3 in the two-variable model, it is easy to show that circumstances may
exist when the biased LS estimator will be preferred to the 0 of (13).
The asymptotic
MSE of /? is just its asymptotic
variance, (A.lO),
which with the identities of sect. 2 inserted becomes
qy,

***,Q_l,y,

mxy,

02

MSE@) =

(14)

nPQ”( 1-V--r))2 .
For the biased LS estimator,
MSE@,,)

’ However,

=

its asymptotic

variance is a2/r@~,

-$+J+
P2(v+d2,

for the two-variable

case, where the equation

so that

(15)
determining

p is

mxy = (mxx-S) 0,
we find that
rn;i

mxy =

plim

ax, u)
(m;;~mxy)=B
V(x)1
[

B(1 -tm;$

V(x) +

which is (10). That is, for the two-variable case, the modified least squares procedure coincides
with ordinary least squares ‘corrected’ for bias. For more than two variables, however, the resemblance ends.

D.J. Aigner, Classification errors in binary variables

using (11). Then the (asymptotic)

relative MSE efficiency

55

of fl is

,.

MSW,,)

MS@)

= (1 -V-n)2[

l+G(v+n)2]

)

(16)

of values for (tin) and
with t2 = f12n~&o 2. For certain combinations
6, the relative MSE efficiency
of fl can be less than one. Of course, for
sufficiently
large n (16) will exceed one whatever is the value of (tin).
It is difficult to assess the importance of these results in general since
t2 also depends on the model parameters,
p and u2. Nonetheless they
serve to illustrate the point. At some ‘large’ sample size,,the modified
LS estimator for (13) will be superior to ordinary LS in MSE. Whether
it compares favorably to maximum likelihood in asymptotic efficiency
is unknown.

4. Conclusions
While the LS procedure in general gives biased and inconsistent parameter estimates in the errors-in-variables
model, through a simple modification of the relevant moment matrix of independent
variables consistent estimation of (y’ 0) can be achieved. For our application,
consistency depends upon a knowledge of c. Since the argument is on
asymptotic
grounds already, one might contemplate
substituting a consistent estimate for r and proceeding.
The difficulty here is the same
one that plagues the classical ‘extraneous
estimation’ literature.
It is
that as a practical matter the estimate for c will generally be obtained
from a different population
than that from which the y1 observations
have been selected. With { estimated from such a source, therefore, it is
not at all clear .what -sort of operation
is implied by plim in
plim [ 1-m,-,‘(~x’u)l [ 1-cm;:
I - ‘, which is central to the consistency
proof. At this point the Bayesian would proceed
directly
to the
‘product’ sample space and claim consistency,
while the strict classical
statistician must be satisfied with a sort of ‘approximate’
consistency
when f is used in place of r.
While the maximum likelihood technique using the same a priori information is available, it may have little to recommend it over modified
least. squares save the possibility of large sample efficiency. Assuming
normality
for E, the likelihood function for this model involves products of linear combinations
of Normal densities, and therefore can be

D.J. Aigner, Classification errors in binary variables

56

expected to present some non-trivial computational
problems6
These
same complexities
in the sample likelihood make for a computationally
difficult solution to an otherwise ‘natural’ Bayesian application.

Acknowledgement
This paper was completed
with financial assistance from several
sources. At the time of this writing the author held the position of
Fulbright Research Scholar at the Center for Operations Research and
Econometrics
of the Universite Catholique de Louvain, on leave from
the University of Wisconsin. Support from the Graduate School of the
University of Wisconsin and the National Science Foundation
is gratefully acknowledged.
J.B. Ramsey and M. Mouchart contributed
helpful
comments on the manuscript,
though neither of them should be held
responsible for any shortcomings that remain.

Appendix
1. Consistency

of the estimators

The proof of consistency
for the vector (5’ 8)’ of (13) is as follows.
Write the equation set which determines (+’ 8)’ as

M Zy =

[MzzMzXI

=Mzzy + MzxP

(A.la)

and

mxy= [Mkx(mxx-

51-1,
(;

) =Mix-y+ Cm,,-S) B

(A.lb)

6 There is, however, one feature of the model that is quite interesting
when judged beside
the maximum
likelihood
formulation
of the classical errors-in-variables
problem in regression.
In such applications
while the main parameters
may be estimated
consistently
by maximum
likelihood,
the ‘incidental’
parameters
(in this instance,
the Xi’s) may not be (Kendall and
Stuart, 1961, p. 384). This situation leads to biased variance estimators
and the possibility
that
the conditions
necessary
for the maximum
likelihood
estimators
to be BAN will not be met.
Unlike the classical case, in the model under consideration
here the number of nuisance parameters does not increase with sample size. In fact, the density function of each Xi is completely
described by but three parameters,
F, n, and IJ.

D.J. Aigner, Classification errors in binary variables

51

From (A. 1b) we find

[m-h
xx

fl=

xx
xy -m-‘M;,rl

which, when substituted

M

[l-~m;~]-’

into (A. 1a) yields

=$Z’ [I-xm;:
(;) xl rl-Zm&!I

zy

+

(A-2)

)

Mzxm,-,‘mxy
1l-b&!

-11 ZY +

’,

I-

(A.3)

where $Z’Z= MZZ,ix'x = m,,,etc., as the equation set determining 9.
Setting [I-xm,-,‘($x’
(1 -fin;:>
- 1] = A, and rearranging (A.3) gives
Z’Ay = Z’AZy ,
or

9 = (Z’AZ)- 1Z’Ay .

(A.4)

For p^we find (by substituting

(A.4) into (A.2)):

B= + m,-,‘x’Il-Z(Z’AZ)- l Z’A)
Usingy

= [Zx]

[y’ 81’ + [e-u/31

y 11-fm,-,l} - 1 .
in (A.4),

9 = -y+ (Z’AZ)-1 Z’Ae + (Z’AZ)-1 Z’AXP
with X = x -u,

as before.

Taking probability

plim 9 = y + plim (:Z’AZ)

(A.5)

(A.61

)

limits,

-’ plim (+Z’AXfl)

.

(A.7)

But.4X=X-(l-~~&!)-1x+(l-~m;~)~1xm;~(~x’~).We
note that
m;j(ix’u)
is a scalar and that plim(Ax’u) = {, and write AX=
=X-x[l-m,-,‘(~x’u)]
[l-{m,-,‘]-l.
Now, plim~Z’AX=plimI-Z’X
- plim XZ’x [ 1-m,-,‘(~x’u)]
[ 1 -@z;J] - ’ , so that if plim [ 1 -m&!($x’U,]
[ 1-&&! I - ’ = 1, plim ;Z’x = plim;Z’X,
thus plim AZ’AX = O*.. Since
* The reader should be surprised
if E(u) = 0 and u is specified
ables are in ‘deviations’ form, hence E(u) = 0.

as in sect.

2. But here vari-

D.J. Aigner, Classification

58

errors in binary variables

plim (Ax’u) = 5, this will be true. Therefore, plim T = 7 and consistency
is proved.
A similar development
follows for plim fl. In this instance,
plim~=Pplim[l-Sjn~~:]-‘-

“x_- nxX
L
Y1
) 11

= /3 plim [ I-{m,-,‘]-’
=

[I-rn;i.(ix’u)l

(A.8)

P.

2. Asymptotic

covariance

matrix

The large-sample variances and covariances
derive.
In the case of T, following (A.4),
C+=$plim

(:Z’AZ)

-’ (+Z’AA’Z)

for 9 and p^are easy to

(kZ'AZ)-'

.

64.9)

Because A is not idempotent
in this non-least squares situation, no further simplification
of (A.9) is possible. The sample estimator for Z:;. is
obvious.
As for the variance of J?, from (A.5) we derive:
V(p^)= $

[ V(x)-cl-2

plim (ixrRRrx)

(A.lO)

,

where R = I- Z(Z’AZ)-l
Z’A and is not idempotent.
Again, V(p^) is
easily estimated from sample information
and a knowledge of the probabilities of misclassification.

References
Bross, I., 1954, Misclassification
in 2 x 2 tables, Biometrics
10, 478-486.
Cochran, W.G., 1968, Errors of measurement
in statistics, Technometrics
Johnston,
J., 1963, Econometric
Methods (McGraw-Hill,
New York).
Kendall, M. and A. Stuart,
1961, The Advanced Theory of Statistics,
New York).

10,637-666.
Vol. 2 (Chas.

Griffin,

D.J. Aigner,

Classification

errors in binary

variclbles

59

Lord, F.M. and M.R. Novick, 1968, Statistical Theories of Mental Test Scores (Addison-Wesley,
Reading, Mass.).
Neyman, J., 1950, First Course in Probability
and Statistics (H. Holt, New York).
Weisbrod,
B., R. Andreano,
R. Baldwin, E. Epstein and A. Kelley, 1973, Disease and Socioeconomic Development
(University of Wisconsin Press).

