{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PC Session 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author:**\n",
    "[Helge Liebert](https://hliebert.github.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Text analysis**: Brexit debate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(\"twfy\")\n",
    "library(\"jsonlite\")\n",
    "library(\"topicmodels\")\n",
    "library(\"textclean\")\n",
    "library(\"wordcloud\")\n",
    "library(\"slam\")\n",
    "library(\"tm\")\n",
    "library(\"data.table\")\n",
    "library(\"tidytext\")\n",
    "library(\"stringr\")\n",
    "library(\"dplyr\")\n",
    "library(\"ggplot2\")\n",
    "library(\"ggrepel\")\n",
    "library(\"uwot\")\n",
    "library(\"udpipe\")\n",
    "library(\"lsa\")\n",
    "library(\"factoextra\")\n",
    "library(\"word2vec\")\n",
    "library(\"plotly\")\n",
    "library(\"fpc\")\n",
    "library(\"text2vec\")\n",
    "library(\"doc2vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data from TheyWorkForYou API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apikey <- \"G3WVqtBtKAbdGVqrd8BKajm8\"\n",
    "set_api_key(apikey)\n",
    "\n",
    "call <- getDebates(type = \"commons\", search = \"Brexit\", num = 1000, page = 1)\n",
    "info <- call$info\n",
    "pages <- ceiling(info$total_results/info$results_per_page)\n",
    "\n",
    "## Form of data.frame, header only\n",
    "brexit.debates <- flatten(as.data.frame(call$rows))[0, ]\n",
    "\n",
    "## read all pages of results\n",
    "for (p in seq(1, pages)) {\n",
    "  call <- getDebates(type = \"commons\",  search = \"Brexit\", num = 1000, page = p)\n",
    "  call$rows$speaker$office <- NULL\n",
    "  brexit.debates <- rbind(brexit.debates, flatten(as.data.frame(call$rows)))\n",
    "}\n",
    "\n",
    "## save to file\n",
    "fwrite(brexit.debates, \"Data/brexit-debates.csv\")\n",
    "saveRDS(brexit.debates, \"Data/brexit-debates.rds\")\n",
    "\n",
    "## read from file\n",
    "## brexit.debates <- readRDS(\"Data/brexit-debates.rds\")\n",
    "\n",
    "## str(brexit.debates)\n",
    "## length(unique(brexit.debates$gid))\n",
    "## length(unique(brexit.debates$hdate))\n",
    "## length(unique(brexit.debates$person_id))\n",
    "names(brexit.debates)\n",
    "\n",
    "## copy before transformations\n",
    "brexit.debates$body.orig <- brexit.debates$body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "brexit.debates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the text\n",
    "\n",
    "Some cleaning and harmonizing, pre-written functions (e.g. `library(textclean)`) convenient compared to writing all regex on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## brief check\n",
    "## check_text(brexit.debates$body[1:100])\n",
    "## Encoding(brexit.debates$body) <- \"UTF-8\"\n",
    "head(brexit.debates$body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brexit.debates$body <- replace_html(brexit.debates$body)\n",
    "head(brexit.debates$body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brexit.debates$body <- replace_non_ascii(brexit.debates$body)\n",
    "head(brexit.debates$body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brexit.debates$body <- gsub(\"&#8212;\", \" - \", brexit.debates$body)\n",
    "head(brexit.debates$body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brexit.debates$body <- gsub(\"&#[0-9]{3,4};\", \" \", brexit.debates$body)\n",
    "head(brexit.debates$body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brexit.debates$body <- replace_names(brexit.debates$body)\n",
    "head(brexit.debates$body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brexit.debates$body <- replace_money(brexit.debates$body, replacement = \"MONEYHERE\")\n",
    "brexit.debates$body <- replace_date(brexit.debates$body, replacement = \"DATEHERE\")\n",
    "brexit.debates$body <- replace_ordinal(brexit.debates$body, num.paste = TRUE)\n",
    "brexit.debates$body <- replace_number(brexit.debates$body, remove = TRUE) ## makes a difference for topics!\n",
    "brexit.debates$body <- add_comma_space(brexit.debates$body)\n",
    "brexit.debates$body <- replace_contraction(brexit.debates$body)\n",
    "brexit.debates$body <- replace_white(brexit.debates$body)\n",
    "head(brexit.debates$body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove blanks\n",
    "## brexit.debates <- brexit.debates[brexit.debates$body != \"\", ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics, both parties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Corpus\n",
    "## corp <- brexit.debates\n",
    "corp <- brexit.debates[, c(\"gid\", \"body\")]\n",
    "setnames(corp, \"gid\", \"doc_id\")\n",
    "setnames(corp, \"body\", \"text\")\n",
    "\n",
    "## initialize dtm\n",
    "dtm <- DocumentTermMatrix(\n",
    "  Corpus(DataframeSource(\n",
    "    corp\n",
    "  )),\n",
    "  control = list(\n",
    "    language = \"english\",\n",
    "    ## weighting = weightTfIdf,\n",
    "    weighting = weightTf,\n",
    "    tolower = TRUE,\n",
    "    removePunctuation = TRUE,\n",
    "    removeNumbers = TRUE,\n",
    "    stopwords = TRUE,\n",
    "    stemming = FALSE,\n",
    "    wordLengths = c(3, Inf)\n",
    "  )\n",
    ")\n",
    "\n",
    "## checks\n",
    "inspect(dtm)\n",
    "## findFreqTerms(dtm, lowfreq = 10)\n",
    "findFreqTerms(dtm, lowfreq = 1000)\n",
    "## dtm <- removeSparseTerms(dtm, sparse=0.90) ## filter some\n",
    "dtm <- dtm[row_sums(dtm) > 0, ] ## documents can't be empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simple visualization\n",
    "## wordcloud(brexit.debates$body, max.words = 100, random.order = FALSE,\n",
    "##          colors = brewer.pal(8, \"Dark2\"))\n",
    "\n",
    "## same plot, works with both tf and tf-idf weighting\n",
    "counts <- sort(colSums(as.matrix(dtm)), decreasing = TRUE)\n",
    "counts <- data.frame(word = names(counts), freq = counts)\n",
    "wordcloud(words = counts$word, freq = counts$freq,\n",
    "          max.words = 100, random.order = FALSE,\n",
    "          colors = brewer.pal(8, \"Dark2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Unsupervised clustering of documents: Topic model\n",
    "tpm <- LDA(dtm, k = 3, control = list(seed = 100))\n",
    "topic <- topics(tpm, 1)\n",
    "freqterms <- terms(tpm, 50)\n",
    "freqterms\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## look at unique terms only per topic\n",
    "duplicates <- c(freqterms)[duplicated(c(freqterms))]\n",
    "distinctterms <- lapply(as.list(as.data.frame(freqterms)), function(x) x[!(x %in% duplicates)])\n",
    "## distinctterms <- as.data.frame(distinctterms)\n",
    "distinctterms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot most frequent terms and associated probabilities by topic\n",
    "tpmat <- tidy(tpm, matrix = \"beta\")\n",
    "topterms <-\n",
    "    tpmat %>%\n",
    "    group_by(topic) %>%\n",
    "    top_n(20, beta) %>%\n",
    "    ungroup() %>%\n",
    "    arrange(topic, -beta)\n",
    "topterms %>%\n",
    "    mutate(term = reorder(term, beta)) %>%\n",
    "    ggplot(aes(term, beta, fill = factor(topic))) +\n",
    "    geom_col(show.legend = FALSE) +\n",
    "    facet_wrap(~ topic, scales = \"free\") +\n",
    "    coord_flip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conservative Party Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## differences by party?\n",
    "table(brexit.debates$speaker.party)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm.con <- DocumentTermMatrix(\n",
    "  Corpus(DataframeSource(\n",
    "    corp[brexit.debates$speaker.party == \"Conservative\" ,]\n",
    "  )),\n",
    "  control = list(\n",
    "    language = \"english\",\n",
    "    weighting = weightTf,\n",
    "    tolower = TRUE,\n",
    "    removePunctuation = TRUE,\n",
    "    removeNumbers = TRUE,\n",
    "    stopwords = TRUE,\n",
    "    stemming = FALSE,\n",
    "    wordLengths = c(3, Inf)\n",
    "  )\n",
    ")\n",
    "dtm.con <- dtm.con[row_sums(dtm.con) > 0, ] ## documents can't be empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Estimate lda topic model\n",
    "tpm.con <- LDA(dtm.con, k = 3, control = list(seed = 100))\n",
    "topic.con <- topics(tpm.con, 1)\n",
    "freqterms.con <- terms(tpm.con, 50)\n",
    "freqterms.con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot most frequent terms and associated probabilities by topic\n",
    "tpmat.con <- tidy(tpm.con, matrix = \"beta\")\n",
    "topterms.con <-\n",
    "    tpmat.con %>%\n",
    "    group_by(topic) %>%\n",
    "    top_n(20, beta) %>%\n",
    "    ungroup() %>%\n",
    "    arrange(topic, -beta)\n",
    "topterms.con %>%\n",
    "    mutate(term = reorder(term, beta)) %>%\n",
    "    ggplot(aes(term, beta, fill = factor(topic))) +\n",
    "    geom_col(show.legend = FALSE) +\n",
    "    facet_wrap(~ topic, scales = \"free\") +\n",
    "    coord_flip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## look at unique terms only per topic\n",
    "duplicates.con <- c(freqterms.con)[duplicated(c(freqterms.con))]\n",
    "distinctterms.con <- lapply(as.list(as.data.frame(freqterms.con)), function(x) x[!(x %in% duplicates.con)])\n",
    "distinctterms.con"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labour Party Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm.lab <- DocumentTermMatrix(\n",
    "  Corpus(DataframeSource(\n",
    "    corp[brexit.debates$speaker.party == \"Labour\", ]\n",
    "  )),\n",
    "  control = list(\n",
    "    language = \"english\",\n",
    "    weighting = weightTf,\n",
    "    tolower = TRUE,\n",
    "    removePunctuation = TRUE,\n",
    "    removeNumbers = TRUE,\n",
    "    stopwords = TRUE,\n",
    "    stemming = FALSE,\n",
    "    wordLengths = c(3, Inf)\n",
    "  )\n",
    ")\n",
    "dtm.lab <- dtm.lab[row_sums(dtm.lab) > 0, ] ## documents can't be empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Estimate LDA topic model\n",
    "tpm.lab <- LDA(dtm.lab, k = 3, control = list(seed = 100))\n",
    "topic.lab <- topics(tpm.lab, 1)\n",
    "freqterms.lab <- terms(tpm.lab, 50)\n",
    "freqterms.lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot most frequent terms and associated probabilities by topic\n",
    "tpmat.lab <- tidy(tpm.lab, matrix = \"beta\")\n",
    "topterms.lab <-\n",
    "    tpmat.lab %>%\n",
    "    group_by(topic) %>%\n",
    "    top_n(20, beta) %>%\n",
    "    ungroup() %>%\n",
    "    arrange(topic, -beta)\n",
    "topterms.lab %>%\n",
    "    mutate(term = reorder(term, beta)) %>%\n",
    "    ggplot(aes(term, beta, fill = factor(topic))) +\n",
    "    geom_col(show.legend = FALSE) +\n",
    "    facet_wrap(~ topic, scales = \"free\") +\n",
    "    coord_flip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## look at unique terms only per topic\n",
    "duplicates.lab <- c(freqterms.lab)[duplicated(c(freqterms.lab))]\n",
    "distinctterms.lab <- lapply(as.list(as.data.frame(freqterms.lab)), function(x) x[!(x %in% duplicates.lab)])\n",
    "distinctterms.lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA/LSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since pca works less well for word similarity tasks and interpretations compared to other embeddings (when applied to the term-term matrix), lets apply it to the document-term-matrix.\n",
    "\n",
    "We aggregate the document-term matrix by speaker, then use pca/lsa to get a reduced dimension that helps assessing document similarity. Then we use the smaller representation to apply k-means clustering to group politicians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check speakers and aggregate\n",
    "## names(brexit.debates)\n",
    "## length(unique((brexit.debates$speaker.name)))\n",
    "## length(unique((brexit.debates$person_id)))\n",
    "## nrow(unique((brexit.debates[, c(\"person_id\", \"speaker.party\")])))\n",
    "\n",
    "## aggregate, in base (more convenient with data.table; or dplyr if you must)\n",
    "## brexit.speakers <- aggregate(body ~ speaker.name + person_id + speaker.party, data = brexit.debates, paste)\n",
    "brexit.speakers <- aggregate(body ~ speaker.name + person_id, data = brexit.debates, paste)\n",
    "\n",
    "## dismiss those who have empty text\n",
    "brexit.speakers <- brexit.speakers[brexit.speakers$body != \"\", ]\n",
    "brexit.speakers <- brexit.speakers[!is.na(brexit.speakers$body), ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## checks\n",
    "dim(brexit.speakers)\n",
    "names(brexit.speakers)\n",
    "## str(brexit.speakers)\n",
    "brexit.speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## corpus base\n",
    "corp <- brexit.speakers[, c(\"speaker.name\", \"body\")]\n",
    "setnames(corp, \"speaker.name\", \"doc_id\")\n",
    "setnames(corp, \"body\", \"text\")\n",
    "\n",
    "## initialize dtm\n",
    "dtm <- DocumentTermMatrix(\n",
    "  Corpus(DataframeSource(\n",
    "    corp\n",
    "  )),\n",
    "  control = list(\n",
    "    language = \"english\",\n",
    "    weighting = weightTf,\n",
    "    tolower = TRUE,\n",
    "    removePunctuation = TRUE,\n",
    "    removeNumbers = TRUE,\n",
    "    stopwords = TRUE,\n",
    "    stemming = FALSE,\n",
    "    wordLengths = c(3, Inf)\n",
    "  )\n",
    ")\n",
    "\n",
    "inspect(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## how many words?\n",
    "brexit.speakers$wordcount <- str_count(as.character(brexit.speakers$body))\n",
    "summary(brexit.speakers$wordcount)\n",
    "qplot(brexit.speakers$wordcount, bins = 100)\n",
    "qplot(brexit.speakers$wordcount[brexit.speakers$wordcount < 50000], bins = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LSA on the document term matrix\n",
    "## ls <- lsa(dtm)\n",
    "ls <- lsa(dtm, 2)\n",
    "str(ls)\n",
    "pcs <- as.data.frame(ls$tk)\n",
    "\n",
    "## if you want to recoup a matrix of the original dimensions\n",
    "M <- as.textmatrix(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "as.matrix(pcs) %*% diag(ls$sk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## kmeans clustering. Try three clusters (three main parties)\n",
    "km <- kmeans(pcs, centers = 3)\n",
    "## km\n",
    "## str(km)\n",
    "fviz_cluster(km, data = pcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## checks\n",
    "summary(brexit.speakers$wordcount)\n",
    "brexit.speakers[brexit.speakers$speaker.name == \"Jeremy Corbyn\", \"wordcount\"]\n",
    "brexit.speakers[brexit.speakers$speaker.name == \"Valerie Vaz\", \"wordcount\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## filter\n",
    "outliers <- c(\"Jeremy Corbyn\", \"Valerie Vaz\")\n",
    "dtm <- DocumentTermMatrix(\n",
    "  Corpus(DataframeSource(\n",
    "    corp[!(corp$doc_id %in% outliers), ]\n",
    "  )),\n",
    "  control = list(\n",
    "    language = \"english\",\n",
    "    weighting = weightTf,\n",
    "    tolower = TRUE,\n",
    "    removePunctuation = TRUE,\n",
    "    removeNumbers = TRUE,\n",
    "    stopwords = TRUE,\n",
    "    stemming = FALSE,\n",
    "    wordLengths = c(3, Inf)\n",
    "  )\n",
    ")\n",
    "\n",
    "## repeat lsa/km\n",
    "ls <- lsa(dtm, 2)\n",
    "pcs <- as.data.frame(ls$tk)\n",
    "M <- as.textmatrix(ls)\n",
    "km <- kmeans(pcs, centers = 3)\n",
    "fviz_cluster(km, data = pcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## filter even more\n",
    "dtm <- DocumentTermMatrix(\n",
    "  Corpus(DataframeSource(\n",
    "    corp[brexit.speakers$wordcount < quantile(brexit.speakers$wordcount, p = 0.95), ]\n",
    "  )),\n",
    "  control = list(\n",
    "    language = \"english\",\n",
    "    weighting = weightTf,\n",
    "    tolower = TRUE,\n",
    "    removePunctuation = TRUE,\n",
    "    removeNumbers = TRUE,\n",
    "    stopwords = TRUE,\n",
    "    stemming = FALSE,\n",
    "    wordLengths = c(3, Inf)\n",
    "  )\n",
    ")\n",
    "\n",
    "## repeat lsa/km\n",
    "ls <- lsa(dtm, 2)\n",
    "pcs <- as.data.frame(ls$tk)\n",
    "M <- as.textmatrix(ls)\n",
    "km <- kmeans(pcs, centers = 3)\n",
    "fviz_cluster(km, data = pcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## filter even more\n",
    "dtm <- DocumentTermMatrix(\n",
    "  Corpus(DataframeSource(\n",
    "    corp[brexit.speakers$wordcount < quantile(brexit.speakers$wordcount, p = 0.95), ]\n",
    "  )),\n",
    "  control = list(\n",
    "    language = \"english\",\n",
    "    weighting = weightTfIdf,\n",
    "    tolower = TRUE,\n",
    "    removePunctuation = TRUE,\n",
    "    removeNumbers = TRUE,\n",
    "    stopwords = TRUE,\n",
    "    stemming = FALSE,\n",
    "    wordLengths = c(3, Inf)\n",
    "  )\n",
    ")\n",
    "\n",
    "## repeat lsa/km\n",
    "ls <- lsa(dtm, 2)\n",
    "pcs <- as.data.frame(ls$tk)\n",
    "M <- as.textmatrix(ls)\n",
    "km <- kmeans(pcs, centers = 3)\n",
    "fviz_cluster(km, data = pcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sampling, adding document length as a column, ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## do the clusters identify party membership?\n",
    "## ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings: Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation and estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this data is not ideal to train embeddings, it is too small.\n",
    "## but it is fast and sufficient for illustration.\n",
    "\n",
    "## use untransformed or only minimally transformed text as input\n",
    "text <- brexit.debates$body.orig\n",
    "text <- replace_html(text)\n",
    "text <- replace_non_ascii(text)\n",
    "text <- gsub(\"&#[0-9]{3,4};\", \" \", text)\n",
    "## text <- replace_ordinal(text, num.paste = TRUE)\n",
    "## text <- replace_number(text, remove = TRUE)\n",
    "## text <- replace_contraction(text)\n",
    "## text <- add_comma_space(text)\n",
    "## text <- replace_white(text)\n",
    "text <- tolower(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train word2vec to learn embeddings\n",
    "## vsmodel <- word2vec(x = text, type = \"skip-gram\", dim = 150, iter = 20)\n",
    "\n",
    "## save model to file\n",
    "## write.word2vec(vsmodel, \"Data/w2v-brexit.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read again (save time)\n",
    "vsmodel <- read.word2vec(\"Data/w2v-brexit.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## all terms\n",
    "terms <- summary(vsmodel, \"vocabulary\")\n",
    "terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## extract embeddings\n",
    "embeddings <- as.matrix(vsmodel)\n",
    "dim(embeddings)\n",
    "head(embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantics and similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## some word associations\n",
    "predict(vsmodel, c(\"johnson\", \"corbyn\", \"bercow\", \"may\", \"starmer\", \"cummings\"),\n",
    "        type = \"nearest\", top_n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(vsmodel, c(\"negotiations\", \"deadline\", \"vote\", \"fisheries\"),\n",
    "        type = \"nearest\", top_n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## analogy tasks (better with pre-trained embeddings in a different context)\n",
    "wv <- predict(vsmodel, newdata = c(\"uk\", \"continent\", \"eu\"), type = \"embedding\")\n",
    "wv <- wv[\"uk\", ] - wv[\"eu\", ] + wv[\"continent\", ]\n",
    "predict(vsmodel, newdata = wv, type = \"nearest\", top_n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## associations: uk without europe\n",
    "wv <- embeddings[\"uk\", ] - embeddings[\"europe\", ]\n",
    "predict(vsmodel, newdata = wv, type = \"nearest\", top_n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  associations: brexit with agreement\n",
    "wv <- embeddings[\"brexit\", ] + embeddings[\"agreement\", ]\n",
    "predict(vsmodel, newdata = wv, type = \"nearest\", top_n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  associations: brexit without agreement\n",
    "wv <- embeddings[\"brexit\", ] - embeddings[\"agreement\", ]\n",
    "predict(vsmodel, newdata = wv, type = \"nearest\", top_n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Project all adjectives in 2 dimensions ===================\n",
    "\n",
    "## pos-tag the text to identify adjectives, takes quite a while, read already annotated file instead\n",
    "## corp <- brexit.debates[, c(\"gid\", \"body.orig\")]\n",
    "## setnames(corp, \"gid\", \"doc_id\")\n",
    "## setnames(corp, \"body.orig\", \"text\")\n",
    "## corp$text <- text\n",
    "## corp.pos <- udpipe(corp, \"english\")\n",
    "## saveRDS(corp.pos, \"Data/brexit-annotated.rds\")\n",
    "corp.pos <- readRDS(\"Data/brexit-annotated.rds\")\n",
    "head(corp.pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get all adjectives in the corpus\n",
    "length(unique(corp.pos[, \"token\"]))\n",
    "length(unique(corp.pos[corp.pos$upos == \"ADJ\", \"token\"]))\n",
    "adjectives <- unique(corp.pos[corp.pos$upos == \"ADJ\", \"token\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get all nouns in the corpus\n",
    "length(unique(corp.pos[, \"token\"]))\n",
    "length(unique(corp.pos[corp.pos$upos == \"NOUN\", \"token\"]))\n",
    "nouns <- unique(corp.pos[corp.pos$upos == \"NOUN\", \"token\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize 2-dimensional projection of all adjectives in the brexit debate data\n",
    "# project on 2dim space\n",
    "viz <- umap(embeddings, n_neighbors = 15, n_threads = 2)\n",
    "# filter for adjectives\n",
    "df  <- data.frame(word = rownames(embeddings),\n",
    "                  xpos = rownames(embeddings),\n",
    "                  x = viz[, 1], y = viz[, 2],\n",
    "                  stringsAsFactors = FALSE)\n",
    "df  <- subset(df, xpos %in% adjectives)\n",
    "head(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot, restrict to first 300 for speed\n",
    "ggplot(df[1:300, ], aes(x = x, y = y, label = word)) +\n",
    "  geom_text_repel() + theme_void() +\n",
    "  labs(title = \"word2vec - adjectives in 2D using UMAP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Interactive plot - unfortunately this does not  work in the notebooks\n",
    "## plot_ly(df[1:300, ], x = ~x, y = ~y, type = \"scatter\", mode = 'text', text = ~word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Similar 2d projection of all nouns\n",
    "embeddings.nouns <- predict(vsmodel, nouns, type = \"embedding\")\n",
    "embeddings.nouns <- embeddings.nouns[complete.cases(embeddings.nouns), ]\n",
    "viz <- umap(embeddings.nouns, n_neighbors = 15, n_threads = 2)\n",
    "df  <- data.frame(word = rownames(embeddings.nouns),\n",
    "                  xpos = rownames(embeddings.nouns),\n",
    "                  x = viz[, 1], y = viz[, 2],\n",
    "                  stringsAsFactors = FALSE)\n",
    "plot_ly(df[1:500, ], x = ~x, y = ~y, type = \"scatter\", mode = 'text', text = ~word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download word2vec, glove or fasttext embeddings\n",
    "## https://github.com/maxoodf/word2vec\n",
    "## https://fasttext.cc/docs/en/crawl-vectors.html\n",
    "## https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## word2vec on English texts corpus, Skip-Gram, Negative Sampling, vector size 500, window 10\n",
    "model <- read.word2vec(file = \"Data/sg_ns_500_10.w2v\", normalize = TRUE)\n",
    "length(summary(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Examples for word similarities, classical analogies and embedding similarities\n",
    "predict(model, newdata = c(\"loan\", \"money\"), type = \"nearest\", top_n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv <- predict(model, newdata = c(\"king\", \"man\", \"woman\"), type = \"embedding\")\n",
    "wv <- wv[\"king\", ] - wv[\"man\", ] + wv[\"woman\", ]\n",
    "predict(model, newdata = wv, type = \"nearest\", top_n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv <- predict(model, newdata = c(\"france\", \"paris\", \"london\"), type = \"embedding\")\n",
    "wv <- wv[\"france\", ] - wv[\"paris\", ] + wv[\"london\", ]\n",
    "predict(model, newdata = wv, type = \"nearest\", top_n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv <- predict(model, newdata = c(\"physician\", \"man\", \"woman\"), type = \"embedding\")\n",
    "wv <- wv[\"physician\", ] - wv[\"man\", ] + wv[\"woman\", ]\n",
    "predict(model, newdata = wv, type = \"nearest\", top_n = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv <- predict(model, newdata = c(\"ideology\", \"person\", \"racist\", \"xenophobia\"), type = \"embedding\")\n",
    "wv <- wv[\"ideology\", ] - wv[\"person\", ] + wv[\"racist\", ]\n",
    "predict(model, newdata = wv, type = \"nearest\", top_n = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings: GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create iterator over tokens\n",
    "tokens <- space_tokenizer(text)\n",
    "str(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create vocabulary. Terms will be unigrams (simple words).\n",
    "it <- itoken(tokens)\n",
    "vocab <- create_vocabulary(it)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove infrequent tokens\n",
    "vocab <- prune_vocabulary(vocab, term_count_min = 5L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use our filtered vocabulary\n",
    "vectorizer <- vocab_vectorizer(vocab)\n",
    "## use window of 5 for context words to construct term-co-occurence matrix\n",
    "tcm <- create_tcm(it, vectorizer, skip_grams_window = 5L)\n",
    "str(tcm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## inspect: standard is decay weighting with offset position\n",
    "## (weight = 1 / distance_from_current_word)\n",
    "tcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fit glove\n",
    "glove <- GlobalVectors$new(rank = 50, x_max = 10)\n",
    "wvmain <- glove$fit_transform(tcm, n_iter = 10, convergence_tol = 0.01, n_threads = 8)\n",
    "dim(wvmain)\n",
    "wvmain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## can also retrieve context vectors\n",
    "wvcontext <- glove$components\n",
    "tail(wvcontext)\n",
    "dim(wvcontext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## could use either of these (typically main),\n",
    "## or aggregate them by averaging or summing them (suggested in glove paper)\n",
    "## summing:\n",
    "wordvectors <- wvmain + t(wvcontext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## analogy tasks work the same\n",
    "## (although not well here as the corpus is too small and specific, requires more data)\n",
    "berlin <- wordvectors[\"paris\", , drop = FALSE] - wordvectors[\"france\", , drop = FALSE] + wordvectors[\"uk\", , drop = FALSE]\n",
    "cosinesim <- sim2(x = wordvectors, y = berlin, method = \"cosine\", norm = \"l2\")\n",
    "head(sort(cosinesim[,1], decreasing = TRUE), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple document-level representation\n",
    "\n",
    "Simple way to get a document representation: just averaging word vectors within a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## isolating common terms, assuming dtm is a document-term-matrix (using the one from above)\n",
    "commonterms <- intersect(colnames(dtm), rownames(wordvectors))\n",
    "commonterms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## filtering dtm (and normalizing)\n",
    "## could also re-weight dtm with tf-idf instead of l1 norm\n",
    "## dtmaveraged <-  as.matrix(dtm)[, common_terms]\n",
    "dtmaveraged <-  normalize(as.matrix(dtm)[, commonterms], \"l1\")\n",
    "dtmaveraged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## get averaged document vectors ('sentence' vectors)\n",
    "docvectors <- dtmaveraged %*% wordvectors[commonterms, ]\n",
    "docvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim(dtmaveraged)\n",
    "dim(wordvectors[commonterms, ])\n",
    "dim(docvectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## analogy tasks work just as before, could use this to find e.g. speakers similar to a person\n",
    "## check which is most similar to first document\n",
    "cosinesim <- sim2(x = docvectors, y = docvectors[1, , drop = FALSE], method = \"cosine\", norm = \"l2\")\n",
    "head(sort(cosinesim[,1], decreasing = TRUE), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning document/paragraph vectors\n",
    "\n",
    "This does not return great results, corpus too small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## input\n",
    "corp <- data.frame(\n",
    "    doc_id = brexit.debates$gid,\n",
    "    text = text,\n",
    "    stringsAsFactors = FALSE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## low dimension, just for illustrations\n",
    "pv.model <- paragraph2vec(\n",
    "  x = corp,\n",
    "  type = \"PV-DM\",\n",
    "  dim = 5,\n",
    "  iter = 3,\n",
    "  min_count = 5,\n",
    "  lr = 0.05,\n",
    "  threads = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## More realistic settings, careful, this will run for a bit. Not worth running with the limited data we use here.\n",
    "## pv.model <- paragraph2vec(\n",
    "##   x = corp,\n",
    "##   type = \"PV-DBOW\",\n",
    "##   dim = 100,\n",
    "##   iter = 20,\n",
    "##   min_count = 5,\n",
    "##   lr = 0.05,\n",
    "##   threads = 4\n",
    "## )\n",
    "## saveRDS(pv.model, \"Data/pv-model.rds\")\n",
    "## pv.model <- readRDS(\"Data/pv-model.rds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract the word embeddings\n",
    "word.embeddings <- as.matrix(pv.model, which = \"words\")\n",
    "head(word.embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract the document embeddings\n",
    "doc.embeddings <- as.matrix(pv.model, which = \"docs\")\n",
    "tail(doc.embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract the vocabulary\n",
    "doc.vocab <- summary(pv.model, which = \"words\")\n",
    "doc.vocab\n",
    "\n",
    "## word.vocab <- summary(pv.model, which = \"docs\")\n",
    "## head(word.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriev word embeddings (as previously)\n",
    "predict(pv.model, \"brexit\", type = \"embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve most similar words to a word (as previously)\n",
    "predict(pv.model,\n",
    "  newdata = \"brexit\",\n",
    "  type = \"nearest\",\n",
    "  which = \"word2doc\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve document embeddings\n",
    "predict(pv.model,\n",
    "  newdata = c(\"2021-02-11b.563.0\", \"2021-02-11b.504.0\", \"2021-02-11b.468.2\"),\n",
    "  type = \"embedding\",\n",
    "  which = \"docs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve most similar documents to a document\n",
    "predict(pv.model,\n",
    "  newdata = \"2021-02-11b.563.0\",\n",
    "  type = \"nearest\",\n",
    "  which = \"doc2doc\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## find document closest to a sentence\n",
    "predict(pv.model,\n",
    "  newdata = list(prophecy = c(\"brexit\", \"will\", \"not\", \"disrupt\", \"trade\")),\n",
    "  type = \"nearest\",\n",
    "  which = \"sent2doc\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get embeddings of sentences.\n",
    "sentences <- list(\n",
    "  sent1 = c(\"germany\", \"and\", \"france\", \"dominate\", \"the\", \"eu\"),\n",
    "  sent2 = c(\"brexit\", \"was\", \"planned\", \"meticulously\")\n",
    ")\n",
    "predict(pv.model, newdata = sentences, type = \"embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
